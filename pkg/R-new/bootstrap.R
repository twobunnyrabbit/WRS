# WRS Package - Bootstrap and Resampling Infrastructure
# Generic bootstrap, permutation, and resampling methods
#
# This module contains:
#   - Generic bootstrap functions: bootse, bootdep, bootcov
#   - BCA (Bias-Corrected Accelerated) methods
#   - Permutation test infrastructure
#   - Bootstrap helpers and subroutines
#
# Note: Specific bootstrap analyses (yuenbt, etc.) are in their
# respective modules (two-sample.R, anova.R, regression.R, etc.)


#' Bootstrap Standard Error Estimate
#'
#' Computes a bootstrap estimate of the standard error for any estimator.
#' Uses simple bootstrap resampling with replacement.
#'
#' @inheritParams common-params
#' @param ... Additional arguments passed to the estimator function `est`.
#'
#' @return Numeric value representing the bootstrap standard error estimate.
#'
#' @details
#' This function generates `nboot` bootstrap samples by randomly sampling
#' with replacement from the data. The estimator `est` is applied to each
#' bootstrap sample, and the standard deviation of these bootstrap estimates
#' is returned as the standard error.
#'
#' The default estimator is the median, but any function that takes a vector
#' and returns a scalar can be used.
#'
#' @export
#' @examples
#' # Bootstrap SE for median
#' x <- rnorm(50)
#' bootse(x, nboot=1000, est=median)
#'
#' # Bootstrap SE for trimmed mean
#' bootse(x, nboot=1000, est=mean, trim=0.2)
#'
#' # Bootstrap SE for MAD
#' bootse(x, nboot=1000, est=mad)
bootse<-function(x,nboot=1000,est=median,SEED=TRUE,...){
if(SEED)set.seed(2)
data<-matrix(sample(x,size=length(x)*nboot,replace=TRUE),nrow=nboot)
bvec<-apply(data,1,est,...)
bootse<-sqrt(var(bvec))
bootse
}



#' Bootstrap Standard Error for Dependent Group Differences
#'
#' Determines the bootstrap standard error for the difference between two
#' measures of location (or scale) for dependent (paired) groups.
#'
#' @inheritParams common-params
#' @param pr Logical. If `TRUE`, prints progress message (default: `TRUE`).
#' @param ... Additional arguments passed to the estimator function `est`.
#'
#' @return Numeric value representing the bootstrap standard error of the
#'   difference between the two groups.
#'
#' @details
#' This function handles dependent (paired) data. The data can be provided
#' as a two-column matrix `x`, or as separate vectors `x` and `y`. Bootstrap
#' samples are generated by resampling pairs (rows) with replacement.
#'
#' Missing values are automatically removed before analysis.
#'
#' @export
#' @examples
#' # Paired data example
#' before <- c(120, 135, 140, 125, 130, 142, 128, 133)
#' after <- c(115, 130, 138, 120, 125, 140, 125, 130)
#' bootdse(before, after, est=median, nboot=500)
#'
#' # Using trimmed mean
#' bootdse(before, after, est=mean, nboot=500, trim=0.2)
bootdse<-function(x,y=NA,est=median,nboot=100,pr=TRUE,...){
if(!is.na(y[1]))x<-cbind(x,y)
x<-elimna(x)
if(pr)print("Taking Bootstrap Samples. Please wait.")
data<-matrix(sample(nrow(x),size=nrow(x)*nboot,replace=TRUE),nrow=nboot)
xmat<-matrix(x[data,1],nrow=nboot,ncol=length(x))
ymat<-matrix(x[data,2],nrow=nboot,ncol=length(x))
bvec<-apply(xmat,1,FUN=est,...)-apply(ymat,1,FUN=est,...)
se<-sqrt(var(bvec))
se
}


#' Percentile Bootstrap CI for Dependent Groups
#'
#' Computes a percentile bootstrap confidence interval for the difference
#' between measures of location or scale when comparing two dependent (paired)
#' groups.
#'
#' @inheritParams common-params
#' @param dif Logical. If `TRUE`, analyzes the difference between groups.
#'   If `FALSE`, analyzes individual groups (default: `TRUE`).
#' @param BA Logical. If `TRUE`, uses Bland-Altman plot approach (default: `FALSE`).
#' @param SR Logical. If `TRUE`, uses the Studentized range when appropriate
#'   (default: `TRUE`).
#' @param ... Additional arguments passed to the estimator function `est`.
#'
#' @return A list containing:
#'   \item{output}{Matrix with confidence intervals and test results}
#'
#' @details
#' By default, uses a one-step M-estimator with Huber's psi function. For
#' a fully iterated M-estimator, use `est=mest`.
#'
#' The function internally calls `rmmcppb` for the bootstrap computations.
#' The `SR` parameter is automatically set to `FALSE` for estimators other
#' than `onestep` and `mom`.
#'
#' @seealso \code{\link{rmmcppb}}, \code{\link{onestep}}, \code{\link{mom}}
#'
#' @export
#' @examples
#' # Paired data
#' x <- c(120, 135, 140, 125, 130, 142, 128, 133)
#' y <- c(115, 130, 138, 120, 125, 140, 125, 130)
#' bootdpci(x, y, nboot=500)
bootdpci<-function(x,y,est=onestep,nboot=NA,alpha=.05,plotit=FALSE,dif=TRUE,BA=FALSE,SR=TRUE,...){
okay=FALSE
if(identical(est,onestep))okay=TRUE
if(identical(est,mom))okay=TRUE
if(!okay)SR=FALSE
output<-rmmcppb(x,y,est=est,nboot=nboot,alpha=alpha,SR=SR,
plotit=plotit,dif=dif,BA=BA,...)$output
list(output=output)
}



#' Bootstrap Samples for Multivariate Dependent Data
#'
#' Generates bootstrap samples and computes trimmed means for each variable
#' in a multivariate dataset. Returns a matrix of bootstrap estimates.
#'
#' @inheritParams common-params
#'
#' @return A matrix of dimension `nboot` by `p` (number of variables),
#'   where each row contains the bootstrap trimmed means for all variables.
#'
#' @details
#' This function handles multivariate dependent data by resampling rows
#' (cases) with replacement. For each bootstrap sample, the trimmed mean
#' is computed for each variable.
#'
#' The input `x` can be either a matrix (n by p) or a list of length p,
#' where each element is a vector of observations.
#'
#' @export
#' @examples
#' # Multivariate data
#' x <- matrix(rnorm(100), ncol=5)
#' boot_means <- bootdep(x, tr=0.2, nboot=1000)
#' # Bootstrap standard errors
#' apply(boot_means, 2, sd)
bootdep<-function(x,tr=.2,nboot=500){
if(is.matrix(x))m1<-x
if(is.list(x)){
m1<-matrix(NA,ncol=length(x))
for(j in 1:length(x))m1[,j]<-x[[j]]
}
data<-matrix(sample(nrow(m1),size=nrow(m1)*nboot,replace=TRUE),nrow=nboot)
bvec<-matrix(NA,ncol=ncol(m1),nrow=nboot)
for(j in 1:ncol(m1)){
temp<-m1[,j]
bvec[,j]<-apply(data, 1., bootdepsub,temp,tr)
}
bvec
}


#' Bootstrap Helper Function for Dependent Data
#'
#' Internal helper function used by `bootdep` to compute trimmed means
#' for bootstrap samples.
#'
#' @param isub Vector of indices for the bootstrap sample.
#' @param x Data vector.
#' @param tr Trimming proportion.
#'
#' @return Trimmed mean of the bootstrap sample.
#'
#' @keywords internal
bootdepsub<-function(isub,x,tr){
tsub<-mean(x[isub],tr)
tsub
}

#' Bootstrap Covariance Matrix for Multivariate Estimator
#'
#' Computes bootstrap estimates of squared standard errors and covariances
#' for a multivariate estimator applied to multivariate data.
#'
#' @inheritParams common-params
#' @param pr Logical. If `TRUE`, prints progress message (default: `TRUE`).
#' @param ... Additional arguments passed to the estimator function `est`.
#'
#' @return Covariance matrix of the bootstrap estimates (p by p matrix,
#'   where p is the number of variables).
#'
#' @details
#' For multivariate data, this function generates bootstrap samples and
#' computes the specified estimator for each variable. The covariance
#' matrix of the bootstrap estimates provides estimates of the squared
#' standard errors (diagonal elements) and covariances (off-diagonal
#' elements) for the estimator.
#'
#' The input can be a matrix or list. Missing values are automatically removed.
#'
#' @export
#' @examples
#' # Multivariate data
#' x <- matrix(rnorm(200), ncol=4)
#' cov_mat <- bootcov(x, est=median, nboot=500)
#' sqrt(diag(cov_mat))  # Bootstrap standard errors
bootcov<-function(x,est=median,nboot=100,pr=TRUE,SEED=FALSE,...){
if(SEED)set.seed(2)
if(is.list(x))x<-matl(x)
x<-elimna(x)
bvec<-matrix(NA,ncol=ncol(x),nrow=nboot)
if(pr)print("Taking Bootstrap Samples. Please wait.")
for(i in 1:nboot){
data<-sample(nrow(x),size=nrow(x),replace=TRUE)
bvec[i,]<-apply(x[data,],2,FUN=est,...)
}
covmat<-var(bvec)
covmat
}

#' Yuen's Bootstrap-t Test for Trimmed Means
#'
#' Computes a confidence interval for the difference between trimmed means
#' of two independent groups using the bootstrap-t method.
#'
#' @inheritParams common-params
#' @param side Logical. If `TRUE`, uses two-sided method with absolute values;
#'   if `FALSE`, uses equal-tailed method (default: `FALSE`).
#' @param nullval Null hypothesis value for testing (default: 0).
#' @param pr Logical. If `TRUE`, prints progress (default: `TRUE`).
#' @param op Plotting option when `plotit=TRUE`: 1 for adaptive kernel density,
#'   2 for ridgeline plot (default: 1).
#'
#' @return A list with components:
#'   \item{ci}{Confidence interval for the difference in trimmed means}
#'   \item{test.stat}{Test statistic value}
#'   \item{p.value}{p-value for the test}
#'   \item{est.1}{Trimmed mean of group 1}
#'   \item{est.2}{Trimmed mean of group 2}
#'   \item{est.dif}{Difference in trimmed means}
#'   \item{n1}{Sample size of group 1}
#'   \item{n2}{Sample size of group 2}
#'
#' @details
#' This function implements Yuen's method using the bootstrap-t approach
#' for comparing trimmed means of two independent groups. The bootstrap-t
#' method tends to provide better coverage than the percentile bootstrap,
#' especially for small samples.
#'
#' The function uses centering (subtracting group trimmed means) before
#' bootstrap resampling to improve performance.
#'
#' @seealso \code{\link{yuen}}, \code{\link{trimse}}
#'
#' @export
#' @examples
#' x <- rnorm(25, mean=100, sd=15)
#' y <- rnorm(30, mean=110, sd=15)
#' yuenbt(x, y, tr=0.2, nboot=500)
yuenbt<-function(x,y,tr=.2,alpha=.05,nboot=199,side=FALSE,nullval=0,pr=TRUE,
plotit=FALSE,op=1,SEED=TRUE){
side<-as.logical(side)
p.value<-NA
ybt<-vector(mode="numeric",length=2)
if(SEED)set.seed(2)
x<-x[!is.na(x)]
y<-y[!is.na(y)]
xcen<-x-mean(x,tr)
ycen<-y-mean(y,tr)
test<-(mean(x,tr)-mean(y,tr))/sqrt(trimse(x,tr=tr)^2+trimse(y,tr=tr)^2)
datax<-matrix(sample(xcen,size=length(x)*nboot,replace=TRUE),nrow=nboot)
datay<-matrix(sample(ycen,size=length(y)*nboot,replace=TRUE),nrow=nboot)
top<-apply(datax,1,mean,tr)-apply(datay,1,mean,tr)
botx<-apply(datax,1,trimse,tr)
boty<-apply(datay,1,trimse,tr)
tval<-top/sqrt(botx^2+boty^2)
if(plotit){
if(op == 1)
akerd(tval)
if(op == 2)
rdplot(tval)
}
if(side)tval<-abs(tval)
tval<-sort(tval)
icrit<-floor((1-alpha)*nboot+.5)
ibot<-floor(alpha*nboot/2+.5)
itop<-floor((1-alpha/2)*nboot+.5)
se<-sqrt((trimse(x,tr))^2+(trimse(y,tr))^2)
ybt[1]<-mean(x,tr)-mean(y,tr)-tval[itop]*se
ybt[2]<-mean(x,tr)-mean(y,tr)-tval[ibot]*se
if(side){
CI=mean(x,tr)-mean(y,tr)-tval[icrit]*se
ybt[1]<-mean(x,tr)-mean(y,tr)-tval[icrit]*se
ybt[2]<-mean(x,tr)-mean(y,tr)+tval[icrit]*se
CI[2]=ybt[2]
p.value<-(sum(abs(test)<=abs(tval)))/nboot
if(ybt[1]>nullval)p.value=min(p.value,alpha)
if(ybt[2]<nullval)p.value=min(p.value,alpha)
}
if(!side){
ibot<-round(alpha*nboot/2)
itop<-nboot-ibot+1
ibot=ibot+1
crit=c(tval[ibot],tval[itop])
CI=mean(x,tr)-mean(y,tr)-tval[itop]*se
ybt=mean(x,tr)-mean(y,tr)-tval[itop]*se
ybt[2]<-mean(x,tr)-mean(y,tr)-tval[ibot]*se
CI[2]=mean(x,tr)-mean(y,tr)-tval[ibot]*se
if(test<0)G=mean(test>tval[1:nboot])
if(test>=0)G=mean(test<tval[1:nboot])
p.value=2*G
}
list(ci=CI,test.stat=test,p.value=p.value,est.1=mean(x,tr),est.2=mean(y,tr),est.dif=mean(x,tr)-mean(y,tr),
n1=length(x),n2=length(y))
}



#' Bootstrap Test for Equal M-Measures of Location
#'
#' Global test for equal M-measures of location across J independent groups
#' using bootstrap methodology. Implements method TM from Wilcox (2017).
#'
#' @inheritParams common-params
#' @param x Data in either matrix, data frame, or list format. If matrix/data frame,
#'   columns represent groups. If list, each element contains data for one group.
#'
#' @return A list with components:
#'   \item{Est.}{Vector of M-estimators (one-step) for each group}
#'   \item{p.value}{Bootstrap p-value for the global test}
#'
#' @details
#' This function tests the null hypothesis that J independent groups have equal
#' M-measures of location (specifically, one-step M-estimators). The test uses
#' a bootstrap approach with centering to ensure proper Type I error control.
#'
#' For each bootstrap sample, the MAD must be greater than zero to avoid
#' numerical issues. The bootstrap samples are centered by subtracting the
#' corresponding group M-estimator.
#'
#' This is method TM described in the 5th edition of "Introduction to Robust
#' Estimation and Hypothesis Testing" by Wilcox.
#'
#' @seealso \code{\link{TM}}, \code{\link{onestep}}
#'
#' @export
#' @examples
#' # Three independent groups
#' x1 <- rnorm(25, mean=100, sd=15)
#' x2 <- rnorm(25, mean=105, sd=15)
#' x3 <- rnorm(25, mean=110, sd=15)
#' boot.TM(list(x1, x2, x3), nboot=500)
boot.TM<-function(x,nboot=599,alpha=.05,SEED=TRUE){
#
# Global test for equal M-measures of location, J independent groups
#
# This is method TM in 5th Ed of Intro to Robust Estimation and Testing
#
if(SEED)set.seed(2)
B=nboot
  if(is.matrix(x) || is.data.frame(x))xlist=listm(x)
  else xlist=x
  xlist=elimna(xlist)
  T.test<-TM(xlist)$TM
  k<-length(xlist)
  ylist<-vector(mode="list",length=k)
  TT<-numeric(B)
  b<-floor((1-alpha)*B)
  onesteps<-sapply(xlist,onestep)
  for (i in 1:B){
   j<-1
   repeat {
    ylist[[j]]<-(sample(xlist[[j]],length(xlist[[j]]),replace=T)-onesteps[j])
    if (mad(ylist[[j]])>0) j<-j+1 #MAD must be greater than zero for every bootstrap sample
    if (j>k)break
   }
   TT[i]<-TM(ylist,alpha)$TM
  }
  TT=sort(TT)
  if(T.test>=TT[b]){1} else{0}
pv=mean(T.test<=TT)
list(Est.=onesteps,p.value=pv)
}




#' Bootstrap Helper for Regression Coefficients
#'
#' Internal helper function that computes regression coefficients for a
#' bootstrap sample. Used by various bootstrap regression functions.
#'
#' @param isub Vector of indices for the bootstrap sample (length n).
#' @param x Matrix of predictor variables (n by p).
#' @param y Vector of response variable values.
#' @param regfun Regression function to use (must return list with `$coef` element).
#' @param ... Additional arguments passed to `regfun`.
#'
#' @return Vector of regression coefficients from the bootstrap sample.
#'
#' @details
#' This function performs regression using the observations indexed by `isub`.
#' The regression function `regfun` must return a list with a component named
#' `coef` containing the coefficient estimates (intercept and slopes).
#'
#' This helper is used internally by bootstrap regression functions to
#' apply regression methods to resampled data.
#'
#' @keywords internal
regboot<-function(isub,x,y,regfun,...){
#
#  Perform regression using x[isub] to predict y[isub]
#  isub is a vector of length n,
#  a bootstrap sample from the sequence of integers
#  1, 2, 3, ..., n
#
#  This function is used by other functions when computing
#  bootstrap estimates.
#
#  regfun is some regression method already stored in R
#  It is assumed that regfun$coef contains the  intercept and slope
#  estimates produced by regfun.  The regression methods written for
#  this  book, plus regression functions in R, have this property.
#
#  x is assumed to be a matrix containing values of the predictors.
#
xmat<-matrix(x[isub,],nrow(x),ncol(x))
vals<-regfun(xmat,y[isub],...)$coef
vals
}



#' Bootstrap Helper for Regression Coefficients (General Version)
#'
#' Internal helper function that computes regression coefficients for a
#' bootstrap sample. Simplified version of `regboot` for general use.
#'
#' @param isub Vector of indices for the bootstrap sample.
#' @param x Matrix of predictor variables.
#' @param y Vector of response variable values.
#' @param regfun Regression function to use (must return list with `$coef` element).
#' @param ... Additional arguments passed to `regfun`.
#'
#' @return Vector of regression coefficients from the bootstrap sample.
#'
#' @details
#' Similar to `regboot` but with simplified matrix indexing. Used internally
#' by various bootstrap regression procedures.
#'
#' @keywords internal
regbootg<-function(isub,x,y,regfun,...){
#
#  Perform regression using x[isub] to predict y[isub]
#  isub is a vector of length n,
#  a bootstrap sample from the sequence of integers
#  1, 2, 3, ..., n
#
#  This function is used by other functions when computing
#  bootstrap estimates.
#
#  regfun is some regression method already stored in R
#  It is assumed that regfun$coef contains the  intercept and slope
#  estimates produced by regfun.  The regression methods written for
#  this  book, plus regression functions in R, have this property.
#
#  x is assumed to be a matrix containing values of the predictors.
#
#xmat<-matrix(x[isub,],nrow(x),ncol(x))
xmat<-x[isub,]
yy<-y[isub]
regboot<-regfun(xmat,y[isub])$coef
#regboot<-regboot$coef
regboot
}


#' Bootstrap Helper for Regression with Parallel Processing
#'
#' Internal helper function for computing regression coefficients in parallel
#' bootstrap procedures using `mclapply`.
#'
#' @param data Vector of indices for the bootstrap sample.
#' @param x Matrix of predictor variables.
#' @param y Vector of response variable values.
#' @param regfun Regression function to use.
#' @param ... Additional arguments passed to `regfun`.
#'
#' @return Vector of regression coefficients.
#'
#' @details
#' Used internally by parallel bootstrap regression functions (those with
#' `MC` suffix). Designed to work with `mclapply` for multicore processing.
#'
#' @keywords internal
regbootMC<-function(data,x,y,regfun,...){
vals=regfun(x[data,],y[data],...)$coef
}


#' Bootstrap ANCOVA for Two Independent Groups
#'
#' Compares two independent groups using a robust ANCOVA method based on
#' running interval smoothers. No parametric assumptions are made about the
#' form of the regression lines.
#'
#' @inheritParams common-params
#' @param x1 Covariate values for group 1.
#' @param y1 Response values for group 1.
#' @param x2 Covariate values for group 2.
#' @param y2 Response values for group 2.
#' @param fr1 Span for running interval smoother in group 1 (default: 1).
#' @param fr2 Span for running interval smoother in group 2 (default: 1).
#' @param pts Design points at which to make comparisons. If `NA`, five
#'   empirically chosen design points are selected (default: `NA`).
#' @param plotit Logical. If `TRUE`, plots the running interval smoothers (default: `TRUE`).
#' @param xout Logical. If `TRUE`, removes outliers in the covariate before analysis
#'   (default: `FALSE`).
#' @param outfun Function for detecting outliers (default: `outpro`).
#'
#' @return A list with components:
#'   \item{output}{Matrix with columns: X (design point), n1, n2, DIF (difference),
#'     TEST (test statistic), ci.low, ci.hi, p.value}
#'   \item{crit}{Critical value used for the bootstrap-t test}
#'
#' @details
#' This function implements the ANCOVA method described in Chapter 12 of
#' Wilcox (2017). It uses running interval smoothers to estimate the conditional
#' means at various design points, without assuming a specific functional form
#' for the regression lines.
#'
#' Confidence intervals are computed using a bootstrap-t method. By default,
#' five design points are chosen empirically to ensure adequate sample sizes
#' (at least 12 observations within the span at each point).
#'
#' The function internally uses `linconb` for linear contrasts with bootstrap.
#'
#' @seealso \code{\link{ancbootg}}, \code{\link{linconb}}, \code{\link{runmean2g}}
#'
#' @export
#' @examples
#' # Two groups with covariate
#' x1 <- runif(50, 0, 10)
#' y1 <- 2*x1 + rnorm(50, 0, 2)
#' x2 <- runif(50, 0, 10)
#' y2 <- 2.5*x2 + rnorm(50, 0, 2)
#' ancboot(x1, y1, x2, y2, nboot=500)
ancboot<-function(x1,y1,x2,y2,fr1=1,fr2=1,tr=.2,nboot=599,pts=NA,plotit=TRUE,xout=FALSE,outfun=outpro,...){
#
# Compare two independent  groups using the ancova method
# in chapter 12 of Wilcox, 2017, Intro to Robust Estimation and Hypothesis Testing.
# No assumption is made about the form of the regression
# lines--a running interval smoother is used.
# Confidence intervals are computed using a bootstrap-t bootstrap
# method. Comparisons are made at five empirically chosen design points.
#
#  Assume data are in x1 y1 x2 and y2
#
if(is.na(pts[1])){
isub<-c(1:5)  # Initialize isub
test<-c(1:5)
m1=elimna(cbind(x1,y1))
x1=m1[,1]
y1=m1[,2]
m1=elimna(cbind(x2,y2))
x2=m1[,1]
y2=m1[,2]
xorder<-order(x1)
y1<-y1[xorder]
x1<-x1[xorder]
xorder<-order(x2)
y2<-y2[xorder]
x2<-x2[xorder]
n1<-1
n2<-1
vecn<-1
for(i in 1:length(x1))n1[i]<-length(y1[near(x1,x1[i],fr1)])
for(i in 1:length(x1))n2[i]<-length(y2[near(x2,x1[i],fr2)])
for(i in 1:length(x1))vecn[i]<-min(n1[i],n2[i])
sub<-c(1:length(x1))
isub[1]<-min(sub[vecn>=12])
isub[5]<-max(sub[vecn>=12])
isub[3]<-floor((isub[1]+isub[5])/2)
isub[2]<-floor((isub[1]+isub[3])/2)
isub[4]<-floor((isub[3]+isub[5])/2)
mat<-matrix(NA,5,8)
dimnames(mat)<-list(NULL,c("X","n1","n2","DIF","TEST","ci.low","ci.hi",
"p.value"))
gv1<-vector("list")
for (i in 1:5){
j<-i+5
temp1<-y1[near(x1,x1[isub[i]],fr1)]
temp2<-y2[near(x2,x1[isub[i]],fr2)]
temp1<-temp1[!is.na(temp1)]
temp2<-temp2[!is.na(temp2)]
mat[i,2]<-length(temp1)
mat[i,3]<-length(temp2)
gv1[[i]]<-temp1
gv1[[j]]<-temp2
}
I1<-diag(5)
I2<-0-I1
con<-rbind(I1,I2)
test<-linconb(gv1,con=con,tr=tr,nboot=nboot)
for(i in 1:5){
mat[i,1]<-x1[isub[i]]
}
mat[,4]<-test$psihat[,2]
mat[,5]<-test$test[,2]
mat[,6]<-test$psihat[,3]
mat[,7]<-test$psihat[,4]
mat[,8]<-test$test[,4]
}
if(!is.na(pts[1])){
n1<-1
n2<-1
vecn<-1
for(i in 1:length(pts)){
n1[i]<-length(y1[near(x1,pts[i],fr1)])
n2[i]<-length(y2[near(x2,pts[i],fr2)])
if(n1[i]<=5)paste("Warning, there are",n1[i]," points corresponding to the design point X=",pts[i])
if(n2[i]<=5)paste("Warning, there are",n2[i]," points corresponding to the design point X=",pts[i])
}
mat<-matrix(NA,length(pts),9)
dimnames(mat)<-list(NULL,c("X","n1","n2","DIF","TEST","se","ci.low","ci.hi",
"p.value"))
gv<-vector("list",2*length(pts))
for (i in 1:length(pts)){
g1<-y1[near(x1,pts[i],fr1)]
g2<-y2[near(x2,pts[i],fr2)]
g1<-g1[!is.na(g1)]
g2<-g2[!is.na(g2)]
j<-i+length(pts)
gv[[i]]<-g1
gv[[j]]<-g2
}
I1<-diag(length(pts))
I2<-0-I1
con<-rbind(I1,I2)
test<-linconb(gv,con=con,tr=tr,nboot=nboot)
mat[,1]<-pts
mat[,2]<-n1
mat[,3]<-n2
mat[,4]<-test$psihat[,2]
mat[,5]<-test$test[,2]
mat[,6]<-test$test[,3]
mat[,7]<-test$psihat[,3]
mat[,8]<-test$psihat[,4]
mat[,9]<-test$test[,4]
}
if(plotit){
if(xout){
flag<-outfun(x1,...)$keep
x1<-x1[flag]
y1<-y1[flag]
flag<-outfun(x2,...)$keep
x2<-x2[flag]
y2<-y2[flag]
}
runmean2g(x1,y1,x2,y2,fr=fr1,est=mean,tr=tr)
}
list(output=mat,crit=test$crit)
}


#' Bootstrap ANCOVA at Specified Design Points
#'
#' Compares two independent groups using robust ANCOVA at user-specified
#' design points. Uses running interval smoothers without parametric assumptions.
#'
#' @inheritParams common-params
#' @param x1 Covariate values for group 1.
#' @param y1 Response values for group 1.
#' @param x2 Covariate values for group 2.
#' @param y2 Response values for group 2.
#' @param pts Vector of design points at which to make comparisons (required).
#' @param fr1 Span for running interval smoother in group 1 (default: 1).
#' @param fr2 Span for running interval smoother in group 2 (default: 1).
#'
#' @return A list with components:
#'   \item{output}{Matrix with columns: X (design point), n1, n2, DIF (difference),
#'     TEST (test statistic), se (standard error), ci.low, ci.hi}
#'   \item{crit}{Critical value used for the bootstrap test}
#'
#' @details
#' This function is similar to `ancboot` but requires the user to specify the
#' design points for comparison. It uses running interval smoothers to estimate
#' conditional means at each design point.
#'
#' The function does not perform automatic design point selection; the user
#' must ensure that adequate sample sizes exist within the span at each point.
#'
#' @seealso \code{\link{ancboot}}, \code{\link{linconb}}
#'
#' @export
#' @examples
#' # Two groups with covariate
#' x1 <- runif(50, 0, 10)
#' y1 <- 2*x1 + rnorm(50, 0, 2)
#' x2 <- runif(50, 0, 10)
#' y2 <- 2.5*x2 + rnorm(50, 0, 2)
#' # Compare at specific design points
#' ancbootg(x1, y1, x2, y2, pts=c(2, 5, 8), nboot=500)
ancbootg<-function(x1,y1,x2,y2,pts,fr1=1,fr2=1,tr=.2,nboot=599){
#
# Compare two independent  groups using the ancova method
# in chapter 9. No assumption is made about the form of the regression
# lines--a running interval smoother is used.
#
#  Assume data are in x1 y1 x2 and y2
#  Comparisons are made at the design points contained in the vector
#  pts
#
m1=elimna(cbind(x1,y1))
x1=m1[,1]
y1=m1[,2]
m1=elimna(cbind(x2,y2))
x2=m1[,1]
y2=m1[,2]
n1<-1
n2<-1
vecn<-1
for(i in 1:length(pts)){
n1[i]<-length(y1[near(x1,pts[i],fr1)])
n2[i]<-length(y2[near(x2,pts[i],fr2)])
}
mat<-matrix(NA,length(pts),8)
dimnames(mat)<-list(NULL,c("X","n1","n2","DIF","TEST","se","ci.low","ci.hi"))
gv<-vector("list",2*length(pts))
for (i in 1:length(pts)){
g1<-y1[near(x1,pts[i],fr1)]
g2<-y2[near(x2,pts[i],fr2)]
g1<-g1[!is.na(g1)]
g2<-g2[!is.na(g2)]
j<-i+length(pts)
gv[[i]]<-g1
gv[[j]]<-g2
}
I1<-diag(length(pts))
I2<-0-I1
con<-rbind(I1,I2)
test<-linconb(gv,con=con,tr=tr,nboot=nboot)
mat[,1]<-pts
mat[,2]<-n1
mat[,3]<-n2
mat[,4]<-test$psihat[,2]
mat[,5]<-test$test[,2]
mat[,6]<-test$test[,3]
mat[,7]<-test$psihat[,3]
mat[,8]<-test$psihat[,4]
list(output=mat,crit=test$crit)
}


#' BCA Confidence Interval for P(X<Y)
#'
#' Computes a bias-corrected and accelerated (BCA) bootstrap confidence interval
#' for the probability P(X<Y) when X and Y are independent samples.
#'
#' @inheritParams common-params
#' @param ... Additional arguments passed to `bcajack`.
#'
#' @return A list with components:
#'   \item{n1}{Sample size of group 1}
#'   \item{n2}{Sample size of group 2}
#'   \item{phat}{Estimated probability P(X<Y)}
#'   \item{ci.low}{Lower confidence limit}
#'   \item{ci.upper}{Upper confidence limit}
#'
#' @details
#' This function computes a BCA bootstrap confidence interval for the
#' probability that a randomly selected observation from X is less than
#' a randomly selected observation from Y.
#'
#' The BCA method (Efron & Tibshirani, 1993) provides more accurate
#' confidence intervals than standard percentile bootstrap by correcting
#' for bias and skewness.
#'
#' When the estimate is exactly 0 or 1, the function automatically switches
#' to an alternative method (`cid`) to avoid numerical issues.
#'
#' Requires the `bcaboot` package.
#'
#' @seealso \code{\link{wmw.bcapv}}, \code{\link{bmp}}, \code{\link{cid}}
#'
#' @export
#' @examples
#' x <- rnorm(30, mean=100, sd=15)
#' y <- rnorm(35, mean=105, sd=15)
#' wmw.bca(x, y, alpha=0.05, nboot=500)
wmw.bca<-function(x,y,alpha=.05,nboot=1000,SEED=TRUE,...){
#
#  BCA confidence interval for P(X<Y), X and Y independent
#  Method: Bias corrected accelerated bootstrap
#
library(bcaboot)
if(SEED)set.seed(2)
n1=length(x)
n2=length(y)
nmax=max(n1,n2)
m=matrix(NA,nmax,2)
m[1:n1,1]=x
m[1:n2,2]=y
e=bmp(x,y)$phat
if(e==0 || e==1)ci=cid(x,y)$ci.p
else{
a=bcajack(m,B=500,wmw.est.only,n1=n1,n2=n2,alpha=alpha/2,verbose=FALSE,...)
ci=c(a$lims[1,1],a$lims[3,1])
}
list(n1=n1,n2=n2,phat=e,ci.low=ci[1],ci.upper=ci[2])
}



#' BCA Confidence Interval and P-value for P(X<Y)
#'
#' Computes a BCA bootstrap confidence interval and p-value for testing
#' H0: P(X<Y) = 0.5 when X and Y are independent samples.
#'
#' @inheritParams common-params
#' @param ... Additional arguments passed to `bcajack`.
#'
#' @return A list with components:
#'   \item{n1}{Sample size of group 1}
#'   \item{n2}{Sample size of group 2}
#'   \item{phat}{Estimated probability P(X<Y)}
#'   \item{ci.low}{Lower confidence limit}
#'   \item{ci.upper}{Upper confidence limit}
#'   \item{p.value}{P-value for testing H0: P(X<Y) = 0.5}
#'
#' @details
#' This function extends `wmw.bca` by also computing a p-value for testing
#' the null hypothesis that P(X<Y) = 0.5. The p-value is determined by
#' finding the smallest alpha level at which the confidence interval
#' excludes 0.5.
#'
#' The function searches across a range of alpha levels to find the
#' critical value where the confidence interval first excludes 0.5.
#'
#' Requires the `bcaboot` package.
#'
#' @seealso \code{\link{wmw.bca}}, \code{\link{wmw.bcapv.v2}}
#'
#' @export
#' @examples
#' x <- rnorm(30, mean=100, sd=15)
#' y <- rnorm(35, mean=105, sd=15)
#' wmw.bcapv(x, y, nboot=500)
wmw.bcapv<-function(x,y,alpha=.05,nboot=1000,SEED=TRUE,...){
#
#  BCA confidence interval for P(X<Y), X and Y independent
#  Method: Bias corrected accelerated bootstrap
#
library(bcaboot)
if(SEED)set.seed(2)
ALPHA=c(seq(.001,.1,.001),seq(.011,.05,.01),seq(.11,.99,.01))
n1=length(x)
n2=length(y)
nmax=max(n1,n2)
m=matrix(NA,nmax,2)
m[1:n1,1]=x
m[1:n2,2]=y
al=length(ALPHA)
il=0
est=bmp(x,y)
a=bcajack(m,nboot,wmw.est.only,alpha=ALPHA/2,verbose=FALSE,...)
iu=nrow(a$lims)+1
for(j in 1:al){
il=il+1
iu=iu-1
pv=ALPHA[j]
ci=c(a$lims[il,1],a$lims[iu,1])
if(a$lims[il,1]>0.5 || a$lims[iu,1]<0.5)break
}
A=wmw.bca(x,y,alpha=alpha,nboot=nboot,SEED=SEED)
list(n1=n1,n2=n2,phat=est$phat,ci.low=A$ci.low,ci.upper=A$ci.upper,p.value=pv)
}



#' BCA P-value for P(X<Y) Using Iteration
#'
#' Computes a BCA bootstrap p-value for testing H0: P(X<Y) = 0.5 using
#' an iterative resampling approach for improved stability.
#'
#' @inheritParams common-params
#' @param iter Number of iterations for averaging p-values (default: 100).
#' @param ... Additional arguments passed to `wmw.bcapv`.
#'
#' @return A list with components:
#'   \item{n1}{Sample size of group 1}
#'   \item{n2}{Sample size of group 2}
#'   \item{phat}{Estimated probability P(X<Y)}
#'   \item{p.value}{Average p-value across iterations}
#'
#' @details
#' This function provides a more stable p-value estimate by computing
#' multiple bootstrap p-values (using `wmw.bcapv`) on subsamples of
#' size min(n1, n2) and averaging the results.
#'
#' Each iteration randomly samples min(n1, n2) observations from both
#' groups and computes a p-value. The final p-value is the average
#' across all iterations.
#'
#' Requires the `bcaboot` package.
#'
#' @seealso \code{\link{wmw.bcapv}}, \code{\link{wmw.bcav2}}
#'
#' @export
#' @examples
#' x <- rnorm(40, mean=100, sd=15)
#' y <- rnorm(35, mean=105, sd=15)
#' wmw.bcapv.v2(x, y, nboot=500, iter=50)
wmw.bcapv.v2<-function(x,y,nboot=500,iter=100,SEED=TRUE,...){
#
#  BCA  p-value for H_0P (X<Y)=.5, X and Y independent
#  Method: Bias corrected accelerated bootstrap
#
library(bcaboot)
if(SEED)set.seed(2)
n1=length(x)
n2=length(y)
nmin=min(n1,n2)
m=matrix(NA,nmin,2)
est=bmp(x,y)$phat
pv.vals=NA
for(i in 1:iter){
ys=sample(y,nmin)
m[1:nmin,1]=sample(x,nmin)
m[1:nmin,2]=sample(y,nmin)
pv.vals[i]=wmw.bcapv(m[,1],m[,2],SEED=FALSE)$p.value
}
list(n1=n1,n2=n2,phat=est,p.value=mean(pv.vals))
}

#' BCA Confidence Interval with Iterative Averaging
#'
#' Computes a BCA bootstrap confidence interval for P(X<Y) using iterative
#' averaging to handle extreme estimates (0 or 1) more robustly.
#'
#' @inheritParams common-params
#' @param iter Number of iterations for averaging confidence limits (default: 100).
#' @param ... Additional arguments passed to `bcajack`.
#'
#' @return A list with components:
#'   \item{n1}{Sample size of group 1}
#'   \item{n2}{Sample size of group 2}
#'   \item{phat}{Estimated probability P(X<Y)}
#'   \item{ci.low}{Average lower confidence limit}
#'   \item{ci.upper}{Average upper confidence limit}
#'
#' @details
#' This function provides more stable confidence intervals when the
#' estimate of P(X<Y) is near 0 or 1. When the estimate is exactly
#' 0 or 1, it uses `cid`. Otherwise, it computes confidence intervals
#' on `iter` subsamples of size min(n1, n2) and averages the limits.
#'
#' The averaging approach can provide better coverage properties when
#' sample sizes are unequal or when the estimate is extreme.
#'
#' Requires the `bcaboot` package.
#'
#' @seealso \code{\link{wmw.bca}}, \code{\link{wmw.bcapv.v2}}
#'
#' @export
#' @examples
#' x <- rnorm(40, mean=100, sd=15)
#' y <- rnorm(30, mean=110, sd=15)
#' wmw.bcav2(x, y, nboot=500, iter=50)
wmw.bcav2<-function(x,y,alpha=.05,nboot=500,iter=100,SEED=TRUE,...){
#
#  BCA confidence interval for P(X<Y), X and Y independent
#  Method: Bias corrected accelerated bootstrap
#  Uses a modification when the estimate is zero or one
#
library(bcaboot)
if(SEED)set.seed(2)
n1=length(x)
n2=length(y)
nmin=min(n1,n2)
cimat=matrix(NA,iter,2)
m=matrix(NA,nmin,2)
e=bmp(x,y)$phat
if(e==1 ||e==0){
ci=cid(x,y)$ci.p
}
else{
for(i in 1:iter){
m[,1]=sample(x,nmin)
m[,2]=sample(y,nmin)
a=bcajack(m,B=nboot,wmw.est.only,alpha=alpha/2,verbose=FALSE,...)
cimat[i,]=c(a$lims[1,1],ci.upper=a$lims[3,1])
}
ci=mean(elimna(cimat[,1]))
ci[2]=mean(elimna(cimat[,2]))
}
list(n1=n1,n2=n2,phat=e,ci.low=ci[1],ci.upper=ci[2])
}


#' BCA Confidence Interval for Correlation with Bad Leverage Point Removal
#'
#' Computes a BCA bootstrap confidence interval for a robust correlation
#' coefficient based on regression with bad leverage points removed.
#'
#' @inheritParams common-params
#' @param regfun Regression function to use (default: `tsreg`).
#' @param varfun Variance function for weighted correlation (default: `pbvar`).
#' @param outfun Function for detecting outliers/leverage points (default: `outpro.depth`).
#' @param plotit Logical. If `TRUE`, creates diagnostic plots (default: `FALSE`).
#' @param ... Additional arguments passed to `bcajack2`.
#'
#' @return A list with components:
#'   \item{cor}{Robust correlation coefficient}
#'   \item{ci}{BCA confidence interval (vector of length 2)}
#'
#' @details
#' This function computes a robust correlation coefficient using the
#' `corblp` method, which removes bad leverage points before computing
#' the correlation. The BCA bootstrap is used to construct confidence
#' intervals.
#'
#' Only bivariate data (single predictor and single response) are allowed.
#' The function will stop with an error if more than one predictor is provided.
#'
#' Requires the `bcaboot` package.
#'
#' @seealso \code{\link{corblp}}, \code{\link{tsreg}}, \code{\link{pbvar}}
#'
#' @export
#' @examples
#' x <- rnorm(50)
#' y <- 0.6*x + rnorm(50)
#' corblp.bca.C(x, y, nboot=500)
corblp.bca.C<-function(x,y,regfun=tsreg,varfun=pbvar,nboot=1000,alpha=.05,outfun=outpro.depth,SEED=TRUE,
plotit=FALSE,...){
#
# Correlation based on a robust regression estimator with bad
# leverage points removes

library(bcaboot)
if(SEED)set.seed(2)
xy=elimna(cbind(x,y))
p1=ncol(xy)
p=p1-1
if(p!=1)stop('Only a single independent variable is allowed')
x=xy[,1]
y=xy[,2]
n=length(y)
est=corblp(x,y,regfun=regfun,varfun=varfun)$cor
a=bcajack2(xy,1000,corblp.sub,alpha=alpha/2,regfun=regfun,varfun=varfun)
ci=c(a$lims[1,1],a$lims[3,1])
list(cor=est,ci=ci)
}


#' Percentile Bootstrap Test with Partial Centering
#'
#' Tests the hypothesis that J independent groups have equal measures of
#' location using the percentile bootstrap method with a partial centering
#' technique.
#'
#' @inheritParams common-params
#' @param x Data in matrix or list format. If matrix, columns are groups.
#'   If list, each element is a group.
#' @param grp Optional vector specifying which groups to compare (default: `NA`
#'   analyzes all groups). Example: `grp=c(1,3,5)` compares groups 1, 3, and 5.
#' @param est Measure of location to use (default: `onestep`).
#' @param ... Additional arguments passed to the estimator function `est`.
#'
#' @return A list with component:
#'   \item{p.value}{Bootstrap p-value for the global test}
#'
#' @details
#' This function tests equality of location measures across J independent
#' groups using a percentile bootstrap approach combined with partial centering.
#' The partial centering technique improves Type I error control.
#'
#' For each group, the data are centered by subtracting the mean of the
#' other groups' location estimates (not including the current group).
#' This centering approach helps control the Type I error rate while
#' maintaining good power.
#'
#' Missing values are automatically removed before analysis.
#'
#' @seealso \code{\link{boot.TM}}, \code{\link{onestep}}
#'
#' @export
#' @examples
#' # Three independent groups
#' x1 <- rnorm(25, mean=100, sd=15)
#' x2 <- rnorm(25, mean=105, sd=15)
#' x3 <- rnorm(25, mean=110, sd=15)
#' pbcan(list(x1, x2, x3), nboot=500)
pbcan<-function(x,nboot=1000,grp=NA,est=onestep,...){
#
#   Test the hypothesis that J independent groups have
#   equal measures of location using the percentile bootstrap method.
#   in conjunction with a partially centering technique.
#
#   The data are assumed to be stored in x
#   which either has list mode or is a matrix.  In the first case
#   x[[1]] contains the data for the first group, x[[2]] the data
#   for the second group, etc. Length(x)=the number of groups = J.
#   If stored in a matrix, the columns of the matrix correspond
#   to groups.
#
#   est is the measure of location and defaults to an M-estimator
#   ... can be used to set optional arguments associated with est
#
#   The argument grp can be used to analyze a subset of the groups
#   Example: grp=c(1,3,5) would compare groups 1, 3 and 5.
#
#   Missing values are allowed.
#
if(is.matrix(x))x<-listm(x)
if(!is.list(x))stop("Data must be stored in list mode or in matrix mode.")
if(!is.na(sum(grp))){  # Only analyze specified groups.
xx<-list()
for(i in 1:length(grp))xx[[i]]<-x[[grp[1]]]
x<-xx
}
J<-length(x)
tempn<-0
vecm<-0
for(j in 1:J){
temp<-x[[j]]
temp<-temp[!is.na(temp)] # Remove missing values.
tempn[j]<-length(temp)
x[[j]]<-temp
vecm[j]<-est(x[[j]],...)
}
xcen<-list()
flag<-rep(TRUE,J)
for(j in 1:J){
flag[j]<-FALSE
temp<-mean(vecm[flag])
xcen[[j]]<-x[[j]]-temp
flag[j]<-T
}
icrit<-round((1-alpha)*nboot)
bvec<-matrix(NA,nrow=J,ncol=nboot)
set.seed(2) # set seed of random number generator so that
#             results can be duplicated.
print("Taking bootstrap samples. Please wait.")
for(j in 1:J){
paste("Working on group ",j)
data<-matrix(sample(xcen[[j]],size=length(x[[j]])*nboot,replace=TRUE),nrow=nboot)
bvec[j,]<-apply(data,1,est,...) # Bootstrapped values for jth group
}
vvec<-NA
for(j in 1:J){
vvec[j]<-sum((bvec[j,]-vecm[j])^2)/(nboot-1)
}
dis<-NA
for(i in 1:nboot){
dis[i]<-sum((bvec[,i]-vecm)^2/vvec)
}
tvec<-sum((0-vecm)^2/vvec)
dis<-sort(dis)
print(tvec)
print(dis[icrit])
print(vecm)
sig<-1-sum((tvec>=dis))/nboot
list(p.value=sig)
}


#' Two-Sample Permutation Test
#'
#' Performs a two-sample permutation test for comparing location or scale
#' measures between two independent groups.
#'
#' @inheritParams common-params
#' @param est Estimator function to use (default: `mean`). Can be any measure
#'   of location or scale.
#' @param nboot Number of permutations (default: 1000).
#'
#' @return A list with components:
#'   \item{dif}{Observed difference between groups}
#'   \item{lower}{Lower critical value}
#'   \item{upper}{Upper critical value}
#'   \item{reject}{Either "yes" or "no" indicating whether to reject H0}
#'
#' @details
#' This function performs a permutation test by randomly permuting the
#' combined data and recomputing the difference in the estimator between
#' the two groups. The distribution of these permuted differences provides
#' the reference distribution for the test.
#'
#' The null hypothesis is rejected if the observed difference falls outside
#' the critical values (at the alpha/2 and 1-alpha/2 quantiles of the
#' permutation distribution).
#'
#' Missing values are automatically removed before analysis.
#'
#' @seealso \code{\link{permg.t}}, \code{\link{perm.rho}}
#'
#' @export
#' @examples
#' x <- rnorm(30, mean=100, sd=15)
#' y <- rnorm(35, mean=105, sd=15)
#' permg(x, y, est=median, nboot=500)
permg<-function(x,y,alpha=.05,est=mean,nboot=1000){
#
# Do a two-sample permutation test based on means or any
# other measure of location or scale indicated by the
# argument est.
#
# The default number of permutations is nboot=1000
#
x<-x[!is.na(x)]
y<-y[!is.na(y)]
xx<-c(x,y)
dif<-est(x)-est(y)
vec<-c(1:length(xx))
v1<-length(x)+1
difb<-NA
temp2<-NA
for(i in 1:nboot){
data <- sample(xx, size = length(xx), replace = FALSE)
temp1<-est(data[c(1:length(x))])
temp2<-est(data[c(v1:length(xx))])
difb[i]<-temp1-temp2
}
difb<-sort(difb)
icl<-floor((alpha/2)*nboot+.5)
icu<-floor((1-alpha/2)*nboot+.5)
reject<-"no"
if(dif>=difb[icu] || dif <=difb[icl])reject<-"yes"
list(dif=dif,lower=difb[icl],upper=difb[icu],reject=reject)
}



#' Permutation Test for Trimmed Means
#'
#' Performs a two-sample permutation test for comparing trimmed means using
#' the Chung-Romano version of the permutation test.
#'
#' @inheritParams common-params
#' @param nboot Number of permutations (default: 1000).
#'
#' @return A list with components:
#'   \item{teststat}{Observed test statistic (Yuen's test statistic)}
#'   \item{lower.crit}{Lower critical value}
#'   \item{upper.crit}{Upper critical value}
#'   \item{reject}{Either "yes" or "no" indicating whether to reject H0}
#'
#' @details
#' This function implements the Chung-Romano version of a permutation test
#' for comparing trimmed means. Instead of permuting the raw differences,
#' it permutes the combined data and recomputes Yuen's test statistic for
#' each permutation.
#'
#' The test statistic is Yuen's test statistic for comparing trimmed means,
#' which accounts for the standard errors of the trimmed means.
#'
#' Missing values are automatically removed before analysis.
#'
#' @references
#' Chung, E. & Romano, J.P. (2013). Exact and asymptotically robust
#' permutation tests. Annals of Statistics, 41, 484-507.
#'
#' @seealso \code{\link{permg}}, \code{\link{yuen}}
#'
#' @export
#' @examples
#' x <- rnorm(30, mean=100, sd=15)
#' y <- rnorm(35, mean=105, sd=15)
#' permg.t(x, y, tr=0.2, nboot=500)
permg.t<-function(x,y,alpha=.05,tr=0,nboot=1000,SEED=TRUE){
#
# Do a two-sample permutation test based on trimmed means using the
# Chung--Romano version of a permuation test.

# The default number of permutations is nboot=1000
#
if(SEED)set.seed(2)
x<-x[!is.na(x)]
y<-y[!is.na(y)]
xx<-c(x,y)
tval<-yuen(x,y,tr=tr)$teststat
vec<-c(1:length(xx))
v1<-length(x)+1
difb<-NA
tv<-NA
for(i in 1:nboot){
data <- sample(xx, size = length(xx), replace = FALSE)
temp1<-data[c(1:length(x))]
temp2<-data[c(v1:length(xx))]
tv[i]<-yuen(temp1,temp2,tr=tr)$teststat
}
tv<-sort(tv)
icl<-floor((alpha/2)*nboot+.5)
icu<-floor((1-alpha/2)*nboot+.5)
reject<-'no'
list(teststat=tval,lower.crit=tv[icl],upper.crit=tv[icu],reject=reject)
}


#' Permutation Test for Pearson Correlation
#'
#' Performs a permutation test for Pearson's correlation using the
#' DiCiccio-Romano version of the permutation test.
#'
#' @inheritParams common-params
#' @param nboot Number of permutations (default: 1000).
#'
#' @return A list with components:
#'   \item{teststat}{Observed test statistic (standardized correlation)}
#'   \item{lower.crit}{Lower critical value}
#'   \item{upper.crit}{Upper critical value}
#'   \item{reject}{Either 0 (do not reject) or 1 (reject H0)}
#'
#' @details
#' This function implements the DiCiccio-Romano version of a permutation
#' test for Pearson's correlation. The test statistic is a standardized
#' version of the correlation coefficient that accounts for its variance.
#'
#' The permutation distribution is generated by randomly permuting the y
#' values while keeping the x values fixed. This preserves the marginal
#' distributions while breaking the association under the null hypothesis.
#'
#' Missing values are automatically removed before analysis.
#'
#' @references
#' DiCiccio, C.J. & Romano, J.P. (2017). Robust permutation tests for
#' correlation and regression coefficients. Journal of the American
#' Statistical Association, 112, 1211-1220.
#'
#' @seealso \code{\link{permg}}, \code{\link{perm.rho.sub}}
#'
#' @export
#' @examples
#' x <- rnorm(50)
#' y <- 0.5*x + rnorm(50)
#' perm.rho(x, y, nboot=500)
perm.rho<-function(x,y,alpha=.05,nboot=1000,SEED=TRUE){
#
# Do a  permutation test based on Pearson's correlation
# Diciccio--Romano version of a permuation test (JASA, 2017, 112, 1211-1220)
#

# The default number of permutations is nboot=1000
#
if(SEED)set.seed(2)
xx<-cbind(x,y)
xx=elimna(xx)
x=xx[,1]
y=xx[,2]
n=length(x)
tval<-perm.rho.sub(x,y)
vec<-c(1:length(xx))
v1<-length(x)+1
difb<-NA
tv<-NA
for(i in 1:nboot){
id=sample(n,n)
tv[i]<-perm.rho.sub(x,y[id])
}
tv<-sort(tv)
icl<-floor((alpha/2)*nboot+.5)
icu<-floor((1-alpha/2)*nboot+.5)
reject<-0
if(tval>=tv[icu] || tval <=tv[icl])reject<-1
list(teststat=tval,lower.crit=tv[icl],upper.crit=tv[icu],reject=reject)
}


#' Helper Function for Permutation Test Statistic
#'
#' Internal helper function that computes the standardized test statistic
#' for the permutation test of Pearson's correlation.
#'
#' @param x First variable.
#' @param y Second variable.
#'
#' @return Standardized test statistic S = sqrt(n) * rho / tau, where
#'   rho is the correlation and tau is an estimate of the standard deviation.
#'
#' @details
#' This function computes a studentized version of Pearson's correlation
#' that is used in the DiCiccio-Romano permutation test. The statistic
#' accounts for the variance of the correlation coefficient.
#'
#' @keywords internal
perm.rho.sub<-function(x,y){
rho=cor(x,y)
n=length(x)
xbar=mean(x)
ybar=mean(y)
m22=sum((x-xbar)^2*(y-ybar)^2)/n
m20=sum((x-xbar)^2)/n
m02=sum((y-ybar)^2)/n
tau=sqrt(m22/(m20*m02))
S=sqrt(n)*rho/tau
S
}


#' Helper Function for Linear Contrasts with WMW Statistics
#'
#' Internal helper function used in parallel bootstrap procedures for
#' computing linear contrasts of WMW-type statistics.
#'
#' @param M Matrix of bootstrap estimates.
#' @param con Contrast matrix.
#'
#' @return Vector of linear contrast values.
#'
#' @details
#' This function computes linear combinations of columns in M according
#' to the contrast matrix. Used internally by `linWMWMC` and related
#' functions for parallel processing.
#'
#' @keywords internal
linWMWMC.sub<-function(M,con){
L=apply(t(con*t(M)),1,sum)
L
}


#' Helper Function for Converting Contrasts to Probabilities
#'
#' Internal helper function that converts linear contrast values to
#' probability estimates for WMW-type statistics.
#'
#' @param L Vector of linear contrast values.
#'
#' @return Probability estimate P(L < 0) + 0.5 * P(L = 0).
#'
#' @details
#' This function computes a probability estimate based on the proportion
#' of negative values plus half the proportion of zero values. Used
#' internally in WMW-based procedures.
#'
#' @keywords internal
linWMWMC.sub2<-function(L){
phat=mean(L<0)+.5*mean(L==0)
phat
}



#' Helper Function for Ridge Regression Global Test
#'
#' Internal helper function for computing F-test statistics in parallel
#' bootstrap procedures for ridge regression.
#'
#' @param x Matrix containing predictors and response (last column).
#' @param p Number of predictors.
#' @param regfun Regression function to use.
#'
#' @return F-test statistic value.
#'
#' @details
#' Used internally by `ridgeGnullMC` for parallel computation of bootstrap
#' F-test statistics in ridge regression global tests.
#'
#' @keywords internal
ridgeGnullMC.sub<-function(x,p,regfun=regfun){
p1=p+1
v=ridgeG.sub(x[,1:p],x[,p1],regfun=regfun)$F.test
v
}



#' Improved Bootstrap-t Method for Multiple Comparisons
#'
#' Computes the B2_tk test statistic for testing equality of trimmed means
#' across k independent samples using an improved bootstrap-t approach.
#'
#' @inheritParams common-params
#' @param alist Data in list format, where each element contains observations
#'   for one group.
#'
#' @return A list with components:
#'   \item{p.value}{P-value for the global test}
#'   \item{teststat}{Observed test statistic}
#'   \item{crit}{Critical value from chi-square distribution}
#'   \item{e}{Number of observations trimmed from each tail in each group}
#'   \item{f}{Effective sample sizes after trimming}
#'   \item{s}{Squared standard errors for trimmed means}
#'   \item{w}{Weights for each group}
#'   \item{tt}{Standardized test statistics for each group}
#'
#' @details
#' This function implements an improved bootstrap-t method (B2_tk) for
#' testing equality of trimmed means across multiple independent groups.
#' The method uses a transformation based on the studentized range to
#' improve upon the standard chi-square approximation.
#'
#' The test statistic is based on weighted deviations from a pooled
#' trimmed mean, with weights inversely proportional to the squared
#' standard errors. A transformation is applied to improve the chi-square
#' approximation for small to moderate sample sizes.
#'
#' Missing values are automatically removed before analysis.
#'
#' @seealso \code{\link{t1way}}, \code{\link{trimse}}
#'
#' @export
#' @examples
#' # Three independent groups
#' x1 <- rnorm(25, mean=100, sd=15)
#' x2 <- rnorm(25, mean=105, sd=15)
#' x3 <- rnorm(25, mean=110, sd=15)
#' btsqrk(list(x1, x2, x3), tr=0.2)
btsqrk<-function(alist,alpha=0.05,tr=0.2){
#computes B2_tk test statistics for k independent samples.
#alist should be a list type object
#s's are computed by trimse which can be found in all Rallfun files written by Wilcox Rand
k<-length(alist)
# Remove any missing values in alist
for (i in 1:k){alist[[i]]<-alist[[i]][!is.na(alist[[i]])]}
zc<-qnorm(alpha/2)
e=trunc(tr*sapply(alist,length))
f<-(sapply(alist,length))-(2*e)
s=sapply(alist,trimse,tr=tr)^2
wden=sum(1/s)
w=(1/s)/wden
yplus<-sum(w*(sapply(alist,mean,trim=tr)))
tt<-((sapply(alist,mean,trim=tr))-yplus)/sqrt(s)
v<-(f-1)
z<-((4*v^2)+(5*((2*(zc^2))+3)/24))/((4*v^2)+v+(((4*(zc^2))+9)/12))*sqrt(v)*(sqrt(log(1+(tt^2/v))))
teststat<-sum(z^2)
crit<-qchisq(1-alpha,k-1)
bt2pvalue<-1-(pchisq(teststat,k-1))
list(p.value=bt2pvalue,teststat=teststat,crit=crit,e=e,f=f,s=s,w=w,tt=tt)
}


