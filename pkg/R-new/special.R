# WRS Package - Special and Specialized Methods
# Extracted from Rallfun-v45.R
#
# This module contains all remaining user-facing functions that don't fit into
# other specialized modules. This is a "catch-all" module containing diverse
# statistical methods including:
#
# - Ophthalmology methods (oph.*, Astig_*): 29 functions
# - Binomial/binary methods (bin*, binom*): 12 functions
# - Run tests (run*): 20 functions
# - Sign tests (sign*): 2 functions
# - Selection methods (sel*): 6 functions
# - ANOVA extensions (*KMS*, *GLOB*): 19 functions
# - Smoothing methods (sm*): 20 functions
# - Trimmed estimators (trim*): 10 functions
# - Stein estimators (stein*, Stein*): 8 functions
# - Multicore variants (*MC*): 4 functions
# - Miscellaneous methods: 644 functions
#
# Total functions: 774
# Extraction date: 2025-12-30
#
# NOTE: This module contains specialized methods that are domain-specific or
# experimental. Some functions may have limited documentation or testing.



# ============================================================================

# Ophthalmology (29 functions)

# ============================================================================


#' Astigmatism Magnitude Analysis Workflow
#'
#' @description
#' Interactive wrapper function for comprehensive astigmatism magnitude analysis.
#' Prompts user to select data file, performs statistical comparisons (dependent or
#' independent groups), and exports results to Excel files.
#'
#' @param dependent Logical. If \code{TRUE} (default), performs dependent group analysis.
#'   If \code{FALSE}, performs independent group analysis.
#'
#' @details
#' This function provides an interactive workflow for astigmatism magnitude data:
#' \enumerate{
#'   \item Prompts user to select tab-delimited data file with headers
#'   \item Validates data (all values must be ≤ 6)
#'   \item Performs appropriate statistical comparisons:
#'     \itemize{
#'       \item Independent: Calls \code{\link{oph.astig.indepcom}} and \code{\link{oph.astig.indepintervals}}
#'       \item Dependent: Calls \code{\link{oph.astig.depcom}} and \code{\link{oph.astig.mcnemar}}
#'     }
#'   \item Exports results to Excel files in same directory as input file
#' }
#'
#' Output files:
#' \itemize{
#'   \item Independent: "18.Mean.xls" (comparisons), "20.Intervals.xls" (confidence intervals)
#'   \item Dependent: "17.Mean.xls" (comparisons), "19.Intervals.xls" (McNemar tests)
#' }
#'
#' @return NULL (invisibly). Results are written to Excel files.
#'
#' @seealso \code{\link{oph.astig.depcom}}, \code{\link{oph.astig.indepcom}},
#'   \code{\link{oph.astig.mcnemar}}, \code{\link{oph.astig.indepintervals}},
#'   \code{\link{Astig_Vector}}
#'
#' @keywords ophthalmology
#' @export
Astig_Magnitude<-function(dependent=T){

#library("readxl")
#library("xlsx")

#src<-choose.files(caption="Please specify source R code file: Rallfun-v40")
#src<-file.choose()
#source(src)


#filein<-choose.files(caption="Please specify your data input file")
filein<-file.choose()
x=read.table(filein,header=T,sep="\t")

f_d<-dependent
max<-as.integer(max(abs(x),na.rm=T))+1
if(max>7) {
#print("A maxium value is > 6 in your data. Stop running.");
stop("A maxium value is > 6 in your data. Stop running.",call.=F);}
cmbn<-names(x)

## fileout<-file.choose()


#dir<-choose.dir(caption="Please specify where to save your results")
dir=dirname(filein)

## if dependent is False
if(f_d==F) {

e1<-try({
name="18.Mean"
res<-as.data.frame(oph.astig.indepcom(x,invalid=max))
colnames(res)[4]="Formula_1"
colnames(res)[5]="Formula_2"
res$Formula_1<-cmbn[match(res$Formula_1,c(1:length(cmbn)))]
res$Formula_2<-cmbn[match(res$Formula_2,c(1:length(cmbn)))]
#write.table(res,file=fileout,sep="\t",quote=F,row.names=F,append=T)
res<-res[,c(1:12)]
write.table(res,file=paste0(dir,"/",name,".xls"),sep="\t", quote=F,row.names=T,append=F,col.names=NA)
}
)

name="20.Intervals"
resList<-oph.astig.indepintervals(x,invalid=max)
resL<-do.call(rbind,resList)
resL1<-as.data.frame(resL)
resL1$D<-row.names(resL)
colnames(resL1)[1]="Formula_1"
colnames(resL1)[2]="Formula_2"
resL1$Formula_1<-cmbn[match(resL1$Formula_1,c(1:length(cmbn)))]
resL1$Formula_2<-cmbn[match(resL1$Formula_2,c(1:length(cmbn)))]
resL1<-resL1[,c(dim(resL1)[2],1:dim(resL1)[2]-1)]
write.table(resL1,file=paste0(dir,"/",name,".xls"),sep="\t", quote=F,row.names=F,append=F)
}

else {

name="17.Mean"
res<-as.data.frame(oph.astig.depcom(x,invalid=max))
colnames(res)[1]="Formula_1"
colnames(res)[2]="Formula_2"
res$Formula_1<-cmbn[match(res$Formula_1,c(1:length(cmbn)))]
res$Formula_2<-cmbn[match(res$Formula_2,c(1:length(cmbn)))]
write.table(res,file=paste0(dir,"/",name,".xls"),sep="\t", quote=F,row.names=T,append=F,col.names=NA)


name="19.Intervals"
resList<-oph.astig.mcnemar(x,invalid=max)
resL<-do.call(rbind,resList)
resL<-as.data.frame(resL)
colnames(resL)[2]="Formula_1"
colnames(resL)[5]="Formula_2"
resL$Formula_1<-cmbn[match(resL$Formula_1,c(1:length(cmbn)))]
resL$Formula_2<-cmbn[match(resL$Formula_2,c(1:length(cmbn)))]
write.table(resL,file=paste0(dir,"/",name,".xls"),sep="\t", quote=F,row.names=T,append=F,col.names=NA)
}
}


#Astigmatism Vector Dataset
#Choose this file (Astig_Vector):  source(file.choose())
#for dependent dataset:  Astig_Vector(dependent=T)
#for independent dataset:  Astig_Vector(dependent=F)




#' Astigmatism Vector Analysis Workflow
#'
#' @description
#' Interactive wrapper function for comprehensive astigmatism vector (bivariate) analysis.
#' Prompts user to select data file, performs convex polygon comparisons, and exports
#' results with visualization plots to Excel/TIFF files.
#'
#' @param dependent Logical. If \code{TRUE} (default), performs dependent group analysis.
#'   If \code{FALSE}, performs independent group analysis.
#'
#' @details
#' This function provides an interactive workflow for bivariate astigmatism vector data:
#' \enumerate{
#'   \item Prompts user to select tab-delimited data file with headers
#'   \item Validates data (all values must be ≤ 6)
#'   \item Computes convex polygon regions for mean and median
#'   \item Generates double angle plots (DAP) for visualization
#'   \item Exports statistical results and plots
#' }
#'
#' Output files created:
#' \itemize{
#'   \item "21.MeanConvexpoly.xls" - Mean convex polygon p-values and centers
#'   \item "22.MedianConvexpoly.xls" - Median convex polygon p-values and centers
#'   \item "23.DatasetMeanConvexpoly.xls" - Dataset-level mean analysis
#'   \item "24.DatasetMedianConvexpoly.xls" - Dataset-level median analysis
#'   \item Multiple .tiff files with double angle plots
#' }
#'
#' @return NULL (invisibly). Results are written to files.
#'
#' @seealso \code{\link{oph.astig.meanconvexpoly}}, \code{\link{oph.astig.medianconvexpoly}},
#'   \code{\link{oph.astig.datasetconvexpoly.mean}}, \code{\link{oph.astig.datasetconvexpoly.median}},
#'   \code{\link{plotDAP}}, \code{\link{Astig_Magnitude}}
#'
#' @keywords ophthalmology
#' @export
Astig_Vector<-function(dependent=T){

#library("readxl")
#library("xlsx")

#src<-choose.files(caption="Please specify source R code file: Rallfun-v40")
#src<-file.choose()
#source(src)

#filein<-choose.files(caption="Please specify your data input file")
filein<-file.choose()
x=read.table(filein,header=T,sep="\t")

f_d<-dependent
max<-as.integer(max(abs(x),na.rm=T))+1
if(max>7) {
#print("A maximum value is > 6 in your data. Stop running.");
stop("A maximum value is > 6 in your data. Stop running.",call.=F);}
cmbn<-names(x)
cmbn1<- unlist(strsplit(cmbn,split="\\."))
cmbn1<-cmbn1[seq_along(cmbn1)%%2 >0]
cmbn1<-cmbn1[seq_along(cmbn1)%%2 >0]
empty<-data.frame()
## fileout<-file.choose()


#dir<-choose.dir(caption="Please specify where to save your results")
dir=dirname(filein)

name="23.DatasetMeanConvexpoly"
res<-oph.astig.datasetconvexpoly.mean(x,plotit=F)
CNT<-as.data.frame(do.call(rbind,res$centers))
colnames(CNT)<-c("center.X","center.Y","N")
row.names(CNT)<-cmbn1
write.table(CNT,file=paste0(dir,"/",name,".xls"),sep="\t", quote=F,row.names=T,append=T,col.names=NA)
write.table(empty,file=paste0(dir,"/",name,".xls"),sep="\t", quote=F,row.names=T,append=T,col.names=NA)
Dm<-list()
for(i in 1:length(res$convex.hull.pts)){
DD<-as.data.frame(res$convex.hull.pts[[i]]);
#Dm<-append(Dm,DD)
Dm[[i]]<-DD
colnames(DD)<-c(paste0(cmbn1[i],".","X"),paste0(cmbn1[i],".","Y"))
write.table(DD,file=paste0(dir,"/",name,".xls"),sep="\t", quote=F,row.names=T,append=T,col.names=NA)
write.table(empty,file=paste0(dir,"/",name,".xls"),sep="\t", quote=F,row.names=T,append=T,col.names=NA)
#Dm<-append(Dm,DD)
}

name="21.MeanConvexpoly"
name1="Mean Convex Polygon"
res<-oph.astig.meanconvexpoly(x,plotit=F)
P<-as.data.frame(do.call(rbind,res$p.values))
colnames(P)<-"P.values"
row.names(P)<-cmbn1
write.table(P,file=paste0(dir,"/",name,".xls"),sep="\t", quote=F,row.names=T,append=T,col.names=NA)
write.table(empty,file=paste0(dir,"/",name,".xls"),sep="\t", quote=F,row.names=T,append=T,col.names=NA)
CNT<-as.data.frame(do.call(rbind,res$centers))
colnames(CNT)<-c("center.X","center.Y","N")
row.names(CNT)<-cmbn1
write.table(CNT,file=paste0(dir,"/",name,".xls"),sep="\t", quote=F,row.names=T,append=T,col.names=NA)
write.table(empty,file=paste0(dir,"/",name,".xls"),sep="\t", quote=F,row.names=T,append=T,col.names=NA)
for(i in 1:length(res$conf.region.points)){
D<-as.data.frame(res$conf.region.points[[i]]);
colnames(D)<-c(paste0(cmbn1[i],".","X"),paste0(cmbn1[i],".","Y"))
write.table(D,file=paste0(dir,"/",name,".xls"),sep="\t", quote=F,row.names=T,append=T,col.names=NA)
write.table(empty,file=paste0(dir,"/",name,".xls"),sep="\t", quote=F,row.names=T,append=T,col.names=NA)
##PDF(paste0(dir,"\\",name,".",cmbn1[i],".PDF"))
ragg::agg_tiff(paste0(dir,"/",name,".",cmbn1[i],".tiff"), width = 6, height = 7, units = "in", res = 300)
plotDAP(x[,c(2*i-1,2*i)],D,CNT[i,],Dm[[i]],paste0(name1," ",cmbn1[i]))
dev.off()
}

name="24.DatasetMedianConvexpoly"
res<-oph.astig.datasetconvexpoly.median(x,plotit=F)
CNT<-as.data.frame(do.call(rbind,res$centers))
colnames(CNT)<-c("center.X","center.Y","N")
row.names(CNT)<-cmbn1
write.table(CNT,file=paste0(dir,"/",name,".xls"),sep="\t", quote=F,row.names=T,append=T,col.names=NA)
write.table(empty,file=paste0(dir,"/",name,".xls"),sep="\t", quote=F,row.names=T,append=T,col.names=NA)
Dm<-list()
for(i in 1:length(res$convex.hull.pts)){
DD<-as.data.frame(res$convex.hull.pts[[i]]);
#Dm<-append(Dm,DD)
Dm[[i]]<-DD
colnames(DD)<-c(paste0(cmbn1[i],".","X"),paste0(cmbn1[i],".","Y"))
write.table(DD,file=paste0(dir,"/",name,".xls"),sep="\t", quote=F,row.names=T,append=T,col.names=NA)
write.table(empty,file=paste0(dir,"/",name,".xls"),sep="\t", quote=F,row.names=T,append=T,col.names=NA)
#Dm<-append(Dm,DD)
}

name="22.MedianConvexpoly"
name1="Median Convex Polygon"
res<-oph.astig.medianconvexpoly(x,plotit=F)
P<-as.data.frame(do.call(rbind,res$p.values))
colnames(P)<-"P.values"
row.names(P)<-cmbn1
write.table(P,file=paste0(dir,"/",name,".xls"),sep="\t", quote=F,row.names=T,append=T,col.names=NA)
write.table(empty,file=paste0(dir,"/",name,".xls"),sep="\t", quote=F,row.names=T,append=T,col.names=NA)
CNT<-as.data.frame(do.call(rbind,res$centers))
colnames(CNT)<-c("center.X","center.Y","N")
row.names(CNT)<-cmbn1
write.table(CNT,file=paste0(dir,"/",name,".xls"),sep="\t", quote=F,row.names=T,append=T,col.names=NA)
write.table(empty,file=paste0(dir,"/",name,".xls"),sep="\t", quote=F,row.names=T,append=T,col.names=NA)
for(i in 1:length(res$conf.region.points)){
D<-as.data.frame(res$conf.region.points[[i]]);
colnames(D)<-c(paste0(cmbn1[i],".","X"),paste0(cmbn1[i],".","Y"))
write.table(D,file=paste0(dir,"/",name,".xls"),sep="\t", quote=F,row.names=T,append=T,col.names=NA)
write.table(empty,file=paste0(dir,"/",name,".xls"),sep="\t", quote=F,row.names=T,append=T,col.names=NA)
##PDF(paste0(dir,"/",name,".",cmbn1[i],".PDF"))
ragg::agg_tiff(paste0(dir,"/",name,".",cmbn1[i],".tiff"), width = 6, height = 7, units = "in", res = 300)
plotDAP(x[,c(2*i-1,2*i)],D,CNT[i,],Dm[[i]],paste0(name1," ",cmbn1[i]))
dev.off()
}

cmbn2<-combn(cmbn1,2,simplify=F)
empty<-data.frame()

## if dependent is False
if(f_d==F) {

e1<-try({
name="26.BivMeans.independent"
res<-oph.astig.indepbivmeans(x)
for(i in 1:length(cmbn1)){
D<-as.data.frame(res[[i]])
colnames(D)<-c(cmbn2[[i]][1],cmbn2[[i]][2],"p.value", "p.adjusted")
rownames(D)<-c("X","Y")
write.table(D,file=paste0(dir,"/",name,".xls"),sep="\t", quote=F,row.names=T,append=T,col.names=NA)
write.table(empty,file=paste0(dir,"/",name,".xls"),sep="\t", quote=F,row.names=T,append=T,col.names=NA)
}
}
)

name="28.Bivmarg.totvars.independent"
res<-oph.astig.indepbivmarg.totvars(x)
for(i in 1:length(cmbn1)){
D<-as.data.frame(res$results[[i]])
colnames(D)<-c(cmbn2[[i]][1],cmbn2[[i]][2],"Ratio","p.value", "p.adjusted")
rownames(D)<-c("X","Y")
write.table(D,file=paste0(dir,"/",name,".xls"),sep="\t", quote=F,row.names=T,append=T,col.names=NA)
write.table(empty,file=paste0(dir,"/",name,".xls"),sep="\t", quote=F,row.names=T,append=T,col.names=NA)
}
D1<-as.data.frame(res$results.total)
colnames(D1)<-c("Formula_1","Formula_2","TotalVar_X","TotalVar_Y","SD_X","SD_Y","p.value", "p.adjusted")
D1$Formula_1<-cmbn1[match(D1$Formula_1,paste0("F ",c(1:length(cmbn1))))]
D1$Formula_2<-cmbn1[match(D1$Formula_2,paste0("F ",c(1:length(cmbn1))))]
write.table(D1,file=paste0(dir,"/",name,".xls"),sep="\t", quote=F,row.names=T,append=T,col.names=NA)

}

else {

e1<-try({
name="25.BivMeans.dependent"
res<-oph.astig.depbivmeans(x)
for(i in 1:length(cmbn1)){
D<-as.data.frame(res[[i]])
colnames(D)<-c(cmbn2[[i]][1],cmbn2[[i]][2],"p.value", "p.adjusted")
rownames(D)<-c("X","Y")
write.table(D,file=paste0(dir,"/",name,".xls"),sep="\t", quote=F,row.names=T,append=T,col.names=NA)
write.table(empty,file=paste0(dir,"/",name,".xls"),sep="\t", quote=F,row.names=T,append=T,col.names=NA)
}
}
)

name="27.Bivmarg.totvars.dependent"
res<-oph.astig.depbivmarg.totvars(x)
for(i in 1:length(cmbn1)){
D<-as.data.frame(res$results[[i]])
colnames(D)<-c(cmbn2[[i]][1],cmbn2[[i]][2],"Ratio","p.value", "p.adjusted")
rownames(D)<-c("X","Y")
write.table(D,file=paste0(dir,"/",name,".xls"),sep="\t", quote=F,row.names=T,append=T,col.names=NA)
write.table(empty,file=paste0(dir,"/",name,".xls"),sep="\t", quote=F,row.names=T,append=T,col.names=NA)
}
D1<-as.data.frame(res$results.total)
colnames(D1)<-c("Formula_1","Formula_2","TotalVar_X","TotalVar_Y","SD_X","SD_Y","p.value", "p.adjusted")
D1$Formula_1<-cmbn1[match(D1$Formula_1,paste0("F ",c(1:length(cmbn1))))]
D1$Formula_2<-cmbn1[match(D1$Formula_2,paste0("F ",c(1:length(cmbn1))))]
write.table(D1,file=paste0(dir,"/",name,".xls"),sep="\t", quote=F,row.names=T,append=T,col.names=NA)


}
}


#' Compare Bivariate Marginal Variances for Astigmatism Data
#'
#' @description
#' Compares variances of dependent bivariate variables for astigmatism prediction errors.
#' Performs pairwise comparisons between formulas using difference scores with outlier adjustment.
#'
#' @param m Matrix or data frame with J columns (J must be even). Columns represent pairs:
#'   columns 1-2 are formula 1, 3-4 are formula 2, etc.
#' @param alpha Significance level for confidence intervals. Default is 0.05.
#'
#' @details
#' Compares each pair of formulas (e.g., columns 1-2 vs 3-4, 1-2 vs 5-6, etc.) using
#' confidence intervals for pairwise difference scores. Estimates are adjusted if outliers
#' are detected using a projection method.
#'
#' @return List with component \code{results} containing pairwise comparison results.
#'
#' @keywords ophthalmology internal
#' @export
oph.astig.bivmarg<-function(m,alpha=.05){
#
# This function is designed to compare two  variances  dependent  variables based
#  prediction errors when dealing with astigmatism.
#
#  Assume m is a matrix or data frame having
#  J columns. First two columns first formula, next two columns next formula..
#
#  So J should be an even integer
#
#  Compare col 1-2 to 3-4, then 1-2 vs 5-6, etc
#
#  returns confidence interval for pairwise difference scores. So using difference scores for 1 and 3 as well as 2 and 4
# alpha = .05 = .95 confidence intervals
#
# Estimates are adjusted if outliers are found based on a projection method.
#
J=ncol(m)
N=J/2
J1=J-1
F=NULL
for(j in 1:N)F=c(F,paste('F',j))
chk.n=names(m)
MAT=matrix(NA,nrow=N,ncol=6)
dimnames(MAT)=list(NULL,c('Form','Form','Est.1','Est.2','Ratio','p.value'))
MAT=as.data.frame(MAT)
if(N != floor(N))stop('Should have an even number of columns')
results=list()
results.total=list()
mat=matrix(NA,nrow=N,ncol=2,byrow=TRUE)
v1=seq(1,J1,2)
mat[,1]=v1
mat[,2]=v1+1
ic=0
for(j in 1:N){
for(k in 1:N){
if(j<k){
ic=ic+1
id1=mat[j,]
id2=mat[k,]
if(!is.null(chk.n))print(paste('Comparing Variances' ,chk.n[mat[j,1]],'minus',chk.n[mat[k,1]], 'and',chk.n[mat[j,2]],'minus',chk.n[mat[k,2]] ))
e=comdvar.astig(m[,id1],m[,id2])
results[[ic]]=e
}}}
list(results=results)  #,results.total=MAT,Total.adj.p.values=pad)
}




# ----------------------------------------------------------------------------

# oph.astig.bivmarg.totvars

# ----------------------------------------------------------------------------

#' Compare Total Variances for Bivariate Astigmatism Data
#'
#' @description
#' Compares total variances of dependent bivariate variables for astigmatism prediction errors.
#' Performs pairwise comparisons between formulas using both marginal and total variances
#' with outlier adjustment and Holm's multiple testing correction.
#'
#' @param m Matrix or data frame with J columns (J must be even). Columns represent pairs:
#'   columns 1-2 are formula 1, 3-4 are formula 2, etc.
#' @param alpha Significance level for hypothesis tests. Default is 0.05.
#'
#' @details
#' This function extends \code{\link{oph.astig.bivmarg}} by also comparing total variances.
#' For each pair of formulas (e.g., columns 1-2 vs 3-4):
#' \itemize{
#'   \item Computes marginal variance comparisons using difference scores
#'   \item Computes total variance for each formula: \code{var(X) + var(Y)}
#'   \item Adjusts for covariance to get variance of sums
#'   \item Uses \code{\link{comdvar.astig}} for marginal comparisons
#'   \item Uses \code{\link{comdvar}} for total variance comparisons
#'   \item Applies Holm's method to control familywise error rate
#' }
#'
#' Estimates are adjusted if outliers are detected using a projection method.
#'
#' @return List with three components:
#' \item{results}{List of marginal variance comparison results from \code{comdvar.astig}}
#' \item{results.total}{Data frame with columns: Form (formula IDs), Est.1, Est.2 (total variances),
#'   Ratio (variance ratio), p.value}
#' \item{Total.adj.p.values}{Holm-adjusted p-values for total variance comparisons}
#'
#' @seealso \code{\link{oph.astig.bivmarg}}, \code{\link{comdvar.astig}}, \code{\link{comdvar}}
#'
#' @keywords ophthalmology
#' @export
oph.astig.bivmarg.totvars<-function(m,alpha=.05){
#
# This function is designed to compare two  variances  dependent  variables based
#  prediction errors when dealing with astigmatism.
#
#  Assume m is a matrix or data frame having
#  J columns. First two columns first formula, next two columns next formula..
#
#  So J should be an even integer
#
#  Compare col 1-2 to 3-4, then 1-2 vs 5-6, etc
#
#  returns confidence interval for pairwise difference scores. So using difference scores for 1 and 3 as well as 2 and 4
# alpha = .05 = .95 confidence intervals
#
# Estimates are adjusted if outliers are found based on a projection method.
#
J=ncol(m)
N=J/2
J1=J-1
F=NULL
for(j in 1:N)F=c(F,paste('F',j))
chk.n=names(m)
MAT=matrix(NA,nrow=N,ncol=6)
dimnames(MAT)=list(NULL,c('Form','Form','Est.1','Est.2','Ratio','p.value'))
MAT=as.data.frame(MAT)
if(N != floor(N))stop('Should have an even number of columns')
results=list()
results.total=list()
mat=matrix(NA,nrow=N,ncol=2,byrow=TRUE)
v1=seq(1,J1,2)
mat[,1]=v1
mat[,2]=v1+1
ic=0
for(j in 1:N){
for(k in 1:N){
if(j<k){
ic=ic+1
id1=mat[j,]
id2=mat[k,]
if(!is.null(chk.n))print(paste('Comparing Variances' ,chk.n[mat[j,1]],'minus',chk.n[mat[k,1]], 'and',chk.n[mat[j,2]],'minus',chk.n[mat[k,2]] ))
e=comdvar.astig(m[,id1],m[,id2])
results[[ic]]=e
X=m[,id1[1]]+m[,id1[2]]-2*cov(m[,id1[1]],m[,id1[2]])  # So variance of sum is now equal to the sum of the variances
Y=m[,id2[1]]+m[,id2[2]]-2*cov(m[,id2[1]],m[,id2[2]])
temp=comdvar(X,Y)
e1=var(m[,id1[1]])+var(m[,id1[2]])
e2=var(m[,id2[1]])+var(m[,id2[2]])
MAT[ic,]=c(F[j],F[k],e1,e2,e1/e2,temp$p.value)
results.total[[ic]]=comdvar(X,Y)
}}}
pv=MAT[,6]
pad=p.adjust(pv,method='holm')
list(results=results,results.total=MAT,Total.adj.p.values=pad)
}




# ----------------------------------------------------------------------------

# oph.astig.Dataset.Means.ConfEllipses

# ----------------------------------------------------------------------------

#' Dataset Means and Confidence Ellipses for Astigmatism Data
#'
#' @description
#' Computes bivariate means and Hotelling's T-squared confidence ellipses for astigmatism
#' datasets. Handles multiple formulas (column pairs) and displays both dataset-level
#' and centroid-level ellipses with rotation adjustment.
#'
#' @param m Matrix or data frame with J columns (J must be even). Columns represent pairs:
#'   columns 1-2 are formula 1, 3-4 are formula 2, etc.
#' @param plotit Logical. If \code{TRUE} (default), plots rotated data with confidence ellipses.
#' @param alpha Significance level for confidence ellipses. Default is 0.05.
#' @param reset Logical. If \code{TRUE}, resets graphics parameters. Default is \code{FALSE}.
#' @param POLY Logical. If \code{TRUE}, uses \code{\link{oph.astig.datasetconvexpoly}} for plotting.
#'   Default is \code{FALSE}.
#' @param xlab,ylab Axis labels for plots.
#' @param pch Plotting character for data points. Default is '.'.
#'
#' @details
#' For each formula (column pair), the function:
#' \enumerate{
#'   \item Performs Shapiro-Wilk normality tests on both variables
#'   \item Computes bivariate mean, standard deviations, and correlation
#'   \item Calculates rotation angle based on covariance structure
#'   \item Computes Hotelling's T-squared critical value: \code{T = sqrt(2(n-1)F/(n-2))}
#'   \item Rotates data to principal axes using \code{\link{rotate.points}}
#'   \item Draws two confidence ellipses:
#'     \itemize{
#'       \item Dataset ellipse (radius = T × SD)
#'       \item Centroid ellipse (radius = T × SE, where SE = SD/sqrt(n))
#'     }
#' }
#'
#' Multiple formulas are displayed in a 2x2 grid layout.
#'
#' @return List with two components:
#' \item{DataSet}{Matrix with columns: N, SW.px (Shapiro-Wilk p-value for X),
#'   SW.py (for Y), Mean.x, Mean.y, sd`x, sd`y (rotated SDs), T (critical value),
#'   Cor (correlation), Ro.Ang.Deg (rotation angle in degrees)}
#' \item{Centroid}{Same as DataSet but with standard errors (se`x, se`y) instead of SDs}
#'
#' @references
#' See Hotelling_Bivariate_Transformation_Rand_10Jul21.docx in rfun
#'
#' @seealso \code{\link{oph.astig.datasetconvexpoly}}, \code{\link{rotate.points}},
#'   \code{\link{oph.astig.meanconvexpoly}}
#'
#' @keywords ophthalmology
#' @export
oph.astig.Dataset.Means.ConfEllipses<-function(m,plotit=TRUE,alpha=.05,reset=FALSE,POLY=FALSE,
xlab='X',ylab='Y',pch='.'){
#
#  See Hotelling_Bivariate_Transformation_Rand_10Jul21.docx  in rfun
#
#
n=nrow(m)
J=ncol(m)
N=J/2
if(N>1)par(mfrow=c(2,2))
if(N != floor(N))stop('Should have an even number of columns')
results=list()
MAT=matrix(NA,nrow=N,ncol=10)
dimnames(MAT)=list(NULL,c('N','SW.px','SW.py','Mean.x','Mean.y','sd`x','sd`y','T','Cor','Ro.Ang.Deg'))
id=c(-1,0)
for(j in 1:N){
id=id+2
d=m[,id]
ntest1=round(shapiro.test(d[,1])$p.value,4)
ntest2=round(shapiro.test(d[,2])$p.value,4)
M=apply(d,2,mean)
M=round(M,4)
sd=apply(d,2,sd)
sd=round(sd,4)
Tsq=2*(n-1)*qf(1-alpha,2,n-2)/(n-2)
Tv=sqrt(Tsq)
P.cor=cor(d[,1],d[,2])
P.cor=round(P.cor,4)
term=(2*P.cor*sd[1]*sd[2])/(var(d[,1])-var(d[,2]))
two.phi.rad=.5*atan(term)
two.phi.degrees=radians.2.degrees(two.phi.rad)  # this actually phi, 2*phi is used when plotting
two.phi.degrees=round(two.phi.degrees,4)
if(sd[1]<sd[2])two.phi.degrees=two.phi.degrees+90
term1=sqrt(var(d[,1])+var(d[,2])+2*sd[1]*sd[2]*sqrt(1-P.cor^2))
term2=sqrt(var(d[,1])+var(d[,2])-2*sd[1]*sd[2]*sqrt(1-P.cor^2))
sdxp=.5*(term1+term2)
sdyp=.5*(term1-term2)
sdxp=round(sdxp,4)
sdyp=round(sdyp,4)
tsdxp=Tv*sdxp
tsdyp=Tv*sdyp
mr=rotate.points(d[,1],d[,2],rad=-1*two.phi.rad)
MAT[j,]=c(n,ntest1,ntest2,M,sdxp,sdyp,Tv,P.cor,two.phi.degrees)
if(plotit){
if(POLY)oph.astig.datasetconvexpoly(mr,plotit = TRUE,reset=reset)
if(!POLY)plot(mr[,1],mr[,2],xlab=xlab,ylab=ylab,pch=pch,xlim=c(-1.5,1.5),ylim=c(-1.5,1.5),asp=1)
}
draw.ellipse(M[1],M[2],tsdxp,tsdyp,border=1,lty=1,angle=two.phi.degrees)
draw.ellipse(M[1],M[2],tsdxp/sqrt(n),tsdyp/sqrt(n),border=1,lty=1,angle=two.phi.degrees)
}
par(mfrow=c(1,1))
MAT2=MAT
MAT2[,6:7]=round(MAT2[,6:7]/sqrt(n),4)
dimnames(MAT2)=list(NULL,c('N','SW.px','SW.py','Mean.x','Mean.y','se`x','se`y','T','Cor','Ro.Ang.Deg'))
list(DataSet=MAT,Centroid=MAT2)
}


 draw.ellipse<-function (x, y, a = 1, b = 1, angle = 0, segment = NULL, arc.only = TRUE,
    deg = TRUE, nv = 100, border = NULL, col = NA, lty = 1, lwd = 1,
    ...)
{
    if (is.null(segment)) {
        if (deg)
            segment <- c(0, 360)
        else segment <- c(0, 2 * pi)
    }
    draw1ellipse <- function(x, y, a = 1, b = 1, angle = 0, segment = NULL,
        arc.only = TRUE, nv = 100, deg = TRUE, border = NULL,
        col = NA, lty = 1, lwd = 1, ...) {
        if (deg) {
            angle <- angle * pi/180
            segment <- segment * pi/180
        }
        z <- seq(segment[1], segment[2], length = nv + 1)
        xx <- a * cos(z)
        yy <- b * sin(z)
        alpha <- xyangle(xx, yy, directed = TRUE, deg = FALSE)
        rad <- sqrt(xx^2 + yy^2)
        xp <- rad * cos(alpha + angle) + x
        yp <- rad * sin(alpha + angle) + y
        if (!arc.only) {
            xp <- c(x, xp, x)
            yp <- c(y, yp, y)
        }
        polygon(xp, yp, border = border, col = col, lty = lty,
            lwd = lwd, ...)
        invisible(NULL)
    }
    xyangle <- function(x, y, directed = FALSE, deg = TRUE) {
        if (missing(y)) {
            y <- x[, 2]
            x <- x[, 1]
        }
        out <- atan2(y, x)
        if (!directed)
            out <- out%%pi
        if (deg)
            out <- out * 180/pi
        out
    }
    if (missing(y)) {
        y <- x[, 2]
        x <- x[, 1]
    }
    n <- length(x)
    if (length(a) < n)
        a <- rep(a, n)[1:n]
    if (length(b) < n)
        b <- rep(b, n)[1:n]
    if (length(angle) < n)
        angle <- rep(angle, n)[1:n]
    if (length(col) < n)
        col <- rep(col, n)[1:n]
    if (length(border) < n)
        border <- rep(border, n)[1:n]
    if (length(nv) < n)
        nv <- rep(nv, n)[1:n]
    if (n == 1)
        draw1ellipse(x, y, a, b, angle = angle, segment = segment,
            arc.only = arc.only, deg = deg, nv = nv, col = col,
            border = border, lty = lty, lwd = lwd, ...)
    else {
        if (length(segment) < 2 * n)
            segment <- matrix(rep(segment, n), n, 2, byrow = TRUE)
        lapply(1:n, function(i) draw1ellipse(x[i], y[i], a[i],
            b[i], angle = angle[i], segment = segment[i, ], arc.only = arc.only,
            deg = deg, nv = nv[i], col = col[i], border = border[i],
            lty = lty, lwd = lwd, ...))
    }
    invisible(NULL)
}

 regIQRcon<-function(x,y,pts=x,regfun=Qreg,...){
#
#
#   Use a quantile regression estimator to estimate the
#   interquartile range of y, given values for x stored in
#   pts
#   Qreg avoids possible computational issues that can arise when
#   using qreg
#
IQR.TOP=Qreghat(x,y,xr=pts,q=.75)
IQR.BOT=Qreghat(x,y,xr=pts,q=.25)
IQR=(IQR.TOP-IQR.BOT)/(qnorm(.75)-qnorm(.25))
IQR
}




# ----------------------------------------------------------------------------

# oph.astig.datasetconvexpoly

# ----------------------------------------------------------------------------

#' Dataset-Level Convex Polygons for Astigmatism Data
#'
#' @description
#' Computes depth-based central regions and convex hull polygons for astigmatism datasets.
#' For each formula (column pair), identifies the central region based on data depth and
#' constructs the convex hull of points in that region.
#'
#' @param m Matrix or data frame with J columns (J must be even). Columns represent pairs:
#'   columns 1-2 are formula 1, 3-4 are formula 2, etc.
#' @param Region Numeric. Determines the central region as the (1-Region) quantile of depths.
#'   Default is 0.05 (95\% central region).
#' @param plotit Logical. If \code{TRUE}, plots data points with convex hull overlay.
#'   Default is \code{FALSE}.
#' @param xlab,ylab Axis labels for plots. Defaults are 'V1' and 'V2'.
#' @param pch Plotting character for data points. Default is '.'.
#' @param reset Logical. If \code{TRUE}, resets graphics parameters after plotting.
#'   Default is \code{TRUE}.
#'
#' @details
#' For each formula, the function:
#' \enumerate{
#'   \item Calls \code{\link{mulcen.region}} to compute multivariate central region
#'   \item Uses \code{\link{fdepth}} to calculate data depths
#'   \item Identifies points with depth >= quantile specified by \code{Region}
#'   \item Computes convex hull using \code{\link[grDevices]{chull}}
#'   \item Optionally plots data with convex hull boundary
#' }
#'
#' Multiple formulas are displayed in a 2x2 grid layout when \code{plotit=TRUE}.
#'
#' @return List with two components:
#' \item{centers}{List of centers for each formula, with elements: V1, V2 (coordinates), N (sample size)}
#' \item{convex.hull.pts}{List of convex hull vertices for each formula}
#'
#' @seealso \code{\link{mulcen.region}}, \code{\link{fdepth}}, \code{\link{oph.astig.datasetconvexpoly.mean}},
#'   \code{\link{oph.astig.meanconvexpoly}}
#'
#' @keywords ophthalmology
#' @export
oph.astig.datasetconvexpoly<-function(m,Region=.05,plotit=FALSE,xlab='V1',ylab='V2',pch='.',reset=TRUE){
#
# region=.05 means that the function
#  determine the 1-.05=.95
#  This is done for each formula
#  Assume m is a matrix or data frame having
#  J columns. First two columns first formula, next two columns next formula..
#
#  So J should be an even integer
#
J=ncol(m)
N=J/2
if(N != floor(N))stop('Should have an even number of columns')
region=list()
centers=list()
id=c(-1,0)
for(j in 1:N){
id=id+2
a=mulcen.region(m[,id],region=Region,plotit=FALSE,xlab=xlab,ylab=ylab)
centers[[j]]=a$center
region[[j]]=a$convex.hull.pts
n=nrow(elimna(m[id,]))
n=as.integer(n)
centers[[j]]=c(a$center,n)
names(centers[[j]])=c('V1','V2','N')
}
if(plotit){
par(pty='s')
M=m
if(N>1)par(mfrow=c(2,2))
id=c(-1,0)
for(j in 1:N){
id=id+2
m=M[,id]
m=as.matrix(m)
temp<-fdepth(m,plotit=FALSE)  #Defaults to using the marginal medians
flag=(temp>=qest(temp,Region))
xx<-m[flag,]
xord<-order(xx[,1])
xx<-xx[xord,]
temp<-chull(xx)
plot(m[,1],m[,2],xlab=xlab,ylab=ylab,pch=pch,xlim=c(-1.5,1.5),ylim=c(-1.5,1.5),asp=1)
lines(xx[temp,])
lines(xx[c(temp[1],temp[length(temp)]),])
}
if(reset)par(mfrow=c(1,1))
}
list(centers=centers,convex.hull.pts=region)
}




# ----------------------------------------------------------------------------

# oph.astig.datasetconvexpoly.mean

# ----------------------------------------------------------------------------

#' Dataset-Level Convex Polygons Using Means for Astigmatism Data
#'
#' @description
#' Computes depth-based central regions and convex hull polygons for astigmatism datasets
#' using means as the measure of central tendency. Identical to \code{\link{oph.astig.datasetconvexpoly}}
#' except uses means instead of medians for computing the center.
#'
#' @param m Matrix or data frame with J columns (J must be even). Columns represent pairs:
#'   columns 1-2 are formula 1, 3-4 are formula 2, etc.
#' @param Region Numeric. Determines the central region as the (1-Region) quantile of depths.
#'   Default is 0.05 (95\% central region).
#' @param plotit Logical. If \code{TRUE}, plots data points with convex hull overlay.
#'   Default is \code{FALSE}.
#' @param xlab,ylab Axis labels for plots. Defaults are 'V1' and 'V2'.
#'
#' @details
#' For each formula (column pair), the function:
#' \enumerate{
#'   \item Removes missing values with \code{\link{elimna}}
#'   \item Calls \code{\link{mulcen.region}} with \code{est=mean} to compute central region using means
#'   \item Uses \code{\link{fdepth}} to calculate data depths
#'   \item Identifies points with depth >= quantile specified by \code{Region}
#'   \item Computes convex hull using \code{\link[grDevices]{chull}}
#'   \item Optionally plots data with convex hull boundary
#' }
#'
#' Multiple formulas are displayed in a 2x2 grid layout when \code{plotit=TRUE}.
#'
#' @return List with two components:
#' \item{centers}{List of mean centers for each formula, with elements: V1, V2 (mean coordinates), N (sample size)}
#' \item{convex.hull.pts}{List of convex hull vertices for each formula}
#'
#' @seealso \code{\link{oph.astig.datasetconvexpoly}}, \code{\link{mulcen.region}}, \code{\link{fdepth}},
#'   \code{\link{oph.astig.meanconvexpoly}}
#'
#' @keywords ophthalmology
#' @export
oph.astig.datasetconvexpoly.mean<-function(m,Region=.05,plotit=FALSE,xlab='V1',ylab='V2'){
#
# region=.05 means that the function
#  determine the 1-.05=.95
#  This is done for each formula
#  Assume m is a matrix or data frame having
#  J columns. First two columns first formula, next two columns next formula..
#
#  So J should be an even integer
#
J=ncol(m)
N=J/2
if(N != floor(N))stop('Should have an even number of columns')
region=list()
centers=list()
id=c(-1,0)
for(j in 1:N){
id=id+2
a=mulcen.region(elimna(m[,id]),region=Region,plotit=FALSE,xlab=xlab,ylab=ylab,est=mean)
centers[[j]]=a$center
region[[j]]=a$convex.hull.pts
n=nrow(elimna(m[id,]))
n=as.integer(n)
centers[[j]]=c(a$center,n)
names(centers[[j]])=c('V1','V2','N')
}
if(plotit){
M=m
if(N>1)par(mfrow=c(2,2))
id=c(-1,0)
for(j in 1:N){
id=id+2
m=M[,id]
m=elimna(m)
m=as.matrix(m)
temp<-fdepth(m,plotit=FALSE)  #Defaults to using the marginal medians
flag=(temp>=qest(temp,Region))
xx<-m[flag,]
xord<-order(xx[,1])
xx<-xx[xord,]
temp<-chull(xx)
plot(m[,1],m[,2],xlab=xlab,ylab=ylab)
lines(xx[temp,])
lines(xx[c(temp[1],temp[length(temp)]),])
}
par(mfrow=c(1,1))
}
list(centers=centers,convex.hull.pts=region)
}



psmm.x=function(x, c, r, nu) {
        snu = sqrt(nu)
        sx = snu * x
        lgx = log(snu) - lgamma(nu/2) + (1 - nu/2) * log(2) +
            (nu - 1) * log(sx) + (-sx^2/2)
        exp(r * log(2 * pnorm(c * x) - 1) + lgx)
        }

psmm = function(x, r, nu) {
        res = integrate(psmm.x, 0, Inf, c = x, r = r, nu = nu)
        res$value
        }

 qsmm<-function(q, r, nu) {
       #r=number of comparisons
        if (!is.finite(nu))
            return(qnorm(1 - 0.5 * (1 - q^(1/r))))
        res = uniroot(function(c, r, nu, q) {
            psmm(c, r = r, nu = nu) - q
            },
             c(0, 100), r = r, nu = nu, q = q)
        res$root
    }

  lincon<-function(x,con=0,tr=.2,alpha=.05,pr=FALSE){
#
#  A heteroscedastic test of d linear contrasts using trimmed means.
#
#  This version uses an improved method for computing the quantiles of a
#  Studentized maximum modulus distriburtion
#
#  The data are assumed to be stored in $x$ in list mode, a matrix
#  or a data frame. If in list mode,
#  length(x) is assumed to correspond to the total number of groups.
#  It is assumed all groups are independent.
#
#  con is a J by d matrix containing the contrast coefficients that are used.
#  If con is not specified, all pairwise comparisons are made.
#
#  Missing values are automatically removed.
#
#  pr=FALSE included to avoid errors using an earlier version of this function when
#   dealing with two-way and higher designs
#
#  Adjusted p-values are based on the Studentized maximum modulus distribution with the
#  goal of controlling FWE
#
#  To apply the Kaiser-Bowden method, use the function kbcon
#
if(tr==.5)stop('Use the R function medpb to compare medians')
if(is.data.frame(x))x=as.matrix(x)
if(is.matrix(x))x<-listm(x)
if(!is.list(x))stop('Data must be stored in a matrix or in list mode.')
con<-as.matrix(con)
J<-length(x)
sam=NA
h<-vector('numeric',J)
w<-vector('numeric',J)
xbar<-vector('numeric',J)
for(j in 1:J){
xx<-!is.na(x[[j]])
val<-x[[j]]
x[[j]]<-val[xx]  # Remove missing values
sam[j]=length(x[[j]])
h[j]<-length(x[[j]])-2*floor(tr*length(x[[j]]))
   # h is the number of observations in the jth group after trimming.
w[j]<-((length(x[[j]])-1)*winvar(x[[j]],tr))/(h[j]*(h[j]-1))
xbar[j]<-mean(x[[j]],tr)
}
if(sum(con^2)==0){
CC<-(J^2-J)/2
psihat<-matrix(0,CC,9)
dimnames(psihat)<-list(NULL,c('Group','Group','psihat','ci.lower','ci.upper',
'p.value','Est.1','Est.2','adj.p.value'))
test<-matrix(NA,CC,6)
dimnames(test)<-list(NULL,c('Group','Group','test','crit','se','df'))
jcom<-0
for (j in 1:J){
for (k in 1:J){
if (j < k){
jcom<-jcom+1
test[jcom,3]<-abs(xbar[j]-xbar[k])/sqrt(w[j]+w[k])
sejk<-sqrt(w[j]+w[k])
test[jcom,5]<-sejk
psihat[jcom,1]<-j
psihat[jcom,2]<-k
test[jcom,1]<-j
test[jcom,2]<-k
psihat[jcom,3]<-(xbar[j]-xbar[k])
df<-(w[j]+w[k])^2/(w[j]^2/(h[j]-1)+w[k]^2/(h[k]-1))
test[jcom,6]<-df
psihat[jcom,6]<-2*(1-pt(test[jcom,3],df))
psihat[jcom,7]=xbar[j]
psihat[jcom,8]=xbar[k]
crit=qsmm(1-alpha,CC,df)
test[jcom,4]<-crit
psihat[jcom,4]<-(xbar[j]-xbar[k])-crit*sejk
psihat[jcom,5]<-(xbar[j]-xbar[k])+crit*sejk
psihat[jcom,9]=1-psmm(test[jcom,3],CC,df)
}}}}
if(sum(con^2)>0){
if(nrow(con)!=length(x)){
stop('The number of groups does not match the number of contrast coefficients.')
}
CC=ncol(con)
psihat<-matrix(0,ncol(con),6)
dimnames(psihat)<-list(NULL,c('con.num','psihat','ci.lower','ci.upper',
'p.value','adj.p.value'))
test<-matrix(0,ncol(con),5)
dimnames(test)<-list(NULL,c('con.num','test','crit','se','df'))
df<-0
for (d in 1:ncol(con)){
psihat[d,1]<-d
psihat[d,2]<-sum(con[,d]*xbar)
sejk<-sqrt(sum(con[,d]^2*w))
test[d,1]<-d
test[d,2]<-sum(con[,d]*xbar)/sejk
df<-(sum(con[,d]^2*w))^2/sum(con[,d]^4*w^2/(h-1))
crit=qsmm(1-alpha,CC,df)
test[d,3]<-crit
test[d,4]<-sejk
test[d,5]<-df
psihat[d,3]<-psihat[d,2]-crit*sejk
psihat[d,4]<-psihat[d,2]+crit*sejk
psihat[d,5]<-2*(1-pt(abs(test[d,2]),df))
psihat[d,6]=1-psmm(abs(test[d,2]),CC,df)
}
}
list(n=sam,test=test,psihat=psihat)
}

#' Compare Total Variances for Dependent Bivariate Astigmatism Data
#'
#' @description
#' Compares total variances of dependent bivariate variables for astigmatism prediction errors.
#' Performs pairwise comparisons between formulas including both marginal and total variances
#' with Holm's multiple testing correction.
#'
#' @param m Matrix or data frame with J columns (J must be even). Columns represent pairs:
#'   columns 1-2 are formula 1, 3-4 are formula 2, etc.
#' @param alpha Significance level for hypothesis tests. Default is 0.05.
#'
#' @details
#' This function is the dependent-groups version of \code{\link{oph.astig.bivmarg.totvars}}.
#' For each pair of formulas:
#' \itemize{
#'   \item Calls \code{\link{oph.astig.bivmarg}} for marginal variance comparisons
#'   \item Computes total variance: \code{var(X) + var(Y)}
#'   \item Adjusts for covariance: subtracts \code{2*cov(X,Y)} from sum
#'   \item Uses \code{\link{comdvar}} to compare total variances
#'   \item Applies Holm's method to control familywise error rate
#' }
#'
#' Estimates are adjusted if outliers are detected using a projection method.
#'
#' @return List with three components:
#' \item{n}{Sample sizes for each column}
#' \item{results}{List of marginal variance comparison results from \code{oph.astig.bivmarg}}
#' \item{results.total}{Data frame with columns: Form (formula IDs), Var.1, Var.2 (total variances),
#'   SD 1, SD 2 (standard deviations), p.value, p.adjusted (Holm-adjusted p-values)}
#'
#' @seealso \code{\link{oph.astig.bivmarg.totvars}}, \code{\link{oph.astig.bivmarg}},
#'   \code{\link{comdvar}}
#'
#' @keywords ophthalmology
#' @export
oph.astig.depbivmarg.totvars<-function(m,alpha=.05){
#
# This function is designed to compare two  variances  dependent  variables based
#  prediction errors when dealing with astigmatism.
#
#  Assume m is a matrix or data frame having
#  J columns. First two columns first formula, next two columns next formula..
#
#  So J should be an even integer
#
#  Compare col 1-2 to 3-4, then 1-2 vs 5-6, etc
#
#  returns confidence interval for pairwise difference scores. So using difference scores for 1 and 3 as well as 2 and 4
# alpha = .05 = .95 confidence intervals
#
# Estimates are adjusted if outliers are found based on a projection method.
#
E1=oph.astig.bivmarg(m)$results
J=ncol(m)
N=J/2
J1=J-1
F=NULL
n=apply(m,2,length)
for(j in 1:N)F=c(F,paste('F',j))
chk.n=names(m)
MAT=matrix(NA,nrow=N,ncol=8)
dimnames(MAT)=list(NULL,c('Form','Form','Var.1','Var.2','SD 1','SD 2','p.value','p.adjusted'))
MAT=as.data.frame(MAT)
if(N != floor(N))stop('Should have an even number of columns')
#results=list()
results.total=list()
mat=matrix(NA,nrow=N,ncol=2,byrow=TRUE)
v1=seq(1,J1,2)
mat[,1]=v1
mat[,2]=v1+1
ic=0
for(j in 1:N){
for(k in 1:N){
if(j<k){
ic=ic+1
id1=mat[j,]
id2=mat[k,]
if(!is.null(chk.n))print(paste('Comparing Variances' ,chk.n[mat[j,1]],'minus',chk.n[mat[k,1]], 'and',chk.n[mat[j,2]],'minus',chk.n[mat[k,2]] ))
#e=comdvar.astig(m[,id1],m[,id2])
#results[[ic]]=e
X=m[,id1[1]]+m[,id1[2]]-2*cov(m[,id1[1]],m[,id1[2]])  # So variance of sum is now equal to the sum of the variances
Y=m[,id2[1]]+m[,id2[2]]-2*cov(m[,id2[1]],m[,id2[2]])
temp=comdvar(X,Y)
e1=var(m[,id1[1]])+var(m[,id1[2]])
e2=var(m[,id2[1]])+var(m[,id2[2]])
#MAT[ic,]=c(F[j],F[k],temp$est1,temp$est2,temp$est1/temp$est2,temp$p.value)
MAT[ic,1:7]=c(F[j],F[k],round(e1,6),round(e2,6),round(sqrt(e1),6),round(sqrt(e2),6),round(temp$p.value,6))
}}}
MAT[,8]=p.adjust(MAT[,7],method='holm')
list(n=n,results=E1,results.total=MAT)  #results.total,Total.p.adjusted=pad)
}




# ----------------------------------------------------------------------------

# oph.astig.depbivmeans

# ----------------------------------------------------------------------------

#' Compare Bivariate Means for Dependent Astigmatism Data
#'
#' @description
#' Compares bivariate means of dependent groups for astigmatism prediction errors.
#' Uses bootstrap methods with trimmed means on difference scores and applies
#' Holm's multiple testing correction.
#'
#' @param m Matrix or data frame with J columns (J must be even). Columns represent pairs:
#'   columns 1-2 are formula 1, 3-4 are formula 2, etc.
#' @param alpha Significance level for hypothesis tests. Default is 0.05.
#' @param nboot Number of bootstrap samples. Default is 1999.
#' @param SEED Logical. If \code{TRUE}, sets random seed for reproducibility. Default is \code{TRUE}.
#' @param tr Trimming proportion (0 to 0.5). Default is 0 (no trimming, uses means).
#'
#' @details
#' For each pair of formulas (e.g., columns 1-2 vs 3-4), the function:
#' \enumerate{
#'   \item Computes difference scores: column 1 minus column 3, column 2 minus column 4
#'   \item Uses \code{\link{trimcibt}} to compute bootstrap confidence intervals for each difference
#'   \item Computes trimmed means for each variable
#'   \item Tests null hypothesis that both differences equal zero
#'   \item Applies Holm's method to adjust p-values for multiple comparisons
#' }
#'
#' @return List of matrices, one for each pairwise comparison. Each matrix has 2 rows
#'   (one per variable) and columns: Mean 1, Mean 2, p.value, p.adjusted.
#'
#' @seealso \code{\link{oph.astig.indepbivmeans}}, \code{\link{trimcibt}},
#'   \code{\link{oph.astig.depbivtotvars}}
#'
#' @keywords ophthalmology
#' @export
oph.astig.depbivmeans<-function(m,alpha=.05,nboot=1999,SEED=TRUE,tr=0){
#
# This function is designed to compare two  bivariate distributions relevant to
#  prediction errors when dealing with astigmatism.
#
#  Assume m is a matrix or data frame having
#  J columns. First two columns first formula, next two columns next formula..
#
#  So J should be an even integer
#
#  Compare col 1-2 to 3-4, then 1-2 vs 5-6, etc
#  using difference scores. That is, col1 and 3, use difference scores, col 2 and 4, then col 1 and 5, etc.
#
#  returns confidence interval for pairwise difference scores.
# alpha = .05 = .95 confidence intervals
#
# Estimates are adjusted if outliers are found based on a projection method.
#
nullv=rep(0,2)
J=ncol(m)
N=J/2
J1=J-1
chk.n=names(m)
MAT=matrix(NA,nrow=2,ncol=4)
dimnames(MAT)=list(NULL,c('Mean 1','Mean 2','p.value','p.adjusted'))
if(N != floor(N))stop('Should have an even number of columns')
results=list()
mat=matrix(NA,nrow=N,ncol=2,byrow=TRUE)
v1=seq(1,J1,2)
mat[,1]=v1
mat[,2]=v1+1
ic=0
for(j in 1:N){
for(k in 1:N){
if(j<k){
ic=ic+1
id1=mat[j,]
id2=mat[k,]
if(!is.null(chk.n))print(paste('Comparing' ,chk.n[mat[j,1]],'minus',chk.n[mat[k,1]], 'and',chk.n[mat[j,2]],'minus',chk.n[mat[k,2]] ))
a=trimcibt(m[,id1[1]]-m[,id2[1]],alpha=alpha,tr=tr)
e1=mean(m[,id1[1]],tr=tr)
e2=mean(m[,id2[1]],tr=tr)
MAT[1,1:3]=c(e1,e2,a$p.value)
a=trimcibt(m[,id1[2]]-m[,id2[2]],alpha=alpha,tr=tr)
e1=mean(m[,id1[2]],tr=tr)
e2=mean(m[,id2[2]],tr=tr)
MAT[2,1:3]=c(e1,e2,a$p.value)
MAT[,4]=p.adjust(MAT[,3],method='holm')
results[[ic]]=MAT  #Dmul.loc2g(m[,id1],m[,id2],alpha=alpha,locfun=locfun,nullv=nullv,nboot=nboot)
}}}
results
}

#' Compare Total Variances for Dependent Bivariate Astigmatism Data
#'
#' @description
#' Compares total variances between pairs of dependent bivariate distributions for
#' astigmatism prediction errors. Data should have an even number of columns (J),
#' with each pair representing one formula.
#'
#' @param m Matrix or data frame with J columns (must be even). First two columns
#'   represent formula 1, next two columns formula 2, etc.
#' @param alpha Significance level (default: 0.05)
#'
#' @details
#' For data with J columns forming J/2 bivariate pairs:
#' \itemize{
#'   \item Compares total variance of columns 1-2 vs 3-4, then 1-2 vs 5-6, etc.
#'   \item Uses difference scores for comparisons (col 1 vs 3, col 2 vs 4, etc.)
#'   \item Returns variance ratios and hypothesis tests
#'   \item Adjusts p-values using Holm's method
#'   \item Outliers are handled using projection-based methods
#' }
#'
#' @return List with components:
#' \describe{
#'   \item{n}{Sample sizes for each column}
#'   \item{results}{List of pairwise marginal variance comparisons}
#'   \item{results.total}{Data frame with total variance comparisons, ratios, and p-values}
#'   \item{Total.adj.p.values}{Holm-adjusted p-values for total variance tests}
#' }
#'
#' @seealso \code{\link{oph.astig.depbivmarg.totvars}}, \code{\link{comdvar.astig}},
#'   \code{\link{oph.astig.indepbivtotvars}}
#'
#' @keywords ophthalmology
#' @export
oph.astig.depbivtotvars<-function(m,alpha=.05){
#
# This function is designed to compare two  variances  dependent  variables based
#  prediction errors when dealing with astigmatism.
#
#  Assume m is a matrix or data frame having
#  J columns. First two columns first formula, next two columns next formula..
#
#  So J should be an even integer
#
#  Compare col 1-2 to 3-4, then 1-2 vs 5-6, etc
#
#  returns confidence interval for pairwise difference scores. So using difference scores for 1 and 3 as well as 2 and 4
# alpha = .05 = .95 confidence intervals
#
# Estimates are adjusted if outliers are found based on a projection method.
#
J=ncol(m)
nv=NA
for(j in 1:J)nv[j]=length(elimna(m[,j]))
N=J/2
J1=J-1
F=NULL
for(j in 1:N)F=c(F,paste('F',j))
chk.n=names(m)
MAT=matrix(NA,nrow=N,ncol=6)
dimnames(MAT)=list(NULL,c('Form', 'Form','Tot Var 1','Tot Var 2','Ratio','p.adjusted'))
MAT=as.data.frame(MAT)
if(N != floor(N))stop('Should have an even number of columns')
results=list()
results.total=list()
mat=matrix(NA,nrow=N,ncol=2,byrow=TRUE)
v1=seq(1,J1,2)
mat[,1]=v1
mat[,2]=v1+1
ic=0
for(j in 1:N){
for(k in 1:N){
if(j<k){
ic=ic+1
id1=mat[j,]
id2=mat[k,]
if(!is.null(chk.n))print(paste('Comparing' ,chk.n[mat[j,1]],'minus',chk.n[mat[k,1]], 'and',chk.n[mat[j,2]],'minus',chk.n[mat[k,2]] ))
e=comdvar.astig(m[,id1],m[,id2])
E1=e[1,1]+e[2,1]
E2=e[1,2]+e[2,2]
results[[ic]]=e
X=m[,id1[1]]-m[,id1[2]]
Y=m[,id2[1]]-m[,id2[2]]
temp=comdvar(X,Y)
MAT[ic,]=c(F[j],F[k],E1,E2,E1/E2,temp$p.value)
results.total[[ic]]=comdvar(X,Y)
}}}
pv=MAT[,6]
pad=p.adjust(pv,method='holm')
list(n=nv,results=results,results.total=MAT,Total.adj.p.values=pad)
}




# ----------------------------------------------------------------------------

# oph.astig.depcom

# ----------------------------------------------------------------------------

#' Compare Dependent Groups for Astigmatism Prediction Errors
#'
#' @description
#' Performs all pairwise comparisons of dependent measures for astigmatism prediction
#' errors using trimmed means and bootstrap confidence intervals. Validates data by
#' checking for invalid values (typically > 4 diopters).
#'
#' @param x Matrix with J columns or list of length J containing prediction error data
#' @inheritParams common_params
#' @param con Contrast matrix (default: 0, performs all pairwise comparisons)
#' @param invalid Maximum valid value in diopters (default: 4). Values exceeding this
#'   threshold are flagged as invalid
#' @param SEED Logical. If \code{TRUE}, sets random seed for reproducibility
#' @param STOP Logical. If \code{TRUE}, stops execution when invalid values detected
#' @param method P-value adjustment method (default: 'holm')
#'
#' @details
#' Compares all pairs of dependent formulas using difference scores. For each pair:
#' \itemize{
#'   \item Computes trimmed mean difference with bootstrap-t confidence interval
#'   \item Validates that all values ≤ \code{invalid} diopters and ≥ 0
#'   \item Reports positions of any invalid or negative values
#'   \item Adjusts p-values for multiple comparisons using specified method
#' }
#'
#' Missing values are automatically removed before analysis.
#'
#' @return List with component:
#' \describe{
#'   \item{output}{Matrix with columns: V1 (group 1), V2 (group 2), n (sample size),
#'     Mean.1, Mean.2, p.value, adj.p.value (adjusted p-value)}
#' }
#'
#' @seealso \code{\link{oph.astig.indepcom}}, \code{\link{oph.astig.depcom.unimean.var}},
#'   \code{\link{trimcibt}}
#'
#' @keywords ophthalmology
#' @export
oph.astig.depcom<-function(x,tr=0,con=0,alpha=.05,nboot=1999,invalid=4,
SEED=TRUE,STOP=TRUE,method='holm'){
#
#  This function is designed specifically for comparing predictions errors
#  relevant to astigmatism.
#
#  Dependent measures are being compared
#
#  It is assumed that any value greater than 4 diopters
#  is invalid.  The argument invalid can be used to change this decision rule.
#
#  Goal:
#   Compute a 1-alpha confidence interval for each pair of measures based in difference scores
#
#  The argument
#   x can be a matrix in with J columns or it can have list mode having length Jy
#
#   Missing values are automatically removed.
#
#   The default number of bootstrap samples is nboot=599
#
#  method: indicates the method for controlling the probability of one or more Type I errors
#
if(is.data.frame(x))x=as.matrix(x)
con<-as.matrix(con)
if(is.matrix(x))x<-listm(x)
if(!is.list(x))stop('Data must be stored in a matrix or in list mode.')
J<-length(x)
for(j in 1:J){
xx<-x[[j]]
x[[j]]<-xx[!is.na(xx)] # Remove any missing values.
}
for(j in 1:J){
flag=abs(elimna(x[[j]]))>invalid
if(sum(flag,na.rm=TRUE)>0){
print(paste('The value of argument invalid indicates that any value greater  than', invalid,' is invalid'))
print(paste('Variable', j, 'has one  or more invalid values'))
print('They occur in the following positions')
nr=c(1:length(x[[j]]))
print(nr[flag])
if(STOP)stop()
}}
for(j in 1:J){
flag=as.logical(x[[j]]<0)
if(sum(flag)>0){
print(paste('Values less than zero were detected'))
print(paste('Variable', j, 'has one  or more values=0'))
print('They occur in the following positions')
nr=c(1:length(x[[j]]))
print(nr[flag])
if(STOP)stop()
}
}
Jm<-J-1
d<-(J^2-J)/2
#
CC<-(J^2-J)/2
output<-matrix(0,CC,7)
dimnames(output)<-list(NULL,c('V1','V2','n','Mean.1','Mean.2','p.value','adj.p.value'))
ic=0
for(j in 1:J){
for(k in 1:J){
if(j<k){
ic=ic+1
a=trimcibt(x[[j]]-x[[k]],tr=tr,alpha=alpha)
output[ic,1]=j
output[ic,2]=k
output[ic,3]=a$n
output[ic,4]=mean(x[[j]],tr=tr)
output[ic,5]=mean(x[[k]],tr=tr)
output[ic,6]=a$p.value
}}}
output[,7]=p.adjust(output[,6],method=method)
list(output=output)
}




# ----------------------------------------------------------------------------

# oph.astig.depcom.unimean.var

# ----------------------------------------------------------------------------

#' Compare Astigmatism Formulas - Dependent Case (Means and Variances)
#'
#' @description
#' Wrapper function that compares both means and variances for dependent astigmatism
#' prediction error data. Combines results from \code{\link{oph.astig.depcom}} for means
#' and \code{\link{comdsd.mcp}} for variances.
#'
#' @param m Matrix or data frame with even number of columns. First two columns are
#'   the first formula, next two are the second formula, etc.
#' @param locfun Location function to use (default: \code{smean})
#' @param method Multiple comparison method for p-value adjustment (default: 'holm')
#' @param invalid Maximum valid absolute value in diopters (default: 4)
#' @param STOP Logical; if TRUE, stop on detection of invalid values
#' @inheritParams common_params
#'
#' @return
#' List with two components:
#' \itemize{
#'   \item \code{mean.results} - Results from comparing means
#'   \item \code{var.results} - Results from comparing variances
#' }
#'
#' @seealso \code{\link{oph.astig.depcom}}, \code{\link{comdsd.mcp}}
#'
#' @keywords ophthalmology
#' @export
oph.astig.depcom.unimean.var<-function(m,alpha=.05,nboot=1999,locfun=smean,tr=0,method='holm',SEED=TRUE,invalid=4,STOP=TRUE){
#
#
r1=oph.astig.depcom(m,alpha=alpha,nboot=nboot,method=method,invalid=invalid,STOP=STOP,SEED=SEED,tr=tr)$output
r2=comdsd.mcp(m,method=method)
list(mean.results=r1,var.results=r2)
}




# ----------------------------------------------------------------------------

# oph.astig.indepbivmarg.totvars

# ----------------------------------------------------------------------------

#' Compare Total Variances for Independent Astigmatism Formulas (Bivariate Marginal)
#'
#' @description
#' Compares total variances of prediction errors for independent astigmatism formulas
#' using bivariate marginal approach. For matrices with J columns (where J is even),
#' compares columns 1-2 vs 3-4, 1-2 vs 5-6, etc. using pairwise difference scores.
#' Estimates are adjusted if outliers are detected using a projection method.
#'
#' @param m Matrix or data frame with even number of columns (J). First two columns
#'   represent first formula, next two the second formula, etc.
#' @inheritParams common_params
#'
#' @details
#' The function:
#' \itemize{
#'   \item Compares total variances between formula pairs
#'   \item Uses difference scores from column pairs (e.g., cols 1 & 3, cols 2 & 4)
#'   \item Applies Holm's method for p-value adjustment
#'   \item Reports variances, standard deviations, and p-values
#' }
#'
#' @return
#' List with components:
#' \itemize{
#'   \item \code{n} - Sample sizes for each column
#'   \item \code{results} - Detailed pairwise comparison results
#'   \item \code{results.total} - Data frame with total variance comparisons and adjusted p-values
#' }
#'
#' @seealso \code{\link{comvar2.astig}}, \code{\link{varcom.IND.MP}}
#'
#' @keywords ophthalmology
#' @export
oph.astig.indepbivmarg.totvars<-function(m,alpha=.05){
#
# This function is designed to compare two  variances  independent  variables based
#  prediction errors when dealing with astigmatism.
#
#  Assume m is a matrix or data frame having
#  J columns. First two columns first formula, next two columns next formula..
#
#  So J should be an even integer
#
#  Compare col 1-2 to 3-4, then 1-2 vs 5-6, etc
#
#  returns confidence interval for pairwise difference scores. So using difference scores for 1 and 3 as well as 2 and 4
# alpha = .05 = .95 confidence intervals
#
# Estimates are adjusted if outliers are found based on a projection method.
#
J=ncol(m)
N=J/2
J1=J-1
F=NULL
nv=NA
for(j in 1:J)nv[j]=length(elimna(m[,j]))
for(j in 1:N)F=c(F,paste('F',j))
chk.n=names(m)
MAT=matrix(NA,nrow=N,ncol=8)
dimnames(MAT)=list(NULL,c('Form','Form','Var.1','Var.2','SD 1','SD 2','p.value','p.adjusted'))
MAT=as.data.frame(MAT)
if(N != floor(N))stop('Should have an even number of columns')
results=list()
results.total=list()
mat=matrix(NA,nrow=N,ncol=2,byrow=TRUE)
v1=seq(1,J1,2)
mat[,1]=v1
mat[,2]=v1+1
ic=0
for(j in 1:N){
for(k in 1:N){
if(j<k){
ic=ic+1
id1=mat[j,]
id2=mat[k,]
if(!is.null(chk.n))print(paste('Comparing Variances' ,chk.n[mat[j,1]],'minus',chk.n[mat[k,1]], 'and',chk.n[mat[j,2]],'minus',chk.n[mat[k,2]] ))
e=comvar2.astig(m[,id1],m[,id2])
results[[ic]]=e
X=m[,id1[1]]+m[,id1[2]]
Y=m[,id2[1]]+m[,id2[2]]
temp=varcom.IND.MP(X,Y)
e1=var(m[,id1[1]],na.rm=TRUE)+var(m[,id1[2]],na.rm=TRUE)
e2=var(m[,id2[1]],na.rm=TRUE)+var(m[,id2[2]],na.rm=TRUE)
MAT[ic,1:7]=c(F[j],F[k],round(e1,6),round(e2,6),round(sqrt(e1),6),round(sqrt(e2),6),round(temp$p.value,6))
}}}
MAT[,8]=p.adjust(MAT[,7],method='holm')
list(n=nv,results=results,results.total=MAT)  #results.total,Total.p.adjusted=pad)
}

#' Compare Bivariate Means for Independent Astigmatism Formulas
#'
#' @description
#' Compares bivariate distributions of prediction errors for independent astigmatism
#' formulas. For matrices with J columns (where J is even), compares columns 1-2 vs 3-4,
#' 1-2 vs 5-6, etc. using difference scores. Estimates are adjusted if outliers are
#' detected using a projection method.
#'
#' @param m Matrix or data frame with even number of columns (J). First two columns
#'   represent first formula, next two the second formula, etc.
#' @param SEED Logical; if TRUE, set random seed for reproducibility
#' @inheritParams common_params
#'
#' @details
#' Independent case bivariate distribution comparison. Uses difference scores (e.g.,
#' columns 1 and 3, then columns 2 and 4) with bootstrap-t method based on trimmed means.
#' Returns confidence intervals with Holm-adjusted p-values.
#'
#' @return
#' List of matrices, one for each pairwise comparison. Each matrix contains means,
#' p-values, and adjusted p-values for the two components being compared.
#'
#' @seealso \code{\link{yuenbt}}
#'
#' @keywords ophthalmology
#' @export
oph.astig.indepbivmeans<-function(m,alpha=.05,nboot=1999,SEED=TRUE,tr=0){
#
# This function is designed to compare two  bivariate distributions relevant to
#  prediction errors when dealing with astigmatism.
#
#  Independent case. That is, the bivariate distributions are independent.
#
#  Assume m is a matrix or data frame having
#  J columns. First two columns first formula, next two columns next formula..
#
#  So J should be an even integer
#
#  Compare col 1-2 to 3-4, then 1-2 vs 5-6, etc
#  using difference scores. That is, col1 and 3, use difference scores, col 2 and 4, then col 1 and 5, etc.
#
#  returns confidence interval for pairwise difference scores.
# alpha = .05 = .95 confidence intervals
#
# Estimates are adjusted if outliers are found based on a projection method.
#
nullv=rep(0,2)
J=ncol(m)
N=J/2
J1=J-1
chk.n=names(m)
MAT=matrix(NA,nrow=2,ncol=4)
dimnames(MAT)=list(NULL,c('Mean 1','Mean 2','p.value','p.adjusted'))
if(N != floor(N))stop('Should have an even number of columns')
results=list()
mat=matrix(NA,nrow=N,ncol=2,byrow=TRUE)
v1=seq(1,J1,2)
mat[,1]=v1
mat[,2]=v1+1
ic=0
for(j in 1:N){
for(k in 1:N){
if(j<k){
ic=ic+1
id1=mat[j,]
id2=mat[k,]
if(!is.null(chk.n))print(paste('Comparing' ,chk.n[mat[j,1]],'minus',chk.n[mat[k,1]], 'and',chk.n[mat[j,2]],'minus',chk.n[mat[k,2]] ))
a=yuenbt(m[,id1[1]],m[,id2[1]],alpha=alpha,tr=tr,side=FALSE)
e1=mean(m[,id1[1]],tr=tr,na.rm=TRUE)
e2=mean(m[,id2[1]],tr=tr,na.rm=TRUE)
MAT[1,1:3]=c(e1,e2,a$p.value)
a=yuenbt(m[,id1[2]],m[,id2[2]],alpha=alpha,tr=tr,side=FALSE)
e1=mean(m[,id1[2]],tr=tr,na.rm=TRUE)
e2=mean(m[,id2[2]],tr=tr,na.rm=TRUE)
MAT[2,1:3]=c(e1,e2,a$p.value)
MAT[,4]=p.adjust(MAT[,3],method='holm')
results[[ic]]=MAT
}}}
results
}




# ----------------------------------------------------------------------------

# oph.astig.indepbivtotvars

# ----------------------------------------------------------------------------

#' Compare Total Variances for Independent Astigmatism Formulas (Bivariate)
#'
#' @description
#' Compares total variances of prediction errors for independent astigmatism formulas
#' using bivariate approach. For matrices with J columns (where J is even), compares
#' columns 1-2 vs 3-4, 1-2 vs 5-6, etc., comparing the variances when testing differences.
#'
#' @param m Matrix or data frame with even number of columns (J). First two columns
#'   represent first formula, next two the second formula, etc.
#' @inheritParams common_params
#'
#' @details
#' Compares total variances using difference scores (columns i-1 minus columns i-2 for
#' each formula pair). Uses Morgan-Pitman test for independent groups. P-values are
#' adjusted using Holm's method.
#'
#' @return
#' List with components:
#' \itemize{
#'   \item \code{n} - Sample sizes for each column
#'   \item \code{results} - Detailed pairwise comparison results
#'   \item \code{results.total} - Data frame with total variances, ratios, and p-values
#'   \item \code{Total.adj.p.values} - Holm-adjusted p-values
#' }
#'
#' @seealso \code{\link{comvar2.astig}}, \code{\link{comvar2}}, \code{\link{varcom.IND.MP}}
#'
#' @keywords ophthalmology
#' @export
oph.astig.indepbivtotvars<-function(m,alpha=.05){
#
# This function is designed to compare two  variances  independent  variables based
#  prediction errors when dealing with astigmatism.
#
#  Assume m is a matrix or data frame having
#  J columns. First two columns first formula, next two columns next formula..
#
#  So J should be an even integer
#
#  Compare col 1-2 to 3-4, then 1-2 vs 5-6, etc
#
#  returns p-values when comparing the variances.
# alpha = .05 is default
#
#
J=ncol(m)
N=J/2
nv=NA
for(j in 1:J)nv[j]=length(elimna(m[,j]))
J1=J-1
F=NULL
for(j in 1:N)F=c(F,paste('F',j))
chk.n=names(m)
MAT=matrix(NA,nrow=N,ncol=6)
dimnames(MAT)=list(NULL,c('Form', 'Form','Tot Var 1','Tot Var 2','Ratio','p.adjusted'))
MAT=as.data.frame(MAT)
if(N != floor(N))stop('Should have an even number of columns')
results=list()
results.total=list()
mat=matrix(NA,nrow=N,ncol=2,byrow=TRUE)
v1=seq(1,J1,2)
mat[,1]=v1
mat[,2]=v1+1
ic=0
for(j in 1:N){
for(k in 1:N){
if(j<k){
ic=ic+1
id1=mat[j,]
id2=mat[k,]
if(!is.null(chk.n))print(paste('Comparing' ,chk.n[mat[j,1]],chk.n[mat[k,1]],chk.n[mat[j,2]],chk.n[mat[k,2]] ))
e=comvar2.astig(m[,id1],m[,id2])
E1=e[1,1]+e[2,1]
E2=e[1,2]+e[2,2]
results[[ic]]=e
X=m[,id1[1]]-m[,id1[2]]
Y=m[,id2[1]]-m[,id2[2]]
temp=varcom.IND.MP(X,Y)
MAT[ic,]=c(F[j],F[k],E1,E2,E1/E2,temp$p.value)
results.total[[ic]]=comvar2(X,Y)
}}}
pv=MAT[,6]
pad=p.adjust(pv,method='holm')
list(n=nv,results=results,results.total=MAT,Total.adj.p.values=pad)
}




# ----------------------------------------------------------------------------

# oph.astig.indepcom

# ----------------------------------------------------------------------------

#' Compare Astigmatism Prediction Errors - Independent Groups
#'
#' @description
#' Compares prediction errors for astigmatism formulas across independent groups.
#' Uses bootstrap-t method with trimmed means (default is mean when tr=0). Validates
#' that values do not exceed the invalid threshold (default 4 diopters) and checks
#' for negative values.
#'
#' @param x Matrix or data frame with J columns, or list of length J
#' @param con Contrast matrix (J by d) specifying contrasts of interest. If unspecified
#'   (con=0), all pairwise comparisons are performed
#' @param invalid Maximum valid absolute value in diopters (default: 4)
#' @param SEED Logical; if TRUE, set random seed for reproducibility
#' @param STOP Logical; if TRUE, stop on detection of invalid values
#' @param method Multiple comparison method for p-value adjustment (default: 'holm')
#' @inheritParams common_params
#'
#' @details
#' The function:
#' \itemize{
#'   \item Checks for invalid values (>invalid diopters or <0)
#'   \item Performs pairwise comparisons or custom contrasts
#'   \item Uses bootstrap-t with centering around trimmed mean
#'   \item Removes missing values automatically
#' }
#'
#' For contrasts: con[,1]=c(1,1,-1,-1,0,0) tests if sum of first two trimmed means
#' equals sum of next two.
#'
#' @return
#' List with:
#' \itemize{
#'   \item \code{output} - Matrix of results with estimates, confidence intervals, and p-values
#'   \item \code{con} - Contrast matrix used
#'   \item Additional bootstrap details
#' }
#'
#' @seealso \code{\link{oph.astig.depcom}}
#'
#' @keywords ophthalmology
#' @export
oph.astig.indepcom<-function(x,con=0,tr=0,alpha=.05,nboot=1999,invalid=4,SEED=TRUE,STOP=TRUE,method='holm'){
#
#  This function is designed specifically for comparing predictions errors
#  relevant to astigmatism.
#  It is assumed that any value greater than 4 diopters
#  is invalid.  The argument invalid can be used to change this decision rule.
#
#  Goal:
#   Compute a 1-alpha confidence interval for each pair of variables
#  using  a trimmed mean and  a bootstrap-t bootstrap method.
#  By default
#  tr=0, meaning means are compared
#
#   Independent groups are assumed.
#
#  The argument
#   x can be a matrix in with J columns or it can have list mode having length J
#
#   Missing values are automatically removed.
#
#  If desired, linear contrasts can be tested.
#   con is a J by d matrix containing the contrast coefficents of interest.
#   If unspecified, all pairwise comparisons are performed.
#   For example, con[,1]=c(1,1,-1,-1,0,0) and con[,2]=c(,1,-1,0,0,1,-1)
#   will test two contrasts: (1) the sum of the first two trimmed means is
#   equal to the sum of the second two, and (2) the difference between
#   the first two is equal to the difference between the trimmed means of
#   groups 5 and 6.
#
#   The default number of bootstrap samples is nboot=599
#
#   This function uses other functions in the WRS or the file Rallffun-v3
if(is.data.frame(x))x=as.matrix(x)
con<-as.matrix(con)
if(is.matrix(x))x<-listm(x)
if(!is.list(x))stop('Data must be stored in a matrix or in list mode.')
J<-length(x)
for(j in 1:J){
xx<-x[[j]]
x[[j]]<-xx[!is.na(xx)] # Remove any missing values.
}
for(j in 1:J){
flag=as.logical(x[[j]]>invalid)
if(sum(flag,na.rm=TRUE)>0){
print(paste('The value of argument invalid indicates that any value greater  than', invalid,' is invalid'))
print(paste('Variable', j, 'has one  or more invalid values'))
print('They occur in the following positions')
nr=c(1:length(x[[j]]))
print(nr[flag])
if(STOP)stop()
}}
for(j in 1:J){
flag=as.logical(x[[j]]<0)
if(sum(flag)>0){
print(paste('Values less than zero were detected'))
print(paste('Variable', j, 'has one  or more values=0'))
print('They occur in the following positions')
nr=c(1:length(x[[j]]))
print(nr[flag])
if(STOP)stop()
}
}
Jm<-J-1
d<-(J^2-J)/2
FLAG=FALSE
if(sum(con^2)==0){
FLAG=TRUE
con=con.all.pairs(J)
}
if(nrow(con)!=length(x))stop('The number of groups does not match the number of contrast coefficients.')
bvec<-array(0,c(J,2,nboot))
if(SEED)set.seed(2) # set seed of random number generator so that
#             results can be duplicated.
nsam=matl(lapply(x,length))
for(j in 1:J){
xcen<-x[[j]]-mean(x[[j]],tr)
data<-matrix(sample(xcen,size=length(x[[j]])*nboot,replace=TRUE),nrow=nboot)
bvec[j,,]<-apply(data,1,trimparts,tr) # A 2 by nboot matrix. The first row
#                     contains the bootstrap trimmed means, the second row
#                     contains the bootstrap squared standard errors.
}
m1<-bvec[,1,]  # J by nboot matrix containing the bootstrap trimmed means
m2<-bvec[,2,]  # J by nboot matrix containing the bootstrap sq. se.
boot<-matrix(0,ncol(con),nboot)
for (d in 1:ncol(con)){
top<-apply(m1,2,trimpartt,con[,d])
#            A vector of length nboot containing psi hat values
consq<-con[,d]^2
bot<-apply(m2,2,trimpartt,consq)
boot[d,]<-abs(top)/sqrt(bot)
}
testb<-apply(boot,2,max)
ic<-floor((1-alpha)*nboot)
ic.crit=ic
testb<-sort(testb)
psihat<-matrix(0,ncol(con),4)
test<-matrix(0,ncol(con),5)
dimnames(psihat)<-list(NULL,c('con.num','psihat','ci.lower','ci.upper'))
dimnames(test)<-list(NULL,c('con.num','test','se','p.value','p.adjusted'))
for (d in 1:ncol(con)){
test[d,1]<-d
psihat[d,1]<-d
testit<-lincon(x,con[,d],tr,pr=FALSE)
test[d,2]<-testit$test[1,2]
pval<-mean((abs(testit$test[1,2])<boot[d,]))
test[d,4]<-pval
psihat[d,3]<-testit$psihat[1,2]-testb[ic]*testit$test[1,4]
psihat[d,4]<-testit$psihat[1,2]+testb[ic]*testit$test[1,4]
psihat[d,2]<-testit$psihat[1,2]
test[d,3]<-testit$test[1,4]
}
test[,5]=p.adjust(test[,4],method=method)
if(FLAG){
rem=psihat
Trem=test
# For all pairwise comparisons, adjust format of output to make it easier to read
CC<-(J^2-J)/2
psihat<-matrix(0,CC,9)
dimnames(psihat)<-list(NULL,c('Group','Group','EST.1-EST.2','ci.lower','ci.upper',
'p.value','Est.1','Est.2','adj.p.value'))
test<-matrix(NA,CC,4)
dimnames(test)<-list(NULL,c('Group','Group','test','se'))
ic=0
for(j in 1:J){
for(k in 1:J){
if(j<k){
ic=ic+1
psihat[ic,1]=j
psihat[ic,2]=k
psihat[ic,3:5]=rem[ic,2:4]
psihat[ic,6]=Trem[ic,4]
psihat[ic,7]=tmean(x[[j]],tr=tr)
psihat[ic,8]=tmean(x[[k]],tr=tr)
psihat[ic,9]=Trem[ic,5]
test[ic,1]=j
test[ic,2]=k
test[ic,3]=Trem[ic,2]
test[ic,4]=Trem[ic,3]
}}}
}
list(n=nsam,psihat=psihat,test=test,crit=testb[ic.crit],con=con)
}




# ----------------------------------------------------------------------------

# oph.astig.indepintervals

# ----------------------------------------------------------------------------

#' Compare Frequency Distributions Across Diopter Intervals - Independent Groups
#'
#' @description
#' For each column of the matrix, compares frequencies across predefined diopter intervals
#' using the KMS (Kolm-Smirnov-type) method. Designed for comparing astigmatism prediction
#' error distributions between independent formulas.
#'
#' @param m Matrix or data frame with J columns representing different formulas
#' @param method Multiple comparison method for p-value adjustment (default: 'holm')
#' @param invalid Maximum valid absolute value in diopters (default: 4)
#'
#' @details
#' Compares frequency distributions at 8 fixed intervals: 0.25, 0.50, 0.75, 1, 1.25,
#' 1.5, 1.75, and 2 diopters. For each pairwise comparison, tests whether the cumulative
#' frequencies differ significantly at each interval using the KMS method.
#'
#' @return
#' List of matrices, one for each pairwise comparison. Each matrix contains interval
#' identifiers, test statistics, p-values, and adjusted p-values.
#'
#' @seealso \code{\link{oph.astig.mcnemar}}, \code{\link{srg1.vs.2}}
#'
#' @keywords ophthalmology
#' @export
oph.astig.indepintervals<-function(m,method='holm',invalid=4){
#
#    For column of x, compare frequencies using KMS method
#
#
#  n: sample sizes
#  x is a matrix or data frame with 8 rows
#
#
E=list()
ic=0
J=ncol(m)
x=m
flag=abs(elimna(x))>invalid
if(sum(flag,na.rm=TRUE)>0){
nr=c(1:nrow(x))
if(sum(flag)>1){
print(paste('The value of argument invalid indicates that any value greater in absolute value than', invalid,' is invalid'))
print('The following rows have invalid values')
}
if(sum(flag)==1){
print(paste('The value of argument invalid indicates that any value greater in absolute value than', invalid,' is invalid'))
print('The following row has an  invalid value')
}
irow=NA
ic=0
N=nrow(x)
for(i  in 1:N){
iflag=abs(x[i,])>invalid
if(sum(iflag,na.rm=TRUE)>0){
ic=ic+1
irow[ic]=i
}}
print(irow)
stop()
}
id=matrix(NA,8,2)
x=matrix(NA,8,2)
INT=c(0.25,0.50, 0.75,1,1.25,1.5,1.75,2)
dimnames(id)=list(NULL,ncol=c('S1','S2'))
for (j in 1:J){
  for (k in 1:J){
    if (j < k){
      ic=ic+1
id[,1]=rep(j,8)
id[,2]=rep(k,8)
#  Next determine frequencies
S1=elimna(m[,j])
S2=elimna(m[,k])
n1=length(S1)
n2=length(S2)
for(L in 1:8){
x[L,1]=sum(S1<=INT[L])
x[L,2]=sum(S2<=INT[L])
}
a=srg1.vs.2(c(n1,n2),x)
Adj.p.value=p.adjust(a[,3],method=method)
E[[ic]]=cbind(id,a,Adj.p.value)
    }}}
E
}




# ----------------------------------------------------------------------------

# oph.astig.mcnemar

# ----------------------------------------------------------------------------

#' Compare Astigmatism Prediction Formulas Using McNemar's Test
#'
#' @description
#' Compares astigmatism prediction formulas using McNemar's test for paired data.
#' Tests whether formulas differ in their frequencies at various diopter intervals
#' (0.25 to 2 diopters in 0.25 increments).
#'
#' @param x Matrix or data frame with J columns representing different formulas
#' @param method Multiple comparison method for p-value adjustment (default: 'holm')
#' @param invalid Maximum valid absolute value in diopters (default: 4)
#'
#' @details
#' For each pair of formulas and each diopter threshold (D = 0.25, 0.50, ..., 2.00),
#' creates a 2x2 contingency table and applies McNemar's test. Reports the number and
#' percentage of cases below each threshold for each formula pair. P-values are adjusted
#' for multiple comparisons.
#'
#' @return
#' List with length equal to number of diopter intervals. Each element is a matrix
#' containing:
#' \itemize{
#'   \item Diopter threshold (D)
#'   \item Formula identifiers
#'   \item Counts and percentages below threshold
#'   \item p-values and adjusted p-values
#' }
#'
#' @seealso \code{\link{oph.mcnemar}}, \code{\link{oph.astig.indepintervals}}
#'
#' @keywords ophthalmology
#' @export
oph.astig.mcnemar<-function(x,method='holm',invalid=4){
#
# Astigmatism: compare prediction formulas
#
if(is.null(dim(x)))stop('x should be a matrix or data frame')
J=ncol(x)  #number of formulas
flag=abs(elimna(x))>invalid
if(sum(flag,na.rm=TRUE)>0){
nr=c(1:nrow(x))
if(sum(flag)>1){
print(paste('The value of argument invalid indicates that any value greater in absolute value than', invalid,' is invalid'))
print('The following rows have invalid values')
}
if(sum(flag)==1){
print(paste('The value of argument invalid indicates that any value greater in absolute value than', invalid,' is invalid'))
print('The following row has an  invalid value')
}
irow=NA
ic=0
N=nrow(x)
for(i  in 1:N){
iflag=abs(x[i,])>invalid
if(sum(iflag,na.rm=TRUE)>0){
ic=ic+1
irow[ic]=i
}}
print(irow)
istop()
}
CC=(J^2-J)/2
output<-matrix(0,CC,9)
dimnames(output)<-list(NULL,c('D', ' Var',  'N< ' ,  '%<',  'Var', 'N<',    '%< ',
'p.value','Adj.p.value'))
E=list()
TAB=list()
D=seq(.25,2,.25)  #D intervals from .25 to 2
for(L in 1:length(D)){
ic=0
for (j in 1:J){
for (k in 1:J){
if (j < k){
ic=ic+1
a=mat2table(x[,c(j,k)],D[L],D[L])
n1=sum(x[[j]]<=D[L])
pn1=mean(x[[j]]<=D[L])
n2=sum(x[[k]]<=D[L])
pn2=mean(x[[k]]<=D[L])
if(sum(is.na(a)>0))print(paste('No data for VAR',j,'VAR',k,'D=',D[L]))
if(sum(is.na(a))==0){
mct=mcnemar.test(a)
output[ic,1]=D[L]
output[ic,2]=j
output[ic,3]=n1
output[ic,4]=pn1
output[ic,5]=k
output[ic,6]=n2
output[ic,7]=pn2
output[ic,8]=mct[[3]]
if(a[1,2]==0 &a[2,1]==0)output[ic,8]=1
}}}}
output[,9]=p.adjust(output[,8],method=method)
E[[L]]=output
}
E
}




# ----------------------------------------------------------------------------

# oph.astig.meanconvexpoly

# ----------------------------------------------------------------------------

#' Compute Confidence Regions for Bivariate Means in Astigmatism Data
#'
#' @description
#' Computes confidence regions for the mean of bivariate astigmatism prediction error
#' data. Creates confidence polygons for each formula pair using bootstrap methods.
#'
#' @param m Matrix or data frame with even number of columns (J). First two columns
#'   represent first formula, next two the second formula, etc.
#' @param plotit Logical; if TRUE, plot the confidence regions
#' @param xlab Label for x-axis (default: 'V1')
#' @param ylab Label for y-axis (default: 'V2')
#' @param MC Logical; if TRUE, use multicore processing
#' @param SEED Logical; if TRUE, set random seed for reproducibility
#' @inheritParams common_params
#'
#' @details
#' For each formula (pair of columns), computes a 1-alpha confidence region for the
#' bivariate mean using bootstrap. The confidence region is represented as a convex
#' polygon. Optionally plots the regions.
#'
#' @return
#' List with components:
#' \itemize{
#'   \item \code{centers} - List of center points (means) for each formula with sample sizes
#'   \item \code{conf.region.points} - List of confidence region boundary points for each formula
#'   \item \code{p.values} - List of p-values from hypothesis tests
#' }
#'
#' @seealso \code{\link{meancr.cord.oph}}, \code{\link{oph.astig.datasetconvexpoly.mean}}
#'
#' @keywords ophthalmology
#' @export
oph.astig.meanconvexpoly<-function(m,alpha=.05,plotit=TRUE,xlab='V1',ylab='V2',nboot=1999,MC=FALSE,
SEED=TRUE){
#
# This function is designed to compute confidence region for  the mean of bivariate data.
#  The function is designed  for dealing with
#  prediction errors when dealing with astigmatism.
#
# alpha=.05 means that the function
#  determine the 1-.05=.95 confidence region for the median of the data cloud.
#  This is done for each formula
#  Assume m is a matrix or data frame having
#  J columns. First two columns first formula, next two columns next formula..
#
#  So J should be an even integer
#
J=ncol(m)
N=J/2
if(N != floor(N))stop('Should have an even number of columns')
if(plotit){
plot.new()
if(N>1)par(mfrow=c(2,2))
}
region=list()
centers=list()
val=list()
pv=list()
CENTERS=list()
id=c(-1,0)
for(j in 1:N){
id=id+2
a=meancr.cord.oph(m[,id],SEED=SEED,plotit=FALSE,xlab=xlab,ylab=ylab,nboot=nboot)
centers[[j]]=a$center
region[[j]]=a$conf.region.points
val[[j]]=a$boot.vals
centers[[j]]=a$center
n=nrow(elimna(m[,id]))
n=as.integer(n)
CENTERS[[j]]=c(a$center,n)
names(CENTERS[[j]])=c('V1','V2','N')
pv[[j]]=a$p.value
}
VAL=val
plot.new()
if(N>1)par(mfrow=c(2,2))
id=c(-1,0)
for(j in 1:N){
id=id+2
n=nrow(m[,id])
crit.level<-.05
if(n<=120)crit.level<-.045
if(n<=80)crit.level<-.04
if(n<=60)crit.level<-.035
if(n<=40)crit.level<-.03
if(n<=30)crit.level<-.025
if(n<=20)crit.level<-.02
ic<-round((1-crit.level)*nboot)
val=VAL[[j]]
est=centers[[j]]
temp3<-est
ic<-round((1-crit.level)*nboot)
if(!MC)temp<-pdis(val,center=est)
if(MC)temp<-pdisMC(val,center=est)
temp.dis<-order(temp)
xx<-val[temp.dis[1:ic],]
xord<-order(xx[,1])
xx<-xx[xord,]
temp<-chull(xx)
region[[j]]=xx[temp,]
if(plotit){
plot(val[,1],val[,2],xlab=xlab,ylab=ylab)
points(temp3[1],temp3[2],pch="+")
lines(xx[temp,])
lines(xx[c(temp[1],temp[length(temp)]),])
}}
par(mfrow=c(1,1))
list(centers=CENTERS,conf.region.points=region,p.values=pv)
}

#' Compare Mean Absolute Deviations - Dependent Groups (Ophthalmology)
#'
#' @description
#' Compares mean absolute deviations (MAD) of prediction errors for intraocular lens
#' power calculation formulas using dependent (paired) samples. Values are centered
#' before computing absolute deviations.
#'
#' @param x Matrix, data frame, or list with J columns/elements representing different formulas
#' @param y Optional second variable; if provided, x is treated as a vector and compared to y
#' @param invalid Maximum valid absolute value in diopters (default: 4)
#' @param method Multiple comparison method for p-value adjustment (default: 'hommel')
#' @param STOP Logical; if TRUE, stop on detection of invalid values
#' @inheritParams common_params
#'
#' @details
#' Computes root-mean-squared absolute errors for dependent measures. Each variable is
#' centered (mean subtracted) before taking absolute values. Uses bootstrap-t method
#' for pairwise comparisons with p-value adjustment for multiple testing.
#'
#' @return
#' Matrix with columns: Var, Var, MAD 1, MAD 2, Dif, p.value, Adj.p.value
#'
#' @seealso \code{\link{oph.indep.comMAD}}, \code{\link{oph.dep.commean}}
#'
#' @keywords ophthalmology
#' @export
oph.dep.comMAD<-function(x, y=NULL, tr=0,invalid=4, method='hommel',STOP=TRUE,nboot=1999){
#
#  This function is designed specifically for dealing with
#  Prediction Error for Intraocular Lens Power Calculation
#
#  Goal: #  Goal: compare mean absolute deviation  of J dependent measures.
#  Strictly speaking, the squared mean  absolute error value is used.
# The estimates reported by the function are the root-mean-squared  absolute errors.
#  All pairwise comparisons are performed using a bootstrap-t method based on means
#

#   x can be a matrix, a data frame or it can have list mode.
#   if y is not NULL, the function assumes x is a vector
#   and the goal is to compare the variances of the data in x and y.
#
#  By default, Hommel's method is used to control the probability of one
#  or more Type I errors
#
if(!is.null(y))x=cbind(x,y)
if(is.list(x)){
n=pool.a.list(lapply(x,length))
if(var(n)!=0)stop('lengths have different values')
x=matl(x)
}
J=ncol(x)
flag=abs(x)>invalid
if(sum(flag,na.rm=TRUE)>0){
nr=c(1:nrow(x))
if(sum(flag)>1){
print(paste('The value of argument invalid indicates that any value greater in absolute value than', invalid,' is invalid'))
print('The following rows have invalid values')
}
if(sum(flag)==1){
print(paste('The value of argument invalid indicates that any value greater in absolute value than', invalid,' is invalid'))
print('The following row has an  invalid value')
}
irow=NA
ic=0
N=nrow(x)
for(i  in 1:N){
iflag=abs(x[i,])>invalid
if(sum(iflag,na.rm=TRUE)>0){
ic=ic+1
irow[ic]=i
}}
print(irow)
if(STOP)stop()
}
CC=(J^2-J)/2
output<-matrix(0,CC,7)
dimnames(output)<-list(NULL,c('Var','Var','MAD 1','MAD 2','Dif','p.value','Adj.p.value'))
ic=0
for(j in 1:J){
mx=mean(x[,j],na.rm=TRUE)
#x[,j]=x[,j]-mean(x[,j],na.rm=TRUE)
x[,j]=x[,j]-mx
}
for (j in 1:J){
for (k in 1:J){
if (j < k){
ic=ic+1
a=ydbt(abs(x[,j]),abs(x[,k]),tr=tr,nboot=nboot)
output[ic,1]=j
output[ic,2]=k
output[ic,3]=mean(abs(x[,j]),tr=tr,na.rm=TRUE)
output[ic,4]=mean(abs(x[,k]),tr=tr,na.rm=TRUE)
output[ic,5]=output[ic,3]-output[ic,4]
output[ic,6]=a$p.value
}}}
output[,7]=p.adjust(output[,6],method=method)
output
}




# ----------------------------------------------------------------------------

# oph.dep.commean

# ----------------------------------------------------------------------------

#' Compare Means - Dependent Groups (Ophthalmology)
#'
#' @description
#' Compares means of prediction errors for intraocular lens power calculation formulas
#' using dependent (paired) samples. Designed for data in diopters with validity checks.
#'
#' @param x Matrix, data frame, or list with J columns/elements representing different formulas
#' @param y Optional second variable; if provided, x is treated as a vector and compared to y
#' @param invalid Maximum valid absolute value in diopters (default: 4)
#' @param method Multiple comparison method for p-value adjustment (default: 'hommel')
#' @param STOP Logical; if TRUE, stop on detection of invalid values
#' @inheritParams common_params
#'
#' @details
#' Performs all pairwise comparisons using bootstrap-t method based on trimmed means
#' (default tr=0 uses means). Checks that all values are between -invalid and +invalid
#' diopters. For more robust estimation, set tr=0.2 for 20% trimmed means.
#'
#' @return
#' Matrix with columns: Var, Var, Mean 1, Mean 2, Dif, p.value, Adj.p.value
#'
#' @seealso \code{\link{oph.indep.commean}}, \code{\link{ydbt}}
#'
#' @keywords ophthalmology
#' @export
oph.dep.commean<-function(x, y=NULL, tr=0,invalid=4, method='hommel',STOP=TRUE){
#
#  This function is designed specifically for dealing with
#  Prediction Error for Intraocular Lens Power Calculation
#  It is assumed that any value less than -4 diopters or greater than 4 diopters
#  is invalid.  The argument invalid can be used to change this decision rule.
#
#  Goal: compare the means of J dependent measures.
#  All pairwise comparisons are performed using a bootstrap-t method based on means
#    To use an even  more robust method using a 20% trimmed mean, set tr=.2
#

#   x can be a matrix, a data frame or it can have list mode.
#   if y is not NULL, the function assumes x is a vector
#   and the goal is to compare the variances of the data in x and y.
#
#  By default, Hommel's method is used to control the probability of one
#  or more Type I errors
#
if(!is.null(y))x=cbind(x,y)
if(is.list(x)){
n=pool.a.list(lapply(x,length))
if(var(n)!=0)stop('lengths have different values')
x=matl(x)
}
J=ncol(x)
flag=abs(x)>invalid
if(sum(flag,na.rm=TRUE)>0){
nr=c(1:nrow(x))
if(sum(flag)>1){
print(paste('The value of argument invalid indicates that any value greater in absolute value than', invalid,' is invalid'))
print('The following rows have invalid values')
}
if(sum(flag)==1){
print(paste('The value of argument invalid indicates that any value greater in absolute value than', invalid,' is invalid'))
print('The following row has an  invalid value')
}
irow=NA
ic=0
N=nrow(x)
for(i  in 1:N){
iflag=abs(x[i,])>invalid
if(sum(iflag,na.rm=TRUE)>0){
ic=ic+1
irow[ic]=i
}}
print(irow)
if(STOP)stop()
}
CC=(J^2-J)/2
output<-matrix(0,CC,7)
dimnames(output)<-list(NULL,c('Var','Var','Mean 1','Mean 2','Dif','p.value','Adj.p.value'))
ic=0
for (j in 1:J){
for (k in 1:J){
if (j < k){
ic=ic+1
a=ydbt(x[,j],x[,k],tr=tr)
output[ic,1]=j
output[ic,2]=k
output[ic,3]=a$Est.1
output[ic,4]=a$Est.2
output[ic,5]=a$Est.1- a$Est.2
output[ic,6]=a$p.value
}}}
output[,7]=p.adjust(output[,6],method=method)
output
}




# ----------------------------------------------------------------------------

# oph.dep.comMeanAE

# ----------------------------------------------------------------------------

#' Compare Mean Absolute Errors - Dependent Groups (Ophthalmology)
#'
#' @description
#' Compares mean absolute prediction errors for intraocular lens power calculation
#' formulas using dependent (paired) samples. Reports root-mean-squared absolute errors.
#'
#' @param x Matrix, data frame, or list with J columns/elements representing different formulas
#' @param y Optional second variable; if provided, x is treated as a vector and compared to y
#' @param invalid Maximum valid absolute value in diopters (default: 4)
#' @param method Multiple comparison method for p-value adjustment (default: 'hommel')
#' @param STOP Logical; if TRUE, stop on detection of invalid values
#' @inheritParams common_params
#'
#' @details
#' Uses squared mean absolute error values internally. Estimates reported are
#' root-mean-squared absolute errors. Uses bootstrap-t method for comparisons.
#' For robust estimation, set tr=0.2 for 20% trimmed means.
#'
#' @return
#' Matrix with columns: Var, Var, Mean.AE 1, Mean.AE 2, Dif, p.value, Adj.p.value
#'
#' @seealso \code{\link{oph.indep.comMeanAE}}, \code{\link{oph.dep.comRMSAE}}
#'
#' @keywords ophthalmology
#' @export
oph.dep.comMeanAE<-function(x, y=NULL, tr=0,invalid=4, method='hommel',STOP=TRUE,nboot=1999){
#
#  This function is designed specifically for dealing with
#  Prediction Error for Intraocular Lens Power Calculation
#
#  Goal: compare the root-mean-square  absolute prediction error  of J dependent measures.
#  Strictly speaking, the squared mean  absolute error value is used.
# The estimates reported by the function are the root-mean-squared  absolute errors.
#  All pairwise comparisons are performed using a bootstrap-t method based on means
#  For an even more robust method using a20% trimmed mean, set tr=.2

#   x can be a matrix, a data frame or it can have list mode.
#   if y is not NULL, the function assumes x is a vector
#   and the goal is to compare the variances of the data in x and y.
#
#  By default, Hommel's method is used to control the probability of one
#  or more Type I errors
#
if(!is.null(y))x=cbind(x,y)
if(is.list(x)){
n=pool.a.list(lapply(x,length))
if(var(n)!=0)stop('lengths have different values')
x=matl(x)
}
J=ncol(x)
flag=abs(elimna(x))>invalid
if(sum(flag,na.rm=TRUE)>0){
nr=c(1:nrow(x))
if(sum(flag)>1){
print(paste('The value of argument invalid indicates that any value greater in absolute value than', invalid,' is invalid'))
print('The following rows have invalid values')
}
if(sum(flag)==1){
print(paste('The value of argument invalid indicates that any value greater in absolute value than', invalid,' is invalid'))
print('The following row has an  invalid value')
}
irow=NA
ic=0
N=nrow(x)
for(i  in 1:N){
iflag=abs(x[i,])>invalid
if(sum(iflag,na.rm=TRUE)>0){
ic=ic+1
irow[ic]=i
}}
print(irow)
if(STOP)stop()
}
CC=(J^2-J)/2
output<-matrix(0,CC,7)
dimnames(output)<-list(NULL,c('Var','Var','Mean.AE 1','Mean.AE 2','Dif','p.value','Adj.p.value'))
ic=0
for (j in 1:J){
for (k in 1:J){
if (j < k){
ic=ic+1
a=ydbt(abs(x[,j]),abs(x[,k]),tr=tr,nboot=nboot)
output[ic,1]=j
output[ic,2]=k
output[ic,3]=mean(abs(x[,j]),tr=tr,na.rm=TRUE)
output[ic,4]=mean(abs(x[,k]),tr=tr,na.rm=TRUE)
output[ic,5]=output[ic,3]-output[ic,4]
output[ic,6]=a$p.value
}}}
output[,7]=p.adjust(output[,6],method=method)
output
}




# ----------------------------------------------------------------------------

# oph.dep.comRMSAE

# ----------------------------------------------------------------------------

#' Compare Root-Mean-Square Absolute Errors - Dependent Groups (Ophthalmology)
#'
#' @description
#' Compares root-mean-square absolute prediction errors (RMSAE) for intraocular lens
#' power calculation formulas using dependent (paired) samples.
#'
#' @param x Matrix, data frame, or list with J columns/elements representing different formulas
#' @param y Optional second variable; if provided, x is treated as a vector and compared to y
#' @param invalid Maximum valid absolute value in diopters (default: 4)
#' @param method Multiple comparison method for p-value adjustment (default: 'hommel')
#' @param STOP Logical; if TRUE, stop on detection of invalid values
#' @inheritParams common_params
#'
#' @details
#' Uses squared mean absolute error values internally for comparisons. Reports
#' root-mean-squared absolute errors. Uses bootstrap-t method based on trimmed means
#' (set tr=0.2 for 20% trimmed means).
#'
#' @return
#' Matrix with columns: Var, Var, RMSAE 1, RMSAE 2, Dif, p.value, Adj.p.value
#'
#' @seealso \code{\link{oph.indep.comRMSAE}}, \code{\link{oph.dep.comMeanAE}}
#'
#' @keywords ophthalmology
#' @export
oph.dep.comRMSAE<-function(x, y=NULL, tr=0,invalid=4, method='hommel',STOP=TRUE,nboot=1999){
#
#  This function is designed specifically for dealing with
#  Prediction Error for Intraocular Lens Power Calculation
#
#  Goal: compare the root-mean-square  absolute prediction error  of J dependent measures.
#  Strictly speaking, the squared mean  absolute error value is used.
# The estimates reported by the function are the root-mean-squared  absolute errors.
#  All pairwise comparisons are performed using a bootstrap-t method based on means
#  To use a  20% trimmed mean, set tr=.2

#   x can be a matrix, a data frame or it can have list mode.
#   if y is not NULL, the function assumes x is a vector
#   and the goal is to compare the variances of the data in x and y.
#
#  By default, Hommel's method is used to control the probability of one
#  or more Type I errors
#
if(!is.null(y))x=cbind(x,y)
if(is.list(x)){
n=pool.a.list(lapply(x,length))
if(var(n)!=0)stop('lengths have different values')
x=matl(x)
}
J=ncol(x)
flag=abs(elimna(x))>invalid
if(sum(flag,na.rm=TRUE)>0){
nr=c(1:nrow(x))
if(sum(flag)>1){
print(paste('The value of argument invalid indicates that any value greater in absolute value than', invalid,' is invalid'))
print('The following rows have invalid values')
}
if(sum(flag)==1){
print(paste('The value of argument invalid indicates that any value greater in absolute value than', invalid,' is invalid'))
print('The following row has an  invalid value')
}
irow=NA
ic=0
N=nrow(x)
for(i  in 1:N){
iflag=abs(x[i,])>invalid
if(sum(iflag,na.rm=TRUE)>0){
ic=ic+1
irow[ic]=i
}}
print(irow)
if(STOP)stop()
}
CC=(J^2-J)/2
output<-matrix(0,CC,7)
dimnames(output)<-list(NULL,c('Var','Var','RMSAE 1','RMSAE 2','Dif','p.value','Adj.p.value'))
ic=0
for (j in 1:J){
for (k in 1:J){
if (j < k){
ic=ic+1
a=ydbt(x[,j]^2,x[,k]^2,tr=tr,nboot=nboot)
output[ic,1]=j
output[ic,2]=k
output[ic,3]=RMSAE(elimna(x[,j]))
output[ic,4]=RMSAE(elimna(x[,k]))
output[ic,5]=output[ic,3]-output[ic,4]
output[ic,6]=a$p.value
}}}
output[,7]=p.adjust(output[,6],method=method)
output
}




# ----------------------------------------------------------------------------

# oph.indep.comMAD

# ----------------------------------------------------------------------------

#' Compare Mean Absolute Deviations - Independent Groups (Ophthalmology)
#'
#' @description
#' Compares mean absolute deviations (MAD) of prediction errors for intraocular lens
#' power calculation formulas using independent samples. Values are centered before
#' computing absolute deviations.
#'
#' @param x Matrix, data frame, or list with J columns/elements representing different formulas
#' @param y Optional second variable; if provided, x is treated as a vector and compared to y
#' @param method Multiple comparison method for p-value adjustment (default: 'hoch')
#' @param invalid Maximum valid absolute value in diopters (default: 4)
#' @param STOP Logical; if TRUE, stop on detection of invalid values
#' @inheritParams common_params
#'
#' @details
#' Uses heteroscedastic Welch method for pairwise comparisons. Each variable is
#' centered (mean subtracted) before computing absolute deviations. For robust
#' estimation, set tr=0.2 for 20% trimmed means.
#'
#' @return
#' Matrix with columns: Var, Var, MAD 1, MAD 2, Dif, p.value, Adj.p.value
#'
#' @seealso \code{\link{oph.dep.comMAD}}, \code{\link{yuenbt}}
#'
#' @keywords ophthalmology
#' @export
oph.indep.comMAD<-function(x,y=NULL,method='hoch',invalid=4,STOP=TRUE,tr=0,nboot=1999){
#
#  This function is designed specifically for dealing with
#  Prediction Error for Intraocular Lens Power Calculation
#  It is assumed that any value less than -4 diopters or greater than 4 diopters
#  is invalid.  The argument invalid can be used to change this decision rule.
#
#  Goal: compare mean absolute deviation  of J independent measures.
#  All pairwise comparisons are performed using a heteroscedastic
#   Welch method
#   x can be a matrix, a data frame or it can have list mode.
#   if y is not NULL, the function assumes x is a vector
#   and the goal is to compare the variances of the data in x and y.
# For a more robust method using a 20% trimme mean, set tr=.2
#
#  By default, Hochberg's method is used to control the probability of one
#  or more TypeI errors
#
if(!is.null(y))x=list(x,y)
if(is.matrix(x) || is.data.frame(x))x=listm(x)
J=length(x)
for(j in 1:J)x[[j]]=elimna(x[[j]])
for(j in 1:J){
flag=abs(x[[j]])>invalid
if(sum(flag,na.rm=TRUE)>0){
print(paste('The value of argument invalid indicates that any value greater in absolute value than', invalid,' is invalid'))
print(paste('Variable', j, 'has one  or more invalid values'))
print('They occur in the following positions')
nr=c(1:length(x[[j]]))
print(nr[flag])
if(STOP)stop()
}
}
CC=(J^2-J)/2
output<-matrix(0,CC,7)
dimnames(output)<-list(NULL,c('Var','Var','MAD 1','MAD 2','Dif','p.value','Adj.p.value'))
ic=0
for(j in 1:J)x[[j]]=x[[j]]-mean(x[[j]])
for (j in 1:J){
for (k in 1:J){
if (j < k){
ic=ic+1
a=yuenbt(abs(x[[j]]),abs(x[[k]]),tr=tr,nboot=nboot)
output[ic,1]=j
output[ic,2]=k
output[ic,3]=mean(abs(x[[j]]),tr=tr)
output[ic,4]=mean(abs(x[[k]]),tr=tr)
output[ic,5]=output[ic,3]-output[ic,4]
output[ic,6]=a$p.value
}}}
output[,7]=p.adjust(output[,6],method=method)
output
}

#' Compare Median Absolute Errors - Independent Groups (Ophthalmology)
#'
#' @description
#' Compares median absolute errors of prediction errors for intraocular lens power
#' calculation formulas using independent samples.
#'
#' @param x Matrix, data frame, or list with J columns/elements representing different formulas
#' @param y Optional second variable; if provided, x is treated as a vector and compared to y
#' @param est Estimator function to use (default: median)
#' @param method Multiple comparison method for p-value adjustment (default: 'hommel')
#' @param invalid Maximum valid absolute value in diopters (default: 4)
#' @param STOP Logical; if TRUE, stop on detection of invalid values
#' @inheritParams common_params
#'
#' @details
#' Uses heteroscedastic method for pairwise comparisons of absolute errors.
#' Default estimator is median, but can be changed via the \code{est} parameter.
#'
#' @return
#' Matrix with columns: Var, Var, Med.AE 1, Med.AE 2, Dif, p.value, Adj.p.value
#'
#' @seealso \code{\link{oph.dep.comMedAE}}, \code{\link{pb2gen}}
#'
#' @keywords ophthalmology
#' @export
oph.indep.comMedAE<-function(x,y=NULL,est=median,method='hommel',invalid=4,STOP=TRUE,nboot=1999){
#
#  This function is designed specifically for dealing with
#  Prediction Error for Intraocular Lens Power Calculation
#  It is assumed that any value less than -4 diopters or greater than 4 diopters
#  is invalid.  The argument invalid can be used to change this decision rule.
#
#  Goal: compare median Absolute Error of J independent measures.
#  All pairwise comparisons are performed using a heteroscedastic method
#   x can be a matrix, a data frame or it can have list mode.
#   if y is not NULL, the function assumes x is a vector
#
#
#  By default, Hommel's method is used to control the probability of one
#  or more TypeI errors
#
if(!is.null(y))x=list(x,y)
if(is.matrix(x) || is.data.frame(x))x=listm(x)
J=length(x)
for(j in 1:J)x[[j]]=elimna(x[[j]])
for(j in 1:J){
flag=abs(x[[j]])>invalid
if(sum(flag,na.rm=TRUE)>0){
print(paste('The value of argument invalid indicates that any value greater in absolute value than', invalid,' is invalid'))
print(paste('Variable', j, 'has one  or more invalid values'))
print('They occur in the following positions')
nr=c(1:length(x[[j]]))
print(nr[flag])
if(STOP)stop()
}
}
CC=(J^2-J)/2
output<-matrix(0,CC,7)
dimnames(output)<-list(NULL,c('Var','Var','Med.AE 1','Med.AE 2','Dif','p.value','Adj.p.value'))
ic=0
for (j in 1:J){
for (k in 1:J){
if (j < k){
ic=ic+1
a=pb2gen(abs(x[[j]]),abs(x[[k]]),est=est,nboot=nboot)
output[ic,1]=j
output[ic,2]=k
output[ic,3]=est(abs(x[[j]]),)
output[ic,4]=est(abs(x[[k]]))
output[ic,5]=output[ic,3]-output[ic,4]
output[ic,6]=a$p.value
}}}
output[,7]=p.adjust(output[,6],method=method)
output
}

#' Compare Means - Independent Groups (Ophthalmology)
#'
#' @description
#' Compares means of prediction errors for intraocular lens power calculation formulas
#' using independent samples. Uses heteroscedastic Welch method.
#'
#' @param x Matrix, data frame, or list with J columns/elements representing different formulas
#' @param y Optional second variable; if provided, x is treated as a vector and compared to y
#' @param method Multiple comparison method for p-value adjustment (default: 'hommel')
#' @param invalid Maximum valid absolute value in diopters (default: 4)
#' @param STOP Logical; if TRUE, stop on detection of invalid values
#' @inheritParams common_params
#'
#' @details
#' Performs all pairwise comparisons using heteroscedastic Welch method based on
#' trimmed means. Default tr=0 uses means; set tr=0.2 for 20% trimmed means for
#' more robust estimation.
#'
#' @return
#' Matrix with columns: Var, Var, Mean 1, Mean 2, Dif, p.value, Adj.p.value
#'
#' @seealso \code{\link{oph.dep.commean}}, \code{\link{yuenbt}}
#'
#' @keywords ophthalmology
#' @export
oph.indep.commean<-function(x,y=NULL,method='hommel',invalid=4,STOP=TRUE,tr=0){
#
#  This function is designed specifically for dealing with
#  Prediction Error for Intraocular Lens Power Calculation
#  It is assumed that any value less than -4 diopters or greater than 4 diopters
#  is invalid.  The argument invalid can be used to change this decision rule.
#
#  Goal: compare means of J independent measures.
#  All pairwise comparisons are performed using a heteroscedastic
#   Welch method
#   x can be a matrix, a data frame or it can have list mode.
#   if y is not NULL, the function assumes x is a vector
#   and the goal is to compare the variances of the data in x and y.
#    To use an even  more robust method using a 20% trimmed mean, set tr=.2
#
#  By default, Hommel's method is used to control the probability of one
#  or more TypeI errors
#
if(!is.null(y))x=list(x,y)
if(is.matrix(x) || is.data.frame(x))x=listm(x)
J=length(x)
for(j in 1:J)x[[j]]=elimna(x[[j]])
for(j in 1:J){
flag=abs(elimna(x[[j]]))>invalid
if(sum(flag,na.rm=TRUE)>0){
print(paste('The value of argument invalid indicates that any value greater in absolute value than', invalid,' is invalid'))
print(paste('Variable', j, 'has one  or more invalid values'))
print('They occur in the following positions')
nr=c(1:length(x[[j]]))
print(nr[flag])
if(STOP)stop()
}
}
CC=(J^2-J)/2
output<-matrix(0,CC,7)
dimnames(output)<-list(NULL,c('Var','Var','Mean 1','Mean 2','Dif','p.value','Adj.p.value'))
ic=0
for (j in 1:J){
for (k in 1:J){
if (j < k){
ic=ic+1
a=yuenbt(x[[j]],x[[k]],tr=tr)
output[ic,1]=j
output[ic,2]=k
output[ic,3:4]=c(a$est.1,a$est.2)
output[ic,5]=output[ic,3]-output[ic,4]
output[ic,6]=a$p.value
}}}
output[,7]=p.adjust(output[,6],method=method)
output
}

#' Compare Median Absolute Errors - Dependent Groups (Ophthalmology)
#'
#' @description
#' Compares median absolute prediction errors for intraocular lens power calculation
#' formulas using dependent (paired) samples.
#'
#' @param x Matrix, data frame, or list with J columns/elements representing different formulas
#' @param y Optional second variable; if provided, x is treated as a vector and compared to y
#' @param est Estimator function to use (default: median)
#' @param dif Logical; method for handling differences in \code{dmedpb} (default: FALSE)
#' @param invalid Maximum valid absolute value in diopters (default: 4)
#' @param method Multiple comparison method for p-value adjustment (default: 'hommel')
#' @param STOP Logical; if TRUE, stop on detection of invalid values
#' @inheritParams common_params
#'
#' @details
#' Uses bootstrap-t method for pairwise comparisons of median absolute errors.
#' Operates on absolute values of the prediction errors.
#'
#' @return
#' Matrix with columns: Var, Var, Med.AE 1, Med.AE 2, Dif, p.value, Adj.p.value
#'
#' @seealso \code{\link{oph.indep.comMedAE}}, \code{\link{dmedpb}}
#'
#' @keywords ophthalmology
#' @export
oph.dep.comMedAE<-function(x, y=NULL, est=median,dif=FALSE, invalid=4, method='hommel',STOP=TRUE,nboot=1999){
#
#  This function is designed specifically for dealing with
#  Prediction Error for Intraocular Lens Power Calculation
#
#  Goal: compare the median absolute prediction error  of J dependent measures.
#
#  All pairwise comparisons are performed using a bootstrap-t method based on means
#
#   x can be a matrix, a data frame or it can have list mode.
#   if y is not NULL, the function assumes x is a vector
#   and the goal is to compare the variances of the data in x and y.
#
#  By default, Hommel's method is used to control the probability of one
#  or more Type I errors
#
if(!is.null(y))x=cbind(x,y)
if(is.list(x)){
n=pool.a.list(lapply(x,length))
if(var(n)!=0)stop('lengths have different values')
x=matl(x)
}
J=ncol(x)
flag=abs(elimna(x))>invalid
if(sum(flag,na.rm=TRUE)>0){
nr=c(1:nrow(x))
if(sum(flag)>1){
print(paste('The value of argument invalid indicates that any value greater in absolute value than', invalid,' is invalid'))
print('The following rows have invalid values')
}
if(sum(flag)==1){
print(paste('The value of argument invalid indicates that any value greater in absolute value than', invalid,' is invalid'))
print('The following row has an  invalid value')
}
irow=NA
ic=0
N=nrow(x)
for(i  in 1:N){
iflag=abs(x[i,])>invalid
if(sum(iflag,na.rm=TRUE)>0){
ic=ic+1
irow[ic]=i
}}
print(irow)
if(STOP)stop()
}
CC=(J^2-J)/2
output<-matrix(0,CC,7)
dimnames(output)<-list(NULL,c('Var','Var','Med.AE 1','Med.AE 2','Dif','p.value','Adj.p.value'))
ic=0
for (j in 1:J){
for (k in 1:J){
if (j < k){
ic=ic+1
a=dmedpb(abs(x[,j]),abs(x[,k]),est=est,dif=dif,nboot=nboot,pr=FALSE,plotit=FALSE)
output[ic,1]=j
output[ic,2]=k
output[ic,3]=est(abs(x[,j]),na.rm=TRUE)
output[ic,4]=est(abs(x[,k]),na.rm=TRUE)
output[ic,5]=output[ic,3]-output[ic,4]
output[ic,6]=a$output[,3]
}}}
output[,7]=p.adjust(output[,6],method=method)
output
}

#' Compare Medians - Independent Groups (Ophthalmology)
#'
#' @description
#' Compares medians of prediction errors for intraocular lens power calculation
#' formulas using independent samples with percentile bootstrap method.
#'
#' @param x Matrix, data frame, or list with J columns/elements representing different formulas
#' @param y Optional second variable; if provided, x is treated as a vector and compared to y
#' @param method Multiple comparison method for p-value adjustment (default: 'hommel')
#' @param invalid Maximum valid absolute value in diopters (default: 4)
#' @param STOP Logical; if TRUE, stop on detection of invalid values
#' @param SEED Logical; if TRUE, set random seed for reproducibility
#'
#' @details
#' Uses heteroscedastic percentile bootstrap method for pairwise comparisons of medians.
#' Assumes values are in diopters with validity checks.
#'
#' @return
#' Matrix with columns: Var, Var, Median 1, Median 2, Dif, p.value, Adj.p.value
#'
#' @seealso \code{\link{oph.dep.commedian}}, \code{\link{medpb2}}
#'
#' @keywords ophthalmology
#' @export
oph.indep.commedian<-function(x,y=NULL,method='hommel',invalid=4,STOP=TRUE,SEED=TRUE){
#
#  This function is designed specifically for dealing with
#  Prediction Error for Intraocular Lens Power Calculation
#  It is assumed that any value less than -4 diopters or greater than 4 diopters
#  is invalid.  The argument invalid can be used to change this decision rule.
#
#  Goal: compare medians of J independent measures.
#  All pairwise comparisons are performed using a heteroscedastic
#  percentile bootstrap  method
#   x can be a matrix, a data frame or it can have list mode.
#   if y is not NULL, the function assumes x is a vector
#   and the goal is to compare the variances of the data in x and y.
#    To use an even  more robust method using a 20% trimmed mean, set tr=.2
#
#  By default, Hommel's method is used to control the probability of one
#  or more TypeI errors
#
if(!is.null(y))x=list(x,y)
if(is.matrix(x) || is.data.frame(x))x=listm(x)
J=length(x)
for(j in 1:J)x[[j]]=elimna(x[[j]])
for(j in 1:J){
flag=abs(x[[j]])>invalid
if(sum(flag,na.rm=TRUE)>0){
print(paste('The value of argument invalid indicates that any value greater in absolute value than', invalid,' is invalid'))
print(paste('Variable', j, 'has one  or more invalid values'))
print('They occur in the following positions')
nr=c(1:length(x[[j]]))
print(nr[flag])
if(STOP)stop()
}
}
CC=(J^2-J)/2
output<-matrix(0,CC,7)
dimnames(output)<-list(NULL,c('Var','Var','Median 1','Median 2','Dif','p.value','Adj.p.value'))
ic=0
for (j in 1:J){
for (k in 1:J){
if (j < k){
ic=ic+1
a=medpb2(x[[j]],x[[k]],SEED=SEED)
output[ic,1]=j
output[ic,2]=k
output[ic,3:4]=c(a$est1,a$est2)
output[ic,5]=output[ic,3]-output[ic,4]
output[ic,6]=a$p.value
}}}
output[,7]=p.adjust(output[,6],method=method)
output
}

#' Compare Medians - Dependent Groups (Ophthalmology)
#'
#' @description
#' Compares medians of prediction errors for intraocular lens power calculation
#' formulas using dependent (paired) samples with percentile bootstrap method.
#'
#' @param x Matrix, data frame, or list with J columns/elements representing different formulas
#' @param y Optional second variable; if provided, x is treated as a vector and compared to y
#' @param invalid Maximum valid absolute value in diopters (default: 4)
#' @param method Multiple comparison method for p-value adjustment (default: 'hommel')
#' @param STOP Logical; if TRUE, stop on detection of invalid values
#' @param SEED Logical; if TRUE, set random seed for reproducibility
#'
#' @details
#' Uses percentile bootstrap method for pairwise comparisons of medians in paired data.
#' Checks that all values are between -invalid and +invalid diopters.
#'
#' @return
#' Matrix with columns: Var, Var, Median 1, Median 2, Dif, p.value, Adj.p.value
#'
#' @seealso \code{\link{oph.indep.commedian}}, \code{\link{dmedpb}}
#'
#' @keywords ophthalmology
#' @export
oph.dep.commedian<-function(x, y=NULL,invalid=4, method='hommel',STOP=TRUE,SEED=TRUE){
#
#  This function is designed specifically for dealing with
#  Prediction Error for Intraocular Lens Power Calculation
#  It is assumed that any value less than -4 diopters or greater than 4 diopters
#  is invalid.  The argument invalid can be used to change this decision rule.
#
#  Goal: compare the medians of J dependent measures.
#  All pairwise comparisons are performed using a percentile bootstrap method
#
#

#   x can be a matrix, a data frame or it can have list mode.
#   if y is not NULL, the function assumes x is a vector
#   and the goal is to compare the variances of the data in x and y.
#
#  By default, Hommel's method is used to control the probability of one
#  or more Type I errors
#
if(!is.null(y))x=cbind(x,y)
if(is.list(x)){
n=pool.a.list(lapply(x,length))
if(var(n)!=0)stop('lengths have different values')
x=matl(x)
}
J=ncol(x)
flag=abs(elimna(x))>invalid
if(sum(flag,na.rm=TRUE)>0){
nr=c(1:nrow(x))
if(sum(flag)>1){
print(paste('The value of argument invalid indicates that any value greater in absolute value than', invalid,' is invalid'))
print('The following rows have invalid values')
}
if(sum(flag)==1){
print(paste('The value of argument invalid indicates that any value greater in absolute value than', invalid,' is invalid'))
print('The following row has an  invalid value')
}
irow=NA
ic=0
N=nrow(x)
for(i  in 1:N){
iflag=abs(x[i,])>invalid
if(sum(iflag,na.rm=TRUE)>0){
ic=ic+1
irow[ic]=i
}}
print(irow)
if(STOP)stop()
}
CC=(J^2-J)/2
output<-matrix(0,CC,7)
dimnames(output)<-list(NULL,c('Var','Var','Median 1','Median 2','Dif','p.value','Adj.p.value'))
ic=0
for (j in 1:J){
for (k in 1:J){
if (j < k){
ic=ic+1
a=dmedpb(x[,j],x[,k],pr=FALSE,plotit=FALSE,nboot=2000,SEED=SEED)
output[ic,1]=j
output[ic,2]=k
output[ic,3]=median(x[,j],na.rm=TRUE)
output[ic,4]=median(x[,k],na.rm=TRUE)
output[ic,5]=a$output[1,2]
output[ic,6]=a$output[1,3]
}}}
output[,7]=p.adjust(output[,6],method=method)
output
}




# ----------------------------------------------------------------------------

# oph.indep.comMeanAE

# ----------------------------------------------------------------------------

#' Compare Mean Absolute Errors - Independent Groups (Ophthalmology)
#'
#' @description
#' Compares mean absolute errors (also called root-mean-square absolute errors, RMSAE)
#' of prediction errors for intraocular lens power calculation formulas using independent samples.
#'
#' @param x Matrix, data frame, or list with J columns/elements representing different formulas
#' @param y Optional second variable; if provided, x is treated as a vector and compared to y
#' @param method Multiple comparison method for p-value adjustment (default: 'hoch')
#' @param invalid Maximum valid absolute value in diopters (default: 4)
#' @param STOP Logical; if TRUE, stop on detection of invalid values
#' @inheritParams common_params
#'
#' @details
#' Uses heteroscedastic Welch method for pairwise comparisons. Operates on absolute
#' values of prediction errors. For robust estimation, set tr=0.2 for 20% trimmed means.
#'
#' @return
#' Matrix with columns: Var, Var, Mean.AE 1, Mean.AE 2, Dif, p.value, Adj.p.value
#'
#' @seealso \code{\link{oph.dep.comMeanAE}}, \code{\link{oph.indep.comRMSAE}}
#'
#' @keywords ophthalmology
#' @export
oph.indep.comMeanAE<-function(x,y=NULL,method='hoch',invalid=4,STOP=TRUE,tr=0,nboot=1999){
#
#  This function is designed specifically for dealing with
#  Prediction Error for Intraocular Lens Power Calculation
#  It is assumed that any value less than -4 diopters or greater than 4 diopters
#  is invalid.  The argument invalid can be used to change this decision rule.
#
#  Goal: compare root-mean-square Absolute Error (RMSAE) of J independent measures.
#  All pairwise comparisons are performed using a heteroscedastic
#   Welch method
#   x can be a matrix, a data frame or it can have list mode.
#   if y is not NULL, the function assumes x is a vector
#   and the goal is to compare the variances of the data in x and y.
# For a more robust method using a 20% trimme mean, set tr=.2
#
#  By default, Hochberg's method is used to control the probability of one
#  or more TypeI errors
#
if(!is.null(y))x=list(x,y)
if(is.matrix(x) || is.data.frame(x))x=listm(x)
J=length(x)
for(j in 1:J)x[[j]]=elimna(x[[j]])
for(j in 1:J){
flag=abs(x[[j]])>invalid
if(sum(flag,na.rm=TRUE)>0){
print(paste('The value of argument invalid indicates that any value greater in absolute value than', invalid,' is invalid'))
print(paste('Variable', j, 'has one  or more invalid values'))
print('They occur in the following positions')
nr=c(1:length(x[[j]]))
print(nr[flag])
if(STOP)stop()
}
}
CC=(J^2-J)/2
output<-matrix(0,CC,7)
dimnames(output)<-list(NULL,c('Var','Var','Mean.AE 1','Mean.AE 2','Dif','p.value','Adj.p.value'))
ic=0
for (j in 1:J){
for (k in 1:J){
if (j < k){
ic=ic+1
a=yuenbt(abs(x[[j]]),abs(x[[k]]),tr=tr,nboot=nboot)
output[ic,1]=j
output[ic,2]=k
output[ic,3]=mean(abs(x[[j]]),tr=tr)
output[ic,4]=mean(abs(x[[k]]),tr=tr)
output[ic,5]=output[ic,3]-output[ic,4]
output[ic,6]=a$p.value
}}}
output[,7]=p.adjust(output[,6],method=method)
output
}




# ----------------------------------------------------------------------------

# oph.indep.comRMSAE

# ----------------------------------------------------------------------------

#' Compare Root-Mean-Square Absolute Errors - Independent Groups (Ophthalmology)
#'
#' @description
#' Compares root-mean-square absolute errors (RMSAE) of prediction errors for
#' intraocular lens power calculation formulas using independent samples.
#'
#' @param x Matrix, data frame, or list with J columns/elements representing different formulas
#' @param y Optional second variable; if provided, x is treated as a vector and compared to y
#' @param method Multiple comparison method for p-value adjustment (default: 'hoch')
#' @param invalid Maximum valid absolute value in diopters (default: 4)
#' @param STOP Logical; if TRUE, stop on detection of invalid values
#' @inheritParams common_params
#'
#' @details
#' Uses heteroscedastic Welch method for comparisons. Compares squared values internally
#' but reports RMSAE. For robust estimation, set tr=0.2 for 20% trimmed means.
#'
#' @return
#' Matrix with columns: Var, Var, RMSAE 1, RMSAE 2, Dif, p.value, Adj.p.value
#'
#' @seealso \code{\link{oph.dep.comRMSAE}}, \code{\link{oph.indep.comMeanAE}}, \code{\link{RMSAE}}
#'
#' @keywords ophthalmology
#' @export
oph.indep.comRMSAE<-function(x,y=NULL,method='hoch',invalid=4,STOP=TRUE,tr=0,nboot=1999){
#
#  This function is designed specifically for dealing with
#  Prediction Error for Intraocular Lens Power Calculation
#  It is assumed that any value less than -4 diopters or greater than 4 diopters
#  is invalid.  The argument invalid can be used to change this decision rule.
#
#  Goal: compare root-mean-square Absolute Error (RMSAE) of J independent measures.
#  All pairwise comparisons are performed using a heteroscedastic
#   Welch method
#   x can be a matrix, a data frame or it can have list mode.
#   if y is not NULL, the function assumes x is a vector
#   and the goal is to compare the variances of the data in x and y.
#   To get an even more robust method using a  20% trimmed mean, set the argument tr=.2
#
#  By default, Hochberg's method is used to control the probability of one
#  or more TypeI errors
#
if(!is.null(y))x=list(x,y)
if(is.matrix(x) || is.data.frame(x))x=listm(x)
J=length(x)
for(j in 1:J)x[[j]]=elimna(x[[j]])
for(j in 1:J){
flag=abs(x[[j]])>invalid
if(sum(flag,na.rm=TRUE)>0){
print(paste('The value of argument invalid indicates that any value greater in absolute value than', invalid,' is invalid'))
print(paste('Variable', j, 'has one  or more invalid values'))
print('They occur in the following positions')
nr=c(1:length(x[[j]]))
print(nr[flag])
if(STOP)stop()
}
}
CC=(J^2-J)/2
output<-matrix(0,CC,7)
dimnames(output)<-list(NULL,c('Var','Var','RMSAE 1','RMSAE 2','Dif','p.value','Adj.p.value'))
ic=0
for (j in 1:J){
for (k in 1:J){
if (j < k){
ic=ic+1
a=yuenbt(x[[j]]^2,x[[k]]^2,tr=tr,nboot=nboot)
output[ic,1]=j
output[ic,2]=k
output[ic,3]=RMSAE(x[[j]])
output[ic,4]=RMSAE(x[[k]])
output[ic,5]=output[ic,3]-output[ic,4]
output[ic,6]=a$p.value
}}}
output[,7]=p.adjust(output[,6],method=method)
output
}




# ----------------------------------------------------------------------------

# oph.indepintervals

# ----------------------------------------------------------------------------

#' Compare Frequency Distributions Across Diopter Intervals (Ophthalmology)
#'
#' @description
#' Compares frequency distributions of prediction errors across predefined diopter
#' intervals using the KMS method for independent samples. Non-astigmatism version
#' that works with absolute values.
#'
#' @param m Matrix or data frame with J columns representing different formulas
#' @param method Multiple comparison method for p-value adjustment (default: 'holm')
#' @param invalid Maximum valid absolute value in diopters (default: 4)
#'
#' @details
#' Takes absolute values of data before comparison. Compares frequency distributions
#' at 8 fixed intervals: 0.25, 0.50, 0.75, 1, 1.25, 1.5, 1.75, and 2 diopters.
#' For each pairwise comparison, tests whether the cumulative frequencies differ
#' significantly at each interval.
#'
#' @return
#' List of matrices, one for each pairwise comparison. Each matrix contains interval
#' identifiers, test statistics, p-values, and adjusted p-values.
#'
#' @seealso \code{\link{oph.astig.indepintervals}}, \code{\link{oph.mcnemar}}
#'
#' @keywords ophthalmology
#' @export
oph.indepintervals<-function(m,method='holm',invalid=4){
#
#    For column of x, compare frequencies using KMS method
#
#
#  n: sample sizes
#  x is a matrix or data frame with 8 rows
#
#
E=list()
ic=0
m=abs(m)
J=ncol(m)
x=m
flag=abs(elimna(x))>invalid
if(sum(flag)>0){
nr=c(1:nrow(x))
if(sum(flag)>1){
print(paste('The value of argument invalid indicates that any value greater in absolute value than', invalid,' is invalid'))
print('The following rows have invalid values')
}
if(sum(flag)==1){
print(paste('The value of argument invalid indicates that any value greater in absolute value than', invalid,' is invalid'))
print('The following row has an  invalid value')
}
irow=NA
ic=0
N=nrow(x)
for(i  in 1:N){
iflag=abs(x[i,])>invalid
if(sum(iflag)>0){
ic=ic+1
irow[ic]=i
}}
print(irow)
stop()
}
id=matrix(NA,8,2)
x=matrix(NA,8,2)
INT=c(0.25,0.50, 0.75,1,1.25,1.5,1.75,2)
dimnames(id)=list(NULL,ncol=c('S1','S2'))
for (j in 1:J){
  for (k in 1:J){
    if (j < k){
      ic=ic+1
id[,1]=rep(j,8)
id[,2]=rep(k,8)
#  Next determine frequencies
S1=elimna(m[,j])
S2=elimna(m[,k])
n1=length(S1)
n2=length(S2)
for(L in 1:8){
x[L,1]=sum(S1<=INT[L])
x[L,2]=sum(S2<=INT[L])
}
a=srg1.vs.2(c(n1,n2),x)
Adj.p.value=p.adjust(a[,3],method=method)
E[[ic]]=cbind(id,a,Adj.p.value)
    }}}
E
}




# ----------------------------------------------------------------------------

# oph.mcnemar

# ----------------------------------------------------------------------------

#' Compare Prediction Formulas Using McNemar's Test (Ophthalmology)
#'
#' @description
#' Compares prediction formulas using McNemar's test for paired data. Tests whether
#' formulas differ in their frequencies at various diopter intervals. Non-astigmatism
#' version that works with absolute values.
#'
#' @param x Matrix or data frame with J columns representing different formulas
#' @param method Multiple comparison method for p-value adjustment (default: 'holm')
#' @param invalid Maximum valid absolute value in diopters (default: 4)
#'
#' @details
#' Takes absolute values of data before comparison. For each pair of formulas and
#' each diopter threshold (D = 0.25, 0.50, ..., 2.00), creates a 2x2 contingency
#' table and applies McNemar's test. Reports the number and percentage of cases
#' below each threshold for each formula pair.
#'
#' @return
#' List with length equal to number of diopter intervals. Each element is a matrix
#' containing:
#' \itemize{
#'   \item Diopter threshold (D)
#'   \item Formula identifiers
#'   \item Counts and percentages below threshold
#'   \item p-values and adjusted p-values
#' }
#'
#' @seealso \code{\link{oph.astig.mcnemar}}, \code{\link{oph.indepintervals}}
#'
#' @keywords ophthalmology
#' @export
oph.mcnemar<-function(x,method='holm',invalid=4){
#
# Astigmatism: compare prediction formulas
#
if(is.null(dim(x)))stop('x should be a matrix or data frame')
x=abs(x)
J=ncol(x)  #number of formulas
flag=max(abs(x),na.rm=TRUE)>invalid
if(flag){
nr=c(1:nrow(x))
if(sum(flag,na.rm=TRUE)>1){
print(paste('The value of argument invalid indicates that any value greater in absolute value than', invalid,' is invalid'))
print('The following rows have invalid values')
}
if(sum(flag,na.rm=TRUE)==1){
print(paste('The value of argument invalid indicates that any value greater in absolute value than', invalid,' is invalid'))
print('The following row has an  invalid value')
}
irow=NA
ic=0
N=nrow(x)
for(i  in 1:N){
iflag=abs(x[i,])>invalid
if(sum(iflag,na.rm=TRUE)>0){
ic=ic+1
irow[ic]=i
}}
print(irow)
stop()
}
CC=(J^2-J)/2
output<-matrix(0,CC,9)
dimnames(output)<-list(NULL,c('D', ' Var',  'N< ' ,  '%<',  'Var', 'N<',    '%< ',
'p.value','Adj.p.value'))
E=list()
TAB=list()
D=seq(.25,2,.25)  #D intervals from .25 to 2
for(L in 1:length(D)){
ic=0
for (j in 1:J){
for (k in 1:J){
if (j < k){
ic=ic+1
a=mat2table(x[,c(j,k)],D[L],D[L])
n1=sum(x[[j]]<=D[L],na.rm=TRUE)
pn1=mean(x[[j]]<=D[L],na.rm=TRUE)
n2=sum(x[[k]]<=D[L],na.rm=TRUE)
pn2=mean(x[[k]]<=D[L],na.rm=TRUE)
if(sum(is.na(a)>0))print(paste('No data for VAR',j,'VAR',k,'D=',D[L]))
if(sum(is.na(a))==0){
mct=mcnemar.test(a)
output[ic,1]=D[L]
output[ic,2]=j
output[ic,3]=n1
output[ic,4]=pn1
output[ic,5]=k
output[ic,6]=n2
output[ic,7]=pn2
output[ic,8]=mct[[3]]
if(a[1,2]==0 &a[2,1]==0)output[ic,8]=1
}}}}
output[,9]=p.adjust(output[,8],method=method)
E[[L]]=output
}
E
}





# ============================================================================

# Binomial/Binary (12 functions)

# ============================================================================


# ----------------------------------------------------------------------------

# bin.best

# ----------------------------------------------------------------------------

#' Select Best Group Based on Binomial Probability of Success
#'
#' @description
#' For J independent groups, identify the group with the highest probability of
#' success. A decision is made if all p-values are less than or equal to their
#' corresponding critical p-values. Uses family-wise error rate control.
#'
#' @param x Vector containing the number of successes for each group
#' @param n Vector indicating the sample sizes for each group
#' @param p.crit Critical p-values for each comparison. If NULL (default), critical
#'   p-values are determined via simulation to control FWE at alpha level
#' @param alpha Family-wise error rate (default 0.05)
#' @param iter Number of simulation iterations for determining critical p-values
#'   (default 5000)
#' @param SEED Logical; if TRUE, set random seed for reproducibility (default TRUE)
#'
#' @details
#' This function identifies which group has the largest probability of success
#' using a multiple comparison procedure that controls the family-wise error rate.
#' The group with the largest estimate is compared to all other groups. If all
#' comparisons are significant (p-value <= p.crit), a decision is made.
#'
#' When p.crit is NULL, critical p-values are determined using simulation under
#' the null hypothesis to achieve the desired FWE control at level alpha.
#'
#' @return
#' An S4 object of class 'BIN' with slots:
#' \itemize{
#'   \item Group.with.largest.estimate: Index of group with highest success rate
#'   \item Larger.than: Groups that the best group is significantly larger than.
#'     Returns 'No Decisions' if not significant, group indices if significant,
#'     or 'All' if best group is significantly larger than all others
#'   \item n: Sample sizes
#'   \item output: Matrix with columns Est.Best, Grp, Est, Dif, ci.low, ci.up,
#'     p.value, p.crit for each comparison
#' }
#'
#' @seealso \code{\link{bin.best.PV}}, \code{\link{bin.best.EQA}},
#'   \code{\link{bi2KMSv2}}
#'
#' @references
#' Wilcox, R.R. (2022) Introduction to Robust Estimation and Hypothesis Testing,
#' 5th Ed. Academic Press.
#'
#' @keywords binomial htest
#' @export
bin.best<-function(x,n,p.crit=NULL,alpha=.05,iter=5000,SEED=TRUE){
#
# For J independent groups,
#  identify the group with highest probability of success.
#  Make a decision if every  p.value<=p.crit
#
#  x is a vector containing the number of successes.
#  n is a vector indicating the sample sizes.
#
#  p.crit:  If NULL, critical p-values are determined so that that FWE is alpha
#  This is done using a simulation to determine the null distribution based on
#  iter=5000 replications.
#
#
#  Returns:
#   Best='No Decision' if not significant
#   Best= the group with largest measure if a decision can be made.
#
#   Confidence intervals having simultaneous probability coverage 1-alpha
#  using the adjusted level.
#
J=length(x)
if(J<2)stop('Should have 2 or more groups')
Jm1=J-1
est=x/n
R=order(est,decreasing = TRUE)
pvec=NA
if(is.null(p.crit)){
phat=sum(x)/sum(n)
pv.mat=bin.best.crit(phat,n=n,iter=iter,SEED=SEED)
init=apply(pv.mat,2,qest,alpha)
z=optim(0,anc.best.fun,init=init,iter=iter,rem=pv.mat,Jm1=Jm1,alpha=alpha,method='Brent',lower=0,upper=1)
p.crit=z$par*init
}
output<-matrix(NA,Jm1,8)
dimnames(output)=list(NULL,c('Est.Best','Grp','Est','Dif','ci.low','ci.up','p.value','p.crit'))
for(i in 2:J){
im1=i-1
a=bi2KMSv2(x[R[1]],n[R[1]],x[R[i]],n[R[i]],alpha=p.crit[im1])
pvec[im1]=a$p.value
output[im1,]=c(a$p1, R[i], a$p2,a$est.dif,a$ci[1],a$ci[2],a$p.value,p.crit[im1])
}

Best='No Decisions'
flag=sum(output[,7]<=output[,8])
id=output[,7]<=output[,8]
if(sum(id>0))Best=output[id,2]
if(flag==Jm1)Best='All'
#setClass('BIN',slots=c('Group.with.largest.estimate','Select.Best.p.value','Larger.than','n','output'))  #not sure select p.value is valid
#put=new('BIN',Group.with.largest.estimate=R[[1]],Select.Best.p.value=dpv,Larger.than=Best,n=n,output=output)
setClass('BIN',slots=c('Group.with.largest.estimate','Larger.than','n','output'))
put=new('BIN',Group.with.largest.estimate=R[[1]],Larger.than=Best,n=n,output=output)
put
}

#' Generate Critical P-Values for bin.best via Simulation
#'
#' @description
#' Internal function used by \code{\link{bin.best}} to determine critical p-values
#' via simulation under the null hypothesis.
#'
#' @param p Probability of success under the null hypothesis (common across groups)
#' @param n Vector of sample sizes for each group
#' @param iter Number of simulation iterations (default 5000)
#' @param SEED Logical; if TRUE, set random seed for reproducibility (default TRUE)
#'
#' @return
#' Matrix with iter rows and J-1 columns, where J is the number of groups.
#' Each row contains p-values from one simulated dataset under the null.
#'
#' @seealso \code{\link{bin.best}}
#'
#' @keywords internal
#' @export
bin.best.crit<-function(p,n,iter=5000,SEED=TRUE){
#
#
#
if(SEED)set.seed(2)
J=length(n)  #Number of groups
Jm1=J-1
pv.mat=matrix(NA,iter,Jm1)
for(i in 1:iter){
x=rbinom(J,n,p)
pv.mat[i,]=bin.best.sub(x,n)
}
pv.mat
}

#' Compute P-Values for bin.best.crit Simulation
#'
#' @description
#' Internal helper function used by \code{\link{bin.best.crit}} to compute
#' p-values for each simulated dataset.
#'
#' @param x Vector containing the number of successes for each group
#' @param n Vector indicating the sample sizes for each group
#' @param p.crit Critical p-values (not used in this helper function)
#' @param alpha Significance level (default 0.05)
#' @param iter Number of iterations (default 5000)
#' @param SEED Logical; if TRUE, set random seed (default TRUE)
#'
#' @return
#' Vector of p-values from comparing the group with largest estimate to all others.
#'
#' @seealso \code{\link{bin.best.crit}}
#'
#' @keywords internal
#' @export
bin.best.sub<-function(x,n,p.crit=NULL,alpha=.05,iter=5000,SEED=TRUE){
#
#  Used by bin.best.crit
#
#  x is a vector containing the number of successes.
#  n is a vector indicating the sample sizes.
#
#
J=length(x)
if(J<3)stop('Should have 3 or more groups')
Jm1=J-1
est=x/n
R=order(est,decreasing = TRUE)
pvec=NA
for(i in 2:J){
im1=i-1
a=bi2KMSv2(x[R[1]],n[R[1]],x[R[i]],n[R[i]],alpha=p.crit[im1])
pvec[im1]=a$p.value
}
pvec=as.vector(matl(pvec))
pvec
}


#' Select Best Group with Overall P-Value
#'
#' @description
#' For J independent groups, identify the group with the highest probability of
#' success. This version also computes an overall p-value for the decision.
#' A decision is made if all pairwise p-values are less than or equal to their
#' corresponding critical p-values.
#'
#' @param x Vector containing the number of successes for each group
#' @param n Vector indicating the sample sizes for each group
#' @param alpha Family-wise error rate (default 0.05). Must be one of the values
#'   .001(.001).1 or .11(.01).99
#' @param iter Number of simulation iterations for determining critical p-values
#'   (default 5000)
#' @param SEED Logical; if TRUE, set random seed for reproducibility (default TRUE)
#'
#' @details
#' Similar to \code{\link{bin.best}}, but also computes an overall p-value for
#' the selection decision. The critical p-values are pre-computed for a range of
#' alpha levels using \code{\link{bin.best.crit.det}}.
#'
#' The overall p-value represents the smallest alpha level at which all comparisons
#' would be significant.
#'
#' @return
#' An S4 object of class 'BIN' with slots:
#' \itemize{
#'   \item Group.with.largest.estimate: Index of group with highest success rate
#'   \item Select.Best.p.value: Overall p-value for the selection decision
#'   \item Larger.than: Groups that the best group is significantly larger than
#'   \item n: Sample sizes
#'   \item output: Matrix with comparison results
#' }
#'
#' @seealso \code{\link{bin.best}}, \code{\link{bin.best.EQA}},
#'   \code{\link{bin.best.crit.det}}
#'
#' @keywords binomial htest
#' @export
bin.best.PV<-function(x,n,alpha=.05,iter=5000,SEED=TRUE){
#
# For J independent groups,
#  identify the group with highest probability of success.
#  Make a decision if every  p.value<=p.crit
#
#  x is a vector containing the number of successes.
#  n is a vector indicating the sample sizes.
#
#  p.crit:  If NULL, critical p-values are determined so that that FWE is alpha
#  This is done using a simulation to determine the null distribution based on
#  iter=5000 replications.
#
#
#  Returns:
#   Best='No Decision' if not significant
#   Best= the group with largest measure if a decision can be made.
#
#   Confidence intervals having simultaneous probability coverage 1-alpha
#  using the adjusted level.
#
J=length(x)
if(J<2)stop('Should have 2 or more groups')
Jm1=J-1
est=x/n
R=order(est,decreasing = TRUE)
pvec=NA

phat=sum(x)/sum(n)


aval=c(seq(.001,.1,.001),seq(.11,.99,.01))
id=which(aval==alpha)
if(length(id)==0)stop('alpha be one one values .001(.001).1 or 11(.01).99')

v=bin.best.crit.det(phat,n=n,iter=iter,SEED=SEED)
p.crit=v[id,]


output<-matrix(NA,Jm1,8)
dimnames(output)=list(NULL,c('Est.Best','Grp','Est','Dif','ci.low','ci.up','p.value','p.crit'))
for(i in 2:J){
im1=i-1
a=bi2KMSv2(x[R[1]],n[R[1]],x[R[i]],n[R[i]],alpha=p.crit[im1])
pvec[im1]=a$p.value
output[im1,1:7]=c(a$p1, R[i], a$p2,a$est.dif,a$ci[1],a$ci[2],a$p.value)
}
output[,8]=p.crit


# Determine p-value for overall decision
na=length(aval)
for(i in 1:na){
chk=sum(output[,7]<=v[i,])
pv=aval[i]
if(chk==Jm1)break
}
Best='No Decisions'
flag=sum(output[,7]<=output[,8])
id=output[,7]<=output[,8]
if(sum(id>0))Best=output[id,2]
if(flag==Jm1)Best='All'
setClass('BIN',slots=c('Group.with.largest.estimate','Select.Best.p.value','Larger.than','n','output'))
put=new('BIN',Group.with.largest.estimate=R[[1]],Select.Best.p.value=pv,Larger.than=Best,n=n,output=output)
put
}


#' Determine Critical P-Values for Range of Alpha Levels
#'
#' @description
#' Pre-compute critical p-values for a range of alpha levels (.001 to .99).
#' Used by \code{\link{bin.best.PV}} to enable p-value computation.
#'
#' @param p Probability of success under the null hypothesis (common across groups)
#' @param n Vector of sample sizes for each group
#' @param iter Number of simulation iterations (default 5000)
#' @param SEED Logical; if TRUE, set random seed for reproducibility (default TRUE)
#'
#' @return
#' Matrix with rows corresponding to alpha levels (.001, .002, ..., .1, .11, ..., .99)
#' and J-1 columns (where J is number of groups). Each entry is a critical p-value.
#'
#' @seealso \code{\link{bin.best.PV}}, \code{\link{bin.best.crit}}
#'
#' @keywords internal
#' @export
bin.best.crit.det<-function(p,n,iter=5000,SEED=TRUE){
#
#
#
if(SEED)set.seed(2)
J=length(n)  #Number of groups
Jm1=J-1
pv.mat=matrix(NA,iter,Jm1)
for(i in 1:iter){
x=rbinom(J,n,p)
pv.mat[i,]=bin.best.sub(x,n)
}
rem=pv.mat
aval=c(seq(.001,.1,.001),seq(.011,.99,.01))
na=length(aval)
fin.crit=matrix(NA,na,Jm1)
for(i in 1:na){
init=apply(rem,2,qest,aval[i])
z=optim(0,anc.best.fun,init=init,iter=iter,rem=rem,Jm1=Jm1,alpha=aval[i],method='Brent',lower=0,upper=1)
fin.crit[i,]=z$par*init
}
fin.crit
}




# ----------------------------------------------------------------------------

# bin.best.EQA

# ----------------------------------------------------------------------------

#' Select Best Group Using Equal Alpha Method
#'
#' @description
#' For J independent groups, identify the group with the highest probability of
#' success using equal critical p-values (alpha) for all comparisons. This is
#' an alternative to \code{\link{bin.best}} which uses unequal critical p-values.
#'
#' @param x Vector containing the number of successes for each group
#' @param n Vector indicating the sample sizes for each group
#' @param p.crit Critical p-value (same for all comparisons). If NULL (default),
#'   determined via simulation to control FWE at alpha level
#' @param alpha Family-wise error rate (default 0.05)
#' @param iter Number of simulation iterations for determining critical p-value
#'   (default 5000)
#' @param SEED Logical; if TRUE, set random seed for reproducibility (default TRUE)
#'
#' @details
#' Unlike \code{\link{bin.best}} which may use different critical p-values for
#' different comparisons, this function uses a single critical p-value for all
#' J-1 comparisons (equal quantile approach, EQA).
#'
#' The group with the largest estimate is compared to all other groups. A decision
#' is made if all comparisons have p-value <= p.crit.
#'
#' @return
#' An S4 object of class 'BIN' with slots:
#' \itemize{
#'   \item Group.with.largest.estimate: Index of group with highest success rate
#'   \item Larger.than: Groups that the best group is significantly larger than
#'   \item n: Sample sizes
#'   \item output: Matrix with comparison results
#' }
#'
#' @seealso \code{\link{bin.best}}, \code{\link{bin.best.PV}}
#'
#' @keywords binomial htest
#' @export
bin.best.EQA<-function(x,n,p.crit=NULL,alpha=.05,iter=5000,SEED=TRUE){
#
#
#  Identify the group with highest probability of success.
#  Make a decision if every  p.value<=p.crit
#
#  x is a vector containing the number of successes.
#  n is a vector indicating the sample sizes.
#
#  p.crit:  If NULL, critical p-values are determined so that that FWE is alpha
#  This is done using a simulation to determine the null distribution based on
#  iter=5000 replications.
#
#
#  Returns:
#   Best='No Decision' if not significant
#   Best= the group with largest measure if a decision can be made.
#
#   Confidence intervals having simultaneous probability coverage 1-alpha
#  using the adjusted level.
#
J=length(x)
if(J<2)stop('Should have 2 or more groups')
Jm1=J-1
est=x/n
R=order(est,decreasing = TRUE)
pvec=NA
init=rep(alpha,Jm1)
if(is.null(p.crit)){
phat=sum(x)/sum(n)
pv.mat=bin.best.crit(phat,n=n,iter=iter,SEED=SEED)
z=optim(0,anc.best.fun,init=init,iter=iter,rem=pv.mat,Jm1=Jm1,alpha=alpha,method='Brent',lower=0,upper=1)
p.crit=z$par*init
}
output<-matrix(NA,Jm1,8)
dimnames(output)=list(NULL,c('Est.Best','Grp','Est','Dif','ci.low','ci.up','p.value','p.crit'))
for(i in 2:J){
im1=i-1
a=bi2KMSv2(x[R[1]],n[R[1]],x[R[i]],n[R[i]],alpha=p.crit[im1])
pvec[im1]=a$p.value
output[im1,]=c(a$p1, R[i], a$p2,a$est.dif,a$ci[1],a$ci[2],a$p.value,p.crit[im1])
}
Best='No Decisions'
flag=sum(output[,7]<=output[,8])
id=output[,7]<=output[,8]
if(sum(id>0))Best=output[id,2]
if(flag==Jm1)Best='All'
setClass('BIN',slots=c('Group.with.largest.estimate','Larger.than','n','output'))
put=new('BIN',Group.with.largest.estimate=R[[1]],Larger.than=Best,n=n,output=output)
put
}



#' Binomial Multiple Comparisons Helper Function
#'
#' @description
#' Internal helper function for binomial multiple comparisons procedures.
#' Performs all pairwise comparisons among J independent groups.
#'
#' @param x Vector containing the number of successes for each group
#' @param n Vector indicating the sample sizes for each group
#' @param p.crit Critical p-value for comparisons (default 0.05)
#' @param alpha Significance level (default 0.05)
#' @param iter Number of iterations (default 5000)
#' @param SEED Logical; if TRUE, set random seed (default TRUE)
#'
#' @details
#' Performs all J(J-1)/2 pairwise comparisons with confidence intervals having
#' simultaneous probability coverage 1-alpha using the adjusted level p.crit.
#'
#' @return
#' Vector of p-values from all pairwise comparisons.
#'
#' @seealso \code{\link{binpair}}
#'
#' @keywords internal
#' @export
binmcp.sub<-function(x,n,p.crit=.05,alpha=.05,iter=5000,SEED=TRUE){
#
#
#  x is a vector containing the number of successes.
#  n is a vector indicating the sample sizes.
#
#  p.crit:  If NULL, critical p-values are determined so that that FWE is alpha
#  This is done using a simulation to determine the null distribution based on
#  iter=5000 replications.
#
#   Confidence intervals having simultaneous probability coverage 1-alpha
#  using the adjusted level.
#
J=length(x)
if(J<2)stop('Should have 2 or more groups')
est=x/n
A=(J^2-J)/2
output<-matrix(NA,A,9)
dimnames(output)=list(NULL,c('Grp','Grp','Est 1','Est 2','Dif','ci.low','ci.up','p.value','p.crit'))

ic=0
for(j in 1:J){
for(k in 1:J){
if(j<k){
ic=ic+1
a=bi2KMSv2(x[j],n[j],x[k],n[k],alpha=p.crit[1])
output[ic,]=c(j,k,a$p1, a$p2,a$est.dif,a$ci[1],a$ci[2],a$p.value,p.crit[1])
}}}
output[,8]
}




# ----------------------------------------------------------------------------

# bin.PMD.PCD

# ----------------------------------------------------------------------------

#' Probability of Making Decision and Probability of Correct Decision
#'
#' @description
#' For J independent binomial groups, determine the probability of making a
#' decision (PMD) and the probability of correct decision given that a decision
#' is made (PCD) when selecting which group has the largest probability of success.
#'
#' @param n Vector of sample sizes for each group
#' @param p Vector of true probabilities of success for each group
#' @param DO Logical; if TRUE (default), use decision-only method; if FALSE,
#'   use bin.best method
#' @param alpha Family-wise error rate (default 0.05)
#' @param p.crit Critical p-values. If NULL (default), determined via simulation
#' @param iter Number of simulation iterations (default 5000)
#' @param SEED Logical; if TRUE, set random seed for reproducibility (default TRUE)
#'
#' @details
#' This function assesses the operating characteristics of the best group selection
#' procedure. It uses simulation to estimate:
#' \itemize{
#'   \item PMD: Probability of Making a Decision (not concluding "no decision")
#'   \item PCD: Probability of Correct Decision (correctly identifying the best
#'     group when a decision is made)
#' }
#'
#' The function generates iter datasets from binomial distributions with the
#' specified probabilities p and sample sizes n, then applies the selection
#' procedure to each dataset.
#'
#' @return
#' List with components:
#' \itemize{
#'   \item PMD: Estimated probability of making a decision
#'   \item PCD: Estimated probability of correct decision given a decision is made
#' }
#'
#' @seealso \code{\link{bin.best}}, \code{\link{bin.best.DO}}
#'
#' @keywords binomial htest
#' @export
bin.PMD.PCD<-function(n,p,DO=TRUE,alpha=.05,p.crit=NULL,iter=5000,SEED=TRUE){
#
#  Which group has the largest probability of success?
#
#  Use an indifference zone. Given
#  n a vector of sample sizes, determine the
#  probability of making a decision and the probability of
#  of correct decision given that a decision is made.
#
#  Number of groups is length(n)
#  x if specified contain the number of succcess in which case p=mean(x/n)
#

if(SEED)set.seed(2)
pmax=which(p==max(p))
J=length(n)
if(!is.null(x))p=rep(mean(x/n),J)
Jm1=J-1
if(J<=1)stop('n should have 2 or more values')
remp=p
id=which(p==max(p))[1]
if(is.null(p.crit)){
pv.mat=bin.best.crit(remp,n,iter=iter,SEED=FALSE)
init=apply(pv.mat,2,qest,alpha)
z=optim(0,anc.best.fun,init=init,iter=iter,rem=pv.mat,Jm1=Jm1,alpha=alpha,method='Brent',lower=0,upper=1)
p.crit=z$par*init
}
PMD=0
PCD=0
for(i in 1:iter){
x=rbinom(J,n,p)
if(!DO){
pv=bin.best(x,n,p.crit=p.crit,SEED=FALSE)
if(pv@Larger.than[1]=='All'){
PMD=PMD+1
if(pv@Group.with.largest.estimate==id)PCD=PCD+1
}}
if(DO){
a=bin.best.DO(x,n)
if(a$p.value<=alpha){
PMD=PMD+1
if(max(a$Est)==pmax)PCD=PCD+1
}}
}
PCD=PCD/PMD
PMD=PMD/iter
list(PMD=PMD,PCD=PCD)
}


#' Extract Matrix Rows Based on Binary Split Criteria
#'
#' @description
#' Extract rows from a matrix based on binary split criteria applied to two
#' variables. Useful for creating binary categorizations based on medians or
#' other cutpoints.
#'
#' @param m Matrix or data frame to extract rows from
#' @param col Vector of two column indices specifying which columns to use for
#'   splitting criteria (default c(1,2))
#' @param int1 Interval bounds for first variable as c(lower, upper) (default
#'   c(.5, .5) for median split)
#' @param int2 Interval bounds for second variable as c(lower, upper) (default
#'   c(.5, .5) for median split)
#' @param INC Logical; if TRUE (default), use <= and >= comparisons; if FALSE,
#'   use < and > comparisons
#'
#' @details
#' This function pulls out rows of matrix m where:
#' \itemize{
#'   \item Column col[1] satisfies: int1[1] <= value <= int1[2] (if INC=TRUE)
#'   \item Column col[2] satisfies: int2[1] <= value <= int2[2] (if INC=TRUE)
#' }
#'
#' By default, splits at the median for both variables (0.5, 0.5). Can be used
#' to create 2x2 contingency tables from continuous data.
#'
#' @return
#' Matrix or vector containing the rows of m that satisfy both criteria.
#'
#' @keywords binomial utilities
#' @export
binmat2v<-function(m,col=c(1,2),int1=c(.5,.5),int2=c(.5,.5),INC=TRUE){
#
# pull out the rows of the matrix m based on the values in the column
# indicated by the argument
#  int1 indicates intervals for first variable
#  int2 indicates intervals for second variable
#  By default,  split at the median for both variables.
#
# col  indicates the columns of m by which the splits are made.
#
if(is.vector(m)){
m=as.matrix(m)
col=NA
}
if(INC){
flag1=m[,col[1]]<=int1[1]
flag2=m[,col[1]]>=int1[2]
flag3=m[,col[2]]<=int2[1]
flag4=m[,col[2]]>=int2[2]
}
if(!INC){
flag1=m[,col[1]]<int1[1]
flag2=m[,col[1]]>int1[2]
flag3=m[,col[2]]<int2[1]
flag4=m[,col[2]]>int2[2]

}
flag=as.logical(flag1*flag2*flag3*flag4)
m[flag,]
}

#' Compare Two Independent Binomial Groups
#'
#' @description
#' Compare proportions from two independent binomial groups using various methods.
#' Provides confidence interval for difference in proportions and p-value.
#'
#' @param r1 Number of successes in group 1 (computed from x if x is provided)
#' @param n1 Sample size for group 1 (computed from x if x is provided)
#' @param r2 Number of successes in group 2 (computed from y if y is provided)
#' @param n2 Sample size for group 2 (computed from y if y is provided)
#' @param x Vector of 1s and 0s for group 1 (optional; if provided, r1 and n1
#'   are computed from x)
#' @param y Vector of 1s and 0s for group 2 (optional; if provided, r2 and n2
#'   are computed from y)
#' @param method Character string specifying method: 'KMS' (Kulinskaya et al.,
#'   default), 'ECP' (empirical critical p-value), 'SK' (Storer-Kim), or
#'   'ZHZ' (Zou et al.)
#' @param binCI Function to compute binomial CI for ZHZ method (default acbinomci)
#' @param alpha Significance level (default 0.05)
#' @param null.value Null hypothesis value for difference (default 0)
#' @param iter Number of iterations for ECP method (default 2000)
#' @param SEED Logical; if TRUE, set random seed for ECP method (default TRUE)
#'
#' @details
#' Four methods are available:
#' \itemize{
#'   \item KMS: Method from Kulinskaya et al. (2008)
#'   \item ECP: Empirical Critical P-value method (often performs best)
#'   \item SK: Storer-Kim method
#'   \item ZHZ: Zou, Hao, and Zhang method
#' }
#'
#' For ZHZ method, binCI can be set to: binomci (Pratt), binomCP (Clopper-Pearson),
#' kmsbinomci (Kulinskaya et al.), wilbinomci (Wilson), binomLCO (Schilling-Doi).
#'
#' @return
#' List with results depending on method. Typically includes:
#' \itemize{
#'   \item Estimates for each group
#'   \item Confidence interval for difference
#'   \item P-value
#'   \item Sample sizes
#' }
#'
#' @seealso \code{\link{binpair}}, \code{\link{binom2g.ZHZ}}, \code{\link{bi2KMSv2}}
#'
#' @references
#' Kulinskaya, E., Morgenthaler, S., and Staudte, R. G. (2008).
#' Meta Analysis: A Guide to Calibrating and Combining Statistical Evidence.
#' Wiley.
#'
#' @keywords binomial htest
#' @export
binom2g<-function(r1 = sum(elimna(x)), n1 = length(elimna(x)), r2 = sum(elimna(y)),
 n2 = length(elimna(y)), x = NA, y = NA, method=c('KMS','ECP','SK','ZHZ'),binCI=acbinomci,alpha = 0.05, null.value = 0,iter=2000,SEED=TRUE){
#
#  r: vector contain number of successes
#  n: corresponding sample size
#   x: contains 1s and 0s for first group.
#   y: contains 1s and 0s for second group.
#   It is assumed that r and n are specified or x and y are specified.
#   If x and y are  specified, r1, r2, n1 and n2 are computed based on the data in x and y
#   Otherwise r and n are used.
#
# ECP: empirical estimate of the critical p-value, seems best in general
#
#  binCI is used by ZHZ and defaults to Agresti--Coull
#  Other choices for binCI:
#  binomci:  Pratt's method
#  binomCP:  Clopper--Pearson
# kmsbinomci:  Kulinskaya et al
#  wilbinomci:  Wilson
#  binomLCO:  Schilling--Doi
#
#
#
type=match.arg(method)
switch(type,
   ECP=binmcp(c(r1,r2),n=c(n1,n2),alpha=alpha,iter=iter,SEED=SEED),
   ZHZ=binom2g.ZHZ(r1,n1,r2,n2,binCI=binCI,alpha=alpha),
    KMS=bi2KMSv2(r1,n1,r2,n2,x=x,y=y,alpha=alpha),
    SK=twobinom(r1,n1,r2,n2,x=x,y=y),
    )
}




# ----------------------------------------------------------------------------

# binom2g.ZHZ

# ----------------------------------------------------------------------------

#' Compare Two Binomial Groups Using Zou et al. Method
#'
#' @description
#' Compare two binomial proportions using the Zou, Hao, and Zhang (2009) method.
#' Computes confidence interval and p-value for difference in proportions.
#'
#' @param r1 Number of successes in group 1 (computed from x if x provided)
#' @param n1 Sample size for group 1 (computed from x if x provided)
#' @param r2 Number of successes in group 2 (computed from y if y provided)
#' @param n2 Sample size for group 2 (computed from y if y provided)
#' @param x Vector of 1s and 0s for group 1 (optional)
#' @param y Vector of 1s and 0s for group 2 (optional)
#' @param nullval Hypothesized difference in proportions (default 0)
#' @param alpha Significance level (default 0.05)
#' @param WARN Logical; if TRUE (default), warn when min(n1,n2) < 40
#' @param method Method for computing binomial CI: 'AC' (Agresti-Coull, default)
#'   or other methods available in binom.conf
#'
#' @details
#' Implements the method from Zou, Hao, and Zhang (2009) published in CSDA.
#' The method combines confidence intervals for individual proportions to
#' construct a CI for their difference.
#'
#' When minimum sample size is less than 40, methods 'SK' or 'KMS' may be
#' more satisfactory.
#'
#' @return
#' List with components:
#' \itemize{
#'   \item n1, n2: Sample sizes
#'   \item p1.hat, p2.hat: Estimated proportions
#'   \item dif: Difference in proportions (p1.hat - p2.hat)
#'   \item ci: Confidence interval for difference
#'   \item p.value: P-value for test of H0: difference = nullval
#' }
#'
#' @seealso \code{\link{binom2g}}, \code{\link{binom2g.ZHZ.main}}
#'
#' @references
#' Zou, G., Hao, J., and Zhang, J. (2009). Construction of confidence limits
#' about effect measures: A general approach. Statistics in Medicine, 28,
#' 1693-1702.
#'
#' @keywords binomial htest
#' @export
binom2g.ZHZ<-function(r1=sum(elimna(x)),n1=length(elimna(x)),
r2=sum(elimna(y)),n2 = length(elimna(y)), x = NA, y = NA, nullval=0,alpha=.05,WARN=TRUE,
method='AC'){
#
#  Compare two binomials using the method in Zou et al.2009 CSDA.
#
#  x and y are vectors of 1s and 0s.
#  Or can use the argument
#  r1 = the number of successes observed among group 1
#  r2 = the number of successes observed among group 2
#  n1 = sample size for group 1
#  n2 = sample size for group 2
#
#  nullval is the hypothesized difference
#
if(min(n1,n2)<40){
if(WARN)print('Minimum sample size is less than or equal  to 40, methods SK and  KMS might be more satisfactory')
}
ci=binom2g.ZHZ.main(r1=r1,n1=n1,r2=r2,n2=n2,alpha=alpha)
p1.hat=r1/n1
p2.hat=r2/n2
alph<-c(1:99)/100
for(i in 1:99){
irem<-i
chkit<-binom2g.ZHZ.main(r1=r1,n1=n1,r2=r2,n2=n2,alpha=alph[i])
if(chkit[1]>nullval || chkit[2]<nullval)break
}
p.value<-irem/100
if(p.value<=.1){
iup<-(irem+1)/100
alph<-seq(.001,iup,.001)
for(i in 1:length(alph)){
p.value<-alph[i]
chkit<-binom2g.ZHZ.main(r1=r1,n1=n1,r2=r2,n2=n2,alpha=alph[i])
if(chkit[1]>nullval || chkit[2]<nullval)break
}}
if(p.value<=.001){
alph<-seq(.0001,.001,.0001)
for(i in 1:length(alph)){
p.value<-alph[i]
chkit<-binom2g.ZHZ.main(r1=r1,n1=n1,r2=r2,n2=n2,alpha=alph[i])
if(chkit[1]>nullval || chkit[2]<nullval)break
}}

list(n1=n1,n2=n2,p1.hat=p1.hat,p2.hat=p2.hat,dif=p1.hat-p2.hat,ci=ci,p.value=p.value)
}




# ----------------------------------------------------------------------------

# binomci

# ----------------------------------------------------------------------------

#' Binomial Confidence Interval Using Pratt's Method
#'
#' @description
#' Compute a confidence interval for the probability of success in a binomial
#' distribution using Pratt's method.
#'
#' @param x Number of successes (computed from y if y is provided)
#' @param nn Sample size (computed from y if y is provided)
#' @param y Vector of 1s and 0s (optional)
#' @param n Deprecated parameter (use nn instead)
#' @param alpha Significance level (default 0.05)
#'
#' @details
#' Implements Pratt's method for computing binomial confidence intervals.
#' Pratt's method provides good coverage properties and handles boundary
#' cases (x=0, x=1, x=n-1, x=n) with special formulas.
#'
#' @return
#' List with components:
#' \itemize{
#'   \item phat: Estimated proportion (x/n)
#'   \item ci: Confidence interval as c(lower, upper)
#'   \item n: Sample size
#' }
#'
#' @seealso \code{\link{binomcipv}}, \code{\link{binomCP}}, \code{\link{binomLCO}}
#'
#' @references
#' Pratt, J. W. (1968). A normal approximation for binomial, F, beta, and other
#' common related tail probabilities. Journal of the American Statistical
#' Association, 63, 1457-1483.
#'
#' @keywords binomial htest
#' @export
binomci<-function(x=sum(y),nn=length(y),y=NULL,n=NA,alpha=.05){
#  Compute a 1-alpha confidence interval for p, the probability of
#  success for a binomial distribution, using Pratt's method
#
#  y is a vector of 1s and 0s.
#  x is the number of successes observed among n trials
#
if(!is.null(y)){
y=elimna(y)
nn=length(y)
}
if(nn==1)stop("Something is wrong: number of observations is only 1")
n<-nn
if(x!=n && x!=0){
z<-qnorm(1-alpha/2)
A<-((x+1)/(n-x))^2
B<-81*(x+1)*(n-x)-9*n-8
C<-(0-3)*z*sqrt(9*(x+1)*(n-x)*(9*n+5-z^2)+n+1)
D<-81*(x+1)^2-9*(x+1)*(2+z^2)+1
E<-1+A*((B+C)/D)^3
upper<-1/E
A<-(x/(n-x-1))^2
B<-81*x*(n-x-1)-9*n-8
C<-3*z*sqrt(9*x*(n-x-1)*(9*n+5-z^2)+n+1)
D<-81*x^2-9*x*(2+z^2)+1
E<-1+A*((B+C)/D)^3
lower<-1/E
}
if(x==0){
lower<-0
upper<-1-alpha^(1/n)
}
if(x==1){
upper<-1-(alpha/2)^(1/n)
lower<-1-(1-alpha/2)^(1/n)
}
if(x==n-1){
lower<-(alpha/2)^(1/n)
upper<-(1-alpha/2)^(1/n)
}
if(x==n){
lower<-alpha^(1/n)
upper<-1
}
phat<-x/n
list(phat=phat,ci=c(lower,upper),n=n)
}




# ----------------------------------------------------------------------------

# binomcipv

# ----------------------------------------------------------------------------

#' Binomial P-Value Using Pratt's Method
#'
#' @description
#' Compute a p-value for testing whether the probability of success equals a
#' specified null value, using Pratt's method for the confidence interval.
#'
#' @param x Number of successes (computed from y if y is provided)
#' @param nn Sample size (computed from y if y is provided)
#' @param y Vector of 1s and 0s (optional)
#' @param n Deprecated parameter (use nn instead)
#' @param alpha Significance level (default 0.05)
#' @param nullval Null hypothesis value for probability of success (default 0.5)
#'
#' @details
#' Computes a p-value by finding the smallest alpha level at which the
#' confidence interval (using Pratt's method) excludes the null value.
#' This provides an exact correspondence between the CI and the p-value.
#'
#' @return
#' List with components:
#' \itemize{
#'   \item n: Sample size
#'   \item phat: Estimated proportion (x/n)
#'   \item ci: Confidence interval at specified alpha level
#'   \item p.value: P-value for test of H0: p = nullval
#' }
#'
#' @seealso \code{\link{binomci}}, \code{\link{binom.conf.pv}}
#'
#' @keywords binomial htest
#' @export
binomcipv<-function(x=sum(y),nn=length(y),y=NULL,n=NA,alpha=.05,nullval=.5){
#  Compute a p-value when testing the hypothesis that the probability of
#  success for a binomial distribution is equal to
#  nullval, which defaults to .5
#  Pratt's  method is used.
#
#  y is a vector of 1s and 0s.
#  Or can use the argument
#  x = the number of successes observed among
#  n=nn trials.
#
if(is.logical(y)){
y=elimna(y)
temp=rep(0,length(y))
temp[y]=1
y=temp
}
res=binomci(x=x,nn=nn,y=y,alpha=alpha)
ci=res$ci
alph<-c(1:99)/100
for(i in 1:99){
irem<-i
chkit<-binomci(x=x,nn=nn,y=y,alpha=alph[i])$ci
if(chkit[1]>nullval || chkit[2]<nullval)break
}
p.value<-irem/100
if(p.value<=.1){
iup<-(irem+1)/100
alph<-seq(.001,iup,.001)
for(i in 1:length(alph)){
p.value<-alph[i]
chkit<-binomci(x=x,nn=nn,y=y,alpha=alph[i])$ci
if(chkit[1]>nullval || chkit[2]<nullval)break
}}
if(p.value<=.001){
alph<-seq(.0001,.001,.0001)
for(i in 1:length(alph)){
p.value<-alph[i]
chkit<-binomci(x=x,nn=nn,y=y,alpha=alph[i])$ci
if(chkit[1]>nullval || chkit[2]<nullval)break
}}
list(n=nn,phat=res$phat,ci=res$ci,p.value=p.value)
}

#' Binomial Confidence Interval Using Clopper-Pearson Method
#'
#' @description
#' Compute a confidence interval for the probability of success using the
#' exact Clopper-Pearson method (also called exact binomial).
#'
#' @param x Number of successes (computed from y if y is provided)
#' @param nn Sample size (computed from y if y is provided)
#' @param y Vector of 1s and 0s (optional)
#' @param n Deprecated parameter (use nn instead)
#' @param alpha Significance level (default 0.05)
#'
#' @details
#' The Clopper-Pearson method provides exact coverage (conservative) by
#' inverting binomial tests. It uses R's built-in binom.test function.
#'
#' @return
#' List with components:
#' \itemize{
#'   \item phat: Estimated proportion (x/n)
#'   \item ci: Confidence interval as c(lower, upper)
#'   \item n: Sample size
#' }
#'
#' @seealso \code{\link{binomci}}, \code{\link{binomLCO}}
#'
#' @keywords binomial htest
#' @export
binomCP<-function(x = sum(y), nn = length(y), y = NULL, n = NA, alpha = 0.05){
#
# Clopper-Pearson
#
#  y is a vector of 1s and 0s.
#  x is the number of successes observed among n trials
#
if(!is.na(n))nn=n
q=binom.test(x,nn,conf.level=1-alpha)[4]
ci=q$conf.int[1:2]
list(phat=x/nn,ci=ci,n=nn)
}




# ----------------------------------------------------------------------------

# binomECP

# ----------------------------------------------------------------------------

#' Compare Two Binomials Using Empirical Critical P-Value
#'
#' @description
#' Compare two independent binomial groups using the empirical critical p-value
#' (ECP) method. This is a wrapper for binmcp with two groups.
#'
#' @param x1 Number of successes in group 1
#' @param n1 Sample size for group 1
#' @param x2 Number of successes in group 2
#' @param n2 Sample size for group 2
#' @param alpha Significance level (default 0.05)
#' @param iter Number of simulation iterations (default 2000)
#' @param SEED Logical; if TRUE, set random seed for reproducibility (default TRUE)
#'
#' @details
#' The ECP (Empirical Critical P-value) method empirically estimates the
#' critical p-value needed to control the Type I error rate at the specified
#' alpha level. Generally performs well across different sample sizes.
#'
#' @return
#' List with components:
#' \itemize{
#'   \item n: Vector of sample sizes
#'   \item output: Matrix with comparison results
#' }
#'
#' @seealso \code{\link{binom2g}}, \code{\link{binmcp}}
#'
#' @keywords binomial htest
#' @export
binomECP<-function(x1,n1,x2,n2,alpha=.05,iter=2000,SEED=TRUE){
#
# ECP= Estimate Critical P-value
a=binmcp(c(x1,x2),n=c(n1,n2),alpha=alpha,iter=iter,SEED=SEED)
list(n=a$n,output=a$output)
}




# ----------------------------------------------------------------------------

# binomLCO

# ----------------------------------------------------------------------------

#' Binomial Confidence Interval Using Schilling-Doi Method
#'
#' @description
#' Compute a confidence interval for the probability of success using the
#' Schilling-Doi LCO (Likelihood-based Coverage Optimization) method.
#'
#' @param x Number of successes (computed from y if y is provided)
#' @param nn Sample size (computed from y if y is provided)
#' @param y Vector of 1s and 0s (optional)
#' @param n Deprecated parameter (use nn instead)
#' @param alpha Significance level (default 0.05)
#'
#' @details
#' Implements the method from Schilling and Doi (2014) which optimizes
#' coverage probability. This method provides excellent coverage properties
#' while maintaining reasonable interval widths.
#'
#' @return
#' List with components:
#' \itemize{
#'   \item phat: Estimated proportion (x/n)
#'   \item ci: Confidence interval as c(lower, upper)
#'   \item n: Sample size
#' }
#'
#' @seealso \code{\link{binomci}}, \code{\link{binomCP}}
#'
#' @references
#' Schilling, M., and Doi, J. (2014). A coverage probability approach to finding
#' an optimal binomial confidence procedure. The American Statistician, 68,
#' 133-145.
#'
#' @keywords binomial htest
#' @export
binomLCO<-function (x = sum(y), nn = length(y), y = NULL, n = NA, alpha = 0.05){
#
# Compute a confidence interval for the probability of success using the method in
#
#  Schilling, M., Doi, J. (2014)
#  A Coverage Probability Approach to Finding
#  an Optimal Binomial Confidence Procedure,
#  The American Statistician, 68, 133-145.
#
if(!is.null(y)){
y=elimna(y)
nn=length(y)
}
if(nn==1)stop('Something is wrong: number of observations is only 1')
cis=LCO.CI(nn,1-alpha,3)
ci=cis[x+1,2:3]
list(phat=x/nn,ci=ci,n=nn)
}

#' Pairwise Comparisons for Multiple Binomial Groups
#'
#' @description
#' Perform all pairwise comparisons among J independent binomial groups.
#' Provides confidence intervals and p-values with multiplicity adjustment.
#'
#' @param r Vector containing number of successes for each group (if x not provided)
#' @param n Vector of sample sizes for each group (if x not provided)
#' @param x List, matrix, or data frame containing 1s and 0s for each of the J
#'   groups. If matrix/data frame, columns correspond to groups. If specified,
#'   r and n are computed from x
#' @param method Character string specifying comparison method: 'KMS'
#'   (Kulinskaya et al., default), 'SK' (Storer-Kim), or 'ZHZ' (Zou et al.)
#' @param alpha Significance level (default 0.05)
#'
#' @details
#' Performs all J(J-1)/2 pairwise comparisons using the specified method.
#' P-values are adjusted for multiple comparisons using the Hochberg method.
#'
#' Three methods are available:
#' \itemize{
#'   \item KMS: Kulinskaya et al. method (generally recommended)
#'   \item SK: Storer-Kim method
#'   \item ZHZ: Zou, Hao, and Zhang method
#' }
#'
#' @return
#' Matrix with one row per comparison containing:
#' \itemize{
#'   \item Group indices
#'   \item Sample sizes
#'   \item Estimated proportions
#'   \item Difference in proportions
#'   \item Confidence interval
#'   \item P-value
#'   \item Hochberg-adjusted p-value
#' }
#'
#' @seealso \code{\link{binom2g}}, \code{\link{bi2KMSv2}}
#'
#' @keywords binomial htest
#' @export
binpair<-function(r=NULL,n=NULL,x=NULL,method='KMS',alpha=.05){
#
#  Do all pairwise comparisons among J independent groups
#
#  Choices for method:
#  'KMS': Kulinskaya et al. method
#  'SK':  Storer--Kim
#  'ZHZ': Zou et al.
#
#  r: vector containing number of successes
#  n: corresponding sample size
#   x: list mode or matrix or data frame containing 1s and 0s for each of the J
#    groups. If a matrix, columns correspond to groups.
#   It is assumed that r and n are specified or x is specified.
#   If x is specified, r and n are computed based on the data in x.
#   Otherwise r and n are used.
#
#
if(!is.null(r)){
J=length(r)  #number of groups
if(J!=length(n))stop('r and n have different lengths')
}
if(is.null(r)){
if(is.matrix(x) || is.data.frame(x))x=listm(x)
r=NA
n=NA
J=length(x)
for(j in 1:length(x)){
r[j]=sum(x[[j]]==1)
n[j]=length(x[[j]])
}}

NP=(J^2-J)/2
output=matrix(NA,nrow=NP,ncol=11)
dimnames(output)=list(NULL,c('Group','Group','n1','n2','Est. 1', 'Est. 2','est.dif',
'ci.low','ci.up','p-value','ADJ.p.value'))
ic=0
for(j in 1:J){
for(k in 1:J){
if(j<k){
ic=ic+1
temp=binom2g(r[j],n[j],r[k],n[k],method=method,alpha=alpha)
if(method=='KMS')output[ic,1:10]=c(j,k,n[j],n[k],r[j]/n[j],r[k]/n[k],temp$est.dif, temp$ci,temp$p.value)
if(method=='SK')output[ic,c(1:7,10)]=c(j,k,n[j],n[k],r[j]/n[j],r[k]/n[k],temp$est.dif,temp$p.value)
if(method=='ZHZ')output[ic,1:10]=c(j,k,n[j],n[k],r[j]/n[j],r[k]/n[k],temp$dif, temp$ci,temp$p.value)
}}}
output[,11]=p.adjust(output[,10],'hoch')
output
}





# ============================================================================

# Run Tests (20 functions)

# ============================================================================


# ----------------------------------------------------------------------------

# run3bo

# ----------------------------------------------------------------------------

#' 3D Running Interval Smoother with Bootstrap
#'
#' @description
#' Compute a 3D running interval smoother with bootstrap aggregating (bagging).
#' Uses bootstrap resampling to create a more stable smooth surface estimate
#' for multivariate predictors.
#'
#' @param x An n by p matrix of predictors (p > 1 for 3D plotting)
#' @param y Vector of response values
#' @param fr Span parameter controlling amount of smoothing (default: 1)
#' @param est Measure of location to use for smoothing (default: tmean)
#' @param theta Azimuthal viewing angle for 3D plot (default: 50)
#' @param phi Colatitude viewing angle for 3D plot (default: 25)
#' @param nmin Minimum number of points needed to estimate y|x (default: 0)
#' @param pyhat Logical: if TRUE, return predicted values; if FALSE, return "Done" (default: FALSE)
#' @param eout Logical: remove outliers based on (x,y) jointly (default: FALSE)
#' @param outfun Outlier detection function (default: out)
#' @param plotit Logical: create 3D plot (default: TRUE)
#' @param xout Logical: remove outliers based on x values (default: FALSE)
#' @param nboot Number of bootstrap samples (default: 40)
#' @param SEED Logical: set random seed for reproducibility (default: TRUE)
#' @param STAND Logical: standardize data for outlier detection (default: TRUE)
#' @param expand Relative length of z-axis in plot (default: 0.5)
#' @param scale Logical: scale the plot (default: FALSE)
#' @param xlab Label for x-axis (default: "X")
#' @param ylab Label for y-axis (default: "Y")
#' @param zlab Label for z-axis (default: "")
#' @param ticktype Type of tick marks for 3D plot (default: "simple")
#' @param ... Additional arguments passed to est function
#'
#' @details
#' This function implements a 3D running interval smoother with bootstrap
#' aggregating to reduce variance. For each point, the function:
#' 1. Draws nboot bootstrap samples from the data
#' 2. Computes a running interval smooth for each bootstrap sample
#' 3. Averages the bootstrap smooths to get final estimates
#'
#' When plotting with p=2 predictors, creates a perspective plot of the
#' smoothed surface. Missing values are automatically removed.
#'
#' @return
#' A list with component:
#' \item{output}{"Done" if pyhat=FALSE, or vector of predicted values if pyhat=TRUE}
#'
#' @seealso \code{\link{rung3hat}}, \code{\link{runmbo}}, \code{\link{runm3d}}
#'
#' @keywords nonparametric smooth bootstrap
#' @export
run3bo<-function(x,y,fr=1,est=tmean,theta = 50, phi = 25,nmin=0,
pyhat=FALSE,eout=FALSE,outfun=out,plotit=TRUE,xout=FALSE,nboot=40,SEED=TRUE,STAND=TRUE,
expand=.5,scale=FALSE,xlab="X",ylab="Y",zlab="",ticktype="simple",...){
#
# running mean using interval method
#
# fr controls amount of smoothing
# tr is the amount of trimming
#
# Missing values are automatically removed.
#
if(SEED)set.seed(2)
temp<-cbind(x,y)
x<-as.matrix(x)
p<-ncol(x)
p1<-p+1
if(p>2)plotit<-FALSE
temp<-elimna(temp) # Eliminate any rows with missing values.
x<-temp[,1:p]
x<-as.matrix(x)
y<-temp[,p1]
if(xout){
keepit<-rep(TRUE,nrow(x))
flag<-outfun(x,plotit=FALSE,STAND=STAND,...)$out.id
keepit[flag]<-FALSE
x<-x[keepit,]
y<-y[keepit]
}
mat<-matrix(NA,nrow=nboot,ncol=length(y))
vals<-NA
for(it in 1:nboot){
idat<-sample(c(1:length(y)),replace=TRUE)
xx<-temp[idat,1:p]
yy<-temp[idat,p1]
tmy<-rung3hat(xx,yy,pts=x,est=est,fr=fr,...)$rmd
mat[it,]<-tmy
}
rmd<-apply(mat,2,mean,na.rm=TRUE)
flag<-!is.na(rmd)
rmd<-elimna(rmd)
x<-x[flag,]
y<-y[flag]
nval<-NA
m<-cov.mve(x)
for(i in 1:nrow(x))nval[i]<-length(y[near3d(x,x[i,],fr,m)])
if(plotit && ncol(x)==2){
#if(ncol(x)!=2)stop("When plotting, x must be an n by 2 matrix")
fitr<-rmd[nval>nmin]
y<-y[nval>nmin]
x<-x[nval>nmin,]
iout<-c(1:length(fitr))
nm1<-length(fitr)-1
for(i in 1:nm1){
ip1<-i+1
for(k in ip1:length(fitr))if(sum(x[i,]==x[k,])==2)iout[k]<-0
}
fitr<-fitr[iout>=1] # Eliminate duplicate points in the x-y plane
#                 This is necessary when doing three dimensional plots
#                 with the R function interp
mkeep<-x[iout>=1,]
fit<-interp(mkeep[,1],mkeep[,2],fitr)
persp(fit,theta=theta,phi=phi,xlab=xlab,ylab=ylab,zlab=zlab,expand=expand,
scale=scale,ticktype=ticktype)
}
last<-"Done"
if(pyhat)last<-rmd
list(output=last)
}

#' 3D Running Interval Smoother Prediction
#'
#' @description
#' Compute predicted y values for specified points using a 3D running
#' interval method with trimmed means. Simpler version without plotting.
#'
#' @param x An n by p matrix of predictors
#' @param y Vector of response values
#' @param pts An m by p matrix of points at which to predict y
#' @param fr Span parameter controlling amount of smoothing (default: 0.8)
#' @param tr Trimming proportion (default: 0.2)
#'
#' @details
#' For each row in pts, finds the nearest neighbors in x based on robust
#' Mahalanobis distance (using MVE covariance), then computes the trimmed
#' mean of corresponding y values. The random seed is set to 12 for
#' reproducibility of the MVE calculation.
#'
#' @return
#' A list with components:
#' \item{rmd}{Vector of predicted values, one for each row of pts}
#' \item{nval}{Vector of sample sizes used for each prediction}
#'
#' @seealso \code{\link{run3bo}}, \code{\link{rung3hat}}, \code{\link{runm3d}}
#'
#' @keywords nonparametric smooth
#' @export
run3hat<-function(x,y,pts,fr=.8,tr=.2){
#
# Compute y hat for each row of data in the matrix pts
# using a running  interval method
#
# fr controls amount of smoothing
# tr is the amount of trimming
# x is an n by p matrix of predictors.
# pts is an m by p matrix, m>=1.
#
set.seed(12)
if(!is.matrix(x))stop("Predictors are not stored in a matrix.")
if(!is.matrix(pts))stop("The third argument, pts, must be a matrix.")
m<-cov.mcd(x)
rmd<-1 # Initialize rmd
nval<-1
for(i in 1:nrow(pts)){
rmd[i]<-mean(y[near3d(x,pts[i,],fr,m)],tr)
nval[i]<-length(y[near3d(x,pts[i,],fr,m)])
}
list(rmd=rmd,nval=nval)
}




# ----------------------------------------------------------------------------

# runbin.CI

# ----------------------------------------------------------------------------

#' Running Interval Confidence Intervals for Binomial Probability
#'
#' @description
#' Compute confidence intervals for the probability of success at specified
#' points using a running interval smoother approach for binary outcome data.
#'
#' @param x Matrix or vector of predictor values
#' @param y Binary response variable (0/1)
#' @param pts Matrix or vector of points at which to estimate probability (default: x)
#' @param fr Span parameter controlling neighborhood size (default: 1.2)
#' @param xout Logical: remove outliers in x before analysis (default: FALSE)
#' @param outfun Outlier detection function (default: outpro)
#'
#' @details
#' For each point in pts, finds the nearest neighbors in x (using robust
#' Mahalanobis distance for multivariate x), then computes a confidence
#' interval for the binomial probability based on the y values of those
#' neighbors. Only points with at least 6 neighbors are included in output.
#'
#' Confidence intervals are computed using the Agresti-Coull method via
#' binom.conf().
#'
#' @return
#' A list with components:
#' \item{points}{Matrix of unique points in pts where CIs were computed}
#' \item{output}{Matrix with columns: n (sample size), pts.no (point number),
#'   Est (estimated probability), ci.low (lower CI), ci.upper (upper CI)}
#'
#' @seealso \code{\link{binom.conf}}, \code{\link{runhat}}
#'
#' @keywords nonparametric smooth
#' @export
runbin.CI<-function(x,y,pts=NULL,fr=1.2,xout=FALSE,outfun=outpro){
#
# Based on running interval smoother, for each point in pts, compute a confidence
# interval for probability of success based on the nearest neighbors
#
xx<-cbind(x,y)
xx<-elimna(xx)
n=nrow(xx)
p1=ncol(xx)
p=p1-1
x=xx[,1:p]
y=xx[,p1]
if(is.null(pts))pts=x
pts=unique(pts)
pts=as.matrix(pts)
x=as.matrix(x)
if(p>1)m=cov.mve(x)
npts=nrow(pts)
output=matrix(NA,npts,5)
dimnames(output)=list(NULL,c('n','pts.no','Est','ci.low','ci.upper'))
for(i in 1:npts){
if(p==1)Z=y[near(x[,1],pts[i,],fr)]
if(p>1)Z=y[near3d(x,pts[i,],fr,m)]
if(length(Z)>5){
a=binom.conf(sum(Z),length(Z),pr=FALSE)
output[i,3]=a$phat
output[i,2]=i
output[i,1]=a$n
output[i,4]=a$ci[1]
output[i,5]=a$ci[2]
}}
list(points=pts,output=output)
}


best.cell.DO<-function(x, FREQ=NULL,LARGE=TRUE){
#
#  For a multinomial distribution, can a decision be made about
#  about which cell has the highest probability?
#
#   x  Assumed to contain raw data, the function then computes the frequencies of each value
#   The  cell frequencies can be specified via the argument
#   FREQ, in which case x is ignored.
#
if(is.null(FREQ)){
a=splot(x,plotit=FALSE)
x=a$frequencies
obs=a$obs.values
est=a$rel.freq
}
if(!is.null(FREQ)){
x=FREQ
x=elimna(x)
n=sum(x)
est=x/n
obs=NA
}
NCELL=length(x)
NCm1=NCELL-1
if(LARGE)id=which(est==max(est)[1])
else id=which(est==min(est)[1])
IND.pv=NA
ic=0
for(j in 1:NCELL){
if(id!=j){
ic=ic+1
IND.pv[ic]=cell.com.pv(x,id,j)
}}
pv=max(IND.pv)
list(obs.value=obs,Est=est,Indivdual.p.values=IND.pv,p.value=pv)
}

bw.es.main<-function(J,K,x,DIF=TRUE,...){
#
#  Main effect sizes
#
if(is.matrix(x) || is.data.frame(x))x=listm(x)
A=list()
JK=J*K
imat=matrix(c(1:JK),J,K,byrow=TRUE)
for(j in 1:J)A[[j]]=pool.a.list(x[imat[j,]])
B=list()
for(k in 1:K)B[[k]]=pool.a.list(x[imat[,k]])
aes=t1wayv2(A,...)
aes=as.vector(aes[7])
aes=aes[[1]]
if(DIF)bes=rmES.dif.pro(B,...)
if(!DIF)bes=rmES.pro(B,...)
list(A.explanatory.effect.size=aes,B.projection.effect.size=bes)
}

bb.es.main<-function(J,K,x,DIF=TRUE,...){
#
#  Main effect sizes  between-by-between
#
if(is.matrix(x) || is.data.frame(x))x=listm(x)
A=list()
JK=J*K
imat=matrix(c(1:JK),J,K,byrow=TRUE)
for(j in 1:J)A[[j]]=pool.a.list(x[imat[j,]])
B=list()
for(k in 1:K)B[[k]]=pool.a.list(x[imat[,k]])
aes=t1wayv2(A,...)
aes=as.vector(aes[7])
aes=aes[[1]]
bes=t1wayv2(B,...)
bes=as.vector(bes[7])
bes=bes[[1]]
list(A.explanatory.effect.size=aes,B.explanatory.effect.size=bes)
}



#' Running Smoother for Discrete Predictors
#'
#' @description
#' Compute a smooth where the predictor x is discrete with a relatively
#' small number of unique values. Estimates location for each unique x value.
#'
#' @param x Discrete predictor variable
#' @param y Response variable
#' @param est Measure of location (default: onestep)
#' @param plotit Logical: create plot (default: TRUE)
#' @param pyhat Logical: if TRUE, return predicted values; if FALSE, return "Done" (default: FALSE)
#' @param ... Additional arguments passed to est function
#'
#' @details
#' For each unique value of x, computes the specified measure of location
#' for the corresponding y values. Creates a plot with the original data
#' points and a line connecting the smoothed estimates.
#'
#' This is appropriate when x has only a small number of distinct values
#' (e.g., categorical or grouped data), as opposed to continuous predictors.
#'
#' @return
#' "Done" if pyhat=FALSE, or vector of fitted values (one per unique x value) if pyhat=TRUE
#'
#' @seealso \code{\link{rungen}}, \code{\link{runmean}}
#'
#' @keywords nonparametric smooth
#' @export
rundis<-function(x,y,est=onestep,plotit=TRUE,pyhat=FALSE,...){
#
# Do a smooth where x is discrete with a
# relatively small number of values.
#
temp<-sort(unique(x))
yhat<-NA
for(i in 1:length(temp)){
flag<-(temp[i]==x)
yhat[i]<-est(y[flag],...)
}
plot(x,y)
lines(temp,yhat)
output<-"Done"
if(pyhat)output<-yhat
output
}

#' General 3D Running Interval Smoother
#'
#' @description
#' Compute a 3D running interval smoother using any specified measure of
#' location. Can optionally create a perspective plot of the smoothed surface.
#'
#' @param x An n by p matrix of predictors
#' @param y Vector of response values
#' @param est Measure of location (default: onestep)
#' @param fr Span parameter controlling amount of smoothing (default: 1)
#' @param plotit Logical: create 3D perspective plot (default: TRUE)
#' @param theta Azimuthal viewing angle for plot (default: 50)
#' @param phi Colatitude viewing angle for plot (default: 25)
#' @param pyhat Logical: if TRUE, return predicted values (default: FALSE)
#' @param LP Logical: apply lowess post-smoothing (default: FALSE)
#' @param expand Relative length of z-axis in plot (default: 0.5)
#' @param scale Logical: scale the plot (default: FALSE)
#' @param zscale Logical: standardize all variables to median 0, MAD 1 (default: TRUE)
#' @param nmin Minimum neighbors needed for estimation (default: 0)
#' @param xout Logical: remove x outliers (default: FALSE)
#' @param eout Logical: remove (x,y) outliers (default: FALSE)
#' @param outfun Outlier detection function (default: out)
#' @param SEED Logical: set random seed for MVE (default: TRUE)
#' @param STAND Logical: standardize for outlier detection (default: TRUE)
#' @param xlab Label for x-axis (default: "X")
#' @param ylab Label for y-axis (default: "Y")
#' @param zlab Label for z-axis (default: "")
#' @param pr Logical: print advisory messages (default: TRUE)
#' @param duplicate How to handle duplicate x values for interp() (default: "error")
#' @param ticktype Type of tick marks for plot (default: "simple")
#' @param ... Additional arguments passed to est function
#'
#' @details
#' Computes a running interval smooth where for each point, the function
#' finds nearest neighbors based on robust Mahalanobis distance (using MVE),
#' then applies the specified measure of location to the corresponding y values.
#'
#' When plotit=TRUE and p=2, creates a perspective plot. If LP=TRUE, applies
#' additional lowess smoothing to the fitted surface. Missing values are
#' automatically removed.
#'
#' @return
#' "Done" if pyhat=FALSE, or vector of predicted values if pyhat=TRUE
#'
#' @seealso \code{\link{runm3d}}, \code{\link{run3bo}}, \code{\link{rungen}}
#'
#' @keywords nonparametric smooth
#' @export
rung3d<-function(x,y,est=onestep,fr=1,plotit=TRUE,theta=50,phi=25,pyhat=FALSE,LP=FALSE,
expand=.5,scale=FALSE,zscale=TRUE,
nmin=0,xout=FALSE,eout=FALSE,outfun=out,SEED=TRUE,STAND=TRUE,
xlab="X",ylab="Y",zlab="",pr=TRUE,duplicate="error",ticktype="simple",...){
#
# running mean using interval method
#

# fr (the span) controls amount of smoothing
# est is the measure of location.
# (Goal is to determine est(y) given x.)
# x is an n by p matrix of predictors.
#
# pyhat=T, predicted values are returned.
#
if(SEED)set.seed(12) # set seed for cov.mve
if(eout && xout)stop("Not allowed to have eout=xout=TRUE")
if(!is.matrix(x))stop("Data are not stored in a matrix.")
if(nrow(x) != length(y))stop("Number of rows in x does not match length of y")
temp<-cbind(x,y)
p<-ncol(x)
p1<-p+1
temp<-elimna(temp) # Eliminate any rows with missing values.
if(eout){
keepit<-outfun(temp,plotit=FALSE)$keep
x<-x[keepit,]
y<-y[keepit]
}
if(xout){
keepit<-outfun(x,plotit=FALSE,STAND=STAND,...)$keep
x<-x[keepit,]
y<-y[keepit]
}
if(zscale){
for(j in 1:p1){
temp[,j]<-(temp[,j]-median(temp[,j]))/mad(temp[,j])
}}
x<-temp[,1:p]
y<-temp[,p1]
m<-cov.mve(x)
iout<-c(1:nrow(x))
rmd<-1 # Initialize rmd
nval<-1
for(i in 1:nrow(x))rmd[i]<-est(y[near3d(x,x[i,],fr,m)],...)
for(i in 1:nrow(x))nval[i]<-length(y[near3d(x,x[i,],fr,m)])
if(ncol(x)==2){
if(plotit){
if(pr){
if(!scale)print("With dependence, suggest using scale=TRUE")
}
fitr<-rmd[nval>nmin]
y<-y[nval>nmin]
x<-x[nval>nmin,]
iout<-c(1:length(fitr))
nm1<-length(fitr)-1
for(i in 1:nm1){
ip1<-i+1
for(k in ip1:length(fitr))if(sum(x[i,]==x[k,])==2)iout[k]<-0
}
fitr<-fitr[iout>=1] # Eliminate duplicate points in the x-y plane
#                 This is necessary when doing three dimensional plots
#                 with the R function interp
if(LP)fitr=lplot(x[iout>=1,],fitr,pyhat=TRUE,pr=FALSE,plotit=FALSE)$yhat
mkeep<-x[iout>=1,]
fit<-interp(mkeep[,1],mkeep[,2],fitr,duplicate=duplicate)
persp(fit,theta=theta,phi=phi,expand=expand,
scale=scale,xlab=xlab,ylab=ylab,zlab=zlab,ticktype=ticktype)
}}
if(pyhat)last<-rmd
if(!pyhat)last <- "Done"
        last
}

#' 3D Running Smoother with Automatic Span Selection
#'
#' @description
#' Compute a 3D running interval smoother with automatic selection of the
#' smoothing span via leave-three-out cross-validation, minimizing the
#' percentage bend scale of residuals.
#'
#' @param x An n by p matrix of predictors
#' @param y Vector of response values
#' @param est Measure of location (default: onestep)
#' @param regfun Regression function for comparison (default: tsreg)
#' @param beta Bending constant for pbvar (default: 0.2)
#' @param plotit Logical: create plot (default: FALSE)
#' @param nmin Minimum neighbors for estimation (default: 0)
#' @param fr Span values to try (default: NA, uses automatic grid search)
#' @param ... Additional arguments passed to est function
#'
#' @details
#' Similar to runm3d but automatically determines the optimal span (fr) by
#' minimizing the percentage bend variance of residuals using leave-three-out
#' cross-validation. If fr is not specified:
#' - First tries fr = 0.7, 0.75, 0.8, ..., 1.2
#' - If minimum occurs at 0.7, also tries fr = 0.2, 0.25, ..., 0.7
#'
#' The function also computes an explanatory measure gamma.L comparing
#' the smoother to a simple regression fit via regfun.
#'
#' @return
#' A list with components:
#' \item{gamma.L}{Explanatory measure comparing smoother to regression}
#' \item{pbcorsq}{Squared percentage bend correlation for regression}
#' \item{etasq}{Squared percentage bend correlation for smoother}
#' \item{fr}{Selected span value}
#' \item{rmd}{Fitted values from smoother}
#' \item{yused}{Y values used (after removing NAs)}
#' \item{varval}{Percentage bend variances for all spans tried}
#'
#' @seealso \code{\link{runm3d}}, \code{\link{rung3d}}, \code{\link{pbcor}}
#'
#' @keywords nonparametric smooth
#' @export
rung3dlchk<-function(x,y,est=onestep,regfun=tsreg,beta=.2,plotit=FALSE,nmin=0,
fr=NA,...){
#
# running mean using interval method
# Same as runm3d, but empirically determine the span, f,
# by maximizing the percentage bend correlation using the
# leave-three-out method.
#
# x is an n by p matrix of predictors.
#
# fr controls amount of smoothing and is determined by this function.
# If fr is missing, function first considers fr=.8(.05)1.2. If
# measure of scale of residuals is mininmized for fr=.8, then consider
# fr=.2(.05).75.
#
#
if(!is.matrix(x))stop("Data are not stored in a matrix.")
plotit<-as.logical(plotit)
chkcor<-1
frtry<-c(.7,.75,.8,.85,.9,.95,1.,1.05,1.1,1.15,1.2)
if(!is.na(fr[1]))frtry<-fr
chkit<-0
for (it in 1:length(frtry)){
fr<-frtry[it]
rmd<-runm3ds1(x,y,fr,tr,FALSE,nmin)  # Using leave-three-out method.
xm<-y[!is.na(rmd)]
rmd<-rmd[!is.na(rmd)]
dif<-xm-rmd
chkcor[it]<-pbvar(dif,beta)
}
if(sum(is.na(chkcor))== length(chkcor))
{stop("A value for the span cannot be determined with these data.")}
tempc<-sort(chkcor)
chkcor[is.na(chkcor)]<-tempc[length(tempc)]
temp<-order(chkcor)
fr1<-frtry[temp[1]]
fr2<-fr1
val1<-min(chkcor)
chkcor2<-0
if(is.na(fr)){
if(temp[1] == 1){
frtry<-c(.2,.25,.3,.35,.4,.45,.5,.55,.6,.65,.7,.75)
for (it in 1:length(frtry)){
fr<-frtry[it]
rmd<-runm3ds1(x,y,fr,tr,FALSE,nmin)
xm<-y[!is.na(rmd)]
rmd<-rmd[!is.na(rmd)]
dif<-xm-rmd
chkcor2[it]<-pbvar(dif,beta)
}
tempc<-sort(chkcor2)
chkcor2[is.na(chkcor2)]<-tempc[length(tempc)]
print(chkcor2)
temp2<-order(chkcor2)
fr2<-frtry[temp2[1]]
}
}
sortc<-sort(chkcor2)
chkcor2[is.na(chkcor2)]<-sortc[length(sortc)]
val2<-min(chkcor2)
fr<-fr1
if(val2 < val1)fr<-fr2
rmd<-runm3d(x,y,fr=fr,tr,plotit=FALSE,nmin,pyhat=TRUE,pr=FALSE)
xm<-y[!is.na(rmd)]
rmd<-rmd[!is.na(rmd)]
etasq<-pbcor(rmd,xm)$cor^2
# Next, fit regression line
temp<-y-regfun(x,y)$res
pbc<-pbcor(temp,y)$cor^2
temp<-(etasq-pbc)/(1-pbc)
list(gamma.L=temp,pbcorsq=pbc,etasq=etasq,fr=fr,rmd=rmd,yused=xm,varval=chkcor)
}




# ----------------------------------------------------------------------------

# rung3hat

# ----------------------------------------------------------------------------

#' General 3D Running Interval Smoother Prediction
#'
#' @description
#' Compute predicted y values for specified points using a 3D running
#' interval method with any specified measure of location.
#'
#' @param x An n by p matrix of predictors
#' @param y Vector of response values
#' @param est Measure of location (default: tmean)
#' @param pts An m by p matrix of points at which to predict y
#' @param fr Span parameter controlling smoothing (default: 1)
#' @param DET Logical: use DETMCD for robust covariance (default: TRUE); if FALSE, uses MVE
#' @param ... Additional arguments passed to est function
#'
#' @details
#' For each row in pts, finds nearest neighbors in x based on robust
#' Mahalanobis distance (using DETMCD by default, or MVE if DET=FALSE),
#' then applies the specified measure of location to corresponding y values.
#'
#' More flexible than run3hat as it allows any measure of location (not just
#' trimmed mean) and choice of robust covariance estimator.
#'
#' @return
#' A list with components:
#' \item{rmd}{Vector of predicted values, one for each row of pts}
#' \item{nval}{Vector of sample sizes used for each prediction}
#'
#' @seealso \code{\link{run3hat}}, \code{\link{rung3d}}, \code{\link{runYhat}}
#'
#' @keywords nonparametric smooth
#' @export
rung3hat<-function(x,y,est=tmean,pts,fr=1,DET=TRUE,...){
#
# Compute y hat for each row of data in the matrix pts
# using a running  interval method
#
# fr controls amount of smoothing
# tr is the amount of trimming
# x is an n by p matrix of predictors.
# pts is an m by p matrix, m>=1.
#
if(!is.matrix(x))stop("Predictors are not stored in a matrix.")
if(!is.matrix(pts))stop("The third argument, pts, must be a matrix.")
if(DET)m=DETMCD(x)
else m<-cov.mve(x)
rmd<-1 # Initialize rmd
nval<-1
for(i in 1:nrow(pts)){
rmd[i]<-est(y[near3d(x,pts[i,],fr,m)],...)
nval[i]<-length(y[near3d(x,pts[i,],fr,m)])
}
list(rmd=rmd,nval=nval)
}


lta.sub<-function(X,theta,h){
np<-ncol(X)
p<-np-1
x<-X[,1:p]
y<-X[,np]
temp<-t(t(x)*theta[2:np])
yhat<-apply(temp,1,sum)+theta[1]
res<-abs(y-yhat)
res<-sort(res)
val<-sum(res[1:h])
val
}
 ltareg<-function(x, y, tr = 0.2, h = NA,op=2)
{
        #
        # Compute the least trimmed absolute value regression estimator.
        # The default amount of trimming is .2
# op=1,  use ltsreg as initial estimate
# op!=1, use tsreg
#
# If h is specfied, use h smallest residuals, and ignore tr
#
x<-as.matrix(x)
if(is.na(h)) h <- length(y) - floor(tr * length(y))
X<-cbind(x,y)
X<-elimna(X)
np<-ncol(X)
p<-np-1
x<-X[,1:p]
x<-as.matrix(x)
y<-X[,np]
if(op==1)temp<-ltsreg(x,y)$coef
if(op!=1)temp<-tsreg(x,y)$coef
START<-temp
coef<-nelderv2(X,np,FN=lta.sub,START=START,h=h)
        res <- y - x%*%coef[2:np] - coef[1]
        list(coef = coef, residuals = res)
}




# ----------------------------------------------------------------------------

# rung3hat.pcrit

# ----------------------------------------------------------------------------

#' Critical P-Value for 3D Running Interval CI
#'
#' @description
#' Compute the critical p-value for multiple testing adjustment in
#' rung3hatCI via simulation under the null hypothesis.
#'
#' @param x An n by p matrix of predictors
#' @param pts Matrix of points at which CIs will be computed (default: x)
#' @param alpha Nominal alpha level (default: 0.05)
#' @param iter Number of simulation iterations (default: 1000)
#' @param tr Trimming proportion (default: 0.2)
#' @param fr Span parameter (default: 1)
#' @param nmin Minimum sample size for CI computation (default: 12)
#' @param ... Additional arguments
#'
#' @details
#' Simulates data under the null hypothesis (Y ~ N(0,1) independent of X)
#' and computes the distribution of minimum p-values across all points in pts.
#' Returns the alpha quantile of this distribution, which can be used as
#' an adjusted critical value for multiple comparisons.
#'
#' @return
#' Critical p-value (alpha quantile of minimum p-value distribution)
#'
#' @seealso \code{\link{rung3hatCI}}
#'
#' @keywords nonparametric smooth htest
#' @export
rung3hat.pcrit<-function(x,pts=x,alpha=.05,iter=1000,tr=.2,fr=1,nmin=12,...){
#
#  Compute critical p-value for rung3hatCI.
#
x=as.matrix(x)
n=nrow(x)
pts=as.matrix(pts)
pvdist=NA
m<-cov.mve(x)
for(i in 1:iter){
y=rnorm(n)
a=rung3hat.sub(x,y,pts=pts,m=m,tr=tr,fr=fr,nmin=nmin)
pvdist[i]=min(a,na.rm=TRUE)
}
pc=hd(pvdist,alpha)
pc
}


#' Helper Function for rung3hat.pcrit
#'
#' @description
#' Internal helper function that computes p-values for trimmed mean tests
#' at each point. Used by rung3hat.pcrit for simulation.
#'
#' @param x Matrix of predictors
#' @param y Response variable
#' @param pts Points at which to compute p-values
#' @param m Robust covariance matrix for distance calculation
#' @param tr Trimming proportion (default: 0.2)
#' @param fr Span parameter
#' @param nmin Minimum sample size
#'
#' @return Vector of p-values from trimmed mean tests
#'
#' @keywords internal
#' @export
rung3hat.sub<-function(x,y,pts,m,tr=.2,fr,nmin){
pv=NA
for(i in 1:nrow(pts)){
flag=near3d(x,pts[i,],fr,m)
if(sum(flag)>nmin)pv[i]=trimci(y[flag],tr=tr,pr=FALSE)$p.value
}
pv
}




# ----------------------------------------------------------------------------

# rung3hatCI

# ----------------------------------------------------------------------------

#' 3D Running Interval Smoother with Confidence Intervals
#'
#' @description
#' Compute confidence intervals for the trimmed mean of Y given X at specified
#' points using a 3D running interval smoother. Optionally adjusts for multiple
#' comparisons.
#'
#' @param x An n by p matrix of predictors
#' @param y Vector of response values
#' @param pts Matrix of points at which to compute CIs (default: x)
#' @param tr Trimming proportion (default: 0.2)
#' @param alpha Significance level (default: 0.05)
#' @param fr Span parameter controlling smoothing (default: 1)
#' @param nmin Minimum neighbors needed for CI (default: 12)
#' @param ADJ Logical: adjust alpha for multiple comparisons (default: FALSE)
#' @param iter Number of iterations for adjusted alpha (default: 1000)
#' @param ... Additional arguments
#'
#' @details
#' For each point in pts, finds nearest neighbors based on robust Mahalanobis
#' distance (MVE), then computes a confidence interval for the trimmed mean
#' of the corresponding y values.
#'
#' If ADJ=TRUE, computes an adjusted critical p-value via simulation to control
#' the familywise error rate across all points. Random seed is set to 12 for
#' reproducibility, then restored.
#'
#' Only points with at least nmin neighbors are included in the output.
#'
#' @return
#' A list with components:
#' \item{pts.used}{Matrix of points where CIs were computed}
#' \item{output}{Matrix with columns: n (sample size), Est. (estimate),
#'   ci.low (lower CI), ci.up (upper CI)}
#' \item{alpha.used}{Actual alpha level used (adjusted if ADJ=TRUE)}
#'
#' @seealso \code{\link{rung3hat.pcrit}}, \code{\link{rung3hat}}, \code{\link{trimci}}
#'
#' @keywords nonparametric smooth htest
#' @export
rung3hatCI<-function(x,y,pts=x,tr=.2,alpha=.05,fr=1,nmin=12,ADJ=FALSE,iter=1000,...){
#
# Compute y hat for each row of data in the matrix pts
# use  a running  interval smoother to compute a confidence interval for trimmed mean of Y given X
#
# fr controls amount of smoothing
# tr is the amount of trimming
# x is an n by p matrix of predictors.
# pts is an m by p matrix, m>=1.
#
oldSeed <- .Random.seed
set.seed(12) # So get consistent results from near3d
if(ADJ)alpha=rung3hat.pcrit(x,pts=pts,tr=tr,nmin=nmin,fr=fr,iter=iter)
x=as.matrix(x)
p=ncol(x)
pts=as.matrix(pts)
m<-cov.mve(x)
rmd<-1 # Initialize rmd
nval<-1
ci=matrix(NA,nrow=nrow(pts),ncol=2)
x.used=matrix(NA,nrow=nrow(pts),ncol=p)
for(i in 1:nrow(pts)){
flag=near3d(x,pts[i,],fr,m)
rmd[i]<-mean(y[flag],tr)
nval[i]<-length(y[flag])
if(nval[i]>nmin){
ci[i,]=trimci(y[flag],tr=tr,alpha=alpha,pr=FALSE)$ci
x.used[i,]=pts[i,]
}
}
flag=!is.na(x.used[,1])
x.used=x.used[flag,]
rmd=rmd[flag]
nval=nval[flag]
ci=ci[flag,]
output=cbind(nval,rmd,ci)
dimnames(output)=list(NULL,c('n','Est.','ci.low','ci.up'))
assign(x='.Random.seed', value=oldSeed, envir=.GlobalEnv)
list(pts.used=x.used,output=output,alpha.used=alpha)
}




# ----------------------------------------------------------------------------

# rungen

# ----------------------------------------------------------------------------

#' General Running Interval Smoother
#'
#' @description
#' Compute a running interval smoother using any specified measure of location
#' or scale. By default uses an M-estimator (onestep) with optional lowess
#' post-smoothing.
#'
#' @param x Predictor variable
#' @param y Response variable
#' @param est Measure of location or scale (default: onestep)
#' @param fr Span parameter controlling smoothing (default: 1)
#' @param plotit Logical: create plot (default: TRUE)
#' @param scat Logical: show scatter plot of data points (default: TRUE)
#' @param pyhat Logical: if TRUE, return predicted values (default: FALSE)
#' @param eout Logical: remove (x,y) outliers (default: FALSE)
#' @param xout Logical: remove x outliers (default: FALSE)
#' @param xlab Label for x-axis (default: "x")
#' @param ylab Label for y-axis (default: "y")
#' @param outfun Outlier detection function (default: out)
#' @param LP Logical: apply lowess post-smoothing (default: TRUE)
#' @param pch Plotting character for data points (default: '.')
#' @param ... Additional arguments passed to est function
#'
#' @details
#' For each x value, finds nearest neighbors and applies the specified measure
#' of location/scale. If LP=TRUE, further smooths the result using lowess (lplot).
#'
#' The function sorts x values for plotting but can handle unsorted input.
#' Missing values are automatically removed. Cannot have both eout=TRUE and
#' xout=TRUE simultaneously.
#'
#' @return
#' A list with component:
#' \item{output}{"Done" if pyhat=FALSE, or vector of fitted values if pyhat=TRUE}
#'
#' @seealso \code{\link{runmean}}, \code{\link{runhat}}, \code{\link{lplot}}
#'
#' @keywords nonparametric smooth
#' @export
rungen<-function(x,y,est=onestep,fr=1,plotit=TRUE,scat=TRUE,pyhat=FALSE,
eout=FALSE,xout=FALSE,xlab="x",ylab="y",outfun=out,LP=TRUE,pch='.',...){
#
# running  interval smoother that can  be used  with any measure
# of location or scale. By default, an M-estimator is used.
#
# LP=TRUE, the plot is further smoothed via lows
#
# fr controls amount of smoothing
plotit<-as.logical(plotit)
scat<-as.logical(scat)
m<-cbind(x,y)
m<-elimna(m)
if(eout && xout)stop("Not allowed to have eout=xout=T")
if(eout){
flag<-outfun(m,plotit=FALSE)$keep
m<-m[flag,]
}
if(xout){
flag<-outfun(m[,1])$keep
m<-m[flag,]
}
x=m[,1]
y=m[,2]
rmd<-c(1:length(x))
for(i in 1:length(x))rmd[i]<-est(y[near(x,x[i],fr)],...)
if(LP){
ord=order(x)
x=x[ord]
rmd=rmd[ord]
y=y[ord]
rmd=lplot(x,rmd,plotit=FALSE,pyhat=TRUE,pr=FALSE,STR=FALSE)$yhat
}
if(plotit){
if(scat){
plot(c(x,x),c(y,rmd),xlab=xlab,ylab=ylab,type="n")
points(x,y,pch=pch)
}
if(!scat)plot(c(x,x),c(y,rmd),type="n",ylab=ylab,xlab=xlab)
points(x,rmd,type="n")
sx<-sort(x)
xorder<-order(x)
sysm<-rmd[xorder]
lines(sx,sysm)
}
if(pyhat)output<-rmd
if(!pyhat)output<-"Done"
list(output=output)
}


#adpchk<-function(x,y,adfun=adrun,gfun=runm3d,xlab="First Fit",




# ----------------------------------------------------------------------------

# rungenv2

# ----------------------------------------------------------------------------

#' Running Interval Smoother Preserving Original Order
#'
#' @description
#' Compute a running interval smoother but return predicted values in the
#' original order of x (not sorted). Wrapper for rungen that preserves input order.
#'
#' @param x Predictor variable
#' @param y Response variable
#' @param est Measure of location (default: onestep)
#' @param fr Span parameter controlling smoothing (default: 1)
#' @param LP Logical: apply lowess post-smoothing (default: TRUE)
#' @param ... Additional arguments passed to rungen
#'
#' @details
#' Calls rungen to compute the smooth, then reorders the predicted values
#' to match the original order of x. Useful when you need predictions aligned
#' with the original data order rather than sorted by x values.
#'
#' @return
#' Vector of fitted values in the original order of x
#'
#' @seealso \code{\link{rungen}}
#'
#' @keywords nonparametric smooth
#' @export
rungenv2<-function(x, y, est = onestep, fr = 1, LP = TRUE, ...){
#
# Return x and predicted y values not sorted in ascending order,
# rather, keep x as originally entered and return corresponding Yhat values
#
xord=order(x)
res=rungen(x,y,est=est,fr=fr,LP=LP,pyhat=TRUE,plotit=FALSE)$output
res[order(xord)]
}




# ----------------------------------------------------------------------------

# runhat

# ----------------------------------------------------------------------------

#' Running Interval Smoother Prediction
#'
#' @description
#' Compute predicted values using a running interval smoother with any
#' specified measure of location or scale. By default uses 20% trimmed mean.
#'
#' @param x Predictor variable
#' @param y Response variable
#' @param pts Points at which to estimate y (default: x)
#' @param est Measure of location or scale (default: tmean)
#' @param fr Span parameter controlling smoothing (default: 1)
#' @param nmin Minimum neighbors needed for estimation (default: 1)
#' @param ... Additional arguments passed to est function
#'
#' @details
#' For each value in pts, finds the nearest neighbors in x (based on the
#' span fr), then applies the specified measure of location/scale to the
#' corresponding y values. Only computes an estimate if at least nmin
#' neighbors are found; otherwise returns NA.
#'
#' This is the workhorse prediction function used by many other smoothers.
#'
#' @return
#' Vector of predicted values, one for each value in pts
#'
#' @seealso \code{\link{rungen}}, \code{\link{runmean}}, \code{\link{runYhat}}
#'
#' @keywords nonparametric smooth
#' @export
runhat<-function(x,y,pts=x,est=tmean,fr=1,nmin=1,...){
#
# running  interval smoother that can  be used  with any measure
# of location or scale. By default, a 20% trimmed mean is used.
# This function computes an estimate of y for each x value stored in pts
#
# fr controls amount of smoothing
rmd<-rep(NA,length(pts))
for(i in 1:length(pts)){
val<-y[near(x,pts[i],fr)]
if(length(val)>=nmin)rmd[i]<-est(val,...)
}
rmd
}




# ----------------------------------------------------------------------------

# runm3d

# ----------------------------------------------------------------------------

#' 3D Running Interval Smoother with Trimmed Means
#'
#' @description
#' Compute a 3D running interval smoother using trimmed means. Can create
#' perspective plots of the smoothed surface.
#'
#' @param x An n by p matrix of predictors
#' @param y Vector of response values
#' @param theta Azimuthal viewing angle for plot (default: 50)
#' @param phi Colatitude viewing angle for plot (default: 25)
#' @param fr Span parameter controlling smoothing (default: 0.8)
#' @param tr Trimming proportion (default: 0.2)
#' @param plotit Logical: create 3D perspective plot (default: TRUE)
#' @param pyhat Logical: if TRUE, return predicted values (default: FALSE)
#' @param nmin Minimum neighbors for estimation (default: 0)
#' @param expand Relative length of z-axis in plot (default: 0.5)
#' @param scale Logical: scale the plot (default: FALSE)
#' @param zscale Logical: standardize variables to median 0, MAD 1 (default: FALSE)
#' @param xout Logical: remove x outliers (default: FALSE)
#' @param outfun Outlier detection function (default: out)
#' @param eout Logical: remove (x,y) outliers (default: FALSE)
#' @param xlab Label for x-axis (default: "X")
#' @param ylab Label for y-axis (default: "Y")
#' @param zlab Label for z-axis (default: "")
#' @param pr Logical: print advisory messages about scale parameter (default: TRUE)
#' @param SEED Logical: set random seed for MVE (default: TRUE)
#' @param ticktype Type of tick marks for plot (default: "simple")
#'
#' @details
#' Computes a 3D running interval smooth using trimmed means. For each point,
#' finds nearest neighbors based on robust Mahalanobis distance (MVE), then
#' computes the trimmed mean of corresponding y values.
#'
#' When plotit=TRUE and p=2, creates a perspective plot. The function prints
#' advisory messages about the scale parameter: scale=FALSE is typically best
#' for independent predictors, scale=TRUE for dependent predictors.
#'
#' Missing values are automatically removed. Cannot have both eout=TRUE and
#' xout=TRUE simultaneously.
#'
#' @return
#' "Done" if pyhat=FALSE, or vector of predicted values if pyhat=TRUE
#'
#' @seealso \code{\link{rung3d}}, \code{\link{run3bo}}, \code{\link{run3hat}}
#'
#' @keywords nonparametric smooth
#' @export
runm3d<-function(x,y,theta=50,phi=25,fr=.8,tr=.2,plotit=TRUE,pyhat=FALSE,nmin=0,
expand=.5,scale=FALSE,zscale=FALSE,xout=FALSE,outfun=out,eout=FALSE,xlab="X",ylab="Y",zlab="",
pr=TRUE,SEED=TRUE,ticktype="simple"){
#
# running mean using interval method
#
# fr controls amount of smoothing
# tr is the amount of trimming
# x is an n by p matrix of predictors.
#
#  Rows of data with missing values are automatically removed.
#
# When plotting, theta and phi can be used to change
# the angle at which the plot is viewed.
#
#  theta is the azimuthal direction and phi the colatitude
#   expand controls relative length of z-axis
#
if(plotit){
if(pr){
print("Note: when there is independence, scale=F is probably best")
print("When there is dependence, scale=T is probably best")
}}
if(!is.matrix(x))stop("x should be a matrix")
if(nrow(x) != length(y))stop("number of rows of x should equal length of y")
temp<-cbind(x,y)
p<-ncol(x)
p1<-p+1
temp<-elimna(temp) # Eliminate any rows with missing values.
if(xout){
keepit<-rep(TRUE,nrow(x))
flag<-outfun(x,plotit=FALSE)$out.id
keepit[flag]<-F
x<-x[keepit,]
y<-y[keepit]
}
if(zscale){
for(j in 1:p1){
temp[,j]<-(temp[,j]-median(temp[,j]))/mad(temp[,j])
}}
x<-temp[,1:p]
y<-temp[,p1]
pyhat<-as.logical(pyhat)
plotit<-as.logical(plotit)
if(SEED)set.seed(12)
m<-cov.mve(x)
iout<-c(1:nrow(x))
rmd<-1 # Initialize rmd
nval<-1
for(i in 1:nrow(x))rmd[i]<-mean(y[near3d(x,x[i,],fr,m)],tr)
for(i in 1:nrow(x))nval[i]<-length(y[near3d(x,x[i,],fr,m)])
if(plotit){
if(ncol(x)!=2)stop("When plotting, x must be an n by 2 matrix")
fitr<-rmd[nval>nmin]
y<-y[nval>nmin]
x<-x[nval>nmin,]
iout<-c(1:length(fitr))
nm1<-length(fitr)-1
for(i in 1:nm1){
ip1<-i+1
for(k in ip1:length(fitr))if(sum(x[i,]==x[k,])==2)iout[k]<-0
}
fitr<-fitr[iout>=1] # Eliminate duplicate points in the x-y plane
#                 This is necessary when doing three dimensional plots
#                 with the R function interp
mkeep<-x[iout>=1,]
fit<-interp(mkeep[,1],mkeep[,2],fitr)
persp(fit,theta=theta,phi=phi,xlab=xlab,ylab=ylab,zlab=zlab,expand=expand,
scale=scale,ticktype=ticktype)
}
last<-"Done"
if(pyhat)last<-rmd
last
}

#' Running Interval Smoother with Bootstrap Aggregating (Bagging)
#'
#' @description
#' Compute a running interval smoother with bootstrap aggregating (bagging)
#' to reduce variance and create more stable estimates.
#'
#' @param x Predictor variable
#' @param y Response variable
#' @param fr Span parameter controlling smoothing (default: 1)
#' @param est Measure of location (default: tmean)
#' @param xlab Label for x-axis (default: "X")
#' @param ylab Label for y-axis (default: "Y")
#' @param pts Points at which to estimate y (default: x)
#' @param RNA Logical: remove NAs when averaging bootstrap estimates (default: FALSE)
#' @param atr Trimming proportion when averaging bootstrap values (default: 0)
#' @param pch Plotting character for data points (default: '*')
#' @param pyhat Logical: if TRUE, return predicted values (default: FALSE)
#' @param eout Logical: remove (x,y) outliers (default: FALSE)
#' @param outfun Outlier detection function (default: out)
#' @param plotit Logical: create plot (default: TRUE)
#' @param xout Logical: remove x outliers (default: FALSE)
#' @param scat Logical: show scatter plot of data points (default: TRUE)
#' @param nboot Number of bootstrap samples (default: 40)
#' @param SEED Logical: set random seed (default: TRUE)
#' @param ... Additional arguments passed to est function
#'
#' @details
#' Implements bootstrap aggregating (bagging) for running interval smoothers:
#' 1. Draws nboot bootstrap samples from the data
#' 2. Computes a running interval smooth for each bootstrap sample
#' 3. Averages the bootstrap smooths (with optional trimming via atr)
#'
#' This approach reduces variance compared to a single smooth. Only works
#' with single predictor (not multivariate). Missing values are automatically
#' removed. Cannot have both eout=TRUE and xout=TRUE simultaneously.
#'
#' @return
#' "Done" if pyhat=FALSE, or vector of fitted values if pyhat=TRUE
#'
#' @seealso \code{\link{runhat}}, \code{\link{run3bo}}, \code{\link{rungen}}
#'
#' @keywords nonparametric smooth bootstrap
#' @export
runmbo<-function(x,y,fr=1,est=tmean,xlab="X",ylab="Y",pts=x,RNA=FALSE,atr=0,pch='*',
pyhat=FALSE,eout=FALSE,outfun=out,plotit=TRUE,xout=FALSE,scat=TRUE,nboot=40,SEED=TRUE,...){
#
# running interval smooth with bagging
#
# fr controls amount of smoothing
# tr is the amount of trimming
#
# Missing values are automatically removed.
#
# RNA=F, do not remove missing values when averaging
# (computing the smooth) at x
# xout=T removes points for which x is an outlier
# eout=F removes points for which (x,y) is an outlier
# nmin  estimate y|x only when number of points close
# to x is > nmin
# atr is amount of trimming when averaging over the bagged
# values
# est is the measure of location to be estimated
# est=tmean means estimate 20% trimmed mean of y given x
#
if(SEED)set.seed(2)
temp<-cbind(x,y)
if(ncol(temp)>2)stop("Use run3bo with more than 1 predictor")
temp<-elimna(temp) # Eliminate any rows with missing values
if(eout && xout)stop("Not allowed to have eout=xout=T")
if(eout){
flag<-outfun(temp,plotit=FALSE)$keep
temp<-temp[flag,]
}
if(xout){
flag<-outfun(x,plotit=FALSE)$keep
temp<-temp[flag,]
}
x<-temp[,1]
y<-temp[,2]
pts<-as.matrix(pts)
mat<-matrix(NA,nrow=nboot,ncol=nrow(pts))
vals<-NA
for(it in 1:nboot){
idat<-sample(c(1:length(y)),replace=TRUE)
xx<-temp[idat,1]
yy<-temp[idat,2]
mat[it,]<-runhat(xx,yy,pts=pts,est=est,fr=fr,...)
}
rmd<-apply(mat,2,mean,na.rm=RNA,tr=atr)
if(plotit){
if(scat){
plot(c(x,x),c(y,rmd),xlab=xlab,ylab=ylab,type="n")
points(x,y,pch=pch)
}
if(!scat)plot(c(x,x),c(y,rmd),type="n",xlab=xlab,ylab=ylab)
points(x, rmd, type = "n")
sx <- sort(x)
xorder <- order(x)
sysm <- rmd[xorder]
lines(sx, sysm)
}
output="Done"
if(pyhat)output<-rmd
output
}




# ----------------------------------------------------------------------------

# runmean

# ----------------------------------------------------------------------------

#' Running Interval Smoother with Trimmed Means
#'
#' @description
#' Compute a running interval smoother using trimmed means. Simple function
#' for basic smoothing with optional plotting.
#'
#' @param x Predictor variable
#' @param y Response variable
#' @param fr Span parameter controlling smoothing (default: 1)
#' @param tr Trimming proportion (default: 0.2)
#' @param pyhat Logical: if TRUE, return predicted values (default: FALSE)
#' @param eout Logical: remove (x,y) outliers (default: FALSE)
#' @param outfun Outlier detection function (default: out)
#' @param plotit Logical: create plot (default: TRUE)
#' @param xout Logical: remove x outliers (default: FALSE)
#' @param xlab Label for x-axis (default: "x")
#' @param ylab Label for y-axis (default: "y")
#'
#' @details
#' For each x value, finds nearest neighbors and computes the trimmed mean
#' of corresponding y values. Creates a scatter plot with smoothed line overlay.
#'
#' Missing values are automatically removed. If both eout=TRUE and xout=TRUE
#' are specified, xout is set to FALSE.
#'
#' @return
#' NULL if pyhat=FALSE and plotit=TRUE, or vector of fitted values if pyhat=TRUE
#'
#' @seealso \code{\link{rungen}}, \code{\link{runhat}}, \code{\link{runm3d}}
#'
#' @keywords nonparametric smooth
#' @export
runmean<-function(x,y,fr=1,tr=.2,pyhat=FALSE,eout=FALSE,outfun=out,plotit=TRUE,xout=FALSE,
xlab="x",ylab="y"){
#
# running mean using interval method
#
# fr controls amount of smoothing
# tr is the amount of trimming
#
# Missing values are automatically removed.
#
if(eout && xout)xout<-FALSE
temp<-cbind(x,y)
temp<-elimna(temp) # Eliminate any rows with missing values
if(eout){
flag<-outfun(temp,plotit=FALSE)$keep
temp<-temp[flag,]
}
if(xout){
flag<-outfun(x,plotit=FALSE)$keep
temp<-temp[flag,]
}
x<-temp[,1]
y<-temp[,2]
pyhat<-as.logical(pyhat)
rmd<-c(1:length(x))
for(i in 1:length(x))rmd[i]<-mean(y[near(x,x[i],fr)],tr)
if(pyhat)return(rmd)
if(plotit){
plot(x,y,xlab=xlab,ylab=ylab)
sx<-sort(x)
xorder<-order(x)
sysm<-rmd[xorder]
tempx<-(!duplicated(sx))
lines(sx[tempx], sysm[tempx])
}}




# ----------------------------------------------------------------------------

# runmq

# ----------------------------------------------------------------------------

#' Running Interval Smoother for Multiple Quantiles
#'
#' @description
#' Create a plot with running interval smoothers for multiple specified
#' quantiles. Can use either Harrell-Davis or standard quantile estimator.
#'
#' @param x Predictor variable
#' @param y Response variable
#' @param HD Logical: use Harrell-Davis quantile estimator (default: FALSE)
#' @param qval Vector of quantiles to estimate (default: c(0.2, 0.5, 0.8))
#' @param xlab Label for x-axis (default: "X")
#' @param ylab Label for y-axis (default: "Y")
#' @param fr Span parameter controlling smoothing (default: 1)
#' @param sm Logical: use bootstrap smoothing (bagging) (default: FALSE)
#' @param nboot Number of bootstrap samples if sm=TRUE (default: 40)
#' @param SEED Logical: set random seed (default: TRUE)
#' @param eout Logical: remove (x,y) outliers (default: FALSE)
#' @param xout Logical: remove x outliers (default: FALSE)
#' @param ... Additional arguments passed to smoother
#'
#' @details
#' Creates a single plot with multiple smoothed quantile curves. For each
#' quantile in qval, computes a running interval smoother using either:
#' - Standard quantile estimator (qest) if HD=FALSE
#' - Harrell-Davis estimator (hd) if HD=TRUE
#'
#' If sm=TRUE, uses bootstrap bagging (runmbo) for more stable estimates.
#' Otherwise uses basic running smoother (rungen).
#'
#' Missing values are automatically removed.
#'
#' @return
#' NULL (function creates plot as side effect)
#'
#' @seealso \code{\link{rungen}}, \code{\link{runmbo}}, \code{\link{hd}}, \code{\link{qest}}
#'
#' @keywords nonparametric smooth
#' @export
runmq<-function(x,y,HD=FALSE,qval=c(.2,.5,.8),xlab="X",ylab="Y",fr=1,
sm=FALSE,nboot=40,SEED=TRUE,eout=FALSE,xout=FALSE,...){
#
# Plot of running interval smoother based on specified quantiles in
# qval
#
# fr controls amount of smoothing
# tr is the amount of trimming
#
# Missing values are automatically removed.
#
rmd1<-NA
xx<-cbind(x,y)
p<-ncol(xx)-1
xx<-elimna(xx)
x<-xx[,1:p]
y<-xx[,ncol(xx)]
plot(x,y,xlab=xlab,ylab=ylab)
sx1<-sort(x)
xorder1<-order(x)
for(it in 1:length(qval)){
if(!sm){
if(!HD)temp<-rungen(x,y,est=qest,fr=fr,pyhat=TRUE,plotit=FALSE,q=qval[it])
if(HD)temp<-rungen(x,y,est=hd,fr=fr,pyhat=TRUE,plotit=FALSE,q=qval[it])
rmd1<-temp[1]$output
sysm1<-rmd1[xorder1]
lines(sx1,sysm1)
}
if(sm){
if(!HD)temp<-runmbo(x,y,est=qest,fr=fr,pyhat=TRUE,plotit=FALSE,SEED=SEED,
nboot=nboot,eout=FALSE,xout=FALSE,q=qval[it])
if(HD)temp<-runmbo(x,y,est=hd,fr=fr,pyhat=TRUE,plotit=FALSE,SEED=SEED,
nboot=nboot,eout=FALSE,xout=FALSE,q=qval[it])
rmd1<-temp
sysm1<-rmd1[xorder1]
lines(sx1,sysm1)
}
}}




# ----------------------------------------------------------------------------

# runpd

# ----------------------------------------------------------------------------

#' 3D Running Smoother Using Projection Distances
#'
#' @description
#' Compute a 3D running interval smoother where distances from a point are
#' determined using a projection method rather than Mahalanobis distance.
#'
#' @param x An n by p matrix of predictors
#' @param y Vector of response values
#' @param pts Matrix of points at which to predict (default: x)
#' @param est Measure of location (default: tmean)
#' @param fr Span parameter controlling smoothing (default: 0.8)
#' @param plotit Logical: create 3D perspective plot (default: TRUE)
#' @param pyhat Logical: if TRUE, return predicted values (default: FALSE)
#' @param nmin Minimum neighbors for estimation (default: 0)
#' @param scale Logical: scale the plot (default: TRUE)
#' @param expand Relative length of z-axis in plot (default: 0.5)
#' @param xout Logical: remove x outliers (default: FALSE)
#' @param outfun Outlier detection function (default: out)
#' @param pr Logical: print advisory message about scale (default: TRUE)
#' @param xlab Label for x-axis (default: "X1")
#' @param ylab Label for y-axis (default: "X2")
#' @param zlab Label for z-axis (default: "")
#' @param LP Logical: apply lowess post-smoothing (default: TRUE)
#' @param theta Azimuthal viewing angle (default: 50)
#' @param phi Colatitude viewing angle (default: 25)
#' @param duplicate How to handle duplicates for interp() (default: "error")
#' @param MC Logical: use parallel computation in pdclose (default: FALSE)
#' @param ticktype Type of tick marks (default: "simple")
#' @param ... Additional arguments passed to est function
#'
#' @details
#' Similar to runm3d but uses projection-based distances (via pdclose) instead
#' of Mahalanobis distances to determine nearest neighbors. This can be more
#' robust in some settings.
#'
#' For each point in pts, finds neighbors using projection distances, then
#' applies the specified measure of location. If LP=TRUE, applies additional
#' lowess smoothing to the surface.
#'
#' When plotit=TRUE and p=2, creates a perspective plot. Missing values are
#' automatically removed.
#'
#' @return
#' "Done" if pyhat=FALSE, or vector of predicted values if pyhat=TRUE
#'
#' @seealso \code{\link{pdclose}}, \code{\link{runm3d}}, \code{\link{rung3d}}
#'
#' @keywords nonparametric smooth
#' @export
runpd<-function(x,y,pts=x,est=tmean,fr=.8,plotit=TRUE,pyhat=FALSE,nmin=0,scale=TRUE,
expand=.5,xout=FALSE,outfun=out,pr=TRUE,xlab="X1",ylab="X2",zlab="",LP=TRUE,
theta=50,phi=25,duplicate="error",MC=FALSE,ticktype="simple",...){
#
# running mean using interval method
# Distances from a point are determined using a projection method
# see function pdclose
#
# fr controls amount of smoothing
# tr is the amount of trimming
# x is an n by p matrix of predictors.
#
if(is.list(x))stop("Data should  not stored be stored in list mode")
x<-as.matrix(x)
pval<-ncol(x)
xx<-cbind(x,y)
xx<-elimna(xx)
x<-xx[,1:pval]
x<-as.matrix(x)
y<-xx[,pval+1]
if(xout){
keepit<-outfun(x,plotit=FALSE)$keep
x<-x[keepit,]
y<-y[keepit]
}
plotit<-as.logical(plotit)
iout<-c(1:nrow(x))
rmd<-1 # Initialize rmd
nval<-1
nmat<-pdclose(x,pts,fr=fr,MC=MC)
for(i in 1:nrow(pts))rmd[i]<-est(y[nmat[i,]],...)
for(i in 1:nrow(pts))nval[i]<-sum(nmat[i,])
if(ncol(x)==2){
if(plotit){
fitr<-rmd[nval>nmin]
y<-y[nval>nmin]
x<-x[nval>nmin,]
iout<-c(1:length(fitr))
nm1<-length(fitr)-1
for(i in 1:nm1){
ip1<-i+1
for(k in ip1:length(fitr))if(sum(x[i,]==x[k,])==2)iout[k]<-0
}
if(plotit){
if(pr){
if(!scale)print("With dependence, suggest using scale=T")
}
fitr<-rmd[nval>nmin]
iout<-c(1:length(fitr))
nm1<-length(fitr)-1
for(i in 1:nm1){
ip1<-i+1
for(k in ip1:length(fitr))if(sum(x[i,]==x[k,])==2)iout[k]<-0
}
fitr<-fitr[iout>=1] # Eliminate duplicate points in the x-y plane
#                 This is necessary when doing three dimensional plots
#                 with the R function interp
mkeep<-x[iout>=1,]
if(LP)fitr=lplot(x[iout>=1,],fitr,pyhat=TRUE,pr=FALSE,plotit=FALSE)$yhat
fit<-interp(mkeep[,1],mkeep[,2],fitr,duplicate=duplicate)
persp(fit,theta=theta,phi=phi,expand=expand,
scale=scale,xlab=xlab,ylab=ylab,zlab=zlab,ticktype=ticktype)
}}}
if(pyhat)last<-rmd
if(!pyhat)last <- "Done"
        last
}

#' Standard Errors for Running Interval Smoother
#'
#' @description
#' Estimate standard errors for predicted values from a running interval
#' smoother based on trimmed means.
#'
#' @param x Predictor variable
#' @param y Response variable
#' @param fr Span parameter controlling smoothing (default: 1)
#' @param tr Trimming proportion (default: 0.2)
#' @param pts Points at which to estimate SE (default: x)
#' @param RNA Logical: remove NAs (default: FALSE, not used)
#' @param outfun Outlier detection function (default: out)
#' @param xout Logical: remove x outliers (default: FALSE)
#' @param SEED Logical: set random seed (default: TRUE)
#'
#' @details
#' For each point in pts, finds the nearest neighbors and computes the
#' standard error of the trimmed mean using trimse(). Also returns the
#' degrees of freedom for each estimate.
#'
#' Only works with single predictor (not multivariate). Missing values are
#' automatically removed.
#'
#' @return
#' A list with components:
#' \item{se}{Vector of standard errors}
#' \item{df}{Vector of degrees of freedom}
#'
#' @seealso \code{\link{trimse}}, \code{\link{runhat}}, \code{\link{runmean}}
#'
#' @keywords nonparametric smooth
#' @export
runse<-function(x,y,fr=1,tr=.2,pts=x,RNA=FALSE,outfun=out,xout=FALSE,SEED=TRUE){
#
# Estimate SE of Yhat when using a running interval smooth
#  based on a trimmed mean.
# fr controls amount of smoothing
#
# Missing values are automatically removed.
#
# RNA=F, do not remove missing values when averaging
# (computing the smooth) at x
# xout=T removes points for which x is an outlier
#
if(SEED)set.seed(2)
temp<-cbind(x,y)
if(ncol(temp)>2)stop(' 1 predictor only is allowed')
temp<-elimna(temp) # Eliminate any rows with missing values
if(xout){
flag<-outfun(x,plotit=FALSE)$keep
temp<-temp[flag,]
}
x<-temp[,1]
y<-temp[,2]
pts<-as.matrix(pts)
vals<-NA
WSE=NA
df=NA
h=NA
for(i in 1:length(pts)){
ysub=y[near(x,pts[i],fr)]
v=trimse(ysub,tr=tr,na.rm=TRUE)
if(is.na(v))v=0
if(v>0){
WSE[i]=trimse(ysub,tr=tr,na.rm=TRUE)
df[i]=length(ysub)-2*floor(tr*length(ysub))-1
}
if(v==0){
df[i]=0
WSE[i]=0
}}
list(se=WSE,df=df)
}

#' Running Interval Smoother Comparing Two Groups
#'
#' @description
#' Create running interval smoothers for two groups defined by a splitting
#' variable. Plots both smooths on the same graph for comparison.
#'
#' @param x1 Predictor variable for both groups
#' @param y1 Response variable for both groups
#' @param x2 Grouping variable used to split data
#' @param val Split point for x2 (default: median(x2))
#' @param est Measure of location (default: tmean)
#' @param sm Logical: use bootstrap smoothing (default: FALSE)
#' @param fr Span parameter controlling smoothing (default: 0.8)
#' @param xlab Label for x-axis (default: "X")
#' @param ylab Label for y-axis (default: "Y")
#' @param ... Additional arguments passed to plotting function
#'
#' @details
#' Splits the data into two groups based on whether x2 < val, then creates
#' a running interval smooth of x1 vs y1 for each group. Both smooths are
#' plotted on the same graph for comparison.
#'
#' If sm=TRUE, uses runmean2g with smoothing. Otherwise uses basic comparison.
#' Missing values are automatically removed.
#'
#' @return
#' NULL (function creates plot as side effect)
#'
#' @seealso \code{\link{runmean2g}}, \code{\link{rungen}}
#'
#' @keywords nonparametric smooth
#' @export
runsm2g<-function(x1,y1,x2,val=median(x2),est=tmean,sm=FALSE,fr=.8,xlab="X",
ylab="Y",...){
#
# Plot of running interval smoother for two groups
# Groups are defined according to whether x2<val,
# then a smooth of x1 versus y1 for both groups is created.
#
# fr controls amount of smoothing
# tr is the amount of trimming
# est is measure of location which defaults to 20% trimming
#
# Missing values are automatically removed.
#
m<-cbind(x1,y1,x2)
m<-elimna(m)
x2<-m[,3]
flag<-(x2<val)
x1<-m[flag,1]
y1<-m[flag,2]
x2<-m[!flag,1]
y2<-m[!flag,2]
runmean2g(x1, y1, x2, y2, fr = fr, est = est,sm=sm,xlab=xlab,
ylab=ylab,...)
}




# ----------------------------------------------------------------------------

# runYhat

# ----------------------------------------------------------------------------

#' General Running Interval Smoother Prediction
#'
#' @description
#' Fit a running interval smoother and estimate the typical value of Y
#' at specified covariate values. Handles both univariate and multivariate
#' predictors automatically.
#'
#' @param x Vector or matrix of predictors
#' @param y Vector of response values
#' @param pts Points at which to predict y (default: NULL, uses all x values)
#' @param est Measure of location (default: tmean)
#' @param fr Span parameter controlling smoothing (default: 1)
#' @param nmin Minimum neighbors for estimation (default: 1)
#' @param xout Logical: remove x outliers (default: FALSE)
#' @param outfun Outlier detection function (default: outpro)
#' @param XY.used Logical: if TRUE, return data used (default: FALSE)
#' @param ... Additional arguments passed to est function
#'
#' @details
#' A general wrapper function that automatically chooses the appropriate
#' smoother based on the dimensionality of x:
#' - If x is univariate, uses runhat()
#' - If x is multivariate, uses rung3hat()
#'
#' If pts=NULL, predictions are made at all x values (after removing missing
#' values). Missing values are automatically removed from the data.
#'
#' @return
#' A list with components:
#' \item{vals}{Vector of predicted values}
#' \item{nvals}{Vector of sample sizes used for each prediction}
#' \item{XY}{Matrix of x and y values used (only if XY.used=TRUE)}
#'
#' @seealso \code{\link{runhat}}, \code{\link{rung3hat}}, \code{\link{rungen}}
#'
#' @keywords nonparametric smooth
#' @export
runYhat<-function(x,y,pts=NULL,est=tmean,fr=1,nmin=1,xout=FALSE,outfun=outpro,XY.used=FALSE,...){
#
# Fit a running interval smoother using the data in x and y
# Use the fit to estimate the typical value of Y
# corresponding to the covariates values in pts
#
#  pts=NULL means all points in x, after missing values are removed, are used. That is, predict y for each x
#
x<-as.matrix(x)
p=ncol(x)
p1=p+1
xx<-cbind(x,y)
xx<-elimna(xx)
x=xx[,1:p]
y=xx[,p1]
if(is.null(pts))pts=x
x=as.matrix(x)
if(xout){
flag<-outfun(x,plotit=FALSE,...)$keep
x<-x[flag,]
y<-y[flag]
x<-as.matrix(x)
}
if(ncol(x)==1){
vals=runhat(x[,1],y,pts=pts,est=est,fr=fr,nmin=nmin,...)
nvals=1
for(i in 1:length(pts)){
nvals[i]<-length(y[near(x[,1],pts[i],fr=fr)])
}
}
if(ncol(x)>1){
temp=rung3hat(x,y,pts=pts,est=est,fr=fr,...)
vals=temp$rmd
nvals=temp$nval
}
XY=NULL
if(XY.used)XY=cbind(x,y)
list(Y.hat=vals,nvals=nvals,xy.used=XY,pts.used=pts)
}

rplot.pred=runYhat

#' Sign Test for Paired Data
#'
#' Performs a sign test for two dependent groups with confidence interval for
#' the probability that x < y.
#'
#' @param x Numeric vector of first group data, or a matrix/data frame with two columns,
#'   or a list with two components when `y = NULL`.
#' @param y Numeric vector of second group data. If `NULL`, `x` should be a matrix
#'   or list with two columns/components (default: `NULL`).
#' @param dif Numeric vector of pre-computed difference scores. If specified, tests
#'   whether P(dif < 0) = 0.5 (default: `NULL`).
#' @inheritParams common-params
#' @param method Character string specifying the confidence interval method:
#'   \itemize{
#'     \item `"AC"`: Agresti-Coull method (default)
#'     \item `"P"`: Pratt's method
#'     \item `"CP"`: Clopper-Pearson method
#'     \item `"KMS"`: Kulinskaya et al. (2008, p. 140)
#'     \item `"WIL"`: Wilson's method
#'     \item `"SD"`: Schilling-Doi method (returns CI but no p-value)
#'   }
#' @param AUTO Logical. If `TRUE` and n < 35, uses Schilling-Doi method
#'   automatically (default: `TRUE`).
#' @param PVSD Logical. If `TRUE`, uses Schilling-Doi method for p-value computation
#'   (default: `FALSE`).
#'
#' @return A list with components:
#'   \item{Prob_x_less_than_y}{Estimated probability that x < y}
#'   \item{ci}{Confidence interval for the probability}
#'   \item{n}{Original sample size}
#'   \item{N}{Number of paired observations that are not equal to one another}
#'   \item{p.value}{P-value for testing if probability equals 0.5}
#'
#' @details
#' The sign test is a non-parametric test for paired data that only uses the
#' direction (sign) of differences, ignoring their magnitude. Zero differences
#' are excluded from the analysis.
#'
#' The function tests the null hypothesis that P(x < y) = 0.5 against a
#' two-sided alternative. Multiple confidence interval methods are available,
#' with automatic selection based on sample size when `AUTO = TRUE`.
#'
#' @export
#' @examples
#' # Paired data comparison
#' x <- c(10, 12, 15, 18, 20, 22, 25, 28)
#' y <- c(12, 14, 16, 20, 22, 24, 27, 30)
#' signt(x, y)
#'
#' # Data in matrix format
#' dat <- cbind(x, y)
#' signt(dat)
#'
#' # Pre-computed differences
#' diff <- x - y
#' signt(dif = diff)
signt<-function(x,y=NULL,dif=NULL,alpha=.05,method='AC',AUTO=TRUE,PVSD=FALSE){
#
if(is.null(dif)){
if(is.null(y[1])){
if(ncol(as.matrix(x))!=2)stop('y is null so x should be a matrix or data frame with two columns')
if(is.matrix(x)||is.data.frame(x))dif<-x[,1]-x[,2]
if(is.list(x))dif<-x[[1]]-x[[2]]
}
if(!is.null(y[1]))dif<-x-y
}
dif=elimna(dif)
n<-length(dif)
dif<-dif[dif!=0]  # Remove any zero values.
flag<-(dif<0)
temp=binom.conf.pv(y=flag,method=method,alpha=alpha,AUTO=AUTO,PVSD=PVSD,pr=FALSE)
list(Prob_x_less_than_y=temp$p.hat,ci=temp$ci,n=n,N=length(flag),p.value=temp$p.value)
}

# NO LONGER NEED THIS NEXT FUNCTION




# ----------------------------------------------------------------------------

# signtpv

# ----------------------------------------------------------------------------

#' Sign Test with P-value (Deprecated)
#'
#' Performs a sign test for two dependent groups, returning primarily a p-value.
#' This function is largely superseded by \code{\link{signt}}, which now returns
#' p-values as well as confidence intervals.
#'
#' @param x Numeric vector of first group data.
#' @param y Numeric vector of second group data.
#' @param nullval Null hypothesis value for the probability that x < y
#'   (default: 0.5).
#' @inheritParams common-params
#' @param AC Logical. If `TRUE`, uses Agresti-Coull method (default: `FALSE`).
#'   This parameter is deprecated; use the `method` parameter in \code{\link{signt}}
#'   instead.
#'
#' @return A list with components:
#'   \item{output}{Full output from \code{\link{signt}}}
#'   \item{p.value}{P-value for testing if probability equals `nullval`}
#'
#' @details
#' This function computes a p-value for the sign test by inverting confidence
#' intervals using a search algorithm. It repeatedly calls \code{\link{signt}}
#' with different alpha levels to find the smallest alpha where the confidence
#' interval excludes `nullval`.
#'
#' The function is deprecated because \code{\link{signt}} now returns p-values
#' directly, making this wrapper function unnecessary.
#'
#' @seealso \code{\link{signt}} for the main sign test function
#'
#' @export
#' @examples
#' x <- c(10, 12, 15, 18, 20, 22, 25, 28)
#' y <- c(12, 14, 16, 20, 22, 24, 27, 30)
#' signtpv(x, y)
signtpv<-function(x,y,nullval=.5,alpha=.05,AC=FALSE){
#
ci<-signt(x,y,alpha=alpha,method='AC')
alph<-c(1:99)/100
for(i in 1:99){
irem<-i
chkit<-signt(x,y,alpha=alph[i],method='AC')$ci
if(chkit[1]>nullval || chkit[2]<nullval)break
}
p.value<-irem/100
if(p.value<=.1){
iup<-(irem+1)/100
alph<-seq(.001,iup,.001)
for(i in 1:length(alph)){
p.value<-alph[i]
chkit<-signt(x,y,alpha=alph[i],method='AC')$ci
if(chkit[1]>nullval || chkit[2]<nullval)break
}}
if(p.value<=.001){
alph<-seq(.0001,.001,.0001)
for(i in 1:length(alph)){
p.value<-alph[i]
chkit<-signt(x,y,alpha=alph[i],method='AC')$ci
if(chkit[1]>nullval || chkit[2]<nullval)break
}}
list(output=ci,p.value=p.value)
}





# ============================================================================

# Selection (7 functions)

# ============================================================================


# ----------------------------------------------------------------------------

# selby

# ----------------------------------------------------------------------------

#' Select Data by Group from Matrix
#'
#' Groups data stored in a matrix or data frame by one grouping variable and
#' returns the selected columns in list format.
#'
#' @param m A matrix or data frame containing the data with group identifiers
#'   and data columns.
#' @param grpc Integer specifying the column index containing group identification
#'   numbers. Must have length 1.
#' @param coln Integer vector specifying which column(s) of data to extract.
#'   Can specify multiple columns.
#'
#' @return A list with components:
#'   \item{x}{List containing the grouped data. If `coln` has k columns and there
#'     are J groups, the list has length J*k, organized with all columns for
#'     group 1, then all columns for group 2, etc.}
#'   \item{grpn}{Vector of unique group identifiers (sorted)}
#'
#' @details
#' This is a utility function for reorganizing data from wide format (matrix/data
#' frame) to list format required by many WRS analysis functions.
#'
#' The function extracts data from specified columns (`coln`) and groups them
#' according to the group identifier in column `grpc`. The output list is
#' organized so that all requested columns for each group are consecutive.
#'
#' For more complex grouping scenarios:
#' \itemize{
#'   \item Use \code{\link{selby2}} for 2-4 grouping factors
#'   \item Use \code{\link{selbybw}} for between-within designs
#'   \item Use \code{\link{selbybbw}} for between-between-within designs
#' }
#'
#' @seealso \code{\link{selby2}}, \code{\link{selbybw}}, \code{\link{selbybbw}}
#'
#' @export
#' @examples
#' # Create sample data: 3 groups, 2 data columns
#' dat <- data.frame(
#'   group = rep(1:3, each = 5),
#'   score1 = rnorm(15),
#'   score2 = rnorm(15)
#' )
#'
#' # Extract score1 grouped by group
#' result1 <- selby(dat, grpc = 1, coln = 2)
#' length(result1$x)  # 3 groups
#'
#' # Extract both scores grouped by group
#' result2 <- selby(dat, grpc = 1, coln = c(2, 3))
#' length(result2$x)  # 6 (3 groups × 2 columns)
selby<-function(m,grpc,coln){
#
if(is.null(dim(m)))stop("Data must be stored in a matrix or data frame")
if(is.na(grpc[1]))stop("The argument grpc is not specified")
if(is.na(coln[1]))stop("The argument coln is not specified")
if(length(grpc)!=1)stop("The argument grpc must have length 1")
x<-vector("list")
grpn<-sort(unique(m[,grpc]))
it<-0
for (ig in 1:length(grpn)){
for (ic in 1:length(coln)){
it<-it+1
flag<-(m[,grpc]==grpn[ig])
x[[it]]<-m[flag,coln[ic]]
}}
list(x=x,grpn=grpn)
}


#' Select Data by Multiple Grouping Factors
#'
#' Groups data by 2-4 grouping factors and returns the selected column in
#' list format, creating all combinations of the grouping factors.
#'
#' @param m A matrix or data frame containing the data with group identifiers
#'   and data columns.
#' @param grpc Integer vector of length 2-4 specifying the column indices
#'   containing group identification numbers for each factor.
#' @param coln Integer specifying which column of data to extract. Unlike
#'   \code{\link{selby}}, this must be a single column (default: `NA`, must
#'   be specified).
#'
#' @return A list with components:
#'   \item{x}{List containing the grouped data. Length equals the product of
#'     the number of levels in each grouping factor (J × K for 2 factors,
#'     J × K × L for 3 factors, etc.)}
#'   \item{grpn}{Matrix where each row contains the group identifiers for one
#'     combination of factor levels. Number of columns equals `length(grpc)`.}
#'
#' @details
#' This function extends \code{\link{selby}} to handle multiple grouping factors
#' (2-4 factors). It creates all combinations of the grouping factors and extracts
#' data for each combination.
#'
#' The function handles:
#' \itemize{
#'   \item 2 grouping factors: Creates J × K groups
#'   \item 3 grouping factors: Creates J × K × L groups
#'   \item 4 grouping factors: Creates J × K × L × M groups
#' }
#'
#' Only combinations that exist in the data are included in the output. Empty
#' combinations are skipped.
#'
#' For simpler designs, see:
#' \itemize{
#'   \item \code{\link{selby}}: Single grouping factor
#'   \item \code{\link{selbybw}}: Between-within designs
#'   \item \code{\link{selbybbw}}: Between-between-within designs
#' }
#'
#' @seealso \code{\link{selby}}, \code{\link{selbybw}}, \code{\link{selbybbw}}
#'
#' @export
#' @examples
#' # Create sample data with 2 factors
#' dat <- expand.grid(
#'   factor1 = 1:2,
#'   factor2 = 1:3,
#'   rep = 1:5
#' )
#' dat$score <- rnorm(nrow(dat))
#'
#' # Group by both factors (assuming columns 1, 2 are factors, column 4 is data)
#' result <- selby2(dat, grpc = c(1, 2), coln = 4)
#' length(result$x)  # 6 (2 × 3 combinations)
#' result$grpn  # Shows which factor levels each list element represents
selby2<-function(m,grpc,coln=NA){
#
if(is.na(coln))stop("The argument coln is not specified")
if(length(grpc)>4)stop("The argument grpc must have length less than or equal to 4")
x<-vector("list")
ic<-0
if(length(grpc)==2){
cat1<-selby(m,grpc[1],coln)$grpn
cat2<-selby(m,grpc[2],coln)$grpn
for (i1 in 1:length(cat1)){
for (i2 in 1:length(cat2)){
temp<-NA
it<-0
for (i in 1:nrow(m)){
if(sum(m[i,c(grpc[1],grpc[2])]==c(cat1[i1],cat2[i2]))==2){
it<-it+1
temp[it]<-m[i,coln]
}
}
if(!is.na(temp[1])){
ic<-ic+1
x[[ic]]<-temp
if(ic==1)grpn<-matrix(c(cat1[i1],cat2[i2]),1,2)
if(ic>1)grpn<-rbind(grpn,c(cat1[i1],cat2[i2]))
}
}}
}
if(length(grpc)==3){
cat1<-selby(m,grpc[1],coln)$grpn
cat2<-selby(m,grpc[2],coln)$grpn
cat3<-selby(m,grpc[3],coln)$grpn
x<-vector("list")
ic<-0
for (i1 in 1:length(cat1)){
for (i2 in 1:length(cat2)){
for (i3 in 1:length(cat3)){
temp<-NA
it<-0
for (i in 1:nrow(m)){
if(sum(m[i,c(grpc[1],grpc[2],grpc[3])]==c(cat1[i1],cat2[i2],cat3[i3]))==3){
it<-it+1
temp[it]<-m[i,coln]
}}
if(!is.na(temp[1])){
ic<-ic+1
x[[ic]]<-temp
if(ic==1)grpn<-matrix(c(cat1[i1],cat2[i2],cat3[i3]),1,3)
if(ic>1)grpn<-rbind(grpn,c(cat1[i1],cat2[i2],cat3[i3]))
}}}}
}
if(length(grpc)==4){
cat1<-selby(m,grpc[1],coln)$grpn
cat2<-selby(m,grpc[2],coln)$grpn
cat3<-selby(m,grpc[3],coln)$grpn
cat4<-selby(m,grpc[4],coln)$grpn
x<-vector("list")
ic<-0
for (i1 in 1:length(cat1)){
for (i2 in 1:length(cat2)){
for (i3 in 1:length(cat3)){
for (i4 in 1:length(cat4)){
temp<-NA
it<-0
for (i in 1:nrow(m)){
if(sum(m[i,c(grpc[1],grpc[2],grpc[3],grpc[4])]==c(cat1[i1],cat2[i2],cat3[i3],cat4[i4]))==4){
it<-it+1
temp[it]<-m[i,coln]
}}
if(!is.na(temp[1])){
ic<-ic+1
x[[ic]]<-temp
if(ic==1)grpn<-matrix(c(cat1[i1],cat2[i2],cat3[i3],cat4[i4]),1,4)
if(ic>1)grpn<-rbind(grpn,c(cat1[i1],cat2[i2],cat3[i3],cat4[i4]))
}}}}}
}
list(x=x,grpn=grpn)
}




# ----------------------------------------------------------------------------

# selbybbw

# ----------------------------------------------------------------------------

#' Select Data for Between-Between-Within Design
#'
#' Organizes data from a between-between-within (mixed) design into list format
#' suitable for analysis with functions like \code{bbwtrim}.
#'
#' @param m A matrix or data frame containing the data. Two columns contain
#'   between-subjects group identifiers, and other columns contain repeated
#'   measures (within-subjects data).
#' @param grpc Integer vector of length 2 specifying the column indices for
#'   the two between-subjects factors.
#' @param coln Integer vector specifying which columns contain the repeated
#'   measures (within-subjects data). Must specify at least 2 columns.
#' @param pr Logical. If `TRUE`, prints the levels for each between-subjects
#'   factor (default: `TRUE`).
#'
#' @return A list of length J × K × L, where:
#'   \itemize{
#'     \item J = number of levels in first between-subjects factor
#'     \item K = number of levels in second between-subjects factor
#'     \item L = number of repeated measures (length of `coln`)
#'   }
#'   Each element contains data for one combination of between-subjects groups
#'   and one time point.
#'
#' @details
#' This function reorganizes data from a between-between-within (BBW) design
#' into the list format required by many WRS analysis functions. In a BBW design:
#' \itemize{
#'   \item Two factors vary between subjects (each subject is in one cell of
#'     the J × K factorial design)
#'   \item One factor varies within subjects (each subject measured L times)
#' }
#'
#' The output list is organized with all L time points for the first combination
#' of between-subjects groups, then all L time points for the second combination,
#' etc.
#'
#' Missing values are removed row-wise before grouping.
#'
#' @seealso \code{\link{selbybw}} for between-within designs,
#'   \code{\link{selby2}} for general multi-factor grouping
#'
#' @export
#' @examples
#' # Create sample BBW data: 2×2 between, 3 time points
#' dat <- expand.grid(
#'   id = 1:5,
#'   factorA = 1:2,
#'   factorB = 1:2
#' )
#' dat$time1 <- rnorm(nrow(dat))
#' dat$time2 <- rnorm(nrow(dat))
#' dat$time3 <- rnorm(nrow(dat))
#'
#' # Organize for analysis (factors in cols 2-3, times in cols 4-6)
#' result <- selbybbw(dat, grpc = c(2, 3), coln = c(4, 5, 6))
#' length(result)  # 12 (2 × 2 × 3)
selbybbw<-function(m,grpc,coln,pr=TRUE){
#
#if(!is.matrix(m))stop("Data must be stored in a matrix")
if(is.na(grpc[1]))stop("The argument grpc is not specified")
if(is.na(coln[1]))stop("The argument coln is not specified")
if(length(grpc)!=2)stop("The argument grpc must have length 2")
mm=m
m<-as.data.frame(elimna(mm))
x<-list()
grp1<-sort(unique(m[,grpc[1]]))
grp2<-sort(unique(m[,grpc[2]]))
if(pr){
print("Levels for first factor:")
print(grp1)
print("Levels for second factor:")
print(grp2)
}
J<-length(grp1)
K<-length(grp2)
L<-length(coln)
JKL<-J*K*L
itt<-0
it=0
mm=as.matrix(m[,coln])
gmat=matrix(NA,ncol=2,nrow=J*K)
for (ig1 in 1:length(grp1)){
for (ig2 in 1:length(grp2)){
itt=itt+1
gmat[itt,]=c(grp1[ig1],grp2[ig2])
for (ic in 1:length(coln)){
it<-it+1
flag<-(m[,grpc[1]]==grp1[ig1])*(m[,grpc[2]]==grp2[ig2])
flag=as.logical(flag)
x[[it]]<-as.numeric(mm[flag,ic])
}}}
x
}




# ----------------------------------------------------------------------------

# selbybw

# ----------------------------------------------------------------------------

#' Select Data for Between-Within Design
#'
#' Organizes data from a between-within (mixed) design into list format
#' suitable for analysis with robust mixed ANOVA functions.
#'
#' @param m A matrix or data frame containing the data. One column contains
#'   between-subjects group identifiers, and other columns contain repeated
#'   measures (within-subjects data).
#' @param grpc Integer specifying the column index for the between-subjects
#'   factor. Must have length 1.
#' @param coln Integer vector specifying which columns contain the repeated
#'   measures (within-subjects data). Must specify at least 2 columns.
#'
#' @return A list with components:
#'   \item{x}{List of length J × K, where J = number of groups and K = number
#'     of repeated measures. Each element contains data for one combination
#'     of group and time point.}
#'   \item{grpn}{Vector of unique group identifiers (sorted)}
#'
#' @details
#' This function reorganizes data from a between-within (BW) design into the
#' list format required by many WRS mixed ANOVA functions. In a BW design:
#' \itemize{
#'   \item One factor varies between subjects (each subject is in one group)
#'   \item One factor varies within subjects (each subject measured K times)
#' }
#'
#' The output list is organized with all K time points for group 1, then all
#' K time points for group 2, etc.
#'
#' Missing values are removed row-wise before grouping.
#'
#' @seealso \code{\link{selbybbw}} for between-between-within designs,
#'   \code{\link{selby}} for single-factor grouping
#'
#' @export
#' @examples
#' # Create sample BW data: 3 groups, 4 time points
#' dat <- expand.grid(
#'   id = 1:10,
#'   group = 1:3
#' )
#' dat$time1 <- rnorm(nrow(dat))
#' dat$time2 <- rnorm(nrow(dat))
#' dat$time3 <- rnorm(nrow(dat))
#' dat$time4 <- rnorm(nrow(dat))
#'
#' # Organize for analysis (group in col 2, times in cols 3-6)
#' result <- selbybw(dat, grpc = 2, coln = c(3, 4, 5, 6))
#' length(result$x)  # 12 (3 groups × 4 times)
#' result$grpn  # Group identifiers
selbybw<-function(m,grpc,coln){
#

#if(!is.matrix(m))stop("Data must be stored in a matrix")
if(is.na(grpc[1]))stop("The argument grpc is not specified")
if(is.na(coln[1]))stop("The argument coln is not specified")
if(length(grpc)!=1)stop("The argument grpc must have length 1")
x<-list()
m=m[,c(grpc,coln)]
m<-as.data.frame(elimna(m))
grpn<-sort(unique(m[,1]))
J<-length(grpn)
K<-length(coln)
JK<-J*K
it<-0
mm=as.data.frame(m[,2:ncol(m)])
for (ig in 1:length(grpn)){
for (ic in 1:length(coln)){
it<-it+1
flag<-(m[,1]==grpn[ig])
x[[it]]<-as.numeric(mm[flag,ic])
}}
list(x=x,grpn=grpn)
}




# ----------------------------------------------------------------------------

# selvar.ind.ex

# ----------------------------------------------------------------------------

#' Compare Group with Smallest Variance to All Others (Internal Helper)
#'
#' Helper function for \code{\link{selvar.ind.MP}}. Identifies the group with
#' smallest variance and compares it to all other groups using the
#' \code{\link{varcom.IND.MP}} test.
#'
#' @param x A matrix, data frame, or list of numeric vectors, one for each group.
#'
#' @return A numeric vector of p-values from comparing the group with smallest
#'   variance to each of the other groups (length = J - 1, where J is the number
#'   of groups).
#'
#' @details
#' This is an internal helper function used by \code{\link{selvar.ind.MP}} to
#' compute raw p-values. It:
#' \enumerate{
#'   \item Computes variance for each group
#'   \item Identifies the group with smallest variance
#'   \item Compares that group to each other group using \code{\link{varcom.IND.MP}}
#' }
#'
#' Users should typically use \code{\link{selvar.ind.MP}} instead, which provides
#' adjusted p-values and proper decision rules.
#'
#' @seealso \code{\link{selvar.ind.MP}}, \code{\link{selvar.ind.crit}},
#'   \code{\link{varcom.IND.MP}}
#'
#' @keywords internal
#' @export
selvar.ind.ex<-function(x){
#
pvec=NA
x=elimna(x)
if(is.matrix(x))x=listm(x)
J=length(x)
EST=lapply(x,var)
EST=matl(EST)
R=order(EST)
ic=0
for(j in 2:J){
ic=ic+1
pvec[ic]=varcom.IND.MP(x[[R[1]]],x[[R[[j]]]],SEED=FALSE)$p.value
}
pvec
}

#' Determine Null Distribution for Smallest Variance Selection
#'
#' Simulates the null distribution of p-values used by \code{\link{selvar.ind.MP}}
#' for selecting the group with smallest variance. Can be pre-computed and reused
#' for multiple analyses with the same design.
#'
#' @param J Integer. Number of groups.
#' @param n Integer vector of sample sizes for each group (length J).
#' @inheritParams common-params
#' @param iter Integer. Number of Monte Carlo iterations for simulation
#'   (default: 1000).
#' @param ... Additional arguments (currently unused).
#'
#' @return A matrix with `iter` rows and J-1 columns, where each row contains
#'   the p-values from one simulation under the null hypothesis (equal variances).
#'
#' @details
#' This function generates the null distribution of p-values by simulating data
#' from normal distributions with equal variances. For each iteration:
#' \enumerate{
#'   \item Generates J samples from standard normal distributions
#'   \item Identifies the group with smallest variance
#'   \item Compares it to all other groups using \code{\link{varcom.IND.MP}}
#'   \item Records the J-1 p-values
#' }
#'
#' The resulting null distribution can be saved and reused in future calls to
#' \code{\link{selvar.ind.MP}} with the same sample sizes, avoiding redundant
#' computation.
#'
#' @seealso \code{\link{selvar.ind.MP}}, \code{\link{selvar.ind.ex}}
#'
#' @export
#' @examples
#' # Pre-compute null distribution for 4 groups with n=20 each
#' null_dist <- selvar.ind.crit(J = 4, n = rep(20, 4), iter = 500)
#'
#' # Use in subsequent analyses
#' # result <- selvar.ind.MP(mydata, rem = null_dist)
selvar.ind.crit<-function(J,n,alpha=.05,iter=1000,...){
#
Jm1=J-1
rem=matrix(NA,iter,Jm1)
XS=list()
for(k in 1:iter){
for(j in 1:J)XS[[j]]=rnorm(n[j])
rem[k,]=selvar.ind.ex(XS)
}
rem
}




# ----------------------------------------------------------------------------

# selvar.ind.MP

# ----------------------------------------------------------------------------

#' Select Group with Smallest Variance (Independent Groups)
#'
#' Identifies which of J independent groups has the smallest variance and
#' determines whether this group is significantly smaller than the others,
#' controlling for multiple comparisons.
#'
#' @param x A matrix, data frame, or list of numeric vectors, one for each group.
#' @inheritParams common-params
#' @param rem Pre-computed null distribution from \code{\link{selvar.ind.crit}}.
#'   If `NULL`, computes it via simulation (default: `NULL`).
#' @param iter Integer. Number of Monte Carlo iterations for computing null
#'   distribution if `rem = NULL` (default: 2000).
#' @param p.crit Deprecated. Critical p-value threshold (not used in current
#'   implementation).
#' @inheritParams common-params
#'
#' @return An S4 object of class 'SSV' with slots:
#'   \item{Group.with.smallest.estimate}{Index of the group with smallest variance}
#'   \item{Less.than}{Character string indicating the decision:
#'     \itemize{
#'       \item `"No Decision"`: Cannot determine if smallest variance is significantly smaller
#'       \item `"Smaller.than.all"`: Smallest variance is significantly smaller than all others
#'       \item Group indices: Smallest variance is significantly smaller than these specific groups
#'     }}
#'   \item{n}{Vector of sample sizes for each group}
#'   \item{output}{Matrix with columns: `Smallest.Est` (smallest variance),
#'     `Grp` (comparison group index), `Est` (comparison group variance),
#'     `p.value` (raw p-value), `p.adj` (Hochberg-adjusted p-value)}
#'
#' @details
#' This function addresses the problem of identifying the group with smallest
#' variance while controlling for multiple comparisons. The procedure:
#' \enumerate{
#'   \item Identifies the group with smallest variance
#'   \item Compares it to each other group using \code{\link{varcom.IND.MP}}
#'   \item Adjusts p-values using Hochberg's method
#'   \item Makes a decision based on which adjusted p-values are significant
#' }
#'
#' P-values are calibrated against a null distribution (computed via simulation
#' or provided in `rem`) to account for the selection process.
#'
#' Requires at least 3 groups. For pairwise variance comparison, use
#' \code{\link{varcom.IND.MP}} directly.
#'
#' @references
#' Wilcox, R.R. (2017). Introduction to Robust Estimation and Hypothesis Testing
#' (4th ed.). Academic Press.
#'
#' @seealso \code{\link{selvar.ind.crit}} for pre-computing null distribution,
#'   \code{\link{selvar.ind.ex}} for internal helper,
#'   \code{\link{varcom.IND.MP}} for pairwise variance comparison
#'
#' @export
#' @examples
#' # Compare variances across 4 groups
#' set.seed(123)
#' x1 <- rnorm(20, sd = 1)
#' x2 <- rnorm(20, sd = 2)
#' x3 <- rnorm(20, sd = 1.5)
#' x4 <- rnorm(20, sd = 1.8)
#' result <- selvar.ind.MP(list(x1, x2, x3, x4), iter = 500)
#' result@Group.with.smallest.estimate
#' result@Less.than
selvar.ind.MP<-function(x,alpha=.05, rem=NULL,iter=2000,p.crit=NULL,SEED=TRUE){
#
if(is.matrix(x) || is.data.frame(x))x=listm(x)
x=elimna(x)
J=length(x)
if(J<3)stop('Should have 3 or more groups')
Jm1=J-1
est=lapply(x,var)
n=lapply(x,length)
est=as.vector(matl(est))
n=as.vector(matl(n))
R=order(est)
pvec=NA
if(is.null(rem))rem=selvar.ind.crit(J=J,n=n,iter=iter,SEED=SEED)
output<-matrix(NA,Jm1,5)
dimnames(output)=list(NULL,c('Smallest.Est','Grp','Est','p.value','p.adj'))
for(i in 2:J){
im1=i-1
SM=est[R[1]]
a=varcom.IND.MP(x[[R[1]]],x[[R[i]]],SEED=SEED)
pvec[im1]=a$p.value
pv=mean(rem[,im1]<=pvec[im1])
output[im1,1:4]=c(SM,R[i],est[R[i]],pv)
}
output[,5]=p.adjust(output[,4],method='hoch')
Best='No Decision'
flag=sum(output[,5]<=alpha)
id=output[,5]<=alpha
if(sum(flag>0))Best=output[id,2]
if(flag==Jm1)Best='Smaller.than.all'
setClass('SSV',slots=c('Group.with.smallest.estimate','Less.than','n','output'))
put=new('SSV',Group.with.smallest.estimate=R[[1]],Less.than=Best,n=n,output=output)
put
}

#' Two-Way ANOVA: Compare Global KMS Effect Sizes Between Levels
#'
#' @description
#' Compares global Kolmogorov-Smirnov (KMS) effect size measures across pairs
#' of levels in a two-way independent groups ANOVA design. Tests whether effect
#' sizes differ significantly between factor levels.
#'
#' @param J Integer. Number of levels for Factor A.
#' @param K Integer. Number of levels for Factor B.
#' @param x Data in one of two formats:
#'   \itemize{
#'     \item Matrix/data frame: J×K design with each cell containing observations
#'     \item List: Length J×K with each element containing a group's data
#'   }
#' @param tr Numeric. Trimming proportion for robust estimation (default: 0.2).
#' @param alpha Numeric. Significance level for confidence intervals (default: 0.05).
#' @param iter Integer. Number of iterations for null distribution (default: 5000).
#' @param nulldist Optional pre-computed null distribution matrix. If NULL, will be computed.
#' @param SEED Logical. If TRUE, sets random seed to 2 for reproducibility (default: TRUE).
#' @param FAC.B Logical. If FALSE (default), compares Factor A levels; if TRUE, compares Factor B levels.
#' @param ... Additional arguments passed to \code{\link{KS.ANOVA.ES}}.
#'
#' @details
#' This function performs pairwise comparisons of global KMS effect sizes:
#' \enumerate{
#'   \item For each pair of Factor A levels (rows), computes global KMS effect size
#'   \item Tests if effect sizes differ significantly using bootstrap-based null distribution
#'   \item Returns differences, p-values, and confidence intervals for all pairs
#' }
#'
#' The global KMS effect size is computed using \code{\link{KS.ANOVA.ES}}, which
#' measures heterogeneity using the Kolmogorov-Smirnov statistic. Setting
#' \code{FAC.B=TRUE} transposes the design to compare Factor B levels instead.
#'
#' Null distribution is generated from g-and-h distributions (g=0.75) to provide
#' robust critical values under non-normality.
#'
#' @return
#' Matrix with one row per pair comparison, containing:
#' \describe{
#'   \item{A.Level/B.Level}{First factor level in comparison}
#'   \item{A.Level/B.Level}{Second factor level in comparison}
#'   \item{B.Est.1/A.Est.1}{Global KMS effect size for first level}
#'   \item{B.Est.2/A.Est.2}{Global KMS effect size for second level}
#'   \item{Dif}{Difference in effect sizes (Est.1 - Est.2)}
#'   \item{p-value}{Two-sided p-value for difference}
#'   \item{ci.low}{Lower bound of (1-alpha) confidence interval}
#'   \item{ci.up}{Upper bound of (1-alpha) confidence interval}
#' }
#'
#' @seealso \code{\link{ANOG2KMS.ND}} for computing null distribution,
#'   \code{\link{AN2GLOB.KMS}} for analyzing both factors,
#'   \code{\link{KS.ANOVA.ES}} for KMS effect size computation
#'
#' @export
#' @examples
#' # 3x2 two-way design
#' set.seed(123)
#' x <- list()
#' for(i in 1:6) x[[i]] <- rnorm(20)
#' result <- ANOG2KMS(J = 3, K = 2, x = x, iter = 1000)
#' result
ANOG2KMS<-function(J,K,x,tr=.2,alpha=.05,iter=5000,nulldist=NULL,SEED=TRUE,FAC.B=FALSE,...){
#
#  Two-way ANOVA independent groups.
#  Compare global KMS measure of effect size for all pairs of rows
# Example: for row of Factor A, compute KMS global effect size. Do the sample for
# row 2 and test the hypothesis that they are the same. Do this for all pairs of rows.
#
#  Can do the same for  Factor B by setting
#  FAC.B=TRUE.
#
#
if(is.matrix(x) || is.data.frame(x))x=listm(x)
JK=J*K
mat=matrix(c(1:JK),nrow=J,byrow=TRUE)
if(FAC.B){
ic=0
y=list()
for(j in 1:J){
for(k in 1:K){
ic=ic+1
y[ic]=x[mat[j,k]]
}}
x=y
rem.J=J
J=K
K=rem.J
mat=t(mat)
}
num=(J^2-J)/2
n=pool.a.list(lapply(x,length))
if(!is.null(nulldist))V=ND
if(is.null(nulldist)){
if(SEED)set.seed(2)
ndist=NA
V=matrix(NA,iter,num)
ic=0
for(j in 1:J){
for(jj in 1:J){
if(j<jj){
ic=ic+1
v=NA
for(i in 1:iter){
dat=list()
for(L in 1:JK)dat[[L]]=ghdist(n[L],g=.75)
v[i]=KS.ANOVA.ES(dat[mat[j,]])-KS.ANOVA.ES(dat[mat[jj,]])
}
V[,ic]=v
}}}}
A=matrix(NA,num,8)
ic=0
for (j in 1:J){
for(jj in 1:J){
if(j<jj){
ic=ic+1
d1=x[mat[j,]]
d2=x[mat[jj,]]
E1=KS.ANOVA.ES(d1,tr=tr)
E2=KS.ANOVA.ES(d2,tr=tr)
DIF=E1-E2
pv=mean(DIF<=V[,ic])
pv=2*min(pv,1-pv)
q1=qest(V[,ic],alpha/2)
q2=qest(V[,ic],1-alpha/2)
A[ic,1]=j
A[ic,2]=jj
A[ic,3]=E1
A[ic,4]=E2
A[ic,5]=DIF
A[ic,6]=pv
A[ic,7]=DIF-q2
A[ic,8]=DIF-q1
}}}
if(!FAC.B)dimnames(A)=list(NULL,c('A.Level', 'A.Level','B.Est.1','B.Est.2','Dif','p-value','ci.low','ci.up'))
if(FAC.B)dimnames(A)=list(NULL,c('B.Level', 'B.Level','A.Est.1','A.Est.2','Dif','p-value','ci.low','ci.up'))
A
}




# ----------------------------------------------------------------------------

# ANOG2KMS.ND

# ----------------------------------------------------------------------------

#' Compute Null Distribution for ANOG2KMS
#'
#' @description
#' Generates the null distribution for two-way ANOVA comparisons of global KMS
#' effect sizes. Used to calibrate p-values in \code{\link{ANOG2KMS}}.
#'
#' @param J Integer. Number of levels for Factor A.
#' @param K Integer. Number of levels for Factor B.
#' @param n Vector or matrix of sample sizes for each cell in the J×K design.
#' @param tr Numeric. Trimming proportion for robust estimation (default: 0.2).
#' @param iter Integer. Number of Monte Carlo iterations (default: 5000).
#' @param SEED Logical. If TRUE, sets random seed to 2 for reproducibility (default: TRUE).
#' @param FAC.B Logical. If FALSE (default), generates null for Factor A; if TRUE, for Factor B.
#' @param ... Additional arguments passed to \code{\link{KS.ANOVA.ES}}.
#'
#' @details
#' Generates null distribution under the assumption of no differences in KMS
#' effect sizes across factor levels:
#' \enumerate{
#'   \item Simulates data from g-and-h distributions (g=0.75) with specified sample sizes
#'   \item Computes pairwise differences in global KMS effect sizes
#'   \item Repeats for \code{iter} iterations to build null distribution
#' }
#'
#' This null distribution can be pre-computed and passed to \code{\link{ANOG2KMS}}
#' via the \code{nulldist} parameter to avoid re-computing for multiple analyses
#' with the same design.
#'
#' @return
#' Matrix with \code{iter} rows and (J²-J)/2 or (K²-K)/2 columns, where each
#' column contains the null distribution for one pairwise comparison.
#'
#' @seealso \code{\link{ANOG2KMS}} for using the null distribution,
#'   \code{\link{KS.ANOVA.ES}} for KMS effect size computation
#'
#' @export
#' @examples
#' # Generate null distribution for 3x2 design with n=20 per cell
#' n <- rep(20, 6)
#' null_dist <- ANOG2KMS.ND(J = 3, K = 2, n = n, iter = 1000)
#' dim(null_dist)  # 1000 rows, 3 columns (for 3 pairs)
ANOG2KMS.ND<-function(J,K,n,tr=.2,iter=5000,SEED=TRUE,FAC.B=FALSE,...){
#
#  Two-way ANOVA independent groups.
#  Compare global KMS measure of effect size for all pairs of rows and columns
# Example: for row of Factor A, compute KMS global effect size. Do the sample for
# row 2 and test the hypothesis that they are the same. Do this for all pairs of rows.
#
#  Repeat for the columns of Factor B.
#
#
if(SEED)set.seed(2)
ndist=NA
JK=J*K
mat=matrix(c(1:JK),nrow=J,byrow=TRUE)
num=(J^2-J)/2
if(FAC.B)num(K^2-K)/2
V=matrix(NA,iter,num)
ic=0
if(!FAC.B){
for(j in 1:J){
for(jj in 1:J){
if(j<jj){
ic=ic+1
v=NA
for(i in 1:iter){
dat=list()
for(L in 1:JK)dat[[L]]=ghdist(n[L],g=.75)
v[i]=KS.ANOVA.ES(dat[mat[j,]])-KS.ANOVA.ES(dat[mat[jj,]])
}
V[,ic]=v
}}}}

if(FAC.B){
for(j in 1:K){
for(jj in 1:K){
if(j<jj){
ic=ic+1
for(i in 1:iter){
dat=list()
for(L in 1:JK)dat[[L]]=ghdist(n[L],g=.75)
v[i]=KS.ANOVA.ES(dat[mat[,j]])-KS.ANOVA.ES(dat[mat[,jj]])
}
V[,ic]=v
}}}}
V
}

#' Two-Way ANOVA: Compare Global KMS for Both Factors
#'
#' @description
#' Wrapper function that compares global KMS effect sizes for both Factor A
#' and Factor B in a two-way ANOVA design. Calls \code{\link{ANOG2KMS}} twice.
#'
#' @param J Integer. Number of levels for Factor A.
#' @param K Integer. Number of levels for Factor B.
#' @param x Data in one of two formats:
#'   \itemize{
#'     \item Matrix/data frame: J×K design with each cell containing observations
#'     \item List: Length J×K with each element containing a group's data
#'   }
#' @param tr Numeric. Trimming proportion for robust estimation (default: 0.2).
#' @param alpha Numeric. Significance level for confidence intervals (default: 0.05).
#' @param iter Integer. Number of iterations for null distribution (default: 5000).
#' @param SEED Logical. If TRUE, sets random seed to 2 for reproducibility (default: TRUE).
#' @param ... Additional arguments passed to \code{\link{ANOG2KMS}}.
#'
#' @details
#' Performs a complete two-way ANOVA analysis using global KMS effect sizes:
#' \enumerate{
#'   \item Calls \code{\link{ANOG2KMS}} with \code{FAC.B=FALSE} to compare Factor A levels
#'   \item Calls \code{\link{ANOG2KMS}} with \code{FAC.B=TRUE} to compare Factor B levels
#'   \item Returns both sets of comparisons in a list
#' }
#'
#' This provides a comprehensive view of effect size differences across both
#' factors in the design.
#'
#' @return
#' List with two components:
#' \describe{
#'   \item{Factor.A}{Matrix of pairwise comparisons for Factor A (from \code{\link{ANOG2KMS}})}
#'   \item{Factor.B}{Matrix of pairwise comparisons for Factor B (from \code{\link{ANOG2KMS}})}
#' }
#'
#' @seealso \code{\link{ANOG2KMS}} for details on comparison output,
#'   \code{\link{AOV2KMS}} for average KMS comparisons
#'
#' @export
#' @examples
#' # 3x2 two-way design
#' set.seed(123)
#' x <- list()
#' for(i in 1:6) x[[i]] <- rnorm(20)
#' result <- AN2GLOB.KMS(J = 3, K = 2, x = x, iter = 1000)
#' result$Factor.A
#' result$Factor.B
AN2GLOB.KMS<-function(J,K,x,tr=.2,alpha=.05,iter=5000,SEED=TRUE,...){
#
#  Two-way ANOVA independent groups.
#  Compare averages KMS measure of effect size for all pairs of rows and columns
#
if(SEED)set.seed(2)
if(is.matrix(x) || is.data.frame(x))x=listm(x)
A=ANOG2KMS(J,K,x,tr=tr,alpha=alpha,SEED=FALSE)
B=ANOG2KMS(J,K,x,tr=tr,alpha=alpha,SEED=FALSE,FAC.B=TRUE)
list(Factor.A=A,Factor.B=B)
}




# ----------------------------------------------------------------------------

# AOV2KMS

# ----------------------------------------------------------------------------

#' Two-Way ANOVA: Compare Average KMS for Both Factors
#'
#' @description
#' Wrapper function that compares average KMS effect sizes for both Factor A
#' and Factor B in a two-way ANOVA design. Calls \code{\link{AOV2KMS.mcp}} twice.
#'
#' @param J Integer. Number of levels for Factor A.
#' @param K Integer. Number of levels for Factor B.
#' @param x Data in one of two formats:
#'   \itemize{
#'     \item Matrix/data frame: J×K design with each cell containing observations
#'     \item List: Length J×K with each element containing a group's data
#'   }
#' @param tr Numeric. Trimming proportion for robust estimation (default: 0.2).
#' @param alpha Numeric. Significance level for confidence intervals (default: 0.05).
#' @param nboot Integer. Number of bootstrap samples (default: 500).
#' @param SEED Logical. If TRUE, sets random seed to 2 for reproducibility (default: TRUE).
#' @param ... Additional arguments passed to \code{\link{AOV2KMS.mcp}}.
#'
#' @details
#' Performs a complete two-way ANOVA analysis using average KMS effect sizes:
#' \enumerate{
#'   \item Calls \code{\link{AOV2KMS.mcp}} with \code{FAC.B=FALSE} to compare Factor A levels
#'   \item Calls \code{\link{AOV2KMS.mcp}} with \code{FAC.B=TRUE} to compare Factor B levels
#'   \item Returns both sets of comparisons in a list
#' }
#'
#' Unlike \code{\link{AN2GLOB.KMS}} which uses global KMS measures, this function
#' uses average KMS measures across factor levels.
#'
#' @return
#' List with two components:
#' \describe{
#'   \item{Factor.A}{Matrix of pairwise comparisons for Factor A (from \code{\link{AOV2KMS.mcp}})}
#'   \item{Factor.B}{Matrix of pairwise comparisons for Factor B (from \code{\link{AOV2KMS.mcp}})}
#' }
#'
#' @seealso \code{\link{AOV2KMS.mcp}} for details on comparison output,
#'   \code{\link{AN2GLOB.KMS}} for global KMS comparisons
#'
#' @export
#' @examples
#' # 3x2 two-way design
#' set.seed(123)
#' x <- list()
#' for(i in 1:6) x[[i]] <- rnorm(20)
#' result <- AOV2KMS(J = 3, K = 2, x = x, nboot = 500)
#' result$Factor.A
#' result$Factor.B
AOV2KMS<-function(J,K,x,tr=.2,alpha=.05,nboot=500,SEED=TRUE,...){
#
#  Two-way ANOVA independent groups.
#  Compare averages KMS measure of effect size for all pairs of rows and columns
#
if(SEED)set.seed(2)
A=AOV2KMS.mcp(J,K,x,tr=tr,alpha=alpha,SEED=FALSE)
B=AOV2KMS.mcp(J,K,x,tr=tr,alpha=alpha,SEED=FALSE,FAC.B=TRUE)
list(Factor.A=A,Factor.B=B)
}




# ----------------------------------------------------------------------------

# AOV2KMS.mcp

# ----------------------------------------------------------------------------

#' Two-Way ANOVA: Pairwise Comparisons of Average KMS Effect Sizes
#'
#' @description
#' Compares average Kolmogorov-Smirnov (KMS) effect size measures across pairs
#' of levels in a two-way independent groups ANOVA design using bootstrap methods.
#'
#' @param J Integer. Number of levels for Factor A.
#' @param K Integer. Number of levels for Factor B.
#' @param x Data in one of two formats:
#'   \itemize{
#'     \item Matrix/data frame: J×K design with each cell containing observations
#'     \item List: Length J×K with each element containing a group's data
#'   }
#' @param tr Numeric. Trimming proportion for robust estimation (default: 0.2).
#' @param alpha Numeric. Significance level for confidence intervals (default: 0.05).
#' @param nboot Integer. Number of bootstrap samples (default: 500).
#' @param SEED Logical. If TRUE, sets random seed to 2 for reproducibility (default: TRUE).
#' @param FAC.B Logical. If FALSE (default), compares Factor A levels; if TRUE, compares Factor B levels.
#' @param ... Additional arguments passed to \code{\link{KMSmcp.ci}}.
#'
#' @details
#' This function performs pairwise comparisons of average KMS effect sizes:
#' \enumerate{
#'   \item For each level, computes KMS effect sizes using \code{\link{KMSmcp.ci}}
#'   \item Averages these effect sizes across the other factor
#'   \item Compares average effect sizes between pairs of levels via bootstrap
#'   \item Returns differences, confidence intervals, and p-values
#' }
#'
#' The bootstrap procedure resamples within each cell to assess variability of
#' the average effect size differences. Unlike \code{\link{ANOG2KMS}} which uses
#' global effect sizes, this uses averages of pairwise comparisons.
#'
#' @return
#' Matrix with one row per pair comparison, containing:
#' \describe{
#'   \item{Level}{First factor level in comparison}
#'   \item{Level}{Second factor level in comparison}
#'   \item{Est. 1}{Average KMS effect size for first level}
#'   \item{Est.2}{Average KMS effect size for second level}
#'   \item{Dif}{Difference in averages (Est. 1 - Est.2)}
#'   \item{ci.low}{Lower bound of bootstrap confidence interval}
#'   \item{ci.up}{Upper bound of bootstrap confidence interval}
#'   \item{p-value}{Two-sided bootstrap p-value}
#' }
#'
#' @seealso \code{\link{AOV2KMS}} for wrapper analyzing both factors,
#'   \code{\link{ANOG2KMS}} for global KMS comparisons,
#'   \code{\link{KMSmcp.ci}} for KMS effect size computation
#'
#' @export
#' @examples
#' # 3x2 two-way design
#' set.seed(123)
#' x <- list()
#' for(i in 1:6) x[[i]] <- rnorm(20)
#' result <- AOV2KMS.mcp(J = 3, K = 2, x = x, nboot = 500)
#' result
AOV2KMS.mcp<-function(J,K,x,tr=.2,alpha=.05,nboot=500,SEED=TRUE,FAC.B=FALSE,...){
#
#  Two-way ANOVA independent groups.
#  Compare average KMS measure of effect size for all pairs of rows and columns
# Example: for row of Factor A, compute KMS global effect size. Do the sample for
# row 2 and test the hypothesis that the averages are the same. Do this for all pairs of rows.
#
#
if(SEED)set.seed(2)
if(is.matrix(x) || is.data.frame(x))x=listm(x)
JK=J*K
mat=matrix(c(1:JK),nrow=J,byrow=TRUE)
if(FAC.B){
ic=0
y=list()
for(j in 1:J){
for(k in 1:K){
ic=ic+1
y[ic]=x[mat[j,k]]
}}
x=y
rem.J=J
J=K
K=rem.J
mat=t(mat)
}
num=(J^2-J)/2
A=matrix(NA,num,8)
ic=0
for (j in 1:J){
for(jj in 1:J){
if(j<jj){
ic=ic+1
d1=x[mat[j,]]
d2=x[mat[jj,]]
E1=mean(KMSmcp.ci(d1,CI=FALSE,tr=tr)[,3])
E2=mean(KMSmcp.ci(d2,CI=FALSE,tr=tr)[,3])
DIF=E1-E2
chk=NA
y=list()
for(i in 1:nboot){
for(k in 1:JK)y[[k]]=sample(x[[k]],replace=TRUE)
d1=y[mat[j,]]
d2=y[mat[jj,]]
e1=KMSmcp.ci(d1,CI=FALSE,tr=tr)[,3]
e2=KMSmcp.ci(d2,CI=FALSE,tr=tr)[,3]
chk[i]=mean(e1)-mean(e2)
}
crit1=hd(chk,alpha/2)
crit2=hd(chk,1-alpha/2)
chk=sort(chk)
ilow<-round((alpha/2) * nboot)
ihi<-nboot - ilow
ilow<-ilow+1
ci=chk[ilow]
ci[2]=chk[ihi]
pv=mean(chk<0)+.5*mean(chk==0)
pv=2*min(pv,1-pv)
A[ic,1]=j
A[ic,2]=jj
A[ic,3]=E1
A[ic,4]=E2
A[ic,5]=DIF
A[ic,6]=ci[1]
A[ic,7]=ci[2]
A[ic,8]=pv
}}}
dimnames(A)=list(NULL,c('Level', 'Level','Est. 1','Est.2','Dif','ci.low','ci.up','p-value'))
A
}




# ----------------------------------------------------------------------------

# bd1GLOB

# ----------------------------------------------------------------------------

#' One-Way Repeated Measures ANOVA: Global Location Test
#'
#' @description
#' Tests the hypothesis of equal location measures for J dependent groups using
#' a percentile bootstrap method with global estimators that account for the
#' overall data structure when handling outliers.
#'
#' @param x Data in one of two formats:
#'   \itemize{
#'     \item List: Length J with each element containing observations from one condition
#'     \item Matrix: n×J with rows representing subjects and columns representing conditions
#'   }
#' @param est Estimator function for location that returns a value in \code{$center}
#'   (default: \code{spatcen} for spatial median). Use \code{dmean.cen} for
#'   Donoho-Gasko trimmed mean.
#' @param nboot Integer. Number of bootstrap samples (default: 599).
#' @param alpha Numeric. Significance level (default: 0.05).
#' @param SEED Logical. If TRUE, sets random seed to 2 for reproducibility (default: TRUE).
#' @param MC Logical. If TRUE, uses parallel processing via \code{mclapply} (default: FALSE).
#' @param ... Additional arguments passed to the estimator function.
#'
#' @details
#' This function is similar to \code{\link{bd1way}} but designed for global estimators
#' that consider the entire data structure when handling outliers:
#' \enumerate{
#'   \item Computes global location estimates for each group
#'   \item Centers the data by subtracting group estimates
#'   \item Bootstrap resamples subjects (rows) with replacement
#'   \item Computes test statistic: (J-1) × variance of location estimates
#'   \item Calibrates p-value against bootstrap distribution
#' }
#'
#' The spatial median and Donoho-Gasko trimmed mean are multivariate estimators
#' that account for the correlation structure across groups.
#'
#' **Note**: Case-wise deletion is used for missing values.
#'
#' @return
#' List with components:
#' \describe{
#'   \item{test}{Test statistic value: (J-1) × variance of location estimates}
#'   \item{estimates}{Vector of location estimates for each group}
#'   \item{p.value}{Bootstrap p-value}
#' }
#'
#' @seealso \code{\link{bd1way}} for standard dependent groups ANOVA,
#'   \code{\link{bd1GLOB1}} for internal bootstrap helper,
#'   \code{\link{spatcen}} for spatial median,
#'   \code{\link{dmean.cen}} for Donoho-Gasko trimmed mean
#'
#' @export
#' @examples
#' # Create dependent groups data (4 conditions, 20 subjects)
#' set.seed(123)
#' x <- matrix(rnorm(80), ncol = 4)
#' result <- bd1GLOB(x, nboot = 500)
#' result$p.value
bd1GLOB<-function(x,est=spatcen,nboot=599,alpha=.05,SEED=TRUE,MC=FALSE,...){
#
#   Test the hypothesis of equal measures of location for J
#   dependent groups using a
#   percentile bootstrap method.
#
#  Same as bd1way, only designed for estimators that take into account the
#  overall structure of the data when dealing with outliers
#
#   By default, use spatial median  estimator
#  est=dmean.cen will use the Donoho-Gasko trimmed mean.
#
#     argument est is location estimator that returns value in $center
#    (So, for example, est=dmean will not run.)
#
#   Data are assumed to be stored  in list mode or an n by J matrix.
#   misran=F means missing values do not occur at random, case wise deletion is used.
#
#if(MC){
#if(identical(est,dmean_C))stop('Using dmean_C with MC=T can cause R to crash. Use MC=F')
#library(parallel)
#}
# Last 3 commands cause an error unless WRScpp is available.
if(!is.list(x) && !is.matrix(x))stop('Data must be store in list mode or in an n by J matrix.')
if(is.list(x)){
m<-matrix(0,length(x[[1]]),length(x))
for (j in 1:length(x))m[,j]<-x[[j]]
}
if(is.matrix(x))m<-x
xcen<-m
locval=est(m,...)$center
locval=as.vector(locval)
for (j in 1:ncol(m))xcen[,j]<-m[,j]-locval[j]
if(SEED)set.seed(2) # set seed of random number generator so that
#             results can be duplicated.
data<-matrix(sample(nrow(m),size=nrow(m)*nboot,replace=TRUE),nrow=nboot)
data=listm(t(data))
if(MC)bvec<-mclapply(data,bd1GLOB1,xcen=xcen,est=est,...)
if(!MC)bvec<-lapply(data,bd1GLOB1,xcen=xcen,est=est,...)
bvec=as.vector(matl(bvec))
# A vector of  nboot test statistics.
icrit<-floor((1-alpha)*nboot+.5)
test<-(length(locval)-1)*var(locval)
pv=mean((test<bvec))
list(test=test,estimates=locval,p.value=pv)
}




# ----------------------------------------------------------------------------

# bd1GLOB1

# ----------------------------------------------------------------------------

#' Internal Helper for bd1GLOB Bootstrap
#'
#' @description
#' Internal function that computes the test statistic for one bootstrap iteration
#' in \code{\link{bd1GLOB}}. Not intended for direct user access.
#'
#' @param isub Integer vector of length n containing bootstrap sample indices
#'   (sampled with replacement from 1:n).
#' @param xcen Numeric matrix (n×J) containing centered data.
#' @param est Estimator function for location that returns a value in \code{$center}.
#' @param ... Additional arguments passed to the estimator function.
#'
#' @details
#' This function:
#' \enumerate{
#'   \item Subsets centered data using bootstrap indices
#'   \item Computes location estimates for each group
#'   \item Returns test statistic: (J-1) × variance of estimates
#' }
#'
#' Called internally by \code{\link{bd1GLOB}} for each bootstrap iteration.
#'
#' @return
#' Numeric scalar: Bootstrap test statistic value.
#'
#' @seealso \code{\link{bd1GLOB}} for the main function
#'
#' @keywords internal
bd1GLOB1<-function(isub,xcen,est,...){
#
#  Compute test statistic for bd1way
#
#  isub is a vector of length n,
#  a bootstrap sample from the sequence of integers
#  1, 2, 3, ..., n
#
#  xcen is an n by J matrix containing the input data
#
val<-NA
val=est(xcen[isub,],...)$center
val=as.vector(val)
test.stat<-(length(val)-1)*var(val)
test.stat
}




# ----------------------------------------------------------------------------

# bi2KMS

# ----------------------------------------------------------------------------

#' Two Independent Binomials: KMS Confidence Interval
#'
#' @description
#' Computes confidence interval for the difference in success probabilities
#' between two independent binomial distributions using the Kulinskaya-Morgenthaler-Staudte
#' (KMS) method.
#'
#' @param r1 Integer. Number of successes in group 1 (default: sum of non-NA values in \code{x}).
#' @param n1 Integer. Sample size for group 1 (default: length of non-NA values in \code{x}).
#' @param r2 Integer. Number of successes in group 2 (default: sum of non-NA values in \code{y}).
#' @param n2 Integer. Sample size for group 2 (default: length of non-NA values in \code{y}).
#' @param x Optional numeric vector for group 1 (binary: 0/1 values). If provided, \code{r1} and \code{n1} are computed from it.
#' @param y Optional numeric vector for group 2 (binary: 0/1 values). If provided, \code{r2} and \code{n2} are computed from it.
#' @param alpha Numeric. Significance level for confidence interval (default: 0.05).
#'
#' @details
#' Implements the KMS method from Kulinskaya, Morgenthaler, and Staudte (2010)
#' for constructing confidence intervals for the difference in binomial proportions
#' (p1 - p2).
#'
#' The method uses a variance-stabilizing transformation based on the arcsine
#' transformation and provides improved coverage properties compared to
#' standard asymptotic methods, especially for small samples or extreme probabilities.
#'
#' **Note**: Function was updated 1/10/19; results may differ very slightly from
#' the original version.
#'
#' @return
#' List with components:
#' \describe{
#'   \item{ci}{Two-element vector: lower and upper bounds of (1-alpha) confidence interval for p1-p2}
#'   \item{p1}{Observed proportion for group 1: r1/n1}
#'   \item{p2}{Observed proportion for group 2: r2/n2}
#' }
#'
#' @references
#' Kulinskaya, E., Morgenthaler, S., & Staudte, R. G. (2010). Variance stabilizing
#' the difference of two binomial proportions. The American Statistician, 64(4), 350-356.
#'
#' @seealso \code{\link{bi2KMSv2}} for version with p-value,
#'   \code{\link{binom2g}} for alternative methods,
#'   \code{\link{binpair}} for paired binomials
#'
#' @export
#' @examples
#' # Test difference in success rates
#' result <- bi2KMS(r1 = 30, n1 = 100, r2 = 45, n2 = 100)
#' result$ci
#'
#' # Using binary vectors
#' x <- c(1, 1, 0, 1, 0, 1, 1, 0)
#' y <- c(1, 0, 0, 1, 1, 1, 1, 1)
#' result <- bi2KMS(x = x, y = y)
#' result
bi2KMS<-function(r1=sum(elimna(x)),n1=length(elimna(x)),r2=sum(elimna(y)),n2=length(elimna(y)),
x=NULL,y=NULL,alpha=.05){
#
# Test the hypothesis that two independent binomials have equal
# probability of success
#
# r1=number of successes in group 1
# n1=number of observations in group 1
#
# Use Kulinskaya et al. method American Statistician, 2010, 64, 350-
#
#  This function was updated 1/10/19; results might differ very slightly compared to the
#  original version.
#
N=n1+n2
u=.5
Dhat=(r1+.5)/(n1+1)-(r2+.5)/(n2+1)
psihat=((r1+.5)/(n1+1)+(r2+.5)/(n2+1))/2
nuhat=(1-2*psihat)*(.5-n2/N)
what=sqrt(2*u*psihat*(1-psihat)+nuhat^2)
se=qnorm(1-alpha/2)*sqrt(u/(2*n1*n2/N))
val1=max(c(-1,(u*Dhat+nuhat)/what))
ci=what*sin(asin(val1)-se)/u-nuhat/u
val2=min(c(1,(u*Dhat+nuhat)/what))
ci[2]=what*sin(asin(val2)+se)/u-nuhat/u
list(ci=ci,p1=r1/n1,p2=r2/n2)
}

#' Two Independent Binomials: KMS Test with P-Value
#'
#' @description
#' Tests the hypothesis that two independent binomial distributions have equal
#' success probabilities using the KMS method. Unlike \code{\link{bi2KMS}}, this
#' version computes a p-value in addition to a confidence interval.
#'
#' @param r1 Integer. Number of successes in group 1 (default: sum of non-NA values in \code{x}).
#' @param n1 Integer. Sample size for group 1 (default: length of non-NA values in \code{x}).
#' @param r2 Integer. Number of successes in group 2 (default: sum of non-NA values in \code{y}).
#' @param n2 Integer. Sample size for group 2 (default: length of non-NA values in \code{y}).
#' @param x Optional numeric vector for group 1 (binary: 0/1 values). Defaults to NA.
#' @param y Optional numeric vector for group 2 (binary: 0/1 values). Defaults to NA.
#' @param nullval Numeric. Null hypothesis value for p1 - p2 (default: 0).
#' @param alpha Numeric. Significance level for confidence interval (default: 0.05).
#'
#' @details
#' Uses the Kulinskaya-Morgenthaler-Staudte (KMS) method to test H0: p1 - p2 = nullval.
#'
#' The p-value is computed by inverting the confidence interval procedure:
#' \enumerate{
#'   \item Searches for the smallest alpha level where the CI excludes the null value
#'   \item First searches in 0.01 increments (0.01 to 0.99)
#'   \item Refines to 0.001 increments if p-value ≤ 0.10
#' }
#'
#' This provides an exact p-value based on the KMS confidence interval method.
#'
#' @return
#' List with components:
#' \describe{
#'   \item{ci}{Two-element vector: lower and upper bounds of (1-alpha) confidence interval}
#'   \item{p1}{Observed proportion for group 1: r1/n1}
#'   \item{p2}{Observed proportion for group 2: r2/n2}
#'   \item{est.dif}{Estimated difference: p1 - p2}
#'   \item{p.value}{P-value for testing H0: p1 - p2 = nullval}
#' }
#'
#' @references
#' Kulinskaya, E., Morgenthaler, S., & Staudte, R. G. (2010). Variance stabilizing
#' the difference of two binomial proportions. The American Statistician, 64(4), 350-356.
#'
#' @seealso \code{\link{bi2KMS}} for version without p-value,
#'   \code{\link{binom2g}} for alternative methods
#'
#' @export
#' @examples
#' # Test H0: p1 = p2
#' result <- bi2KMSv2(r1 = 30, n1 = 100, r2 = 45, n2 = 100)
#' result$p.value
#' result$ci
bi2KMSv2<-function(r1=sum(elimna(x)),n1=length(elimna(x)),r2=sum(elimna(y)),n2=length(elimna(y)),
x=NA,y=NA,nullval=0,alpha=.05){
#
# Test the hypothesis that two independent binomials have equal
# probability of success using method KMS.
#
#  Unlike the function bi2KMS, a p-value is returned
#
# r1=number of successes in group 1
# n1=number of observations in group 1
#
# Uses Kulinskaya et al. method American Statistician, 2010, 64, 350-
#
#  null value is the hypothesized value for p1-p2
#
alph<-c(1:99)/100
for(i in 1:99){
irem<-i
chkit<-bi2KMS(r1=r1,n1=n1,r2=r2,n2=n2,x=x,y=x,alpha=alph[i])
if(chkit$ci[1]>nullval || chkit$ci[2]<nullval)break
}
p.value<-irem/100
if(p.value<=.1){
iup<-(irem+1)/100
alph<-seq(.001,iup,.001)
for(i in 1:length(alph)){
p.value<-alph[i]
chkit<-bi2KMS(r1=r1,n1=n1,r2=r2,n2=n2,x=x,y=x,alpha=alph[i])
if(chkit$ci[1]>nullval || chkit$ci[2]<nullval)break
}}
est=bi2KMS(r1=r1,n1=n1,r2=r2,n2=n2,x=x,y=y,alpha=alpha)
list(ci=est$ci,p1=est$p1,p2=est$p2,est.dif=est$p1-est$p2,p.value=p.value)
}

#' Between-Within Design: Compare Global Projection Effect Sizes
#'
#' @description
#' Compares global projection effect sizes between two independent groups in a
#' 2×K between-within design. Tests whether projection-based effect sizes differ
#' significantly between groups.
#'
#' @param M1 Numeric matrix for group 1 with K columns (repeated measures).
#' @param M2 Numeric matrix for group 2 with K columns (repeated measures).
#' @param tr Numeric. Trimming proportion for robust estimation (default: 0.2).
#' @param alpha Numeric. Significance level (default: 0.05).
#' @param nboot Integer. Number of bootstrap samples (default: 1000).
#' @param MM Logical. If TRUE, uses marginal medians in projection; if FALSE, uses trimmed means (default: FALSE).
#' @param SEED Logical. If TRUE, sets random seed to 2 for reproducibility (default: TRUE).
#' @param ND Optional pre-computed null distribution. If NULL, will be computed via \code{\link{bwESP.GLOB.B.NULL}}.
#'
#' @details
#' Compares projection-based effect sizes for dependent (repeated) measures across
#' two independent groups:
#' \enumerate{
#'   \item For each group, computes projection effect size using \code{\link{rmESPRO.est}}
#'   \item Effect size is based on standardized distance via projection onto the
#'         line connecting the null vector to the vector of location estimates
#'   \item Tests H0: Effect size for group 1 = Effect size for group 2
#'   \item Calibrates p-value using null distribution (simulated or provided)
#' }
#'
#' The projection method accounts for the correlation structure in repeated
#' measures when computing effect sizes.
#'
#' @return
#' List with components:
#' \describe{
#'   \item{pv}{P-value for test of equal projection effect sizes}
#'   \item{E1}{Projection effect size for group 1}
#'   \item{E2}{Projection effect size for group 2}
#'   \item{DIF}{Difference in effect sizes: E1 - E2}
#' }
#'
#' @seealso \code{\link{bwESP.GLOB.B.NULL}} for computing null distribution,
#'   \code{\link{rmESPRO.est}} for projection effect size computation
#'
#' @export
#' @examples
#' # 2x3 between-within design
#' set.seed(123)
#' M1 <- matrix(rnorm(60), ncol = 3)
#' M2 <- matrix(rnorm(60, mean = 0.5), ncol = 3)
#' result <- bwESP.GLOB.B(M1, M2, nboot = 500)
#' result$pv
bwESP.GLOB.B<-function(M1,M2,tr=.2,alpha=.05,nboot=1000,MM=FALSE,SEED=TRUE,ND=NULL){
#
#  Between-by-within design.  A 2-by-K design here.
#  Basically,
#  M1 and M2 are matrices each with K columns and sampled from some unknown
#  multivariate distribution.
#
#  Compare projection effect size of M1 to the projection effect size of M2
#
#  The effect for the dependent groups is based on
#  a standardized distance based on a projection of the
#  data onto the line connecting the null vector to the
#  vector of estimated trimmed means
#
#  The default amount of trimming is tr=.2
#
if(SEED)set.seed(2)
pv=NULL
M1=elimna(M1)
M2=elimna(M2)
K=ncol(M1)
if(K!=ncol(M2))stop('Number of columns for M1 and M2 differ')
if(is.matrix(x) || is.data.frame(x))x=listm(x)
E1=rmESPRO.est(M1,MM=MM)
E2=rmESPRO.est(M2,MM=MM)
DIF=E1-E2
n1=nrow(M1)
n2=nrow(M2)
if(is.null(ND))ND=bwESP.GLOB.B.NULL(n1,n2,K,MM=MM,tr=tr)
pv=mean(DIF<ND)
pv=2*min(pv,1-pv)
#
#  Confidence intervals can be computed using various versions of the code below
#  But they can result in slightly different results in terms of rejecting and the
#  Type I error probability
#
#q1=qest(ND,alpha/2)
#q2=qest(ND,1-alpha/2)
#q=(q2-q1)/2
#ci=DIF-q2
#ci=DIF-q
#ci[2]=DIF+q1
#ci[2]=DIF+q
#if(pv<alpha){
#if(DIF>0  & ci[1]<0)ci=0.001
#if(DIF<0  & ci[2]>0)ci[2]=0-0.001
#}
#Eif(pv>=alpha){
#if(DIF>0  & ci[1]>0)ci[1]=0-0.001
#if(DIF<0  & ci[2]<0)ci[2]=-0.001
#}
list(n1=n1,n2=n2,Est1=E1,Est2=E2,p.value=pv)
}

#' Compute Null Distribution for bwESP.GLOB.B
#'
#' @description
#' Generates null distribution for comparing global projection effect sizes in
#' a 2×K between-within design. Used to calibrate p-values in \code{\link{bwESP.GLOB.B}}.
#'
#' @param n1 Integer. Sample size for group 1.
#' @param n2 Integer. Sample size for group 2.
#' @param K Integer. Number of repeated measures (columns).
#' @param tr Numeric. Trimming proportion for robust estimation (default: 0.2).
#' @param MM Logical. If TRUE, uses marginal medians; if FALSE, uses trimmed means (default: FALSE).
#' @param SEED Logical. If TRUE, sets random seed to 2 for reproducibility (default: TRUE).
#' @param iter Integer. Number of Monte Carlo iterations (default: 1000).
#' @param g Numeric. g parameter for g-and-h distribution (default: 0 for normal).
#' @param rho Numeric. Common correlation for simulated data (default: 0).
#'
#' @details
#' Generates null distribution under H0: Equal projection effect sizes:
#' \enumerate{
#'   \item Simulates data from multivariate g-and-h distributions with specified correlation
#'   \item Computes projection effect size for each group via \code{\link{rmESPRO.est}}
#'   \item Computes difference in effect sizes
#'   \item Repeats for \code{iter} iterations
#' }
#'
#' The null distribution can be pre-computed and passed to \code{\link{bwESP.GLOB.B}}
#' via the \code{ND} parameter to avoid re-computation.
#'
#' @return
#' Numeric vector of length \code{iter} containing null distribution of
#' effect size differences.
#'
#' @seealso \code{\link{bwESP.GLOB.B}} for using the null distribution,
#'   \code{\link{rmESPRO.est}} for projection effect size computation
#'
#' @export
#' @examples
#' # Generate null distribution for 2x3 design
#' ND <- bwESP.GLOB.B.NULL(n1 = 20, n2 = 20, K = 3, iter = 500)
#' hist(ND)
bwESP.GLOB.B.NULL<-function(n1,n2,K,tr=.2,MM=FALSE,SEED=TRUE,iter=1000,g=0.,rho=0){
if(SEED)set.seed(2)
ND=NA
for(i in 1:iter){
M1=rmul(n1,p=K,g=g,rho=rho)
M2=rmul(n2,p=K,g=g,rho=rho)
ND[i]=rmESPRO.est(M1,MM=MM)-rmESPRO.est(M2,MM=MM)
}
ND
}



 rmESPRO.est<-function(x,est=tmean,MM=FALSE,...){
    #
    #  Estimate projection measure of effect size
    #
    if(is.list(x))x=matl(x)
    x=elimna(x)
    n=nrow(x)
    E=apply(x,2,est,...)
    GM=mean(E)
    J=ncol(x)
    GMvec=rep(GM,J)
    GMvec=rep(GM,J)
    DN=pdis(x,E,center=GMvec,MM=MM)
    DN
}

#' KMS Effect Size: Confidence Interval for Two Groups
#'
#' @description
#' Computes bootstrap confidence interval for the difference between two
#' Kolmogorov-Smirnov (KMS) effect size measures, with optional hypothesis test.
#'
#' @param x Numeric vector for group 1.
#' @param y Numeric vector for group 2.
#' @param tr Numeric. Trimming proportion (default: 0.2). Passed to \code{\link{kms.effect}}.
#' @param alpha Numeric. Significance level for confidence interval (default: 0.05).
#' @param null.val Numeric. Null hypothesis value for effect size difference (default: 0).
#' @param nboot Integer. Number of bootstrap samples (default: 500).
#' @param SEED Logical. If TRUE, sets random seed to 2 for reproducibility (default: TRUE).
#' @param ... Additional arguments passed to \code{\link{kms.effect}}.
#'
#' @details
#' Computes the KMS effect size for each group and their difference, with bootstrap
#' confidence interval:
#' \enumerate{
#'   \item Computes KMS effect size using \code{\link{kms.effect}}
#'   \item Bootstrap resamples within each group with replacement
#'   \item Computes effect size for each bootstrap sample
#'   \item Constructs percentile confidence interval
#'   \item Computes two-sided p-value for H0: effect size = null.val
#' }
#'
#' The KMS effect size measures group separation using the Kolmogorov-Smirnov
#' statistic, providing a robust alternative to standardized mean differences.
#'
#' @return
#' List with components:
#' \describe{
#'   \item{n1}{Sample size for group 1}
#'   \item{n2}{Sample size for group 2}
#'   \item{effect.size}{Observed KMS effect size}
#'   \item{ci}{Bootstrap percentile confidence interval}
#'   \item{p.value}{Two-sided p-value for testing H0: effect size = null.val}
#' }
#'
#' @seealso \code{\link{kms.effect}} for KMS effect size computation,
#'   \code{\link{KMSmcp.ci}} for multiple comparisons,
#'   \code{\link{akp.effect}} for alternative effect size
#'
#' @export
#' @examples
#' # Compare two groups
#' set.seed(123)
#' x <- rnorm(30)
#' y <- rnorm(30, mean = 0.5)
#' result <- KMS.ci(x, y, nboot = 500)
#' result$effect.size
#' result$ci
KMS.ci<-function(x,y,tr=.2,alpha=.05,null.val=0,nboot=500,SEED=TRUE,...){
#
# confidence interval for the difference between to KMS
#  measures of effect size.
#
if(SEED)set.seed(2)
x=elimna(x)
y=elimna(y)
n1=length(x)
n2=length(y)
v=NA
ef=kms.effect(x,y)$effect.size
for(i in 1:nboot){
X=sample(x,n1,replace=TRUE)
Y=sample(y,n2,replace=TRUE)
v[i]=kms.effect(X,Y)$effect.size
}
v=sort(v)
ilow<-round((alpha/2) * nboot)
ihi<-nboot - ilow
ilow<-ilow+1
ci=v[ilow]
ci[2]=v[ihi]
pv=mean(v<null.val)
pv=2*min(pv,1-pv)
list(n1=n1,n2=n2,effect.size=ef,ci=ci,p.value=pv)
}

#' 2×2 Design: Test Interaction via KMS Effect Sizes
#'
#' @description
#' For a 2×2 design, compares the explanatory power (KMS effect sizes) between
#' the two levels of the first factor to test for interaction.
#'
#' @param x Data in one of two formats:
#'   \itemize{
#'     \item List: Length 4 with elements ordered as [level 1,1], [level 1,2],
#'           [level 2,1], [level 2,2]
#'     \item Matrix/data frame: Will be converted to list
#'   }
#' @param tr Numeric. Trimming proportion for robust estimation (default: 0.2).
#' @param alpha Numeric. Significance level for confidence interval (default: 0.05).
#' @param nboot Integer. Number of bootstrap samples (default: 1000).
#' @param SEED Logical. If TRUE, sets random seed to 2 for reproducibility (default: TRUE).
#' @param SW Logical. If TRUE, switches rows and columns (tests Factor B instead of Factor A; default: FALSE).
#'
#' @details
#' Tests for interaction by comparing KMS effect sizes across factor levels:
#' \enumerate{
#'   \item Computes KMS effect size for Factor A at level 1 of Factor B
#'   \item Computes KMS effect size for Factor A at level 2 of Factor B
#'   \item Tests if difference in effect sizes equals zero
#'   \item Uses percentile bootstrap for confidence interval and p-value
#' }
#'
#' If \code{SW=TRUE}, transposes the design to test the other factor's effect sizes.
#'
#' A significant difference indicates an interaction: the effect of Factor A
#' depends on the level of Factor B.
#'
#' @return
#' List with components:
#' \describe{
#'   \item{Est.1}{KMS effect size at first level}
#'   \item{Est.2}{KMS effect size at second level}
#'   \item{Dif}{Difference in effect sizes: Est.1 - Est.2}
#'   \item{ci}{Bootstrap percentile confidence interval for difference}
#'   \item{p.value}{Two-sided p-value for H0: no interaction}
#' }
#'
#' @seealso \code{\link{KMS2way}} for complete 2-way analysis,
#'   \code{\link{kms.effect}} for KMS effect size computation,
#'   \code{\link{KMSinter.mcp}} for multiple interaction tests
#'
#' @export
#' @examples
#' # 2x2 design
#' set.seed(123)
#' x <- list(
#'   rnorm(20), rnorm(20, mean = 0.5),
#'   rnorm(20, mean = 0.3), rnorm(20, mean = 1.2)
#' )
#' result <- KMS.inter.pbci(x, nboot = 500)
#' result$p.value
KMS.inter.pbci<-function(x,tr=.2,alpha=.05,nboot=1000,SEED=TRUE,SW=FALSE){
#
# For a 2-by-2 design, compare
# explanatory power associated with the two levels of the first factor
#
#  SW=TRUE, switches rows and column

if(SEED)set.seed(2)
if(is.matrix(x) || is.data.frame(x))x=listm(x)
if(length(x)!=4)stop('Should have four groups exactly')
for(j in 1:4)x[[j]]=elimna(x[[j]])
if(SW)x=x[c(1,3,2,4)]
v=list()
dif=NA
for(i in 1:nboot){
for(j in 1:4)v[[j]]=sample(x[[j]],replace=TRUE)
a1=kms.effect(v[[1]],v[[2]],tr=tr)$effect.size
a2=kms.effect(v[[3]],v[[4]],tr=tr)$effect.size
dif[i]=a1-a2
}
dif=sort(dif)
ilow<-round((alpha/2) * nboot)
ihi<-nboot - ilow
ilow<-ilow+1
ci=dif[ilow]
ci[2]=dif[ihi]
pv=mean(dif<0)+.5*mean(dif==0)
pv=2*min(pv,1-pv)
a1=kms.effect(x[[1]],x[[2]],tr=tr)$effect.size
a2=kms.effect(x[[3]],x[[4]],tr=tr)$effect.size
Dif=a1-a2
list(Est.1=a1, Est.2=a2,Dif=Dif,ci=ci,p.value=pv)
}




# ----------------------------------------------------------------------------

# KMS2way

# ----------------------------------------------------------------------------

#' Two-Way ANOVA: Complete KMS Effect Size Analysis
#'
#' @description
#' Performs a complete two-way ANOVA analysis using KMS (Kolmogorov-Smirnov)
#' effect sizes for main effects and interactions. Provides robust,
#' heteroscedastic analysis of group differences.
#'
#' @param J Integer. Number of levels for Factor A.
#' @param K Integer. Number of levels for Factor B.
#' @param x Data in one of two formats:
#'   \itemize{
#'     \item Matrix/data frame: J×K design with observations
#'     \item List: Length J×K with each element containing a group's data
#'   }
#' @param tr Numeric. Trimming proportion for robust estimation (default: 0.2).
#' @param alpha Numeric. Significance level for confidence intervals (default: 0.05).
#' @param nboot Integer. Number of bootstrap samples (default: 999).
#' @param SEED Logical. If TRUE, sets random seed to 2 for reproducibility (default: TRUE).
#' @param SW Logical. If TRUE, switches interpretation of factors (default: FALSE).
#'
#' @details
#' Performs comprehensive two-way ANOVA using KMS effect sizes:
#' \enumerate{
#'   \item **Main Effect A**: Pools data across Factor B levels, computes all
#'         pairwise KMS comparisons among Factor A levels
#'   \item **Main Effect B**: Pools data across Factor A levels, computes all
#'         pairwise KMS comparisons among Factor B levels
#'   \item **Interactions**: Tests all pairwise interactions using
#'         \code{\link{KMS.inter.pbci}}
#' }
#'
#' The KMS effect size is a robust, distribution-free measure based on the
#' Kolmogorov-Smirnov statistic, making it appropriate for non-normal data and
#' heterogeneous variances.
#'
#' @return
#' List with components:
#' \describe{
#'   \item{Factor.A}{Matrix of pairwise KMS comparisons for Factor A main effects}
#'   \item{Factor.B}{Matrix of pairwise KMS comparisons for Factor B main effects}
#'   \item{Interaction}{Matrix of all pairwise interaction tests}
#' }
#'
#' Each matrix contains effect sizes, confidence intervals, and p-values.
#'
#' @seealso \code{\link{KMSmcp.ci}} for multiple comparisons,
#'   \code{\link{KMS.inter.pbci}} for interaction tests,
#'   \code{\link{kms.effect}} for KMS effect size computation,
#'   \code{\link{AOV2KMS}} for alternative approach
#'
#' @export
#' @examples
#' # 3x2 two-way design
#' set.seed(123)
#' x <- list()
#' for(i in 1:6) x[[i]] <- rnorm(20)
#' result <- KMS2way(J = 3, K = 2, x = x, nboot = 500)
#' result$Factor.A
#' result$Interaction
KMS2way<-function(J,K,x,tr=.2,alpha=.05,nboot=999,SEED=TRUE,SW=FALSE){
#
# Compare robust, heteroscedastic measures of effect size, the KMS measure effect size
#  For main effects pool the data over levels and do all pairwise comparisons
#
#  Do all interactions
#
if(is.matrix(x)  || is.data.frame(x))x=listm(x)
JK=J*K
mat=matrix(1:JK,J,K,byrow=TRUE)
# First do Factor A
dat=list()
for(j in 1:J){
DAT=NULL
for(k in 1:K){
DAT=c(as.vector(matl(x[mat[j,k]])))
}
dat[[j]]=DAT
}
A=KMSmcp.ci(dat,SEED=SEED)
#
# Factor B next
#
dat=list()
for(k in 1:K){
DAT=NULL
for(j in j:J){
DAT=c(as.vector(matl(x[mat[j,k]])))
}
dat[[k]]=DAT
}
B=KMSmcp.ci(dat,SEED=SEED)
AB=KMSinter.mcp(J,K,x,tr=tr,SEED=SEED,SW=SW)
list(Factor.A=A,Factor.B=B,Interactions=AB)
}




# ----------------------------------------------------------------------------

# KMSgrid.mcp

# ----------------------------------------------------------------------------

#' Grid-Based KMS Analysis: Multiple Comparisons Across Quantile Grids
#'
#' @description
#' Performs KMS (Kolmogorov-Smirnov) effect size comparisons across groups defined
#' by quantile splits of two independent variables, creating a grid-based two-way
#' ANOVA design.
#'
#' @param x Matrix or data frame containing independent variables.
#' @param y Numeric vector containing the dependent variable.
#' @param IV Integer vector of length 2 specifying columns in \code{x} to use as IVs (default: c(1,2)).
#' @param Qsplit1 Numeric vector of quantiles for splitting first IV (default: 0.5 for median split).
#' @param Qsplit2 Numeric vector of quantiles for splitting second IV (default: 0.5 for median split).
#' @param VAL1 Optional numeric vector of specific values for splitting first IV (overrides \code{Qsplit1}).
#' @param VAL2 Optional numeric vector of specific values for splitting second IV (overrides \code{Qsplit2}).
#' @param alpha Numeric. Significance level (default: 0.05).
#' @param SW Logical. If TRUE, switches row/column interpretation in analysis (default: FALSE).
#' @param nulldist Optional pre-computed null distribution.
#' @param est Estimator function for location (default: \code{tmean}).
#' @param iter Integer. Number of iterations for null distribution (default: 1000).
#' @param pr Logical. If TRUE, prints messages about eliminated groups (default: TRUE).
#' @param method Character. Multiple comparison adjustment method (default: 'hoch' for Hochberg).
#' @param xout Logical. If TRUE, removes outliers from IVs before analysis (default: FALSE).
#' @param outfun Function for outlier detection (default: \code{outpro}).
#' @param SEED Logical. If TRUE, sets random seed for reproducibility (default: TRUE).
#' @param ... Additional arguments passed to estimator and outlier functions.
#'
#' @details
#' Creates a grid-based analysis by:
#' \enumerate{
#'   \item Splitting data into bins based on quantiles of two IVs (e.g., quartiles, median splits)
#'   \item Creating a J×K design where J = length(Qsplit1)+1 and K = length(Qsplit2)+1
#'   \item Computing KMS effect sizes using signed (not squared) version
#'   \item Testing main effects via \code{\link{KMSinter.mcp}}
#'   \item Eliminating groups with n ≤ 5
#' }
#'
#' Example splits:
#' \itemize{
#'   \item \code{Qsplit1=c(.25,.5,.75), Qsplit2=.5}: Quartiles × median split
#'   \item \code{Qsplit1=.5, Qsplit2=.5}: 2×2 median split design
#' }
#'
#' @return
#' Result from \code{\link{KMSinter.mcp}} containing main effects and interaction tests
#' for the grid-based design.
#'
#' @seealso \code{\link{KMSinter.mcp}} for interaction analysis,
#'   \code{\link{KMSgridAB}} for alternative grid approach,
#'   \code{\link{KMSgridRC}} for variation-based version,
#'   \code{\link{smgrid.GLOB}} for smoothing-based grid analysis
#'
#' @export
#' @examples
#' # Create 2x2 grid via median splits
#' set.seed(123)
#' x <- matrix(rnorm(200), ncol = 2)
#' y <- rnorm(100)
#' result <- KMSgrid.mcp(x, y, Qsplit1 = .5, Qsplit2 = .5)
KMSgrid.mcp<-function(x,y,IV=c(1,2),Qsplit1=.5,Qsplit2=.5,VAL1=NULL,VAL2=NULL,alpha=05,SW=FALSE,
nulldist=NULL,est=tmean,iter=1000,pr=TRUE,method='hoch',
xout=FALSE,outfun=outpro,SEED=TRUE,...){
#
# Compare robust, heteroscedastic measures of effect size, the KMS measure for two or more groups
# among grids defined by quantiles of two IVs.
# Uses the sign version of KMS (the two group) case rather than the variation measure used by KMSgridRC
#
#   The method tests for main effects based on the
#   signed version (not the squared version) of the  KMS  measure of effect size.
#
# Qsplit: split the independent variable based on the
#         quantiles indicated by Qsplit
#  Example
#   Qsplit1=c(.25,.5,.75)
#   Qsplit2=.5
#   would split based on the quartiles for the first independent variable and the median
#   for the second independent variable
#
#   Basically, reduce the data to a two-way ANOVA  design and examine main effects.
#
#
if(!is.null(VAL1))Qsplit1=PVALS(x[,IV[1]],VAL1)
if(!is.null(VAL2))Qsplit2=PVALS(x[,IV[2]],VAL2)
J=length(Qsplit1)+1
K=length(Qsplit2)+1
x=as.matrix(x)
p=ncol(x)
if(p==1)stop('There should be two or more independent variables')
p1=p+1
xy<-elimna(cbind(x,y))
x<-xy[,1:p]
y<-xy[,p1]
v=NULL
if(xout){
flag<-outfun(x,plotit=FALSE,...)$keep
x<-x[flag,]
y<-y[flag]
xy=cbind(x,y)
}
z=list()
group=list()
N.int=length(Qsplit1)+1
N.int2=length(Qsplit2)+1
est.mat=matrix(NA,nrow=N.int,ncol=N.int2)
n.mat=matrix(NA,nrow=N.int,ncol=N.int2)
DV.mat=matrix(NA,nrow=N.int,ncol=N.int2)
L1=NULL
L2=NULL
for(i in 1:N.int)L1[i]=paste('IV1.G',i)
for(i in 1:N.int2)L2[i]=paste('IV2.G',i)
dimnames(est.mat)=list(L1,L2)
qv=quantile(x[,IV[1]],Qsplit1)
qv=c(min(x[,IV[1]]),qv,max(x[,IV[1]]))
qv2=quantile(x[,IV[2]],Qsplit2)
qv2=c(min(x[,IV[2]]),qv2,max(x[,IV[2]]))
ic=0
for(j in 1:N.int){
j1=j+1
xsub=binmat(xy,IV[1],qv[j],qv[j1])
for(k in 1:N.int2){
k1=k+1
xsub2=binmat(xsub,IV[2],qv2[k],qv2[k1])
est.mat[j,k]=est(xsub2[,p1],...)
n.mat[j,k]=length(xsub2[,p1])
ic=ic+1
z[[ic]]=xsub2[,p1]
if(length(z[[ic]])>6)group[[ic]]=summary(xsub2[,1:p])
}
}
n=NA
for(j in 1:length(z)){
n[j]=length(z[[j]])
}
if(min(n)<=5){
id=which(n>5)
del=which(n<=5)
n=n[id]
if(pr)print(paste('eliminated group',del,'The sample size is less than 6'))
}
a=KMSinter.mcp(J,K,z,tr=tr,SW=SW)
a
}




# ----------------------------------------------------------------------------

# KMSgridAB

# ----------------------------------------------------------------------------

#' Grid-Based Analysis: Test Main Effects Across Quantile Grids
#'
#' @description
#' Splits data based on quantiles of two independent variables and tests hypotheses
#' of equal location measures across the resulting grid cells. Designed for exploring
#' main effects in a flexible grid-based framework.
#'
#' @param x Matrix or data frame containing independent variables.
#' @param y Numeric vector containing the dependent variable.
#' @param IV Integer vector of length 2 specifying columns in \code{x} to use as IVs (default: c(1,2)).
#' @param Qsplit1 Numeric vector of quantiles for splitting first IV (default: 0.5 for median split).
#' @param Qsplit2 Numeric vector of quantiles for splitting second IV (default: 0.5 for median split).
#' @param tr Numeric. Trimming proportion for robust estimation (default: 0.2).
#' @param VAL1 Optional numeric vector of specific values for splitting first IV (overrides \code{Qsplit1}).
#' @param VAL2 Optional numeric vector of specific values for splitting second IV (overrides \code{Qsplit2}).
#' @param PB Logical. If TRUE, uses percentile bootstrap; if FALSE, uses parametric methods (default: FALSE).
#' @param est Estimator function for location (default: \code{tmean}).
#' @param nboot Integer. Number of bootstrap samples (default: 1000).
#' @param pr Logical. If TRUE, prints detailed results (default: TRUE).
#' @param fun Function for computing effect size summaries (default: \code{ES.summary}).
#' @param xout Logical. If TRUE, removes outliers from IVs before analysis (default: FALSE).
#' @param outfun Function for outlier detection (default: \code{outpro}).
#' @param SEED Logical. If TRUE, sets random seed for reproducibility (default: TRUE).
#' @param ... Additional arguments passed to estimator and outlier functions.
#'
#' @details
#' Creates a quantile-based grid analysis:
#' \enumerate{
#'   \item Splits data into bins based on quantiles of both IVs
#'   \item Tests hypothesis of equal location measures across grid cells
#'   \item Can use either quantile splits (Qsplit1/2) or specific values (VAL1/2)
#'   \item Supports both parametric and bootstrap inference
#' }
#'
#' Unlike \code{\link{KMSgrid.mcp}} which focuses on KMS effect sizes, this function
#' tests location differences directly.
#'
#' @return
#' Object containing test results for main effects across the grid, with effect size
#' summaries and p-values.
#'
#' @seealso \code{\link{KMSgrid.mcp}} for KMS-based grid analysis,
#'   \code{\link{KMSgridAV}} for grid analysis with ANOVA,
#'   \code{\link{smgrid.GLOB}} for smoothing-based approach
#'
#' @export
#' @examples
#' # 2x2 grid analysis
#' set.seed(123)
#' x <- matrix(rnorm(200), ncol = 2)
#' y <- rnorm(100)
#' result <- KMSgridAB(x, y, Qsplit1 = .5, Qsplit2 = .5)
KMSgridAB<-function(x,y,IV=c(1,2),Qsplit1=.5,Qsplit2=.5,tr=.2,VAL1=NULL,VAL2=NULL,PB=FALSE,est=tmean,nboot=1000,pr=TRUE,fun=ES.summary,
xout=FALSE,outfun=outpro,SEED=TRUE,...){
#
# Split on two variables, not just one.
#
# Qsplit: split the independent variable based on the
#         quantiles indicated by Qsplit
#  Example
#   Qsplit1=c(.25,.5,.75)
#   Qsplit2=.5
#   would split based on the quartiles for the first independent variable and the median
#   for the second independent variable
#
#  Alternatively, can split the data based on specified values indicating by the arguments
#  VAL1 and VAL2
#
#  Then test the hypothesis of equal measures of location
#  IV[1]: indicates the column of containing the first independent variable to use.
#  IV[2]:  indicates the column of containing the second independent variable to use.
#
#  if(length(unique(y)>2))stop('y should be binary')
x=as.matrix(x)
p=ncol(x)
if(p==1)stop('There should be two or more independent variables')
p1=p+1
xy<-elimna(cbind(x,y))
x<-xy[,1:p]
y<-xy[,p1]
v=NULL
if(xout){
flag<-outfun(x,plotit=FALSE,...)$keep
x<-x[flag,]
y<-y[flag]
xy=cbind(x,y)
}
J=length(Qsplit1)+1
K=length(Qsplit2)+1
z=list()
group=list()
if(is.null(VAL1) || is.null(VAL2)){
N.int=length(Qsplit1)+1
N.int2=length(Qsplit2)+1
}
else {
J=length(VAL1)+1
K=length(VAL2)+1
N.int=length(VAL1)+1
N.int2=length(VAL2)+1
}
JK=J*K
MAT=matrix(1:JK,J,K,byrow=TRUE)
est.mat=matrix(NA,nrow=N.int,ncol=N.int2)
n.mat=matrix(NA,nrow=N.int,ncol=N.int2)
DV.mat=matrix(NA,nrow=N.int,ncol=N.int2)
L1=NULL
L2=NULL
if(is.null(VAL1) || is.null(VAL2)){
qv=quantile(x[,IV[1]],Qsplit1)
qv=c(min(x[,IV[1]]),qv,max(x[,IV[1]]))
qv2=quantile(x[,IV[2]],Qsplit2)
qv2=c(min(x[,IV[2]]),qv2,max(x[,IV[2]]))
}
else{
qv=VAL1
qv=c(min(x[,IV[1]]),qv,max(x[,IV[1]]))
qv2=VAL2
qv2=c(min(x[,IV[2]]),qv2,max(x[,IV[2]]))
}
ic=0
for(j in 1:N.int){
j1=j+1
xsub=binmat(xy,IV[1],qv[j],qv[j1])
for(k in 1:N.int2){
k1=k+1
xsub2=binmat(xsub,IV[2],qv2[k],qv2[k1])
est.mat[j,k]=est(xsub2[,p1],...)
n.mat[j,k]=length(xsub2[,p1])
ic=ic+1
z[[ic]]=xsub2[,p1]
if(length(z[[ic]])>6)group[[ic]]=summary(xsub2[,1:p])
}
}
n=NA
for(j in 1:length(z)){
n[j]=length(z[[j]])
}
if(min(n)<=5){
id=which(n>5)
del=which(n<=5)
n=n[id]
if(pr)print(paste('For group',del,'the sample size is less than 6'))
}
A=list()
B=list()
for(j in 1:J)A[[j]]=lincon(z[MAT[j,]],tr=tr,pr=FALSE)$psihat
for(j in 1:K)B[[j]]=lincon(z[MAT[,j]],tr=tr,pr=FALSE)$psihat
list(est.loc.4.DV=est.mat,n=n.mat,A=A,B=B,A.effect.sizes=A,B.effect.sizes=B)
}




# ----------------------------------------------------------------------------

# KMSgridAV

# ----------------------------------------------------------------------------

#' Grid-Based KMS ANOVA: Average Pairwise Effect Sizes
#'
#' @description
#' Compares effect sizes among groups defined by quantile splits of two independent
#' variables. Tests main effects based on average pairwise KMS effect sizes for each
#' factor level in a grid-based two-way ANOVA framework.
#'
#' @param x Matrix or data frame containing independent variables.
#' @param y Numeric vector containing the dependent variable.
#' @param IV Integer vector of length 2 specifying columns in \code{x} to use as IVs (default: c(1,2)).
#' @param Qsplit1 Numeric vector of quantiles for splitting first IV (default: 0.5 for median split).
#' @param Qsplit2 Numeric vector of quantiles for splitting second IV (default: 0.5 for median split).
#' @param alpha Numeric. Significance level (default: 0.05).
#' @param est Estimator function for location (default: \code{tmean}).
#' @param nboot Integer. Number of bootstrap samples (default: 1000).
#' @param pr Logical. If TRUE, prints messages about eliminated groups (default: TRUE).
#' @param method Character. Multiple comparison adjustment method (default: 'hoch' for Hochberg).
#' @param xout Logical. If TRUE, removes outliers from IVs before analysis (default: FALSE).
#' @param outfun Function for outlier detection (default: \code{outpro}).
#' @param SEED Logical. If TRUE, sets random seed for reproducibility (default: TRUE).
#' @param ... Additional arguments passed to estimator and outlier functions.
#'
#' @details
#' Performs grid-based ANOVA using average KMS effect sizes:
#' \enumerate{
#'   \item Splits data into J×K grid based on quantiles of two IVs
#'   \item For each factor level, computes pairwise KMS effect sizes
#'   \item Averages KMS values within each factor level
#'   \item Tests main effects using \code{\link{AOV2KMS.mcp}}
#'   \item Eliminates groups with n ≤ 5
#' }
#'
#' Unlike \code{\link{KMSgridRC}} which uses global KMS, this function uses
#' average pairwise comparisons, which can be more sensitive to specific
#' pairwise differences.
#'
#' @return
#' List with components:
#' \describe{
#'   \item{Factor.A}{Results for Factor A main effects from \code{\link{AOV2KMS.mcp}}}
#'   \item{Factor.B}{Results for Factor B main effects from \code{\link{AOV2KMS.mcp}}}
#' }
#'
#' @seealso \code{\link{AOV2KMS.mcp}} for average KMS comparisons,
#'   \code{\link{KMSgridRC}} for global KMS version,
#'   \code{\link{KMSgrid.mcp}} for signed KMS version
#'
#' @export
#' @examples
#' # 2x2 grid with average KMS effect sizes
#' set.seed(123)
#' x <- matrix(rnorm(200), ncol = 2)
#' y <- rnorm(100)
#' result <- KMSgridAV(x, y, Qsplit1 = .5, Qsplit2 = .5, nboot = 500)
#' result$Factor.A
KMSgridAV<-function(x,y,IV=c(1,2),Qsplit1=.5,Qsplit2=.5,alpha=.05,est=tmean,nboot=1000,pr=TRUE,method='hoch',
xout=FALSE,outfun=outpro,SEED=TRUE,...){
#
# Compare measures of effect among grids defined by quantiles of two IVs.
#
#  The method tests for main effects based on the average of  pairwise KMS measures of effect size for each level of a Factor
#
# Qsplit: split the independent variable based on the
#         quantiles indicated by Qsplit
#  Example
#   Qsplit1=c(.25,.5,.75)
#   Qsplit2=.5
#   would split based on the quartiles for the first independent variable and the median
#   for the second independent variable
#
#   Basically, reduce the data to a two-way ANOVA and examine main effects.
#
#
J=length(Qsplit1)+1
K=length(Qsplit2)+1
x=as.matrix(x)
p=ncol(x)
if(p==1)stop('There should be two or more independent variables')
p1=p+1
xy<-elimna(cbind(x,y))
x<-xy[,1:p]
y<-xy[,p1]
v=NULL
if(xout){
flag<-outfun(x,plotit=FALSE,...)$keep
x<-x[flag,]
y<-y[flag]
}
z=list()
group=list()
N.int=length(Qsplit1)+1
N.int2=length(Qsplit2)+1
est.mat=matrix(NA,nrow=N.int,ncol=N.int2)
n.mat=matrix(NA,nrow=N.int,ncol=N.int2)
DV.mat=matrix(NA,nrow=N.int,ncol=N.int2)
L1=NULL
L2=NULL
for(i in 1:N.int)L1[i]=paste('IV1.G',i)
for(i in 1:N.int2)L2[i]=paste('IV2.G',i)
dimnames(est.mat)=list(L1,L2)
qv=quantile(x[,IV[1]],Qsplit1)
qv=c(min(x[,IV[1]]),qv,max(x[,IV[1]]))
qv2=quantile(x[,IV[2]],Qsplit2)
qv2=c(min(x[,IV[2]]),qv2,max(x[,IV[2]]))
ic=0
for(j in 1:N.int){
j1=j+1
xsub=binmat(xy,IV[1],qv[j],qv[j1])
for(k in 1:N.int2){
k1=k+1
xsub2=binmat(xsub,IV[2],qv2[k],qv2[k1])
est.mat[j,k]=est(xsub2[,p1],...)
n.mat[j,k]=length(xsub2[,p1])
ic=ic+1
z[[ic]]=xsub2[,p1]
if(length(z[[ic]])>6)group[[ic]]=summary(xsub2[,1:p])
}
}
n=NA
for(j in 1:length(z)){
n[j]=length(z[[j]])
}
if(min(n)<=5){
id=which(n>5)
del=which(n<=5)
n=n[id]
if(pr)print(paste('eliminated group',del,'The sample size is less than 6'))
}
a=AOV2KMS.mcp(J,K,z,tr=tr,nboot=nboot)
b=AOV2KMS.mcp(J,K,z,tr=tr,nboot=nboot,FAC.B=TRUE)
list(Factor.A=a,Factor.B=b)
}




# ----------------------------------------------------------------------------

# KMSgridRC

# ----------------------------------------------------------------------------

#' Grid-Based KMS ANOVA: Global Effect Sizes Across Rows and Columns
#'
#' @description
#' Compares global KMS (Kolmogorov-Smirnov) effect sizes among groups defined
#' by quantile splits of two independent variables. Tests main effects based on
#' the global (variation-based) KMS measure rather than signed comparisons.
#'
#' @param x Matrix or data frame containing independent variables.
#' @param y Numeric vector containing the dependent variable.
#' @param IV Integer vector of length 2 specifying columns in \code{x} to use as IVs (default: c(1,2)).
#' @param Qsplit1 Numeric vector of quantiles for splitting first IV (default: 0.5 for median split).
#' @param Qsplit2 Numeric vector of quantiles for splitting second IV (default: 0.5 for median split).
#' @param VAL1 Optional numeric vector of specific values for splitting first IV (overrides \code{Qsplit1}).
#' @param VAL2 Optional numeric vector of specific values for splitting second IV (overrides \code{Qsplit2}).
#' @param alpha Numeric. Significance level (default: 0.05).
#' @param nulldist.a Optional pre-computed null distribution for Factor A.
#' @param nulldist.b Optional pre-computed null distribution for Factor B.
#' @param est Estimator function for location (default: \code{tmean}).
#' @param iter Integer. Number of iterations for null distribution (default: 5000).
#' @param pr Logical. If TRUE, prints messages about eliminated groups (default: TRUE).
#' @param method Character. Multiple comparison adjustment method (default: 'hoch' for Hochberg).
#' @param xout Logical. If TRUE, removes outliers from IVs before analysis (default: FALSE).
#' @param outfun Function for outlier detection (default: \code{outpro}).
#' @param SEED Logical. If TRUE, sets random seed for reproducibility (default: TRUE).
#' @param ... Additional arguments passed to estimator and outlier functions.
#'
#' @details
#' Performs grid-based ANOVA using global KMS effect sizes:
#' \enumerate{
#'   \item Splits data into J×K grid based on quantiles (or specified values) of two IVs
#'   \item Computes global KMS effect sizes for each factor's levels
#'   \item Tests main effects using \code{\link{ANOG2KMS}}
#'   \item Uses variation-based (squared) KMS measure, not signed version
#'   \item Can accept pre-computed null distributions for faster repeated analyses
#' }
#'
#' The "RC" stands for "Rows and Columns", indicating analysis across both
#' dimensions of the grid.
#'
#' @return
#' List with components:
#' \describe{
#'   \item{Factor.A}{Results for Factor A (rows) from \code{\link{ANOG2KMS}}}
#'   \item{Factor.B}{Results for Factor B (columns) from \code{\link{ANOG2KMS}}}
#' }
#'
#' @seealso \code{\link{ANOG2KMS}} for global KMS comparisons,
#'   \code{\link{KMSgridAV}} for average KMS version,
#'   \code{\link{KMSgrid.mcp}} for signed KMS version
#'
#' @export
#' @examples
#' # 2x2 grid with global KMS effect sizes
#' set.seed(123)
#' x <- matrix(rnorm(200), ncol = 2)
#' y <- rnorm(100)
#' result <- KMSgridRC(x, y, Qsplit1 = .5, Qsplit2 = .5, iter = 1000)
#' result$Factor.A
KMSgridRC<-function(x,y,IV=c(1,2),Qsplit1=.5,Qsplit2=.5,VAL1=NULL,VAL2=NULL,
alpha=.05,nulldist.a=NULL,nulldist.b=NULL,est=tmean,iter=5000,pr=TRUE,method='hoch',
xout=FALSE,outfun=outpro,SEED=TRUE,...){
#
# Compare robust, heteroscedastic measures of effect size, the KMS measure for two or more groups
# among grids defined by quantiles of two IVs.
#
#   The method tests for main effects based on the global KMS  measure of effect size.
#
# Qsplit: split the independent variable based on the
#         quantiles indicated by Qsplit
#  Example
#   Qsplit1=c(.25,.5,.75)
#   Qsplit2=.5
#   would split based on the quartiles for the first independent variable and the median
#   for the second independent variable
#
#   Basically, reduce the data to a two-way ANOVA  design and examine main effects.
#
#   Can split data based on specified values via VAL1 and VAL2
#
#
if(!is.null(VAL1))Qsplit1=PVALS(x[,IV[1]],VAL1)
if(!is.null(VAL2))Qsplit2=PVALS(x[,IV[2]],VAL2)
J=length(Qsplit1)+1
K=length(Qsplit2)+1
x=as.matrix(x)
p=ncol(x)
if(p==1)stop('There should be two or more independent variables')
p1=p+1
xy<-elimna(cbind(x,y))
x<-xy[,1:p]
y<-xy[,p1]
v=NULL
if(xout){
flag<-outfun(x,plotit=FALSE,...)$keep
x<-x[flag,]
y<-y[flag]
xy=cbind(x,y)
}
z=list()
group=list()
N.int=length(Qsplit1)+1
N.int2=length(Qsplit2)+1
est.mat=matrix(NA,nrow=N.int,ncol=N.int2)
n.mat=matrix(NA,nrow=N.int,ncol=N.int2)
DV.mat=matrix(NA,nrow=N.int,ncol=N.int2)
L1=NULL
L2=NULL
for(i in 1:N.int)L1[i]=paste('IV1.G',i)
for(i in 1:N.int2)L2[i]=paste('IV2.G',i)
dimnames(est.mat)=list(L1,L2)
qv=quantile(x[,IV[1]],Qsplit1)
qv=c(min(x[,IV[1]]),qv,max(x[,IV[1]]))
qv2=quantile(x[,IV[2]],Qsplit2)
qv2=c(min(x[,IV[2]]),qv2,max(x[,IV[2]]))
ic=0
for(j in 1:N.int){
j1=j+1
xsub=binmat(xy,IV[1],qv[j],qv[j1])
for(k in 1:N.int2){
k1=k+1
xsub2=binmat(xsub,IV[2],qv2[k],qv2[k1])
est.mat[j,k]=est(xsub2[,p1],...)
n.mat[j,k]=length(xsub2[,p1])
ic=ic+1
z[[ic]]=xsub2[,p1]
if(length(z[[ic]])>6)group[[ic]]=summary(xsub2[,1:p])
}
}
n=NA
for(j in 1:length(z)){
n[j]=length(z[[j]])
}
if(min(n)<=5){
id=which(n>5)
del=which(n<=5)
n=n[id]
if(pr)print(paste('eliminated group',del,'The sample size is less than 6'))
}
a=ANOG2KMS(J,K,z,tr=tr,iter=iter,nulldist=nulldist.a)
b=ANOG2KMS(J,K,z,tr=tr,iter=iter,FAC.B=TRUE,nulldist=nulldist.b)
list(Factor.A=a,Factor.B=b)
}

#' Two-Way ANOVA: Interaction Tests Using KMS Effect Sizes
#'
#' @description
#' Tests all pairwise interactions in a J×K two-way ANOVA design using KMS
#' (Kolmogorov-Smirnov) effect sizes with Hochberg adjustment for multiple comparisons.
#'
#' @param J Integer. Number of levels for Factor A.
#' @param K Integer. Number of levels for Factor B.
#' @param x Data in one of two formats:
#'   \itemize{
#'     \item Matrix/data frame: Will be converted to list
#'     \item List: Length J×K with each element containing a group's data
#'   }
#' @param tr Numeric. Trimming proportion for robust estimation (default: 0.2).
#' @param alpha Numeric. Significance level for confidence intervals (default: 0.05).
#' @param nboot Integer. Number of bootstrap samples (default: 999).
#' @param SEED Logical. If TRUE, sets random seed for reproducibility (default: TRUE).
#' @param SW Logical. If TRUE, switches factor interpretation (default: FALSE).
#'
#' @details
#' Performs comprehensive interaction analysis:
#' \enumerate{
#'   \item Generates all pairwise interaction contrasts using \code{\link{con2way}}
#'   \item For each contrast, computes KMS-based interaction test via \code{\link{KMS.inter.pbci}}
#'   \item Adjusts p-values using Hochberg's method to control family-wise error rate
#'   \item Returns effect sizes, confidence intervals, and adjusted p-values
#' }
#'
#' An interaction is detected when the KMS effect size difference between factor
#' levels varies significantly across the other factor's levels.
#'
#' @return
#' List with components:
#' \describe{
#'   \item{CON}{Matrix with rows for each interaction contrast containing:
#'     \itemize{
#'       \item Con.num: Contrast number
#'       \item Est.1: KMS effect size at first level
#'       \item Est.2: KMS effect size at second level
#'       \item Dif: Difference in effect sizes
#'       \item ci.low, ci.up: Bootstrap confidence interval
#'       \item p.value: Unadjusted p-value
#'       \item p.adjusted: Hochberg-adjusted p-value
#'     }}
#'   \item{con}{Contrast matrix showing which groups are compared}
#' }
#'
#' @seealso \code{\link{KMS.inter.pbci}} for single interaction test,
#'   \code{\link{KMS2way}} for complete two-way analysis,
#'   \code{\link{con2way}} for contrast generation
#'
#' @export
#' @examples
#' # 3x2 design interaction tests
#' set.seed(123)
#' x <- list()
#' for(i in 1:6) x[[i]] <- rnorm(20)
#' result <- KMSinter.mcp(J = 3, K = 2, x = x, nboot = 500)
#' result$CON
KMSinter.mcp<-function(J,K,x,tr=.2,alpha=.05,nboot=999,SEED=TRUE,SW=FALSE){
#
#  Interactions based on KMS measure of effect size
#
if(is.matrix(x)  || is.data.frame(x))x=listm(x)
con=con2way(J,K)$conAB
if(SW){
JK=J*K
M=matrix(c(1:JK),nrow=J,byrow=TRUE)
M=as.vector(M)
x=x[M]
con=con2way(K,J)$conAB
}
num=ncol(con)
CON=matrix(NA,nrow=num,ncol=8)
dimnames(CON)=list(NULL,c('Con.num','Est.1','Est.2','Dif','ci.low','ci.up','p.value','p.adjusted'))
#CON=list()
for(j in 1:ncol(con)){
id=which(con[,j]!=0)
dat=x[id]
temp=pool.a.list(KMS.inter.pbci(dat,tr=tr,alpha=alpha,nboot=nboot,SEED=SEED))
CON[j,1]=j
CON[j,2:7]=temp
}
CON[,8]=p.adjust(CON[,7],method='hoch')
list(CON=CON,con=con)
}




# ----------------------------------------------------------------------------

# KMSmcp.ci

# ----------------------------------------------------------------------------

#' One-Way ANOVA: KMS Effect Sizes for All Pairwise Comparisons
#'
#' @description
#' Computes KMS (Kolmogorov-Smirnov) effect sizes for all pairwise group comparisons
#' in a one-way independent groups design, with optional confidence intervals and
#' adjusted p-values.
#'
#' @param x Data in one of two formats:
#'   \itemize{
#'     \item Matrix/data frame: Each column represents a group
#'     \item List: Each element contains data for one group
#'   }
#' @param tr Numeric. Trimming proportion for robust estimation (default: 0.2).
#' @param alpha Numeric. Significance level for confidence intervals (default: 0.05).
#' @param SEED Logical. If TRUE, sets random seed for reproducibility (default: TRUE).
#' @param nboot Integer. Number of bootstrap samples for CIs (default: 500).
#' @param CI Logical. If TRUE, computes bootstrap confidence intervals (default: TRUE).
#' @param method Character. P-value adjustment method (default: 'hoch' for Hochberg).
#'
#' @details
#' Performs all pairwise KMS comparisons in a one-way design:
#' \enumerate{
#'   \item Computes KMS effect size for each pair using \code{\link{kms.effect}}
#'   \item If \code{CI=TRUE}, computes bootstrap confidence intervals via \code{\link{KMS.ci}}
#'   \item Adjusts p-values using specified method (default: Hochberg)
#'   \item Returns comprehensive results matrix
#' }
#'
#' The KMS effect size is a distribution-free measure based on the Kolmogorov-Smirnov
#' statistic, providing robustness to outliers and non-normality.
#'
#' @return
#' Matrix with one row per pairwise comparison, containing:
#' \describe{
#'   \item{Group}{First group number}
#'   \item{Group}{Second group number}
#'   \item{Effect.Size}{KMS effect size}
#'   \item{low.ci}{Lower bound of confidence interval (if CI=TRUE)}
#'   \item{up.ci}{Upper bound of confidence interval (if CI=TRUE)}
#'   \item{p.value}{Unadjusted p-value (if CI=TRUE)}
#'   \item{p.adjust}{Adjusted p-value using specified method}
#' }
#'
#' @seealso \code{\link{kms.effect}} for KMS effect size computation,
#'   \code{\link{KMS.ci}} for confidence interval calculation,
#'   \code{\link{akp.effect}} for alternative effect size
#'
#' @export
#' @examples
#' # Compare 4 groups
#' set.seed(123)
#' x <- list(
#'   rnorm(20),
#'   rnorm(20, mean = 0.5),
#'   rnorm(20, mean = 1),
#'   rnorm(20, mean = 0.3)
#' )
#' result <- KMSmcp.ci(x, nboot = 500)
#' result
KMSmcp.ci<-function(x,tr=.2,alpha=0.05,SEED=TRUE,nboot=500,CI=TRUE,method='hoch'){
#
#  Estimate KMS effect size  when comparing all
#  pairs of groups in a one-way (independent) groups design
#
#  CI=TRUE: confidence intervals for the measure of effect size are computed.
#
if(is.matrix(x) || is.data.frame(x))x<-listm(x)
if(!is.list(x))stop('Data must be stored in a matrix or in list mode.')
J=length(x)
Jall=(J^2-J)/2
con1=con1way(J)
output=matrix(NA,nrow=Jall,ncol=7)
dimnames(output)=list(NULL,c('Group','Group','Effect.Size','low.ci','up.ci','p.value','p.adjust'))
ic=0
for(j in 1:J){
for(k in 1:J){
if(j<k){
ic=ic+1
output[ic,1]=j
output[ic,2]=k
if(!CI)output[ic,3]=kms.effect(x[[j]],x[[k]],tr=tr)$effect.size
if(CI){
ci=KMS.ci(x[[j]],x[[k]],tr=tr,nboot=nboot,pr=FALSE,SEED=SEED)
output[ic,3]=ci$effect.size
output[ic,4]=ci$ci[1]
output[ic,5]=ci$ci[2]
output[ic,6]=ci$p.value
}}}}
output[,7]=p.adjust(output[,6],method=method)
output
}




# ----------------------------------------------------------------------------

# smgrid.GLOB

# ----------------------------------------------------------------------------

#' Global Test for Location Differences with Two-Variable Splitting
#'
#' @description
#' Splits data based on quantiles or specific values of two independent variables
#' and performs a global test of equal location measures across all resulting groups.
#' Uses GLOB (Global test) methodology.
#'
#' @param x Matrix or data frame of predictor variables
#' @param y Response variable vector
#' @param IV Vector of length 2 specifying which columns of x to use for splitting.
#'   Default is \code{c(1,2)} (first two columns).
#' @param Qsplit1 Quantile(s) for splitting first independent variable. Default is 0.5.
#'   Ignored if \code{VAL1} is specified.
#' @param Qsplit2 Quantile(s) for splitting second independent variable. Default is 0.5.
#'   Ignored if \code{VAL2} is specified.
#' @param tr Trimming proportion (0 to 0.5). Default is 0.2.
#' @param VAL1 Optional vector of specific values for splitting first IV. If provided,
#'   overrides \code{Qsplit1}. Default is \code{NULL}.
#' @param VAL2 Optional vector of specific values for splitting second IV. If provided,
#'   overrides \code{Qsplit2}. Default is \code{NULL}.
#' @param PB Logical. If \code{TRUE}, use percentile bootstrap. Default is \code{FALSE}.
#' @param est Location estimator function. Default is \code{\link{tmean}}.
#' @param nboot Number of bootstrap samples (if \code{PB=TRUE}). Default is 1000.
#' @param pr Logical. If \code{TRUE}, print warnings. Default is \code{TRUE}.
#' @param fun Effect size summary function. Default is \code{\link{ES.summary}}.
#' @param xout Logical. If \code{TRUE}, remove outliers from x. Default is \code{FALSE}.
#' @param outfun Outlier detection function. Default is \code{\link{outpro}}.
#' @param SEED Logical or numeric. Controls random seed. Default is \code{TRUE}.
#' @param ... Additional arguments passed to estimator or outlier function.
#'
#' @details
#' This function:
#' \enumerate{
#'   \item Splits data into J × K groups based on two variables
#'   \item Computes location estimates for each cell
#'   \item Performs global test using \code{\link{t2way}} (ANOVA for trimmed means)
#' }
#'
#' Groups with fewer than 6 observations generate warnings. Unlike \code{\link{smgrid}}
#' which performs pairwise comparisons, this function performs a global omnibus test.
#'
#' @return Result from \code{\link{t2way}} containing global test statistics.
#'
#' @seealso \code{\link{smgrid}} for pairwise comparisons,
#'   \code{\link{t2way}} for two-way ANOVA with trimmed means
#'
#' @export
#' @examples
#' # Global test with two-way split
#' set.seed(123)
#' x <- matrix(rnorm(200), ncol=2)
#' y <- x[,1] + x[,2] + rnorm(100)
#' result <- smgrid.GLOB(x, y, Qsplit1=.5, Qsplit2=.5)
smgrid.GLOB<-function(x,y,IV=c(1,2),Qsplit1=.5,Qsplit2=.5,tr=.2,VAL1=NULL,VAL2=NULL,PB=FALSE,est=tmean,nboot=1000,pr=TRUE,fun=ES.summary,
xout=FALSE,outfun=outpro,SEED=TRUE,...){
#
# Split on two variables, not just one.
#
# Qsplit: split the independent variable based on the
#         quantiles indicated by Qsplit
#  Example
#   Qsplit1=c(.25,.5,.75)
#   Qsplit2=.5
#   would split based on the quartiles for the first independent variable and the median
#   for the second independent variable
#
#  Alternatively, can split the data based on specified values indicating by the arguments
#  VAL1 and VAL2
#
#  Then test the hypothesis of equal measures of location
#  IV[1]: indicates the column of containing the first independent variable to use.
#  IV[2]:  indicates the column of containing the second independent variable to use.
#
#  if(length(unique(y)>2))stop('y should be binary')
x=as.matrix(x)
p=ncol(x)
if(p==1)stop('There should be two or more independent variables')
p1=p+1
xy<-elimna(cbind(x,y))
x<-xy[,1:p]
y<-xy[,p1]
v=NULL
if(xout){
flag<-outfun(x,plotit=FALSE,...)$keep
x<-x[flag,]
y<-y[flag]
xy=cbind(x,y)
}
J=length(Qsplit1)+1
K=length(Qsplit2)+1
z=list()
group=list()
if(is.null(VAL1) || is.null(VAL2)){
N.int=length(Qsplit1)+1
N.int2=length(Qsplit2)+1
}
else {
J=length(VAL1)+1
K=length(VAL2)+1
N.int=length(VAL1)+1
N.int2=length(VAL2)+1
}
JK=J*K
MAT=matrix(1:JK,J,K,byrow=TRUE)
est.mat=matrix(NA,nrow=N.int,ncol=N.int2)
n.mat=matrix(NA,nrow=N.int,ncol=N.int2)
DV.mat=matrix(NA,nrow=N.int,ncol=N.int2)
L1=NULL
L2=NULL
if(is.null(VAL1) || is.null(VAL2)){
qv=quantile(x[,IV[1]],Qsplit1)
qv=c(min(x[,IV[1]]),qv,max(x[,IV[1]]))
qv2=quantile(x[,IV[2]],Qsplit2)
qv2=c(min(x[,IV[2]]),qv2,max(x[,IV[2]]))
}
else{
qv=VAL1
qv=c(min(x[,IV[1]]),qv,max(x[,IV[1]]))
qv2=VAL2
qv2=c(min(x[,IV[2]]),qv2,max(x[,IV[2]]))
}
ic=0
for(j in 1:N.int){
j1=j+1
xsub=binmat(xy,IV[1],qv[j],qv[j1])
for(k in 1:N.int2){
k1=k+1
xsub2=binmat(xsub,IV[2],qv2[k],qv2[k1])
est.mat[j,k]=est(xsub2[,p1],...)
n.mat[j,k]=length(xsub2[,p1])
ic=ic+1
z[[ic]]=xsub2[,p1]
if(length(z[[ic]])>6)group[[ic]]=summary(xsub2[,1:p])
}
}
n=NA
for(j in 1:length(z)){
n[j]=length(z[[j]])
}
if(min(n)<=5){
id=which(n>5)
del=which(n<=5)
n=n[id]
if(pr)print(paste('For group',del,'the sample size is less than 6'))
}
A=t2way(J,K,z)
A
}





# ============================================================================

# Smoothing (20 functions)

# ============================================================================


# ----------------------------------------------------------------------------

# sm.inter

# ----------------------------------------------------------------------------

sm.inter<-function(x,y,IV=c(1,2),Qsplit1=.5,Qsplit2=.5,PB=FALSE,est=tmean,tr=.2,nboot=1000,pr=TRUE,con=NULL,
xout=FALSE,outfun=outpro,SEED=TRUE,...){
#
# Split on two variables.
# Look for interactions
# PB=FALSE: use trimmed means
# PB=TRUE:  use percentile bootstrap.
#
#  TR: amount of trimming when using a non-bootstrap method. To alter the amount of trimming when using
#        a bootstrap method use
#        tr.  Example, tr=.25 would use 25% trimming.
#
#  est=median would use medians
#  est=hd  would use the Harrell-Davis estimator for the median.
#
# Qsplit: split the independent variable based on the
#         quantiles indicated by Qsplit
#  Example
#   Qsplit1=c(.25,.5,.75)
#   Qsplit2=.5
#   would split based on the quartiles for the first independent variable and the median
#   for the second independent variable
#

#  Then test the hypothesis of equal measures of location
#  IV[1]: indicates the column of containing the first independent variable to use.
#  IV[2]:  indicates the column of containing the second independent variable to use.
#
#  if(length(unique(y)>2))stop('y should be binary')
x=as.matrix(x)
p=ncol(x)
if(p==1)stop('There should be two or more independent variables')
p1=p+1
xy<-elimna(cbind(x,y))
x<-xy[,1:p]
y<-xy[,p1]
v=NULL
if(xout){
flag<-outfun(x,plotit=FALSE,...)$keep
x<-x[flag,]
y<-y[flag]
}
if(identical(est,median))PB=TRUE
if(identical(est,hd))PB=TRUE
z=list()
group=list()
N.int=length(Qsplit1)+1
N.int2=length(Qsplit2)+1
est.mat=matrix(NA,nrow=N.int,ncol=N.int2)
L1=NULL
L2=NULL
for(i in 1:N.int)L1[i]=paste('IV1.G',i)
for(i in 1:N.int2)L2[i]=paste('IV2.G',i)
dimnames(est.mat)=list(L1,L2)
qv=quantile(x[,IV[1]],Qsplit1)
qv=c(min(x[,IV[1]]),qv,max(x[,IV[1]]))
qv2=quantile(x[,IV[2]],Qsplit2)
qv2=c(min(x[,IV[2]]),qv2,max(x[,IV[2]]))
ic=0
for(j in 1:N.int){
j1=j+1
xsub=binmat(xy,IV[1],qv[j],qv[j1])
for(k in 1:N.int2){
k1=k+1
xsub2=binmat(xsub,IV[2],qv2[k],qv2[k1])
est.mat[j,k]=est(xsub2[,p1],...)
ic=ic+1
z[[ic]]=xsub2[,p1]
if(length(z[[ic]])>6)group[[ic]]=summary(xsub2[,1:p])
}
}
n=NA
for(j in 1:length(z)){
n[j]=length(z[[j]])
}
if(min(n)<=5){
id=which(n>5)
del=which(n<=5)
n=n[id]
if(pr)print(paste('For group',del,'The sample size is less than 6'))
}
test=NULL
if(is.null(con))con=con2way(N.int,N.int2)$conAB
if(!PB){
a=lincon(z,con=con,tr=tr,pr=FALSE)
test=a$test
psihat=a$psihat
}
if(PB){
a=linconpb(z,con=con,est=est,...)
psihat=a$output
}
ES=IND.PAIR.ES(z,con=con)$effect.size
list(Group.summary=group,loc.est=est.mat,test=test,psihat=psihat,con=con,Effect.Sizes=ES)
}

smgridLC=sm.inter

#' Compare Two Smoothing Methods Visually
#'
#' @description
#' Plots predictions from two different smoothing methods against each other to
#' assess agreement. If smoothers give similar results, points should cluster
#' tightly around the line y=x (slope=1, intercept=0).
#'
#' @param x Predictor variable vector
#' @param y Response variable vector
#' @param method1 First smoothing method. Default is 'RUN' (running interval smoother)
#' @param method2 Second smoothing method. Default is 'RF' (random forest smoother)
#' @param xout Logical. If \code{TRUE}, remove outliers in x before smoothing.
#'   Default is \code{FALSE}.
#' @param outfun Outlier detection function. Default is \code{\link{outpro}}.
#' @param xlab Label for x-axis (predictions from method1). Default is 'Est1'.
#' @param ylab Label for y-axis (predictions from method2). Default is 'Est2'.
#' @param pch Plotting character for points. Default is '.'.
#' @param pr Logical. If \code{TRUE}, print messages. Default is \code{TRUE}.
#' @param xoutL Logical. If \code{TRUE}, check for outliers when creating the
#'   plot. Default is \code{FALSE}.
#' @param ... Additional arguments passed to \code{\link{smpred}}
#'
#' @details
#' This function:
#' \enumerate{
#'   \item Computes predictions using both smoothing methods via \code{\link{smpred}}
#'   \item Creates scatter plot of method1 vs method2 predictions
#'   \item Adds reference line y=x (dashed) for perfect agreement
#' }
#'
#' Agreement between smoothers suggests robust results. Systematic deviations
#' indicate sensitivity to smoothing method choice.
#'
#' @return Creates a plot comparing the two smoothers (invisible NULL).
#'
#' @seealso \code{\link{smpred}} for computing smooth predictions
#'
#' @export
#' @examples
#' set.seed(123)
#' x <- 1:100
#' y <- 2*x + 10*rnorm(100)
#' sm.vs.sm(x, y, method1='RUN', method2='RF')
sm.vs.sm<-function(x,y,method1='RUN',method2='RF',xout=FALSE,outfun=outpro,xlab='Est1',
ylab='Est2',pch='.',pr=TRUE,xoutL=FALSE,...){
#
# If the smoothers give similar results, the plot returned here should be
# tightly clustered around a line having slope=1 and intercept=0, indicated
# by a dashed line.
#
# if(!xoutL)print('Suggest also looking at result using xoutL=TRUE)
e1=smpred(x,y,method=method1,xout=xout,outfun=outfun,...)
e2=smpred(x,y,method=method2,xout=xout,outfun=outfun,...)
lplot(e1,e2,xlab=xlab,ylab=ylab,pc=pch,xout=xoutL,pr=FALSE)
abline(0,1,lty=2)
}

#' Test if Identifying Group with Largest Location is Reasonable
#'
#' @description
#' Determines whether it is statistically reasonable to identify which group
#' has the largest measure of location (trimmed mean) by testing all contrasts
#' that compare the maximum group to all others.
#'
#' @param x Data as matrix, data frame, or list. Each column/element represents
#'   a group.
#' @param tr Trimming proportion (0 to 0.5). Default is 0.2 (20% trimming).
#' @param ... Additional arguments (currently unused).
#'
#' @details
#' This function:
#' \enumerate{
#'   \item Computes trimmed means for all groups
#'   \item Identifies the group with the largest trimmed mean
#'   \item Creates contrasts comparing that group to all others
#'   \item Tests all contrasts using \code{\link{lincon}}
#'   \item Returns the maximum p-value across all tests
#' }
#'
#' A small p-value (e.g., < 0.05) suggests it is reasonable to declare which
#' group has the largest location measure. A large p-value indicates uncertainty.
#'
#' @return List with components:
#' \describe{
#'   \item{Est.}{Vector of trimmed means for each group}
#'   \item{p.value}{Maximum p-value from testing all relevant contrasts}
#' }
#'
#' @seealso \code{\link{rmbestPB.DO}} for percentile bootstrap version,
#'   \code{\link{lincon}} for linear contrast testing
#'
#' @export
#' @examples
#' # Test which of 3 groups has largest mean
#' set.seed(123)
#' x <- list(rnorm(20), rnorm(20, mean=1), rnorm(20, mean=0.5))
#' result <- best.DO(x)
#' result
best.DO<-function(x,tr=.2,...){
#
#  Determine whether it is reasonable to
#  decide which group has largest measure of location
#
#
chk=0
if(is.matrix(x)||is.data.frame(x))x<-listm(x)
x=elimna(x)
J=length(x)
e=lapply(x,tmean,tr)
e=pool.a.list(e)
id=which(e==max(e))
CON=conCON(J,id)$conCON
a=lincon(x,con=CON,pr=FALSE)
pv=max(a$psihat[,5])
list(Est.=e,p.value=pv)
}

#' Test if Identifying Group with Largest Location is Reasonable (Bootstrap)
#'
#' @description
#' Bootstrap version of \code{\link{best.DO}} that determines whether it is
#' statistically reasonable to identify which group has the largest measure of
#' location using percentile bootstrap methods and a flexible estimator.
#'
#' @param x Data as matrix, data frame, or list. Each column/element represents
#'   a group.
#' @param est Location estimator function. Default is \code{\link{tmean}}.
#'   Can be \code{median}, \code{mean}, etc.
#' @param nboot Number of bootstrap samples. If \code{NA}, a default value is used.
#'   Default is \code{NA}.
#' @param SEED Logical or numeric. Controls random seed for reproducibility.
#'   Default is \code{TRUE}.
#' @param ... Additional arguments passed to the estimator function.
#'
#' @details
#' This function:
#' \enumerate{
#'   \item Computes location estimates for all groups using specified estimator
#'   \item Identifies the group with the largest estimate
#'   \item Creates contrasts comparing that group to all others
#'   \item Tests contrasts using percentile bootstrap (\code{\link{rmmcppb}})
#'   \item Returns the maximum p-value across all tests
#' }
#'
#' A small p-value suggests it is reasonable to declare which group has the
#' largest location. Unlike \code{\link{best.DO}}, this version uses bootstrap
#' methods and supports any location estimator.
#'
#' @return List with components:
#' \describe{
#'   \item{Est.}{Vector of location estimates for each group}
#'   \item{p.value}{Maximum p-value from testing all relevant contrasts}
#'   \item{con}{Contrast matrix used in testing}
#' }
#'
#' @seealso \code{\link{best.DO}} for trimmed mean version without bootstrap,
#'   \code{\link{rmmcppb}} for repeated measures bootstrap testing
#'
#' @export
#' @examples
#' # Test which of 3 groups has largest median (bootstrap)
#' set.seed(123)
#' x <- list(rnorm(20), rnorm(20, mean=1), rnorm(20, mean=0.5))
#' result <- rmbestPB.DO(x, est=median, nboot=500)
#' result
rmbestPB.DO<-function(x,est=tmean,nboot=NA,SEED=TRUE,...){
#
#  Determine whether it is reasonable to
#  decide which group has largest measure of location
#
#
if(is.list(x))x=matl(x)
x=elimna(x)
x<-listm(x)
J=length(x)
e=lapply(x,est,...)
e=pool.a.list(e)
id=which(e==max(e))
CON=conCON(J,id)$conCON
a=rmmcppb(x,con=CON,dif=FALSE,est=est,nboot=nboot,SEED=SEED,pr=FALSE,...)
pv=max(a$output[,3])
list(Est.=e,p.value=pv,con=CON)
}



#' Test Interactions for Binary Outcome with Two-Variable Splitting
#'
#' @description
#' Splits data based on quantiles of two independent variables and tests for
#' interactions when the dependent variable is binary. Uses binomial methods
#' to compare probabilities across groups.
#'
#' @param x Matrix or data frame of predictor variables
#' @param y Binary dependent variable vector (0/1 or two unique values)
#' @param IV Vector of length 2 specifying which columns of x to use for splitting.
#'   Default is \code{c(1,2)} (first two columns).
#' @param Qsplit1 Quantile(s) for splitting first independent variable. Default is 0.5
#'   (median split). Can be vector for multiple splits (e.g., \code{c(.25,.5,.75)} for quartiles).
#' @param Qsplit2 Quantile(s) for splitting second independent variable. Default is 0.5.
#' @param alpha Significance level. Default is 0.05.
#' @param con Optional contrast matrix for testing specific comparisons. If \code{NULL},
#'   default interaction contrasts are used.
#' @param xout Logical. If \code{TRUE}, remove outliers from x before analysis.
#'   Default is \code{FALSE}.
#' @param outfun Outlier detection function. Default is \code{\link{outpro}}.
#' @param SEED Logical or numeric. Controls random seed. Default is \code{TRUE}.
#' @param ... Additional arguments passed to outlier function.
#'
#' @details
#' This function:
#' \enumerate{
#'   \item Splits data into groups based on quantiles of two independent variables
#'   \item Computes probability of success (proportion of 1s) in each group
#'   \item Tests interaction contrasts using \code{\link{lincon.bin}}
#' }
#'
#' The dependent variable y must be binary (contain exactly 2 unique values).
#' Groups with fewer than 6 observations generate warnings.
#'
#' @return List with components:
#' \describe{
#'   \item{Group.summary}{Summaries for each group}
#'   \item{Prob.est}{Matrix of probability estimates for each cell}
#'   \item{output}{Confidence intervals and tests from \code{\link{lincon.bin}}}
#'   \item{con}{Contrast matrix used}
#' }
#'
#' @seealso \code{\link{lincon.bin}} for binomial linear contrasts,
#'   \code{\link{smbin.test}} for one-variable splitting,
#'   \code{\link{smbinAB}} for main effects testing
#'
#' @export
#' @examples
#' # Test interaction for binary outcome
#' set.seed(123)
#' x <- matrix(rnorm(200), ncol=2)
#' y <- rbinom(100, 1, plogis(x[,1] + x[,2] + x[,1]*x[,2]))
#' result <- smbin.inter(x, y, Qsplit1=.5, Qsplit2=.5)
smbin.inter<-function(x,y,IV=c(1,2),Qsplit1=.5,Qsplit2=.5,alpha=.05,con=NULL,xout=FALSE,outfun=outpro,SEED=TRUE,...){
#
# Split on two variables.
# Look for interactions when dealing with binary dependent variable.
#
# Qsplit: split the independent variable based on the
#         quantiles indicated by Qsplit
#  Example
#   Qsplit1=c(.25,.5,.75)
#   Qsplit2=.5
#   would split based on the quartiles for the first independent variable and the median
#   for the second independent variable
#
#  IV[1]: indicates the column of containing the first independent variable to use.
#  IV[2]:  indicates the column of containing the second independent variable to use.
#
x=as.matrix(x)
p=ncol(x)
if(p==1)stop('There should be two or more independent variables')
p1=p+1
xy<-elimna(cbind(x,y))
x<-xy[,1:p]
y<-xy[,p1]
 if(length(unique(y))>2)stop('y should be binary')
v=NULL
if(xout){
flag<-outfun(x,plotit=FALSE,...)$keep
x<-x[flag,]
y<-y[flag]
}
z=list()
group=list()
N.int=length(Qsplit1)+1
N.int2=length(Qsplit2)+1
if(is.null(con))con=con2way(N.int,N.int2)$conAB
est.mat=matrix(NA,nrow=N.int,ncol=N.int2)
L1=NULL
L2=NULL
for(i in 1:N.int)L1[i]=paste('IV1.G',i)
for(i in 1:N.int2)L2[i]=paste('IV2.G',i)
dimnames(est.mat)=list(L1,L2)
qv=quantile(x[,IV[1]],Qsplit1)
qv=c(min(x[,IV[1]]),qv,max(x[,IV[1]]))
qv2=quantile(x[,IV[2]],Qsplit2)
qv2=c(min(x[,IV[2]]),qv2,max(x[,IV[2]]))
ic=0
for(j in 1:N.int){
j1=j+1
xsub=binmat(xy,IV[1],qv[j],qv[j1])
for(k in 1:N.int2){
k1=k+1
xsub2=binmat(xsub,IV[2],qv2[k],qv2[k1])
est.mat[j,k]=mean(xsub2[,p1],...)
ic=ic+1
z[[ic]]=xsub2[,p1]
if(length(z[[ic]])>6)group[[ic]]=summary(xsub2[,1:p])
}
}
r=NA
n=NA
for(j in 1:length(z)){
r[j]=sum(z[[j]])
n[j]=length(z[[j]])
}
if(min(n)<=5){
del=which(n<=5)
if(pr)print(paste('For group',del,'the sample size is less than 6'))
}
test=lincon.bin(r,n,con=con)
list(Group.summary=group,Prob.est=est.mat,output=test$CI,con=con)
}

#' Test Binary Outcome Probabilities Across Quantile-Based Groups
#'
#' @description
#' Splits data based on quantiles of one independent variable and compares
#' probabilities of success (binary outcome) across resulting groups using
#' pairwise binomial tests.
#'
#' @param x Matrix or data frame of predictor variables
#' @param y Binary dependent variable vector (0/1 or two unique values)
#' @param IV Column index in x for splitting. Default is 1 (first column).
#' @param Qsplit Quantile(s) for splitting. Default is 0.5 (median split).
#'   Can be vector (e.g., \code{c(.25,.5,.75)} for quartiles).
#' @param method Method for pairwise comparisons passed to \code{\link{binpair}}.
#'   Default is 'SK'. Options include 'SK', 'KMS', etc.
#' @param nboot Number of bootstrap samples (if method requires). Default is 1000.
#' @param xout Logical. If \code{TRUE}, remove outliers before analysis.
#'   Default is \code{FALSE}.
#' @param outfun Outlier detection function. Default is \code{\link{outpro}}.
#' @param SEED Logical or numeric. Controls random seed. Default is \code{TRUE}.
#' @param ... Additional arguments passed to outlier function.
#'
#' @details
#' This function:
#' \enumerate{
#'   \item Splits data into groups based on quantiles of one variable
#'   \item Computes number of successes and total observations per group
#'   \item Performs all pairwise comparisons using \code{\link{binpair}}
#' }
#'
#' The dependent variable y must be binary. Unlike \code{\link{smbin.inter}},
#' this function splits on only one variable.
#'
#' @return List with components:
#' \describe{
#'   \item{Group.summary}{Summary statistics for each group}
#'   \item{output}{Pairwise comparison results from \code{\link{binpair}}}
#' }
#'
#' @seealso \code{\link{binpair}} for pairwise binomial tests,
#'   \code{\link{smbin.inter}} for two-variable splitting
#'
#' @export
#' @examples
#' # Compare binary outcome across median split
#' set.seed(123)
#' x <- matrix(rnorm(200), ncol=2)
#' y <- rbinom(100, 1, plogis(x[,1]))
#' result <- smbin.test(x, y, IV=1, Qsplit=.5)
smbin.test<-function(x,y,IV=1,Qsplit=.5,method='SK',nboot=1000,
xout=FALSE,outfun=outpro,SEED=TRUE,...){
#
#
# Qsplit: split the independent variable indicated by the argument
# IV; it defaults to 1, the first variable , column 1, in x.
#  Example:
#  IV=1: split on the first independent variable,
#  IV=2:  split on the second independent variable,
#
# Qsplit indicates the  quantiles to be used and defaults to .5.
#  Example Qsplit=c(.25,.5,.75) would split based on the quartiles
#
#  Then compare the probability of success corresponding to the groups.
#
#  if(length(unique(y)>2))stop('y should be binary')
x=as.matrix(x)
p=ncol(x)
if(p==1)stop('There should be two or more independent variables')
p1=p+1
xy<-elimna(cbind(x,y))
x<-xy[,1:p]
y<-xy[,p1]
v=NULL
if(xout){
flag<-outfun(x,plotit=FALSE,...)$keep
m<-xy[flag,]
x<-m[,1:p]
y<-m[,p1]
}
z=list()
N.int=length(Qsplit)+1
qv=quantile(x[,IV],Qsplit)
qv=c(min(x[,IV]),qv,max(x[,IV]))
group=list()
for(j in 1:N.int){
j1=j+1
xsub=binmat(xy,IV,qv[j],qv[j1])
z[[j]]=xsub[,p1]
group[[j]]=summary(xsub[,IV])
}
r=NA
n=NA
for(j in 1:N.int){
r[j]=sum(z[[j]])
n[j]=length(z[[j]])
}
a=binpair(r,n,method=method)
list(Group.summary=group,output=a)
}




# ----------------------------------------------------------------------------

# smbinAB

# ----------------------------------------------------------------------------

#' Test Main Effects for Binary Outcome with Two-Variable Splitting
#'
#' @description
#' Splits data based on quantiles of two independent variables and tests main
#' effects (marginal comparisons) for each variable when the dependent variable
#' is binary. Tests probabilities across levels of each factor separately.
#'
#' @param x Matrix or data frame of predictor variables
#' @param y Binary dependent variable vector (0/1 or two unique values)
#' @param IV Vector of length 2 specifying which columns of x to use for splitting.
#'   Default is \code{c(1,2)} (first two columns).
#' @param Qsplit1 Quantile(s) for splitting first independent variable. Default is 0.5
#'   (median split). Can be vector (e.g., \code{c(.25,.5,.75)} for quartiles).
#' @param Qsplit2 Quantile(s) for splitting second independent variable. Default is 0.5.
#' @param tr Trimming proportion (for related continuous analyses). Default is 0.2.
#' @param method Method for binomial comparisons passed to \code{\link{lincon.bin}}.
#'   Default is 'KMS'. Other options include 'AC', 'SK', etc.
#' @param xout Logical. If \code{TRUE}, remove outliers from x before analysis.
#'   Default is \code{FALSE}.
#' @param outfun Outlier detection function. Default is \code{\link{outpro}}.
#' @param ... Additional arguments passed to outlier function.
#'
#' @details
#' This function:
#' \enumerate{
#'   \item Splits data into J × K groups based on quantiles of two variables
#'   \item Computes probabilities (proportions) for each cell
#'   \item Tests main effect A: compares probability across levels of IV1
#'   \item Tests main effect B: compares probability across levels of IV2
#' }
#'
#' Unlike \code{\link{smbin.inter}} which tests interactions, this function
#' tests marginal effects. The dependent variable y must be binary.
#'
#' @return List with components:
#' \describe{
#'   \item{est.loc.4.DV}{Matrix of probability estimates for each cell}
#'   \item{n}{Matrix of sample sizes for each cell}
#'   \item{A}{List of main effect results for first IV (across rows)}
#'   \item{B}{List of main effect results for second IV (across columns)}
#' }
#'
#' @seealso \code{\link{lincon.bin}} for binomial linear contrasts,
#'   \code{\link{smbin.inter}} for interaction tests,
#'   \code{\link{smbinRC}} for row-column comparisons
#'
#' @export
#' @examples
#' # Test main effects for binary outcome
#' set.seed(123)
#' x <- matrix(rnorm(200), ncol=2)
#' y <- rbinom(100, 1, plogis(x[,1] + x[,2]))
#' result <- smbinAB(x, y, Qsplit1=.5, Qsplit2=.5)
smbinAB<-function(x,y,IV=c(1,2),Qsplit1=.5,Qsplit2=.5,tr=.2,method='KMS',
xout=FALSE,outfun=outpro,...){
#
#  y is assumed to be binary
#
#  x a matrix or data frame
#
# Split on two indepnddent variables.
#
# Qsplit: split the independent variable based on the
#         quantiles indicated by Qsplit
#  Example
#   Qsplit1=c(.25,.5,.75)
#   Qsplit2=.5
#   would split based on the quartiles for the first independent variable and the median
#   for the second independent variable
#

#  Then test the hypotheses about the probability of a success.
#
#  IV[1]: indicates the column containing the first independent variable to use.
#  IV[2]:  indicates the column of containing the second independent variable to use.
#
#
x=as.matrix(x)
p=ncol(x)
if(p==1)stop('There should be two or more independent variables')
p1=p+1
xy<-elimna(cbind(x,y))
x<-xy[,1:p]
y<-xy[,p1]
if(length(unique(y))>2)stop('y should be binary')
flag=max(y)
y[flag]=1
y[!flag]=0
v=NULL
if(xout){
flag<-outfun(x,plotit=FALSE,...)$keep
x<-x[flag,]
y<-y[flag]
}
J=length(Qsplit1)+1
K=length(Qsplit2)+1
JK=J*K
MAT=matrix(1:JK,J,K,byrow=TRUE)
z=list()
group=list()
N.int=length(Qsplit1)+1
N.int2=length(Qsplit2)+1
est.mat=matrix(NA,nrow=N.int,ncol=N.int2)
n.mat=matrix(NA,nrow=N.int,ncol=N.int2)
Nsuc=matrix(NA,nrow=N.int,ncol=N.int2)
DV.mat=matrix(NA,nrow=N.int,ncol=N.int2)
L1=NULL
L2=NULL
qv=quantile(x[,IV[1]],Qsplit1)
qv=c(min(x[,IV[1]]),qv,max(x[,IV[1]]))
qv2=quantile(x[,IV[2]],Qsplit2)
qv2=c(min(x[,IV[2]]),qv2,max(x[,IV[2]]))
ic=0
for(j in 1:N.int){
j1=j+1
xsub=binmat(xy,IV[1],qv[j],qv[j1])
for(k in 1:N.int2){
k1=k+1
xsub2=binmat(xsub,IV[2],qv2[k],qv2[k1])
Nsuc[j,k]=sum(xsub2[,p1]==1)
est.mat[j,k]=mean(xsub2[,p1])
n.mat[j,k]=length(xsub2[,p1])
ic=ic+1
z[[ic]]=xsub2[,p1]
if(length(z[[ic]])>6)group[[ic]]=summary(xsub2[,1:p])
}
}
n=NA
for(j in 1:length(z)){
n[j]=length(z[[j]])
}
if(min(n)<=5){
id=which(n>5)
del=which(n<=5)
n=n[id]
if(pr)print(paste('For group',del,'the sample size is less than 6'))
}
A=list()
B=list()
for(j in 1:J)A[[j]]=lincon.bin(Nsuc[j,],n.mat[j,],method=method)$CI
for(j in 1:K)B[[j]]=lincon.bin(Nsuc[,j],n.mat[,j],method=method)$CI
list(est.loc.4.DV=est.mat,n=n.mat,A=A,B=B)
}




# ----------------------------------------------------------------------------

# smbinRC

# ----------------------------------------------------------------------------

#' Row-Column Pairwise Comparisons for Binary Outcome with Two-Variable Splitting
#'
#' @description
#' Splits data based on quantiles of two independent variables and performs all
#' pairwise comparisons of probabilities within rows and within columns of the
#' resulting J × K table. Uses binomial methods with FWE adjustment.
#'
#' @param x Matrix or data frame of predictor variables
#' @param y Binary dependent variable vector (0/1 or two unique values)
#' @param IV Vector of length 2 specifying which columns of x to use for splitting.
#'   Default is \code{c(1,2)} (first two columns).
#' @param Qsplit1 Quantile(s) for splitting first independent variable. Default is 0.5
#'   (median split). Can be vector (e.g., \code{c(.25,.5,.75)} for quartiles).
#' @param Qsplit2 Quantile(s) for splitting second independent variable. Default is 0.5.
#' @param method Method for pairwise comparisons passed to \code{\link{binpair}}.
#'   Default is 'ACSK'. Other options include 'SK', 'KMS', etc.
#' @param nboot Number of bootstrap samples (if method requires). Default is 1000.
#' @param est Location estimator for continuous variables (not used for binary).
#'   Default is \code{\link{tmean}}.
#' @param alpha Significance level. Default is 0.05.
#' @param FWE.method Family-wise error rate adjustment method. Default is 'hoch'
#'   (Hochberg). Other options: 'holm', 'bonferroni', etc.
#' @param xout Logical. If \code{TRUE}, remove outliers from x before analysis.
#'   Default is \code{FALSE}.
#' @param outfun Outlier detection function. Default is \code{\link{outpro}}.
#' @param SEED Logical or numeric. Controls random seed. Default is \code{TRUE}.
#' @param ... Additional arguments passed to outlier function.
#'
#' @details
#' This function:
#' \enumerate{
#'   \item Splits data into J × K groups based on quantiles of two variables
#'   \item For each row: performs all pairwise comparisons across columns
#'   \item For each column: performs all pairwise comparisons across rows
#'   \item Applies FWE adjustment to control family-wise error rate
#' }
#'
#' Unlike \code{\link{smbinAB}} which tests marginal effects, this function
#' performs all pairwise comparisons within each level of the factors.
#'
#' @return List with components:
#' \describe{
#'   \item{Independent.variables.summary}{Summary statistics for groups}
#'   \item{Res.4.IV1}{Pairwise comparison results within rows (across IV1 levels)}
#'   \item{Res.4.IV2}{Pairwise comparison results within columns (across IV2 levels)}
#' }
#'
#' @seealso \code{\link{binpair}} for pairwise binomial tests,
#'   \code{\link{smbinAB}} for main effects,
#'   \code{\link{smbin.inter}} for interaction tests
#'
#' @export
#' @examples
#' # Pairwise comparisons for binary outcome
#' set.seed(123)
#' x <- matrix(rnorm(200), ncol=2)
#' y <- rbinom(100, 1, plogis(x[,1] + x[,2]))
#' result <- smbinRC(x, y, Qsplit1=.5, Qsplit2=.5)
smbinRC<-function(x,y,IV=c(1,2),Qsplit1=.5,Qsplit2=.5,method='ACSK',nboot=1000,est=tmean,alpha=.05,FWE.method='hoch',
xout=FALSE,outfun=outpro,SEED=TRUE,...){
#
# y is assumed to be binary
#
#  Split data based on the covariates indicated by
#  IV
#
# Qsplit: split the independent variable based on the
#         quantiles indicated by Qsplit
#  Example
#   Qsplit1=c(.25,.5,.75)
#   Qsplit2=.5
#   would split based on the quartiles for the first independent variable and the median
#   for the second independent variable
#
#  Then compare the probability of a success corresponding to the resulting groups
#
#  IV[1]: indicates the column of containing the first independent variable to use.
#  IV[2]:  indicates the column of containing the second independent variable to use.
#
p=ncol(x)
if(p==1)stop('There should be two or more independent variables')
p1=p+1
xy<-elimna(cbind(x,y))
x<-xy[,1:p]
y<-xy[,p1]
v=NULL
# Next convert y to 0 and 1s
n=length(y)
yy=rep(0,n)
flag=which(y==max(y))
yy[flag]=1
y=yy
if(xout){
flag<-outfun(x,plotit=FALSE,...)$keep
x<-x[flag,]
y<-y[flag]
}
if(length(unique(y))>2)stop('y should be binary')
z=list()
group=list()
N.int=length(Qsplit1)+1
N.int2=length(Qsplit2)+1
est.mat=matrix(NA,nrow=N.int,ncol=N.int2)
L1=NULL
L2=NULL
for(i in 1:N.int)L1[i]=paste('IV1.G',i)
for(i in 1:N.int2)L2[i]=paste('IV2.G',i)
dimnames(est.mat)=list(L1,L2)
qv=quantile(x[,IV[1]],Qsplit1)
qv=c(min(x[,IV[1]]),qv,max(x[,IV[1]]))
qv2=quantile(x[,IV[2]],Qsplit2)
qv2=c(min(x[,IV[2]]),qv2,max(x[,IV[2]]))
ic=0
for(j in 1:N.int){
j1=j+1
xsub=binmat(xy,IV[1],qv[j],qv[j1])
for(k in 1:N.int2){
k1=k+1
xsub2=binmat(xsub,IV[2],qv2[k],qv2[k1])
est.mat[j,k]=est(xsub2,...)
ic=ic+1
z[[ic]]=xsub2[,p1]
if(length(z[[ic]])>6)group[[ic]]=summary(xsub2[,1:p])
}
}
n=NA
for(j in 1:length(z)){
n[j]=length(z[[j]])
}
if(min(n)<=5){
id=which(n>5)
del=which(n<=5)
n=n[id]
if(pr)print(paste('eliminated group',del,'The sample size is less than 6'))
}
NT=N.int*N.int2
MID=matrix(c(1:NT),nrow=N.int,ncol=N.int2,byrow=TRUE)
# pull out groups indicated by the columns of MID and do tests
IV1res=NULL
a=NULL
r=NA
n=NA
for(j in 1:N.int2){
zsub=z[MID[,j]]
r=lapply(zsub,sum)
n=lapply(zsub,length)
r=as.vector(matl(r))
n=as.vector(matl(n))
a=binpair(r,n,method=method,alpha=alpha)
IV1res=rbind(IV1res,a[,3:11])
}
#  update adjusted p-value
IV1res[,9]=p.adjust(IV1res[,8],method=FWE.method)
#Now do IV2
IV2res=NULL
r=NA
n=NA
a=NULL
for(j in 1:N.int){
zsub=z[MID[j,]]
r=lapply(zsub,sum)
n=lapply(zsub,length)
r=as.vector(matl(r))
n=as.vector(matl(n))
a=binpair(r,n,method=method,alpha=alpha)
IV2res=rbind(IV2res,a[,3:11])
IV2res[,9]=p.adjust(IV2res[,8],method=FWE.method)
}
list(Independent.variables.summary=group,Res.4.IV1=IV1res,Res.4.IV2=IV2res)
}




# ----------------------------------------------------------------------------

# smgrid

# ----------------------------------------------------------------------------

#' Pairwise Comparisons with Two-Variable Splitting
#'
#' @description
#' Splits data based on quantiles of two independent variables and performs all
#' pairwise comparisons of location measures across the resulting J × K groups.
#' Can use trimmed means or percentile bootstrap with flexible estimators.
#'
#' @param x Matrix or data frame of predictor variables
#' @param y Response variable vector
#' @param IV Vector of length 2 specifying which columns of x to use for splitting.
#'   Default is \code{c(1,2)} (first two columns).
#' @param Qsplit1 Quantile(s) for splitting first independent variable. Default is 0.5.
#'   Can be vector (e.g., \code{c(.25,.5,.75)} for quartiles).
#' @param Qsplit2 Quantile(s) for splitting second independent variable. Default is 0.5.
#' @param tr Trimming proportion (0 to 0.5). Default is 0.2.
#' @param PB Logical. If \code{FALSE}, use trimmed means. If \code{TRUE}, use
#'   percentile bootstrap with estimator specified by \code{est}. Default is \code{FALSE}.
#' @param est Location estimator function (used when \code{PB=TRUE}). Default is
#'   \code{\link{tmean}}.
#' @param nboot Number of bootstrap samples (if \code{PB=TRUE}). Default is 1000.
#' @param pr Logical. If \code{TRUE}, print warnings. Default is \code{TRUE}.
#' @param xout Logical. If \code{TRUE}, remove outliers from x. Default is \code{FALSE}.
#' @param outfun Outlier detection function. Default is \code{\link{outpro}}.
#' @param SEED Logical or numeric. Controls random seed. Default is \code{TRUE}.
#' @param ... Additional arguments passed to estimator or outlier function.
#'
#' @details
#' This function:
#' \enumerate{
#'   \item Splits data into J × K groups based on quantiles of two variables
#'   \item Computes location estimates for each cell
#'   \item Performs all pairwise comparisons using \code{\link{lincon}} or \code{\link{linpairpb}}
#'   \item Computes effect sizes for all pairs
#' }
#'
#' Groups with fewer than 6 observations are excluded with warnings.
#'
#' @return List with components:
#' \describe{
#'   \item{Group.summary}{Summary statistics for each group}
#'   \item{loc.est}{Matrix of location estimates for each cell}
#'   \item{test}{Global test results (if \code{PB=FALSE})}
#'   \item{output}{Matrix of pairwise comparison results}
#'   \item{num.sig}{Number of significant pairwise differences}
#'   \item{Effect.Sizes}{Effect sizes for all pairwise comparisons}
#' }
#'
#' @seealso \code{\link{smgrid.GLOB}} for global test only,
#'   \code{\link{lincon}} for linear contrasts,
#'   \code{\link{linpairpb}} for bootstrap pairwise comparisons
#'
#' @export
#' @examples
#' # Pairwise comparisons with median split on two variables
#' set.seed(123)
#' x <- matrix(rnorm(200), ncol=2)
#' y <- x[,1] + x[,2] + rnorm(100)
#' result <- smgrid(x, y, Qsplit1=.5, Qsplit2=.5)
smgrid<-function(x,y,IV=c(1,2),Qsplit1=.5,Qsplit2=.5,tr=.2,PB=FALSE,est=tmean,nboot=1000,pr=TRUE,
xout=FALSE,outfun=outpro,SEED=TRUE,...){
#
# Split on two variables, not just one.
#
# Qsplit: split the independent variable based on the
#         quantiles indicated by Qsplit
#  Example
#   Qsplit1=c(.25,.5,.75)
#   Qsplit2=.5
#   would split based on the quartiles for the first independent variable and the median
#   for the second independent variable
#

#  Then test the hypothesis of equal measures of location
#  IV[1]: indicates the column of containing the first independent variable to use.
#  IV[2]:  indicates the column of containing the second independent variable to use.
#
#  if(length(unique(y)>2))stop('y should be binary')
x=as.matrix(x)
p=ncol(x)
if(p==1)stop('There should be two or more independent variables')
p1=p+1
xy<-elimna(cbind(x,y))
x<-xy[,1:p]
y<-xy[,p1]
v=NULL
if(xout){
flag<-outfun(x,plotit=FALSE,...)$keep
x<-x[flag,]
y<-y[flag]
}
z=list()
group=list()
N.int=length(Qsplit1)+1
N.int2=length(Qsplit2)+1
est.mat=matrix(NA,nrow=N.int,ncol=N.int2)
L1=NULL
L2=NULL
for(i in 1:N.int)L1[i]=paste('IV1.G',i)
for(i in 1:N.int2)L2[i]=paste('IV2.G',i)
dimnames(est.mat)=list(L1,L2)
qv=quantile(x[,IV[1]],Qsplit1)
qv=c(min(x[,IV[1]]),qv,max(x[,IV[1]]))
qv2=quantile(x[,IV[2]],Qsplit2)
qv2=c(min(x[,IV[2]]),qv2,max(x[,IV[2]]))
ic=0
for(j in 1:N.int){
j1=j+1
xsub=binmat(xy,IV[1],qv[j],qv[j1])
for(k in 1:N.int2){
k1=k+1
xsub2=binmat(xsub,IV[2],qv2[k],qv2[k1])
est.mat[j,k]=est(xsub2,...)
ic=ic+1
z[[ic]]=xsub2[,p1]
if(length(z[[ic]])>6)group[[ic]]=summary(xsub2[,1:p])
}
}
n=NA
for(j in 1:length(z)){
n[j]=length(z[[j]])
}
if(min(n)<=5){
id=which(n>5)
del=which(n<=5)
n=n[id]
if(pr)print(paste('eliminated group',del,'The sample size is less than 6'))
}
test=NULL
ES=IND.PAIR.ES(z)$effect.size
if(!PB){
a=lincon(z,tr=tr,pr=FALSE)
chk.sig=sign(a$psihat[,4])*sign(a$psihat[,5])
num.sig=sum(chk.sig>=0)
test=a$test
res=a$psihat
}
if(PB){
a=linpairpb(z,est=est,nboot=nboot,...)
num.sig=a$num.sig
res=a$output
}
list(Group.summary=group,loc.est=est.mat,test=test,output=res,num.sig=num.sig,Effect.Sizes=ES)
}


#' Estimate Location for Grid Cells Defined by Two-Variable Splitting
#'
#' @description
#' Splits data based on quantiles of two independent variables and computes
#' location estimates for each cell of the resulting J × K grid. Returns only
#' the matrix of estimates without performing tests.
#'
#' @param x Matrix or data frame of predictor variables
#' @param y Response variable vector
#' @param est Location estimator function. Default is \code{\link{tmean}}.
#' @param IV Vector of length 2 specifying which columns of x to use for splitting.
#'   Default is \code{c(1,2)} (first two columns).
#' @param Qsplit1 Quantile(s) for splitting first independent variable. Default is
#'   \code{c(.3,.7)} (tertiles).
#' @param Qsplit2 Quantile(s) for splitting second independent variable. Default is
#'   \code{c(.3,.7)} (tertiles).
#' @param tr Trimming proportion (0 to 0.5). Default is 0.2.
#' @param xout Logical. If \code{TRUE}, remove outliers from x. Default is \code{FALSE}.
#' @param outfun Outlier detection function. Default is \code{\link{outpro}}.
#' @param ... Additional arguments passed to estimator or outlier function.
#'
#' @details
#' This is a simplified version of \code{\link{smgrid}} that only computes estimates
#' without performing hypothesis tests. Useful for exploring location estimates
#' across grid cells defined by two variables.
#'
#' @return Matrix of location estimates with rows corresponding to levels of IV1
#'   and columns to levels of IV2.
#'
#' @seealso \code{\link{smgrid}} for version with hypothesis testing
#'
#' @export
#' @examples
#' # Compute trimmed means for grid cells
#' set.seed(123)
#' x <- matrix(rnorm(200), ncol=2)
#' y <- x[,1] + x[,2] + rnorm(100)
#' estimates <- smgrid.est(x, y, Qsplit1=c(.3,.7), Qsplit2=c(.3,.7))
smgrid.est<-function(x,y,est=tmean,IV=c(1,2),Qsplit1 = c(.3,.7),Qsplit2 = c(.3,.7),tr=.2,xout=FALSE,outfun=outpro,...){
#
#  Splits the data into groups
#
x=as.matrix(x)
p=ncol(x)
if(p==1)stop('There should be two or more independent variables')
p1=p+1
xy<-elimna(cbind(x,y))
x<-xy[,1:p]
y<-xy[,p1]
v=NULL
if(xout){
flag<-outfun(x,plotit=FALSE,...)$keep
x<-x[flag,]
y<-y[flag]
}
J=length(Qsplit1)+1
K=length(Qsplit2)+1
JK=J*K
qv=quantile(x[,IV[1]],Qsplit1)
qv=c(min(x[,IV[1]]),qv,max(x[,IV[1]]))
qv2=quantile(x[,IV[2]],Qsplit2)
qv2=c(min(x[,IV[2]]),qv2,max(x[,IV[2]]))
ic=0
N.int=length(Qsplit1)+1
N.int2=length(Qsplit2)+1
est.mat=matrix(NA,nrow=N.int,ncol=N.int2)
for(j in 1:N.int){
j1=j+1
xsub=binmat(xy,IV[1],qv[j],qv[j1])
for(k in 1:N.int2){
k1=k+1
xsub2=binmat(xsub,IV[2],qv2[k],qv2[k1])
if(!is.null(dim(xsub2)))
est.mat[j,k]=est(xsub2[,p1],...)
}}
est.mat
}

#' Two-Way ANOVA for Grid with Main Effects Test
#'
#' @description
#' Splits data based on quantiles of two independent variables and performs
#' a two-way ANOVA to test main effects using \code{\link{t2way}}. Designed for
#' testing main effects in a J × K factorial design created by splitting.
#'
#' @param x Matrix or data frame of predictor variables
#' @param y Response variable vector
#' @param IV Vector of length 2 specifying which columns of x to use for splitting.
#'   Default is \code{c(1,2)} (first two columns).
#' @param Qsplit1 Quantile(s) for splitting first independent variable. Default is 0.5.
#' @param Qsplit2 Quantile(s) for splitting second independent variable. Default is 0.5.
#' @param tr Trimming proportion (0 to 0.5). Default is 0.2.
#' @param PB Logical. If \code{TRUE}, use percentile bootstrap. Default is \code{FALSE}.
#' @param est Location estimator function (used when \code{PB=TRUE}). Default is
#'   \code{\link{tmean}}.
#' @param nboot Number of bootstrap samples (if \code{PB=TRUE}). Default is 1000.
#' @param pr Logical. If \code{TRUE}, print warnings. Default is \code{TRUE}.
#' @param fun Effect size summary function. Default is \code{\link{ES.summary}}.
#' @param xout Logical. If \code{TRUE}, remove outliers from x. Default is \code{FALSE}.
#' @param outfun Outlier detection function. Default is \code{\link{outpro}}.
#' @param SEED Logical or numeric. Controls random seed. Default is \code{TRUE}.
#' @param ... Additional arguments passed to estimator or outlier function.
#'
#' @details
#' This function:
#' \enumerate{
#'   \item Splits data into J × K groups based on quantiles of two variables
#'   \item Performs two-way ANOVA using \code{\link{t2way}}
#'   \item Tests main effects and interactions for trimmed means
#' }
#'
#' Groups with fewer than 6 observations are excluded with warnings.
#'
#' @return Result from \code{\link{t2way}} containing F-statistics and p-values
#'   for main effects and interactions.
#'
#' @seealso \code{\link{t2way}} for two-way ANOVA,
#'   \code{\link{smgridAB}} for pairwise comparisons of main effects
#'
#' @export
#' @examples
#' # Two-way ANOVA with median splits
#' set.seed(123)
#' x <- matrix(rnorm(200), ncol=2)
#' y <- x[,1] + 0.5*x[,2] + rnorm(100)
#' result <- smgrid2M(x, y, Qsplit1=.5, Qsplit2=.5)
smgrid2M<-function(x,y,IV=c(1,2),Qsplit1=.5,Qsplit2=.5,tr=.2,PB=FALSE,est=tmean,nboot=1000,pr=TRUE,fun=ES.summary,
xout=FALSE,outfun=outpro,SEED=TRUE,...){
#
# Split on two variables,
#
# Qsplit: split the independent variable based on the
#         quantiles indicated by Qsplit
#  Example
#   Qsplit1=c(.25,.5,.75)
#   Qsplit2=.5
#   would split based on the quartiles for the first independent variable and the median
#   for the second independent variable
#

#  Then test main effects based on trimmed means
#  IV[1]: indicates the column of containing the first independent variable to use.
#  IV[2]:  indicates the column of containing the second independent variable to use.
#
#  if(length(unique(y)>2))stop('y should be binary')
x=as.matrix(x)
p=ncol(x)
if(p==1)stop('There should be two or more independent variables')
p1=p+1
xy<-elimna(cbind(x,y))
x<-xy[,1:p]
y<-xy[,p1]
v=NULL
if(xout){
flag<-outfun(x,plotit=FALSE,...)$keep
x<-x[flag,]
y<-y[flag]
xy=cbind(x,y)
}
J=length(Qsplit1)+1
K=length(Qsplit2)+1
JK=J*K
MAT=matrix(1:JK,J,K,byrow=TRUE)
z=list()
group=list()
N.int=length(Qsplit1)+1
N.int2=length(Qsplit2)+1
est.mat=matrix(NA,nrow=N.int,ncol=N.int2)
n.mat=matrix(NA,nrow=N.int,ncol=N.int2)
DV.mat=matrix(NA,nrow=N.int,ncol=N.int2)
L1=NULL
L2=NULL
qv=quantile(x[,IV[1]],Qsplit1)
qv=c(min(x[,IV[1]]),qv,max(x[,IV[1]]))
qv2=quantile(x[,IV[2]],Qsplit2)
qv2=c(min(x[,IV[2]]),qv2,max(x[,IV[2]]))
ic=0
for(j in 1:N.int){
j1=j+1
xsub=binmat(xy,IV[1],qv[j],qv[j1])
for(k in 1:N.int2){
k1=k+1
xsub2=binmat(xsub,IV[2],qv2[k],qv2[k1])
est.mat[j,k]=est(xsub2[,p1],...)
n.mat[j,k]=length(xsub2[,p1])
ic=ic+1
z[[ic]]=xsub2[,p1]
if(length(z[[ic]])>6)group[[ic]]=summary(xsub2[,1:p])
}
}
n=NA
for(j in 1:length(z)){
n[j]=length(z[[j]])
}
if(min(n)<=5){
id=which(n>5)
del=which(n<=5)
n=n[id]
if(pr)print(paste('For group',del,'the sample size is less than 6'))
}
A=t2way(J,K,z,tr=tr)
A
}




# ----------------------------------------------------------------------------

# smgridAB

# ----------------------------------------------------------------------------

#' Pairwise Comparisons of Main Effects with Two-Variable Splitting
#'
#' @description
#' Splits data based on quantiles or specific values of two independent variables
#' and performs all pairwise comparisons for each main effect (Factor A and Factor B).
#' Computes effect sizes for comparisons within each factor level.
#'
#' @param x Matrix or data frame of predictor variables
#' @param y Response variable vector
#' @param IV Vector of length 2 specifying which columns of x to use for splitting.
#'   Default is \code{c(1,2)} (first two columns).
#' @param Qsplit1 Quantile(s) for splitting first independent variable. Default is 0.5.
#'   Ignored if \code{VAL1} is specified.
#' @param Qsplit2 Quantile(s) for splitting second independent variable. Default is 0.5.
#'   Ignored if \code{VAL2} is specified.
#' @param tr Trimming proportion (0 to 0.5). Default is 0.2.
#' @param VAL1 Optional vector of specific values for splitting first IV. Default is \code{NULL}.
#' @param VAL2 Optional vector of specific values for splitting second IV. Default is \code{NULL}.
#' @param PB Logical. If \code{TRUE}, use percentile bootstrap. Default is \code{FALSE}.
#' @param est Location estimator function (used when \code{PB=TRUE}). Default is
#'   \code{\link{tmean}}.
#' @param nboot Number of bootstrap samples (if \code{PB=TRUE}). Default is 1000.
#' @param pr Logical. If \code{TRUE}, print warnings. Default is \code{TRUE}.
#' @param fun Effect size summary function. Default is \code{\link{ES.summary}}.
#' @param xout Logical. If \code{TRUE}, remove outliers from x. Default is \code{FALSE}.
#' @param outfun Outlier detection function. Default is \code{\link{outpro}}.
#' @param SEED Logical or numeric. Controls random seed. Default is \code{TRUE}.
#' @param ... Additional arguments passed to estimator or outlier function.
#'
#' @details
#' This function:
#' \enumerate{
#'   \item Splits data into J × K groups based on two variables
#'   \item For Factor A: performs pairwise comparisons across rows (IV1 levels)
#'   \item For Factor B: performs pairwise comparisons across columns (IV2 levels)
#'   \item Computes effect sizes for all comparisons
#' }
#'
#' Unlike \code{\link{smgrid2M}} which tests overall main effects, this function
#' provides detailed pairwise comparisons within each main effect.
#'
#' @return List with components:
#' \describe{
#'   \item{est.loc.4.DV}{Matrix of location estimates for each cell}
#'   \item{n}{Matrix of sample sizes for each cell}
#'   \item{A}{List of pairwise comparison results for Factor A (IV1)}
#'   \item{B}{List of pairwise comparison results for Factor B (IV2)}
#'   \item{A.effect.sizes}{Effect sizes for Factor A comparisons}
#'   \item{B.effect.sizes}{Effect sizes for Factor B comparisons}
#' }
#'
#' @seealso \code{\link{smgrid2M}} for overall main effects test,
#'   \code{\link{smgridRC}} for row-column pairwise comparisons
#'
#' @export
#' @examples
#' # Pairwise comparisons for main effects
#' set.seed(123)
#' x <- matrix(rnorm(200), ncol=2)
#' y <- x[,1] + 0.5*x[,2] + rnorm(100)
#' result <- smgridAB(x, y, Qsplit1=.5, Qsplit2=.5)
smgridAB<-function(x,y,IV=c(1,2),Qsplit1=.5,Qsplit2=.5,tr=.2,VAL1=NULL,VAL2=NULL,
PB=FALSE,est=tmean,nboot=1000,pr=TRUE,fun=ES.summary,
xout=FALSE,outfun=outpro,SEED=TRUE,...){
#
# Split on two variables, not just one.
#
# Qsplit: split the independent variable based on the
#         quantiles indicated by Qsplit
#  Example
#   Qsplit1=c(.25,.5,.75)
#   Qsplit2=.5
#   would split based on the quartiles for the first independent variable and the median
#   for the second independent variable
#
#  Alternatively, can split the data based on specified values indicating by the arguments
#  VAL1 and VAL2
#
#  Then test the hypothesis of equal measures of location
#  IV[1]: indicates the column of containing the first independent variable to use.
#  IV[2]:  indicates the column of containing the second independent variable to use.
#
#  if(length(unique(y)>2))stop('y should be binary')
x=as.matrix(x)
p=ncol(x)
if(p==1)stop('There should be two or more independent variables')
p1=p+1
xy<-elimna(cbind(x,y))
x<-xy[,1:p]
y<-xy[,p1]
v=NULL
if(xout){
flag<-outfun(x,plotit=FALSE,...)$keep
x<-x[flag,]
y<-y[flag]
xy=cbind(x,y)
}
J=length(Qsplit1)+1
K=length(Qsplit2)+1
z=list()
group=list()
if(is.null(VAL1) || is.null(VAL2)){
N.int=length(Qsplit1)+1
N.int2=length(Qsplit2)+1
}
else {
J=length(VAL1)+1
K=length(VAL2)+1
N.int=length(VAL1)+1
N.int2=length(VAL2)+1
}
JK=J*K
MAT=matrix(1:JK,J,K,byrow=TRUE)
est.mat=matrix(NA,nrow=N.int,ncol=N.int2)
n.mat=matrix(NA,nrow=N.int,ncol=N.int2)
DV.mat=matrix(NA,nrow=N.int,ncol=N.int2)
L1=NULL
L2=NULL
if(is.null(VAL1) || is.null(VAL2)){
qv=quantile(x[,IV[1]],Qsplit1)
qv=c(min(x[,IV[1]]),qv,max(x[,IV[1]]))
qv2=quantile(x[,IV[2]],Qsplit2)
qv2=c(min(x[,IV[2]]),qv2,max(x[,IV[2]]))
}
else{
qv=VAL1
qv=c(min(x[,IV[1]]),qv,max(x[,IV[1]]))
qv2=VAL2
qv2=c(min(x[,IV[2]]),qv2,max(x[,IV[2]]))
}
ic=0
for(j in 1:N.int){
j1=j+1
xsub=binmat(xy,IV[1],qv[j],qv[j1])
for(k in 1:N.int2){
k1=k+1
xsub2=binmat(xsub,IV[2],qv2[k],qv2[k1])
est.mat[j,k]=est(xsub2[,p1],...)
n.mat[j,k]=length(xsub2[,p1])
ic=ic+1
z[[ic]]=xsub2[,p1]
if(length(z[[ic]])>6)group[[ic]]=summary(xsub2[,1:p])
}
}
n=NA
for(j in 1:length(z)){
n[j]=length(z[[j]])
}
if(min(n)<=5){
id=which(n>5)
del=which(n<=5)
n=n[id]
if(pr)print(paste('For group',del,'the sample size is less than 6'))
}
A=list()
B=list()
A.ES=list()
B.ES=list()
for(j in 1:J)A.ES[[j]]=IND.PAIR.ES(z[MAT[j,]],fun=fun)$effect.size[[1]]
for(j in 1:K)B.ES[[j]]=IND.PAIR.ES(z[MAT[,j]],fun=fun)$effect.size[[1]]
if(!PB){
for(j in 1:J)A[[j]]=lincon(z[MAT[j,]],tr=tr,pr=FALSE)$psihat
for(j in 1:K)B[[j]]=lincon(z[MAT[,j]],tr=tr,pr=FALSE)$psihat
}
if(PB){
for(j in 1:J)A[[j]]=linpairpb(z[MAT[j,]],est=est,nboot=nboot,...)$output
for(j in 1:K)B[[j]]=linpairpb(z[MAT[,j]],est=est,nboot=nboot,...)$output
}
list(est.loc.4.DV=est.mat,n=n.mat,A=A,B=B,A.effect.sizes=A.ES,B.effect.sizes=B.ES)
}


smgridRC<-function(x,y,IV=c(1,2),Qsplit1=.5,Qsplit2=.5,tr=.2,alpha=.05,PB=FALSE,est=tmean,nboot=1000,pr=TRUE,method='hoch',
xout=FALSE,outfun=outpro,SEED=TRUE,...){
#
# Compare measures of location among grids defined by quantiles of two IVs. By default 20% trimming is used
#  est=median would use medians
#  est=hd  would use the Harrell-Davis estimator for the median.
#
# Qsplit: split the independent variable based on the
#         quantiles indicated by Qsplit
#  Example
#   Qsplit1=c(.25,.5,.75)
#   Qsplit2=.5
#   would split based on the quartiles for the first independent variable and the median
#   for the second independent variable
#  Then compare binomials#
#
#  IV[1]: indicates the column of containing the first independent variable to use.
#  IV[2]:  indicates the column of containing the second independent variable to use.
#
#  tr: amount of trimming
#
x=as.matrix(x)
p=ncol(x)
if(p==1)stop('There should be two or more independent variables')
p1=p+1
xy<-elimna(cbind(x,y))
x<-xy[,1:p]
y<-xy[,p1]
v=NULL
if(xout){
flag<-outfun(x,plotit=FALSE)$keep
x<-x[flag,]
y<-y[flag]
}
if(identical(est,median))PB=TRUE
if(identical(est,hd))PB=TRUE
z=list()
group=list()
N.int=length(Qsplit1)+1
N.int2=length(Qsplit2)+1
est.mat=matrix(NA,nrow=N.int,ncol=N.int2)
n.mat=matrix(NA,nrow=N.int,ncol=N.int2)
DV.mat=matrix(NA,nrow=N.int,ncol=N.int2)
L1=NULL
L2=NULL
for(i in 1:N.int)L1[i]=paste('IV1.G',i)
for(i in 1:N.int2)L2[i]=paste('IV2.G',i)
dimnames(est.mat)=list(L1,L2)
qv=quantile(x[,IV[1]],Qsplit1)
qv=c(min(x[,IV[1]]),qv,max(x[,IV[1]]))
qv2=quantile(x[,IV[2]],Qsplit2)
qv2=c(min(x[,IV[2]]),qv2,max(x[,IV[2]]))
ic=0
for(j in 1:N.int){
j1=j+1
xsub=binmat(xy,IV[1],qv[j],qv[j1])
for(k in 1:N.int2){
k1=k+1
xsub2=binmat(xsub,IV[2],qv2[k],qv2[k1])
est.mat[j,k]=est(xsub2[,p1],...)
n.mat[j,k]=length(xsub2[,p1])
ic=ic+1
z[[ic]]=xsub2[,p1]
if(length(z[[ic]])>6)group[[ic]]=summary(xsub2[,1:p])
}
}
n=NA
for(j in 1:length(z)){
n[j]=length(z[[j]])
}
if(min(n)<=5){
id=which(n>5)
del=which(n<=5)
n=n[id]
if(pr)print(paste('eliminated group',del,'The sample size is less than 6'))
}
NT=N.int*N.int2
MID=matrix(c(1:NT),nrow=N.int,ncol=N.int2,byrow=TRUE)
# pull out groups indicated by the columns of MID and do tests
IV1res=NULL
a=NULL
for(j in 1:N.int2){
zsub=z[MID[,j]]
DV.mat[,j]=matl(lapply(zsub,est,...))
if(!PB)a=lincon(zsub,tr=tr,pr=FALSE,alpha=alpha)$psihat[,3:8]

if(PB){
if(identical(est,tmean))a=linpairpb(zsub,nboot=nboot,alpha=alpha,est=est,SEED=SEED,tr=tr)$output[,c(3:9)]
else
a=linpairpb(zsub,nboot=nboot,alpha=alpha,est=est,method=method,SEED=SEED,...)$output[,c(3:9)]
}
IV1res=rbind(IV1res,a)
}
#Now do IV2
IV2res=NULL
a=NULL
for(j in 1:N.int){
zsub=z[MID[j,]]
if(!PB){
a=lincon(zsub,tr=tr,pr=FALSE,alpha=alpha)$psihat[,3:8]
}
if(PB){
print(zsub)
if(identical(est,tmean))a=linpairpb(zsub,nboot=nboot,alpha=alpha,est=est,SEED=SEED,tr=tr)$output[,c(3:9)]
else
a=linpairpb(zsub,nboot=nboot,alpha=alpha,est=est,SEED=SEED,...)$output[,c(3:9)]
}
IV2res=rbind(IV2res,a)
}
if(!PB){  #fix labels add adjusted p-value
IV1res=cbind(IV1res[,1:4],p.adjust(IV1res[,4],method=method),IV1res[,5:6])
IV2res=cbind(IV2res[,1:4],p.adjust(IV2res[,4],method=method),IV2res[,5:6])
dimnames(IV1res)=list(NULL,c('psihat','ci.lower','ci.upper','p.value','p.adjust','Est.1','Est.2'))
dimnames(IV2res)=list(NULL,c('psihat','ci.lower','ci.upper','p.value','p.adjust','Est.1','Est.2'))
}
if(PB){
IV1res[,3]=p.adjust(IV1res[,2],method=method)
IV2res[,3]=p.adjust(IV2res[,2],method=method)
IV1res=IV1res[,c(1,4,5,2,3,6,7)]
IV2res=IV2res[,c(1,4,5,2,3,6,7)]
dimnames(IV1res)=list(NULL,c('psihat','ci.lower','ci.upper','p.value','p.adjust','Est.1','Est.2'))
dimnames(IV2res)=list(NULL,c('psihat','ci.lower','ci.upper','p.value','p.adjust','Est.1','Est.2'))
}
list(est.loc.4.DV=est.mat,n=n.mat,Independent.variables.summary=group,Res.4.IV1=IV1res,Res.4.IV2=IV2res)
}




# ----------------------------------------------------------------------------

# smgridVRC

# ----------------------------------------------------------------------------

smgridVRC<-function(x,y,IV=c(1,2),Qsplit1=.5,Qsplit2=.5,TR=.2,alpha=.05,VAL1=NULL,VAL2=NULL,PB=FALSE,est=tmean,nboot=1000,pr=TRUE,method='hoch',
xout=FALSE,outfun=outpro,SEED=TRUE,...){
#
# Compare measures of location among grids defined by quantiles of two IVs. By default 20% trimming is used
#  est=median would use medians
#  est=hd  would use the Harrell-Davis estimator for the median.
#
# Qsplit: split the independent variable based on the
#         quantiles indicated by Qsplit
#  Example
#   Qsplit1=c(.25,.5,.75)
#   Qsplit2=.5
#   would split based on the quartiles for the first independent variable and the median
#   for the second independent variable
#

#  Then test the hypothesis of equal measures of location
#  IV[1]: indicates the column of containing the first independent variable to use.
#  IV[2]:  indicates the column of containing the second independent variable to use.
#
#  TR: amount of trimming when using a non-bootstrap method. To alter the amount of trimming when using
#        a bootstrap method use
#        tr.  Example, tr=.25 would use 25% trimming.
#
x=as.matrix(x)
p=ncol(x)
if(p==1)stop('There should be two or more independent variables')
p1=p+1
xy<-elimna(cbind(x,y))
x<-xy[,1:p]
y<-xy[,p1]
v=NULL
if(xout){
flag<-outfun(x,plotit=FALSE,...)$keep
x<-x[flag,]
y<-y[flag]
}
if(identical(est,median))PB=TRUE
if(identical(est,hd))PB=TRUE
z=list()
group=list()
if(is.null(VAL1) || is.null(VA2)){
N.int=length(Qsplit1)+1
N.int2=length(Qsplit2)+1
}
else {
N.int=length(VAL1)+1
N.int2=length(VAL2)+1
}
est.mat=matrix(NA,nrow=N.int,ncol=N.int2)
n.mat=matrix(NA,nrow=N.int,ncol=N.int2)
DV.mat=matrix(NA,nrow=N.int,ncol=N.int2)
L1=NULL
L2=NULL
for(i in 1:N.int)L1[i]=paste('IV1.G',i)
for(i in 1:N.int2)L2[i]=paste('IV2.G',i)
dimnames(est.mat)=list(L1,L2)

if(is.null(VAL1) || is.null(VA2)){
qv=quantile(x[,IV[1]],Qsplit1)
qv=c(min(x[,IV[1]]),qv,max(x[,IV[1]]))
qv2=quantile(x[,IV[2]],Qsplit2)
qv2=c(min(x[,IV[2]]),qv2,max(x[,IV[2]]))
}
else{
qv=VAL1
qv=c(min(x[,IV[1]]),qv,max(x[,IV[1]]))
qv2=VAL2
qv2=c(min(x[,IV[2]]),qv2,max(x[,IV[2]]))
}
ic=0
for(j in 1:N.int){
j1=j+1
xsub=binmat(xy,IV[1],qv[j],qv[j1])
for(k in 1:N.int2){
k1=k+1
xsub2=binmat(xsub,IV[2],qv2[k],qv2[k1])
est.mat[j,k]=est(xsub2[,p1],...)
n.mat[j,k]=length(xsub2[,p1])
ic=ic+1
z[[ic]]=xsub2[,p1]
if(length(z[[ic]])>6)group[[ic]]=summary(xsub2[,1:p])
}
}
n=NA
for(j in 1:length(z)){
n[j]=length(z[[j]])
}
if(min(n)<=5){
id=which(n>5)
del=which(n<=5)
n=n[id]
if(pr)print(paste('eliminated group',del,'The sample size is less than 6'))
}
NT=N.int*N.int2
MID=matrix(c(1:NT),nrow=N.int,ncol=N.int2,byrow=TRUE)
# pull out s indicated by the columns of MID and do tests
IV1res=NULL
a=NULL
for(j in 1:N.int2){
zsub=z[MID[,j]]
DV.mat[,j]=matl(lapply(zsub,est,...))
if(!PB)a=lincon(zsub,tr=TR,pr=FALSE,alpha=alpha)$psihat[,3:8]
if(PB)a=linpairpb(zsub,nboot=nboot,alpha=alpha,SEED=SEED,...)$output[,c(3:9)]
IV1res=rbind(IV1res,a)
}
#Now do IV2
IV2res=NULL
a=NULL
for(j in 1:N.int){
zsub=z[MID[j,]]
if(!PB){
a=lincon(zsub,tr=TR,pr=FALSE,alpha=alpha)$psihat[,3:8]
}
if(PB){
a=linpairpb(zsub,nboot=nboot,alpha=alpha,est=est,SEED=SEED,...)$output[,c(3:9)]
}
IV2res=rbind(IV2res,a)
}
if(!PB){  #fix labels add adjusted p-value
IV1res=cbind(IV1res[,1:4],p.adjust(IV1res[,4],method=method),IV1res[,5:6])
IV2res=cbind(IV2res[,1:4],p.adjust(IV2res[,4],method=method),IV2res[,5:6])
}
if(PB){
IV1res[,3]=p.adjust(IV1res[,2],method=method)
IV2res[,3]=p.adjust(IV2res[,2],method=method)
IV1res=IV1res[,c(1,4,5,2,3,6,7)]
IV2res=IV2res[,c(1,4,5,2,3,6,7)]
}
nr=nrow(IV1res)
Lnam1=NULL
for(j in 1:nr)Lnam1=c(Lnam1,paste(' IV1Level',j))
print(dim(IV1res))
print(Lnam1)
nr=nrow(IV2res)
Lnam2=NULL
for(j in 1:nr)Lnam2=c(Lnam2,paste('IV2 Level',j))
dimnames(IV1res)=list(Lnam1,c('psihat','ci.lower','ci.upper','p.value','p.adjust','Est.1','Est.2'))
dimnames(IV2res)=list(Lnam2,c('psihat','ci.lower','ci.upper','p.value','p.adjust','Est.1','Est.2'))
list(est.loc.4.DV=est.mat,n=n.mat,Independent.variables.summary=group,Res.4.IV1=IV1res,Res.4.IV2=IV2res)
}



# --------------------------------------------------------------
# Code adapted from RGenData::GenDataPopulation from John Ruscio
# --------------------------------------------------------------

# Reference
# Ruscio, J. & Kaczetow, W. (2008)
# Simulating Multivariate Nonnormal Data Using an Iterative Algorithm.
# Multivariate Behav Res, 43, 355-381.
# https://www.ncbi.nlm.nih.gov/pubmed/26741201

# License: MIT
# Copyright <2018> <John Ruscio>

# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the "Software"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.

# Original
# https://github.com/cran/RGenData/blob/master/R/EFAGenData.R

# Simulate multivariate g-and-h data using an iterative algorithm
#
# Args:
#   n.cases            : Number of observations for each variable - default 1000
#   n.variables        : Number of variables - default 2
#   g                  : g parameter of the g-and-h distribution - default 0
#   h                  : h parameter of the g-and-h distribution - default 0
#   rho                : Target correlation between variables - default 0
#   corr.type          : Type of correlation - default "pearson", alternative "spearman"
#
# Returns:
#   data               : Population of data - matrix n.cases rows by n.variables columns
#




# ----------------------------------------------------------------------------

# smmval

# ----------------------------------------------------------------------------

#' Studentized Maximum Modulus Distribution Quantile
#'
#' @description
#' Determines the upper 1-alpha quantile of the maximum of K independent Student's
#' t random variables. This distribution is similar to a Studentized maximum modulus
#' distribution, but the t statistics are not based on an estimate of an assumed
#' common variance.
#'
#' @param dfvec Vector of length K containing the degrees of freedom for each
#'   independent t distribution.
#' @param iter Number of Monte Carlo iterations. Default is 10000.
#' @param alpha Significance level. Default is 0.05.
#' @param SEED Logical. If \code{TRUE} (default), sets random seed to 1 for
#'   reproducibility.
#'
#' @details
#' The function generates \code{iter} samples of K independent Student's t random
#' variables with degrees of freedom specified in \code{dfvec}, computes the maximum
#' of their absolute values for each iteration, and returns the (1-alpha) quantile
#' of these maxima.
#'
#' This is useful for multiple comparison procedures where different tests may have
#' different degrees of freedom.
#'
#' @return Numeric value: the upper (1-alpha) quantile of the distribution.
#'
#' @seealso \code{\link{smmvalv2}}, \code{\link{smmvalv3}}
#'
#' @keywords multivariate
#' @export
smmval<-function(dfvec,iter=10000,alpha=.05,SEED=TRUE){
#
#  Determine the upper 1-alpha quantile of the maximum of
#  K independent Student's T random variables.
#  dfvec is a vector of length K containing the degrees of freedom
#
#  So this distribution is similar to a Studentized maximum modulus distribution but
#  the T statistics are not based on an estimate of an assumed common variance.
#
if(SEED)set.seed(1)
vals<-NA
tvals<-NA
J<-length(dfvec)
for(i in 1:iter){
for(j in 1:J){
tvals[j]<-rt(1,dfvec[j])
}
vals[i]<-max(abs(tvals))
}
vals<-sort(vals)
ival<-round((1-alpha)*iter)
qval<-vals[ival]
qval
}

#' Studentized Maximum Modulus Distribution Quantile (Vectorized Version)
#'
#' @description
#' Vectorized version of \code{\link{smmval}} that more efficiently determines the
#' upper 1-alpha quantile of the maximum of K independent Student's t random variables.
#'
#' @param dfvec Vector of length K containing the degrees of freedom for each
#'   independent t distribution.
#' @param iter Number of Monte Carlo iterations. Default is 20000.
#' @param alpha Significance level. Default is 0.05.
#' @param SEED Logical. If \code{TRUE} (default), sets random seed to 1 for
#'   reproducibility.
#'
#' @details
#' This function is a more efficient implementation of \code{\link{smmval}} using
#' vectorized matrix operations. It generates all random values at once and uses
#' \code{apply} to compute row-wise maxima, making it faster for large numbers of
#' iterations.
#'
#' @return Numeric value: the upper (1-alpha) quantile of the distribution.
#'
#' @seealso \code{\link{smmval}}, \code{\link{smmvalv3}}
#'
#' @keywords multivariate
#' @export
smmvalv2<-function(dfvec,iter=20000,alpha=.05,SEED=TRUE){
#
if(SEED)set.seed(1)
vals<-NA
tvals<-NA
J<-length(dfvec)
z=matrix(nrow=iter,ncol=J)
for(j in 1: J)z[,j]=abs(rt(iter,dfvec[j]))
vals=apply(z,1,max)
vals<-sort(vals)
ival<-round((1-alpha)*iter)
qval<-vals[ival]
qval
}

#' Studentized Maximum Modulus Distribution Quantile (Equal DF Version)
#'
#' @description
#' Specialized version of \code{\link{smmval}} for the case where all t statistics
#' have the same degrees of freedom. Computes the upper 1-alpha quantile of the
#' maximum of ntests independent Student's t random variables.
#'
#' @param ntests Number of tests (t statistics) to be performed.
#' @param df Degrees of freedom (same for all t statistics).
#' @param iter Number of Monte Carlo iterations. Default is 20000.
#' @param alpha Significance level. Default is 0.05.
#' @param SEED Logical. If \code{TRUE} (default), sets random seed to 1 for
#'   reproducibility.
#'
#' @details
#' This function is optimized for the special case where all t statistics have equal
#' degrees of freedom. It internally creates a vector with \code{rep(df, ntests)} and
#' uses the vectorized approach from \code{\link{smmvalv2}}.
#'
#' Useful for multiple comparison procedures with equal sample sizes across groups.
#'
#' @return Numeric value: the upper (1-alpha) quantile of the distribution.
#'
#' @seealso \code{\link{smmval}}, \code{\link{smmvalv2}}
#'
#' @keywords multivariate
#' @export
smmvalv3<-function(ntests,df,iter=20000,alpha=.05,SEED=TRUE){
#
#  ntests=number of tests to be performed
#
if(SEED)set.seed(1)
vals<-NA
tvals<-NA
dfvec=rep(df,ntests)
z=matrix(nrow=iter,ncol=ntests)
for(j in 1: ntests)z[,j]=rt(iter,dfvec[j])
vals=apply(abs(z),1,max)
vals<-sort(vals)
ival<-round((1-alpha)*iter)
qval<-vals[ival]
qval
}

#' Smoother Predictions Wrapper
#'
#' @description
#' Unified interface for generating predictions from various smoothing methods
#' including running interval smoothers, LOESS, random forests, and binary smoothers.
#'
#' @param x Matrix or vector of predictor values.
#' @param y Vector of response values.
#' @param pts Points at which to generate predictions. Default is \code{x}.
#' @param xout Logical. If \code{TRUE}, remove outliers from predictor space.
#'   Default is \code{FALSE}.
#' @param outfun Outlier detection function. Default is \code{\link{outpro}}.
#' @param method Character string specifying smoothing method. One of:
#'   \itemize{
#'     \item \code{"RUN"}: Running interval smoother (default)
#'     \item \code{"LOESS"}: Cleveland's LOESS
#'     \item \code{"RF"}: Random Forest regression
#'     \item \code{"BIN"}: Binary dependent variable smoother (logSM)
#'   }
#' @param ... Additional arguments passed to the specific smoother function.
#'
#' @details
#' This wrapper function provides a consistent interface to multiple smoothing methods:
#' \itemize{
#'   \item \code{RUN}: Calls \code{\link{rplot.pred}} for running interval smoothing
#'   \item \code{LOESS}: Calls \code{\link{lplot.pred}} for LOESS smoothing
#'   \item \code{RF}: Calls \code{\link{RFreg}} for random forest regression
#'   \item \code{BIN}: Calls \code{\link{logSMpred}} for binary response smoothing
#' }
#'
#' @return Vector of predicted values at the specified points.
#'
#' @seealso \code{\link{rplot.pred}}, \code{\link{lplot.pred}}, \code{\link{RFreg}},
#'   \code{\link{logSMpred}}
#'
#' @keywords smooth regression
#' @export
smpred<-function(x,y,pts=x,xout=FALSE,outfun=outpro,
method=c('RUN','LOESS','RF','BIN'),...){
#
#  RUN: running interval smoother
#  LOESS: Cleveland's LOESS
#  Random Forest
#  BIN:  Binary DV, use logSM
#
x=as.matrix(x)  #need this for method RF
type=match.arg(method)
switch(type,
    RUN=rplot.pred(x=x,y=y,pts=pts,xout=xout,outfun=outfun,...)$Y.hat,
    LOESS=lplot.pred(x=x,y=y,pts=pts,xout=xout,outfun=outfun,...)$yhat,
    RF=RFreg(x=x,y=y,pts=pts,xout=xout,outfun=outfun,pyhat=TRUE,plotit=FALSE,...),
    BIN=logSMpred(x=x,y=y,pts=pts,xout=xout,outfun=outfun,...))
}




# ----------------------------------------------------------------------------

# smRstr

# ----------------------------------------------------------------------------

#' Estimate Explanatory Strength of Association Using Running Interval Smoother
#'
#' @description
#' Estimates the explanatory strength of an association (a generalization of
#' Pearson's correlation) based on a running interval smoother and leave-one-out
#' cross-validation. Also estimates prediction error.
#'
#' @param x Matrix or vector of predictor values.
#' @param y Vector of response values.
#' @param fr Span (fraction of data) for the running interval smoother. Default is 1.
#' @param est Measure of location to be used. Default is \code{\link{tmean}} (20% trimmed mean).
#' @param nmin Minimum number of points required in running interval. Default is 1.
#' @param varfun Measure of variation used when estimating prediction error.
#'   Default is \code{\link{winvar}} (winsorized variance). Example: \code{varfun=pbvar}
#'   would use the percentage bend measure of variation.
#' @param xout Logical. If \code{TRUE}, remove outliers from predictor space.
#'   Default is \code{FALSE}.
#' @param outfun Outlier detection function. Default is \code{\link{outpro}}.
#' @param ... Additional arguments passed to \code{est} and other functions.
#'
#' @details
#' The function uses leave-one-out cross-validation to estimate:
#' \enumerate{
#'   \item Prediction error based on the specified variance function
#'   \item Explanatory strength as sqrt((variance without predictor - prediction error) /
#'         variance without predictor)
#' }
#'
#' The explanatory strength ranges from 0 (no association) to 1 (perfect prediction),
#' providing a robust alternative to R-squared that generalizes to nonlinear relationships.
#'
#' @return List with components:
#'   \item{str}{Explanatory strength of the association (0 to 1)}
#'   \item{pred.error}{Prediction error based on cross-validation}
#'
#' @seealso \code{\link{runhat}}, \code{\link{rung3hat}}, \code{\link{smstrcom}}
#'
#' @keywords regression robust
#' @export
smRstr<-function(x,y,fr=1,est=tmean,nmin=1,varfun=winvar,xout=FALSE,outfun=outpro,...){
#
# Estimate explanatory strength of an association (a generlization of
# Pearson's correlation) based on a running interval smoother
# and a leave-one-out cross-validation technique.
# Prediction error is estimated as well.
#
# Arguments:
# est: the measure of location to be used by rplot
# varfun: the measure of variation used when estimating prediction error
#   Example: varfun=pbvar would compute the percentage bend measure of
#            of varition between observed and predicted values of the
#            dependent variable.
# fr: the span used by rplot
#
# Function returns
# str: strength of the association
# pred.error: prediction error
#
xy=elimna(cbind(x,y))
if(xout){
flag=outfun(x,plotit=FALSE)$keep
xy=xy[flag,]
}
p=ncol(xy)
pm1=p-1
x=xy[,1:pm1]
y=xy[,p]
x=as.matrix(x)
px=ncol(x)
px1=px+1
n=nrow(xy)
val=NA
for(i in 1:n){
if(px==1)val[i]=runhat(xy[-i,1:px],xy[-i,px1],x[i,1:px],fr=fr,nmin=nmin,est=est)
if(px>1)val[i]=rung3hat(xy[-i,1:px],xy[-i,px1],pts=t(as.matrix(x[i,1:px])),fr=fr,est=est,...)$rmd
}
dif=y-val
dif=elimna(dif)
pe=varfun(dif)
nopre=locCV(y,varfun=varfun,locfun=est,...)# no predictor
rat=(nopre-pe)/nopre
str=0
if(rat>0)str=sqrt(rat)
list(str=str,pred.error=pe)
}




# ----------------------------------------------------------------------------

# smstrcom

# ----------------------------------------------------------------------------

#' Compare Strength of Association Between Two Independent Bivariate Datasets
#'
#' @description
#' Compares the strength of association between (x1, y1) and (x2, y2) for two
#' independent groups using a robust measure based on Cleveland's LOESS smoother
#' and bootstrap resampling.
#'
#' @param x1 Vector of predictor values for group 1.
#' @param y1 Vector of response values for group 1.
#' @param x2 Vector of predictor values for group 2.
#' @param y2 Vector of response values for group 2.
#' @param nboot Number of bootstrap samples. Default is 200.
#' @param pts Points for plotting (deprecated parameter). Default is \code{NA}.
#' @param plotit Logical. If \code{TRUE} (default), plot the smooths for both groups.
#' @param SEED Logical. If \code{TRUE} (default), set random seed for reproducibility.
#' @param varfun Measure of variation. Default is \code{\link{pbvar}} (percentage bend variance).
#' @param fr Span for the running interval smoother plot. Default is 0.8.
#' @param xout Logical. If \code{TRUE}, remove outliers from predictor space. Default is \code{FALSE}.
#' @param outfun Outlier detection function. Default is \code{\link{out}}.
#' @param ... Additional arguments passed to outlier detection function.
#'
#' @details
#' The function:
#' \enumerate{
#'   \item Computes strength of association for each group using \code{\link{lplotv2}}
#'   \item Generates bootstrap samples to estimate the distribution of strength differences
#'   \item Computes p-value based on bootstrap distribution
#'   \item Determines critical p-value (\code{pcrit.05}) adjusted for sample sizes
#'   \item Optionally plots the smooths using \code{\link{runmean2g}}
#' }
#'
#' Reject the null hypothesis of equal association strength at the 0.05 level if
#' \code{p.value <= pcrit.05}.
#'
#' @return List with components:
#'   \item{p.value}{Bootstrap p-value for testing equal association strength}
#'   \item{pcrit.05}{Critical p-value for 0.05 level test (adjusted for sample sizes)}
#'   \item{cor1}{Estimated strength of association for group 1}
#'   \item{cor2}{Estimated strength of association for group 2}
#'
#' @seealso \code{\link{smRstr}}, \code{\link{lplotv2}}, \code{\link{runmean2g}}
#'
#' @keywords robust regression htest
#' @export
smstrcom<-function(x1,y1,x2,y2,nboot=200,pts=NA,plotit=TRUE,
SEED=TRUE,varfun=pbvar,fr=.8,xout=FALSE,outfun=out,...){
#
# Compare the association of the two variables x1 and y1 to the
# association between x2 and y2
# (two independent  groups) using a robust measure of the
# strength of the association associated with Cleveland's LOWESS smoother.
#
#  Assume data are in x1 y1 x2 and y2
#
# Reject at the .05 level if the reported p-value is less than or
# equal to p.crit, which is returned by the function.
#
m<-elimna(cbind(x1,y1))
x1<-m[,1]
y1<-m[,2]
if(xout){
flag<-outfun(x1,...)$keep
x1<-x1[flag]
y1<-y1[flag]
}
m<-elimna(cbind(x2,y2))
x2<-m[,1]
y2<-m[,2]
if(xout){
flag<-outfun(x2,...)$keep
x2<-x2[flag]
y2<-y2[flag]
}
if(SEED)set.seed(2)
estmat1=NA
estmat2=NA
data1<-matrix(sample(length(y1),size=length(y1)*nboot,replace=TRUE),nrow=nboot)
data2<-matrix(sample(length(y2),size=length(y2)*nboot,replace=TRUE),nrow=nboot)
#
for(ib in 1:nboot){
estmat1[ib]=lplotv2(x1[data1[ib,]],y1[data1[ib,]],plotit=FALSE,
varfun=varfun)$Strength.Assoc
estmat2[ib]=lplotv2(x2[data2[ib,]],y2[data2[ib,]],
varfun=varfun,plotit=FALSE)$Strength.Assoc
}
dif<-(estmat1<estmat2)
dif0<-(estmat1==estmat2)
p.value=mean(dif)+.5*mean(dif0)
p.value=2*min(c(p.value,1-p.value))
n1=length(y1)
n2=length(y2)
p1=.05
p2=.05
temp1=tsreg(c(100,200),c(.08,.05),SEED=FALSE)$coef
temp2=tsreg(c(50,100),c(.21,.08),SEED=FALSE)$coef
temp3=tsreg(c(30,50),c(.3,.21),SEED=FALSE)$coef
if(n1<200)p1=temp1[1]+temp1[2]*n1
if(n1<100)p1=temp2[1]+temp2[2]*n1
#if(n1<50)p1=temp3[1]+temp3[2]*n1
#if(n1<30)p1=.3
if(n2<200)p2=temp1[1]+temp1[2]*n2
if(n2<100)p2=temp2[1]+temp2[2]*n2
pcrit=(n2*p1+n1*p2)/(n1+n2)
names(pcrit)=NULL
est1=lplotv2(x1,y1,plotit=FALSE,varfun=varfun)$Strength.Assoc
est2=lplotv2(x2,y2,plotit=FALSE,varfun=varfun)$Strength.Assoc
if(plotit)runmean2g(x1,y1,x2,y2,fr=fr,est=tmean,sm=TRUE)
list(p.value=p.value,pcrit.05=pcrit,cor1=est1,cor2=est2)
}

#' Test for Equal Location Measures Across Quantile-Defined Groups
#'
#' @description
#' Tests the hypothesis of equal measures of location across groups defined by
#' quantile splits of an independent variable in multivariate data.
#'
#' @param x Matrix of independent variables (predictors).
#' @param y Vector of dependent variable (response).
#' @param IV Integer specifying which independent variable to split on (column index).
#'   Default is 1 (first column).
#' @param Qsplit Vector of quantiles for splitting the independent variable.
#'   Default is 0.5 (median split). Example: \code{c(.25, .5, .75)} splits at quartiles.
#' @param nboot Number of bootstrap samples when using percentile bootstrap.
#'   Default is 1000.
#' @param est Measure of location. Default is \code{\link{tmean}} (20% trimmed mean).
#' @param tr Amount of trimming when using non-bootstrap method. Default is 0.2.
#' @param PB Logical. If \code{TRUE}, use percentile bootstrap method. Default is \code{FALSE}.
#'   Automatically set to \code{TRUE} if \code{est} is \code{median} or \code{hd}.
#' @param xout Logical. If \code{TRUE}, remove multivariate outliers from predictors.
#'   Default is \code{FALSE}.
#' @param outfun Outlier detection function. Default is \code{\link{outpro}}.
#' @param SEED Logical. If \code{TRUE} (default), set random seed for reproducibility.
#' @param ... Additional arguments passed to \code{est} or other functions.
#'
#' @details
#' The function:
#' \enumerate{
#'   \item Splits data into groups based on quantiles of the specified independent variable
#'   \item Extracts the dependent variable values for each group
#'   \item Tests for equal location measures using either:
#'     \itemize{
#'       \item \code{\link{lincon}} for trimmed means (when \code{PB=FALSE})
#'       \item \code{\link{linconpb}} for percentile bootstrap (when \code{PB=TRUE})
#'     }
#' }
#'
#' Example: \code{Qsplit=c(.25, .5, .75)} creates 4 groups (below Q1, Q1-Q2, Q2-Q3, above Q3).
#'
#' @return List with components:
#'   \item{Indep.var.summary.for.each.group}{Summary statistics for the independent
#'     variable within each group}
#'   \item{output}{Results from \code{\link{lincon}} or \code{\link{linconpb}}}
#'
#' @seealso \code{\link{lincon}}, \code{\link{linconpb}}, \code{\link{binmat}},
#'   \code{\link{smgrid}}
#'
#' @keywords htest robust
#' @export
smtest<-function(x,y,IV=1,Qsplit=.5,nboot=1000,est=tmean,tr=.2,PB=FALSE,
xout=FALSE,outfun=outpro,SEED=TRUE,...){
#
#
# Qsplit: split the independent variable based on the
#         quantiles indicated by Qsplit
#  Example Qsplit=c(.25,.5,.75) would split based on the quartiles
#
#  Then test the hypothesis of equal measures of location
#  IV=1: split on the first independent variable,
#  IV=2:  split on the second independent variable, etc.

x=as.matrix(x)
p=ncol(x)
if(p==1)stop('There should be two or more independent variables')
p1=p+1
xy<-elimna(cbind(x,y))
x<-xy[,1:p]
y<-xy[,p1]
v=NULL
if(xout){
flag<-outfun(x,plotit=FALSE)$keep
m<-xy[flag,]
x<-m[,1:p]
y<-m[,p1]
}
if(identical(est,median))PB=TRUE
if(identical(est,hd))PB=TRUE
z=list()
N.int=length(Qsplit)+1
qv=quantile(x[,IV],Qsplit)
qv=c(min(x[,IV]),qv,max(x[,IV]))
group=list()
for(j in 1:N.int){
j1=j+1
xsub=binmat(xy,IV,qv[j],qv[j1])
z[[j]]=xsub[,p1]
group[[j]]=summary(xsub[,IV])
}
if(PB)a=linconpb(z,est=est,SEED=SEED,nboot=nboot,...)
if(!PB)a=lincon(z,tr=tr,pr=FALSE)
list(Indep.var.summary.for.each.group=group,output=a)
}




# ----------------------------------------------------------------------------

# smvar

# ----------------------------------------------------------------------------

#' Estimate Conditional Variance Using Bagged Running Interval Smoother
#'
#' @description
#' Estimates VAR(Y|X), the conditional variance of Y given X, using a bagged
#' (bootstrap aggregated) running interval smoother. Provides robust estimation
#' of heteroscedasticity patterns.
#'
#' @param x Vector of predictor values.
#' @param y Vector of response values.
#' @param fr Span (fraction of data) for the running interval smoother. Default is 0.6.
#' @param xout Logical. If \code{TRUE} (default), eliminate points where x is an outlier.
#' @param eout Logical. If \code{TRUE}, eliminate points where (x,y) is a bivariate outlier.
#'   Default is \code{FALSE}.
#' @param xlab Label for x-axis. Default is "X".
#' @param ylab Label for y-axis. Default is "VAR(Y|X)".
#' @param pyhat Logical. If \code{TRUE}, return variance estimates for each x value.
#'   If \code{FALSE} (default), return "Done" message.
#' @param plotit Logical. If \code{TRUE} (default), create scatterplot of squared residuals
#'   with smooth curve showing estimated conditional variance.
#' @param nboot Number of bootstrap samples for bagging. Default is 40.
#' @param RNA Logical. If \code{TRUE}, remove missing values when applying smooth.
#'   If \code{FALSE} (default), may return NA for some predicted values.
#' @param SEED Logical. If \code{TRUE} (default), set random seed for reproducibility.
#'
#' @details
#' The function:
#' \enumerate{
#'   \item Fits LOESS smooth to estimate E(Y|X)
#'   \item Computes squared residuals (Y - Yhat)^2
#'   \item Uses bagged running interval smoother (\code{\link{runmbo}}) to estimate variance
#'   \item Optionally plots squared residuals with smooth variance curve
#' }
#'
#' This is useful for detecting and modeling heteroscedasticity in regression relationships.
#'
#' @return If \code{pyhat=TRUE}, returns vector of estimated conditional variance values
#'   at each x point. If \code{pyhat=FALSE}, returns character string "Done".
#'
#' @seealso \code{\link{runmbo}}, \code{\link{lplot}}, \code{\link{runmean}}
#'
#' @keywords regression smooth
#' @export
smvar<-function(x,y,fr=.6,xout=TRUE,eout=FALSE,xlab="X",ylab="VAR(Y|X)",pyhat=FALSE,plotit=TRUE,nboot=40,
RNA=FALSE,SEED=TRUE){
#
# Estimate VAR(Y|X) using bagged version of running interval method
#
# xout=T eliminates all points for which x is an outlier.
# eout=F eliminates all points for which (x,y) is an outlier.
#
# pyhat=T will return estimate for each x.
#
# RNA=T removes missing values when applying smooth
# with RNA=F, might get NA for some pyhat values.
#
# plotit=TRUE, scatterplot of points x versus square of
# predicted y minus y
# stemming from a smooth. Then plots a line indicating
# var(y|x) using bagged smooth
#
temp <- cbind(x, y)
temp <- elimna(temp)
x <- temp[, 1]
y <- temp[, 2]
yhat<-lplot(x, y, pyhat = TRUE, plotit = FALSE)$yhat.values
yvar<-(y-yhat)^2
estvar<-runmbo(x,y,est=var,pyhat=TRUE,fr=fr,plotit=FALSE,RNA=RNA,nboot=nboot)
if(plotit){
plot(c(x,x),c(yvar,estvar),type="n",xlab=xlab,ylab=ylab)
points(x,yvar)
sx<-sort(x)
xorder<-order(x)
sysm<-estvar[xorder]
lines(sx,sysm)
}
output <- "Done"
if(pyhat)output <- estvar
output
}

#' Multivariate Two-Sample Trimmed Mean Comparison
#'
#' @description
#' For two independent p-variate distributions, applies Yuen's test (\code{\link{yuen}})
#' to each variable (column) separately. Controls familywise error rate (FWE) using
#' Hochberg's method.
#'
#' @param x Matrix with p columns for group 1, or data in list mode.
#' @param y Matrix with p columns for group 2, or data in list mode.
#' @param tr Amount of trimming (proportion of observations to trim from each tail).
#'   Default is 0.2 (20% trimming).
#' @param alpha Familywise error rate. Default is 0.05.
#'
#' @details
#' The function applies Yuen's trimmed mean test to each of p variables independently,
#' then adjusts p-values using Hochberg's step-down method to control the familywise
#' error rate. This provides strong control of Type I error when testing multiple
#' hypotheses simultaneously.
#'
#' For each variable, the test compares trimmed means between the two independent groups.
#'
#' @return List with components:
#'   \item{n}{Vector of sample sizes (n1, n2) for the two groups}
#'   \item{test}{Matrix with columns: Variable index, test statistic, p-value,
#'     critical p-value (Hochberg adjusted), and standard error}
#'   \item{psihat}{Matrix with columns: Variable index, estimated difference,
#'     lower CI, upper CI}
#'   \item{num.sig}{Number of significant tests using adjusted critical values}
#'
#' @seealso \code{\link{yuen}}, \code{\link{trimcimul}}, \code{\link{lincon}}
#'
#' @keywords multivariate htest robust
#' @export
trim2gmul<-function(x,y, tr = 0.2, alpha = 0.05){
#
# For two independent  p-variate distributions,  apply yuen to each column of data
# FWE controlled with Hochberg's method
#
# x and y are  matrices having p columns. (Can have list mode as well.)
#
if(!is.matrix(x))x<-matl(x)
if(!is.matrix(x))stop('Data must be stored in a matrix or in list mode.')
if(!is.matrix(y))y<-matl(y)
if(!is.matrix(y))stop('Data must be stored in a matrix or in list mode.')

J<-ncol(x)
if(J!=ncol(y))stop('x and y should have the same number of columns')

xbar<-vector('numeric',J)
ncon<-J
dvec<-alpha/c(1:ncon)
psihat<-matrix(0,J,4)
dimnames(psihat)<-list(NULL,c('Variable','difference','ci.lower','ci.upper'))
test<-matrix(0,J,5)
dimnames(test)<-list(NULL,c('Variable','test','p.value','p.crit','se'))
temp1<-NA
nval=NULL
for (d in 1:J){
psihat[d,1]<-d
#dval=na.omit(x[,d])
#nval[d]=length(dval)
temp=yuen(x[,d],y[,d],tr=tr)
test[d,1]<-d
test[d,2]<-temp$teststat
test[d,3]=temp$p.value
test[d,5]<-temp$se
psihat[d,2]<-temp$dif
psihat[d,3]<-temp$ci[1]
psihat[d,4]<-temp$ci[2]
}
temp1=test[,3]
temp2<-order(0-temp1)
zvec<-dvec[1:ncon]
test[temp2,4]<-zvec
num.sig=sum(test[,3]<=test[,4])
list(n=c(nrow(x),nrow(y)),test=test,psihat=psihat,num.sig=num.sig)
}

#' Trimmed Mean Confidence Interval for Paired Differences
#'
#' @description
#' Convenience wrapper for computing confidence interval for trimmed mean of
#' paired differences (x - y). Simply calls \code{\link{trimci}} on the differences.
#'
#' @param x Vector of observations for first condition.
#' @param y Vector of observations for second condition (paired with x).
#' @param alpha Significance level for confidence interval. Default is 0.05.
#' @param tr Amount of trimming. Default is 0.2 (20% trimming).
#'
#' @details
#' This is a convenience function equivalent to \code{trimci(x-y, alpha=alpha, tr=tr, pr=FALSE)}.
#' It computes the trimmed mean of the differences and provides a confidence interval.
#'
#' @return Same output as \code{\link{trimci}}: list with estimate, confidence interval,
#'   test statistic, p-value, and sample size.
#'
#' @seealso \code{\link{trimci}}, \code{\link{yuend}}
#'
#' @keywords htest robust
#' @export
trimci.dif<-function(x,y,alpha=.05,tr=.2){
#
# For convenince
#
a=trimci(x-y,alpha=alpha,tr=tr,pr=FALSE)
a
}

#' Bootstrap-t Confidence Interval for Trimmed Mean
#'
#' @description
#' Computes a confidence interval for the trimmed mean using a bootstrap percentile-t
#' method (Tukey-McLaughlin method). Provides more accurate coverage than standard
#' methods, especially with non-normal distributions.
#'
#' @param x Vector of observations.
#' @param tr Amount of trimming. Default is 0.2 (20% trimming).
#' @param alpha Significance level. Default is 0.05.
#' @param nboot Number of bootstrap samples. Default is 1999.
#' @param side Logical. If \code{TRUE} (default), uses symmetric two-sided method.
#'   If \code{FALSE}, yields equal-tailed confidence interval.
#' @param plotit Logical. If \code{TRUE}, plot the estimated null distribution of the
#'   test statistic. Default is \code{FALSE}.
#' @param op Integer (1 or 2). When \code{plotit=TRUE}, controls plot type:
#'   \code{op=1} uses adaptive kernel density estimator, \code{op=2} uses expected
#'   frequency curve (faster for large \code{nboot}).
#' @param nullval Null hypothesis value to test against. Default is 0.
#' @param SEED Logical. If \code{TRUE} (default), set random seed to 2 for reproducibility.
#' @param prCRIT Logical. If \code{TRUE}, print critical values. Default is \code{FALSE}.
#' @param pr Logical. If \code{TRUE} (default), print informational messages.
#' @param xlab Label for x-axis when \code{plotit=TRUE}. Default is empty string.
#'
#' @details
#' The bootstrap-t method:
#' \enumerate{
#'   \item Generates bootstrap samples and computes t-statistics
#'   \item Uses the bootstrap distribution of t-statistics to determine critical values
#'   \item Forms confidence interval using these critical values
#' }
#'
#' The symmetric method (\code{side=TRUE}) uses absolute values of t-statistics and
#' provides p-values. The equal-tailed method (\code{side=FALSE}) uses separate lower
#' and upper critical values but does not compute p-values.
#'
#' @return List with components:
#'   \item{estimate}{Trimmed mean estimate}
#'   \item{ci}{Confidence interval (vector of length 2)}
#'   \item{test.stat}{Test statistic for testing \code{nullval}}
#'   \item{p.value}{P-value (only when \code{side=TRUE}, otherwise NA)}
#'   \item{n}{Sample size}
#'
#' @seealso \code{\link{trimci}}, \code{\link{trimpb}}, \code{\link{trimse}}
#'
#' @keywords htest robust bootstrap
#' @export
trimcibt<-function(x,tr=.2,alpha=.05,nboot=1999,side=TRUE,plotit=FALSE,op=1,
nullval=0,SEED=TRUE,prCRIT=FALSE,pr=TRUE,xlab=""){
#
#  Compute a 1-alpha confidence interval for the trimmed mean
#  using a bootstrap percentile t method.
#
#  The default amount of trimming is tr=.2
#  side=T, for true,  indicates the symmetric two-sided method
#
#  Side=F yields an equal-tailed confidence interval
#
#  NOTE: p.value is reported when side=T only.
#
#  prCRIT=TRUE, if side=FALSE, lower and upper critical values are returned.
# plotit=TRUE, plots an estimate of the null distribution of the Tukey--McLaughlin test statistic.
# op=1, use adaptive kernel density estimator when plotit=TRUE
# op=2, uses the expected frequency curve, which will have a lower execution if nboot is large.
#
x=elimna(x)
side<-as.logical(side)
p.value<-NA
if(SEED)set.seed(2) # set seed of random number generator so that
#   results can be duplicated.
test<-(mean(x,tr)-nullval)/trimse(x,tr)
data<-matrix(sample(x,size=length(x)*nboot,replace=TRUE),nrow=nboot)
data<-data-mean(x,tr)
top<-apply(data,1,mean,tr)
bot<-apply(data,1,trimse,tr)
tval<-top/bot
if(plotit){
if(op==1)akerd(tval,xlab=xlab)
if(op==2)rdplot(tval,xlab=xlab)
}
if(side)tval<-abs(tval)
tval<-sort(tval)
icrit<-round((1-alpha)*nboot)
ibot<-round(alpha*nboot/2)
itop<-nboot-ibot #altered code very slightly to correspond to recent versions of my books.
if(!side){
if(prCRIT)print(paste("Lower crit=",tval[ibot],"Upper crit=",tval[itop]))
if(prCRIT)print(paste(".025 Lower Type I=",mean(tval<=0-1.96)))
if(prCRIT)print(paste(".05 Lower Type I=",mean(tval<=0-1.645)))
if(prCRIT)print(paste(".025 Upper Type I=",mean(tval>=1.96)))
if(prCRIT)print(paste(".05 Upper Type I=",mean(tval>=1.645)))
ci<-mean(x,tr)-tval[itop]*trimse(x,tr)
ci[2]<-mean(x,tr)-tval[ibot]*trimse(x,tr)
if(pr)print("NOTE: p.value is computed only when side=T")
}
if(side){
if(prCRIT)print(paste("Symmetric Crit.val=",tval[icrit]))
ci<-mean(x,tr)-tval[icrit]*trimse(x,tr)
ci[2]<-mean(x,tr)+tval[icrit]*trimse(x,tr)
p.value<-(sum(abs(test)<=abs(tval)))/nboot
}
list(estimate=mean(x,tr),ci=ci,test.stat=test,p.value=p.value,n=length(x))
}

#' Multivariate Trimmed Mean Tests with FWE Control
#'
#' @description
#' For J dependent variables, applies \code{\link{trimci}} to each variable separately.
#' Controls familywise error rate (FWE) using the Rom-Hochberg method and provides
#' simultaneous confidence intervals.
#'
#' @param x Matrix with J columns (one per variable), or data in list mode.
#' @param tr Amount of trimming. Default is 0.2 (20% trimming).
#' @param alpha Familywise error rate. Default is 0.05.
#' @param null.value Null hypothesis value for each trimmed mean. Default is 0.
#'
#' @details
#' The function:
#' \enumerate{
#'   \item Applies one-sample trimmed mean test to each variable
#'   \item Adjusts p-values using the Rom-Hochberg method for strong FWE control
#'   \item Computes adjusted confidence intervals with simultaneous coverage 1-alpha
#' }
#'
#' The Rom-Hochberg method uses special critical values (stored for alpha=0.05 and 0.01)
#' that provide better power than Bonferroni while maintaining FWE control.
#'
#' @return List with components:
#'   \item{n}{Vector of sample sizes for each variable (after removing NAs)}
#'   \item{test}{Matrix with columns: Variable index, test statistic, p-value,
#'     adjusted critical p-value, and standard error}
#'   \item{psihat}{Matrix with columns: Variable index, estimate, unadjusted CI lower/upper,
#'     adjusted CI lower/upper}
#'   \item{num.sig}{Number of significant tests using adjusted critical values}
#'
#' @seealso \code{\link{trimci}}, \code{\link{trim2gmul}}, \code{\link{trimmulCI}}
#'
#' @keywords multivariate htest robust
#' @export
trimcimul<-function(x, tr = 0.2, alpha = 0.05,null.value=0){
#
# For J dependent random variables, apply trimci to each
# FWE controlled with Rom-Hochberg  method
#
# x is a matrix having J columns. (Can have list mode as well.)
if(!is.matrix(x))x<-matl(x)
if(!is.matrix(x))stop('Data must be stored in a matrix or in list mode.')
J<-ncol(x)
xbar<-vector('numeric',J)
ncon<-J
if(alpha==.05){
dvec<-c(.05,.025,.0169,.0127,.0102,.00851,.0073,.00639,.00568,.00511)
if(ncon > 10){
avec<-.05/c(11:ncon)
dvec<-c(dvec,avec)
}}
if(alpha==.01){
dvec<-c(.01,.005,.00334,.00251,.00201,.00167,.00143,.00126,.00112,.00101)
if(ncon > 10){
avec<-.01/c(11:ncon)
dvec<-c(dvec,avec)
}}
if(alpha != .05 && alpha != .01)dvec<-alpha/c(1:ncon)
psihat<-matrix(0,J,6)
dimnames(psihat)<-list(NULL,c('Variable','estimate','ci.lower','ci.upper','adj.ci.lower','adj.ci.upper'))
test<-matrix(0,J,5)
dimnames(test)<-list(NULL,c('Variable','test','p.value','p.crit','se'))
temp1<-NA
nval=NULL
for (d in 1:J){
psihat[d,1]<-d
dval=na.omit(x[,d])
nval[d]=length(dval)
temp=trimci(dval,tr=tr,pr=FALSE,null.value=null.value)
test[d,1]<-d
test[d,2]<-temp$test.stat
test[d,3]=temp$p.value
test[d,5]<-temp$se
psihat[d,2]<-temp$estimate
psihat[d,3]<-temp$ci[1]
psihat[d,4]<-temp$ci[2]
}
temp1=test[,3]
temp2<-order(0-temp1)
zvec<-dvec[1:ncon]
test[temp2,4]<-zvec
num.sig=sum(test[,3]<=test[,4])
# compute adjusted confidence intervals having simultaneous probability coverage 1-alpha
for(d in 1:J){
dval=na.omit(x[,d])
psihat[d,5:6]=trimci(dval,tr=tr,alpha=test[d,4],pr=FALSE,null.value=null.value)$ci
}
list(n=nval,test=test,psihat=psihat,num.sig=num.sig)
}

#' Trimmed Mean CI with Quantile Shift Effect Size
#'
#' @description
#' Computes confidence interval for the trimmed mean (same as \code{\link{trimci}})
#' plus a quantile shift measure of effect size.
#'
#' @param x Vector of observations.
#' @param tr Amount of trimming. Default is 0.2 (20% trimming).
#' @param alpha Significance level. Default is 0.05.
#' @param null.value Null hypothesis value. Default is 0.
#' @param pr Logical. If \code{TRUE} (default), print informational messages.
#' @param nullval Deprecated. Use \code{null.value} instead.
#'
#' @details
#' This function extends \code{\link{trimci}} by also computing a quantile shift
#' effect size using \code{\link{depQS}}. The quantile shift measures the shift
#' between distributions in terms of quantiles, providing a robust effect size measure.
#'
#' @return List with components from \code{\link{trimci}} plus:
#'   \item{Q.effect}{Quantile shift effect size}
#'
#' @seealso \code{\link{trimci}}, \code{\link{trimciv2}}, \code{\link{depQS}}
#'
#' @keywords htest robust
#' @export
trimciQS<-function(x,tr=.2,alpha=.05,null.value=0,pr=TRUE,nullval=NULL){
#
#  Compute a 1-alpha confidence interval for the trimmed mean
#  Same as trimci plus quantile shift measure of effect size.
#
#  The default amount of trimming is tr=.2
#
if(pr){
print("The p-value returned by this function is based on the")
print("null value specified by the argument null.value, which defaults to 0")
print('To get a measure of effect size using a Winsorized measure of scale,  use trimciv2')
}
if(!is.null(nullval))null.value=nullval
x<-elimna(x)
se<-sqrt(winvar(x,tr))/((1-2*tr)*sqrt(length(x)))
trimci<-vector(mode="numeric",length=2)
df<-length(x)-2*floor(tr*length(x))-1
trimci[1]<-mean(x,tr)-qt(1-alpha/2,df)*se
trimci[2]<-mean(x,tr)+qt(1-alpha/2,df)*se
test<-(mean(x,tr)-null.value)/se
sig<-2*(1-pt(abs(test),df))
QS=depQS(x,locfun=tmean,tr=tr)$Q.effect
list(ci=trimci,estimate=mean(x,tr),test.stat=test,se=se,p.value=sig,n=length(x),Q.effect=QS)
}

#' Trimmed Mean CI with Standardized Effect Size
#'
#' @description
#' Computes confidence interval for the trimmed mean (same as \code{\link{trimci}})
#' plus a standardized effect size based on Winsorized standard deviation.
#'
#' @param x Vector of observations.
#' @param tr Amount of trimming. Default is 0.2 (20% trimming).
#' @param alpha Significance level. Default is 0.05.
#' @param null.value Null hypothesis value. Default is 0.
#' @param pr Logical. If \code{TRUE} (default), print informational messages.
#'
#' @details
#' This function extends \code{\link{trimci}} by computing a standardized effect size:
#' the difference between the trimmed mean and null value divided by the Winsorized
#' standard deviation, rescaled to estimate the standard deviation when sampling from
#' a normal distribution.
#'
#' The rescaling factor adjusts for the trimming to provide an effect size comparable
#' to Cohen's d when data are normally distributed.
#'
#' @return List with components from \code{\link{trimci}} plus:
#'   \item{Effect.Size}{Standardized effect size based on Winsorized standard deviation}
#'
#' @seealso \code{\link{trimci}}, \code{\link{trimciQS}}, \code{\link{winvar}}
#'
#' @keywords htest robust
#' @export
trimciv2<-function(x,tr=.2,alpha=.05,null.value=0,pr=TRUE){
#
#  Compute a 1-alpha confidence interval for the trimmed mean
#  Same as trimci, only a standardized measure of effect size is reported:
# the difference between the trimmed mean and hypothesized value divided by
# the Winsorized standard deviation, rescaled to estimate the standard deviation
# when sampling from a normal distribution.
#
#  The default amount of trimming is tr=.2
#
x<-elimna(x)
se<-sqrt(winvar(x,tr))/((1-2*tr)*sqrt(length(x)))
trimci<-vector(mode="numeric",length=2)
df<-length(x)-2*floor(tr*length(x))-1
trimci[1]<-mean(x,tr)-qt(1-alpha/2,df)*se
trimci[2]<-mean(x,tr)+qt(1-alpha/2,df)*se
test<-(mean(x,tr)-null.value)/se
sig<-2*(1-pt(abs(test),df))
if(tr==0)term=1
if(tr>0)term=sqrt(area(dnormvar,qnorm(tr),qnorm(1-tr))+2*(qnorm(tr)^2)*tr)
epow=(mean(x,tr)-null.value)*term/sqrt(winvar(x,tr=tr,na.rm=TRUE))
list(ci=trimci,estimate=mean(x,tr),test.stat=test,se=se,p.value=sig,n=length(x),Effect.Size=epow)
}




# ----------------------------------------------------------------------------

# trimmulCI

# ----------------------------------------------------------------------------

#' Multivariate Trimmed Mean Tests with Simultaneous CIs
#'
#' @description
#' For J dependent variables, applies trimmed mean test to each with simultaneous
#' confidence intervals having probability coverage 1-alpha. Particularly useful
#' when J is large (>20) and sample size is small (<=20).
#'
#' @param x Matrix with J columns (one per variable), or data in list mode.
#' @param tr Amount of trimming. Default is 0.2 (20% trimming).
#' @param alpha Familywise error rate. Default is 0.05.
#' @param null.value Null hypothesis value for each trimmed mean. Default is 0.
#' @param nboot Number of bootstrap samples for determining critical p-value.
#'   Default is 2000.
#' @param SEED Logical. If \code{TRUE} (default), set random seed to 2 for reproducibility.
#' @param MC Logical. If \code{TRUE} (default), use parallel processing via \code{mclapply}.
#'
#' @details
#' This function uses bootstrap methods to determine a critical p-value that controls
#' the familywise error rate. Unlike \code{\link{trimcimul}} which uses Rom-Hochberg
#' critical values, this function estimates the critical value via simulation, making
#' it more appropriate when the number of tests is large or sample size is small.
#'
#' The simultaneous confidence intervals are constructed to have joint coverage 1-alpha.
#'
#' @return List with components:
#'   \item{test}{Matrix with columns: Variable index, test statistic, p-value, standard error}
#'   \item{psihat}{Matrix with columns: Variable index, estimate, CI lower, CI upper}
#'   \item{p.crit}{Critical p-value for controlling FWE at level alpha}
#'   \item{num.sig}{Number of significant tests}
#'
#' @seealso \code{\link{trimcimul}}, \code{\link{trimci}}, \code{\link{trimmul.crit}}
#'
#' @keywords multivariate htest robust bootstrap
#' @export
trimmulCI<-function(x, tr = 0.2, alpha = 0.05,null.value=0,nboot=2000,SEED=TRUE,MC=TRUE){
#
# For J dependent random variables, apply trimci to each.
# Confidence intervals are designed to have simultaneous probability coverage 1-alpha
# Useful when the number of variables is large say >20 and n is very small <=20
#
# x is a matrix having J columns. (Can have list mode as well.)
#
# Output:
#  num.sig =  number of significant results.
#
#
if(SEED)set.seed(2)
if(!is.matrix(x))x<-matl(x)
if(!is.matrix(x))stop('Data must be stored in a matrix or in list mode.')
J<-ncol(x)
xbar<-vector('numeric',J)
ncon<-J
psihat<-matrix(0,J,4)
dimnames(psihat)<-list(NULL,c('Variable','estimate','ci.lower','ci.upper'))
test<-matrix(0,J,4)
dimnames(test)<-list(NULL,c('Variable','test','p.value','se'))
#  Determine critical p-value
p.crit=trimmul.crit(x,tr=tr,nboot=nboot,alpha=alpha,SEED=SEED,MC=MC)
#
for (d in 1:J){
psihat[d,1]<-d
dval=na.omit(x[,d])
nval[d]=length(dval)
temp=trimci(dval,tr=tr,pr=FALSE,null.value=null.value,alpha=p.crit)
test[d,1]<-d
test[d,2]<-temp$test.stat
test[d,3]=temp$p.value
test[d,4]<-temp$se
psihat[d,2]<-temp$estimate
psihat[d,3]<-temp$ci[1]
psihat[d,4]<-temp$ci[2]
}
CI.sig=sum(psihat[,3]>null.value)+sum(psihat[,4]<null.value)
list(n=nval,test=test,psihat=psihat,crit.p.value=p.crit,num.sig=CI.sig)
}

#' Compute Trimmed Mean Components
#'
#' @description
#' Internal utility function that computes the trimmed mean and squared standard error.
#' Used by other trimmed mean functions.
#'
#' @param x Vector of observations.
#' @param tr Amount of trimming. Default is 0.2 (20% trimming).
#'
#' @details
#' This function computes two components:
#' \enumerate{
#'   \item Trimmed mean
#'   \item Squared standard error based on Winsorized variance
#' }
#'
#' The squared standard error is computed as:
#' (n-1) * winvar(x,tr) / (h * (h-1))
#' where h is the effective sample size after trimming.
#'
#' @return Numeric vector of length 2: (trimmed mean, squared standard error).
#'
#' @seealso \code{\link{trimse}}, \code{\link{winvar}}
#'
#' @keywords internal
#' @export
trimparts<-function(x,tr=.2){
#
#  Compute the trimmed mean, effective sample size, and squared standard error.
#  The default amount of trimming is tr=.2.
#
#  This function is used by other functions described in chapter 6.
#
tm<-mean(x,tr)
h1<-length(x)-2*floor(tr*length(x))
sqse<-(length(x)-1)*winvar(x,tr)/(h1*(h1-1))
trimparts<-c(tm,sqse)
trimparts
}




# ----------------------------------------------------------------------------

# trimpartt

# ----------------------------------------------------------------------------

#' Compute Linear Contrast of Trimmed Means
#'
#' @description
#' Internal utility function that computes a linear combination (contrast) of values.
#' Used by trimmed mean ANOVA functions.
#'
#' @param x Vector of values (typically trimmed means).
#' @param con Vector of contrast coefficients.
#'
#' @details
#' Simply computes the sum of element-wise products: sum(con * x).
#' This is used in ANOVA procedures to compute contrasts among trimmed means.
#'
#' @return Numeric value: the linear combination sum(con * x).
#'
#' @seealso \code{\link{trimparts}}, \code{\link{lincon}}
#'
#' @keywords internal
#' @export
trimpartt<-function(x,con){
#
#  This function is used by other functions described in chapter 6.
#
trimpartt<-sum(con*x)
trimpartt
}

#' Two-Way Repeated Measures ANOVA with Trimmed Means
#'
#' @description
#' Performs a J by K ANOVA using trimmed means with repeated measures on both factors.
#' Uses a within-subjects design where all subjects are measured under all J*K conditions.
#'
#' @param J Number of levels for Factor A.
#' @param K Number of levels for Factor B.
#' @param x Data in list mode or matrix form. If list, \code{x[[1]]} contains data for
#'   condition (1,1), \code{x[[2]]} for condition (1,2), ..., \code{x[[K]]} for (1,K),
#'   \code{x[[K+1]]} for (2,1), etc.
#' @param grp Vector indicating which groups to analyze. Default is \code{1:p} (all groups).
#' @param p Total number of groups. Default is J*K.
#' @param tr Amount of trimming. Default is 0.2 (20% trimming).
#'
#' @details
#' This function performs a two-way repeated measures ANOVA using trimmed means.
#' The data should be structured so that each element of the list (or column of the matrix)
#' contains observations for one combination of Factor A and Factor B levels, with all
#' observations from the same subjects measured repeatedly.
#'
#' The function:
#' \enumerate{
#'   \item Computes trimmed means for each cell
#'   \item Estimates the covariance matrix among cells using \code{\link{covmtrim}}
#'   \item Tests main effects for Factor A and Factor B
#'   \item Tests the A × B interaction
#' }
#'
#' @return List with test results for main effects and interaction.
#'
#' @seealso \code{\link{bwtrim}}, \code{\link{rmanova}}, \code{\link{covmtrim}}
#'
#' @keywords htest robust
#' @export
trimww<-function(J,K,x,grp=c(1:p),p=J*K,tr=.2){
#
#  Perform a J by K anova using trimmed means with
#  repeated measures on both factors.
#
#  tr=.2 is default trimming
#
#  The R variable data is assumed to contain the raw
#  data stored in list mode. data[[1]] contains the data
#  for the first level of both factors: level 1,1.
#  data[[2]] is assumed to contain the data for level 1 of the
#  first factor and level 2 of the second: level 1,2
#  data[[K]] is the data for level 1,K
#  data[[K+1]] is the data for level 2,1, data[2K] is level 2,K, etc.
#
#  It is assumed that data has length JK, the total number of
#  groups being tested, but a subset of the data can be analyzed
#  using grp
#
if(is.list(x))x<-elimna(matl(x))
if(is.matrix(x))x<-elimna(x)
data<-x
if(is.matrix(data))data<-listm(data)
if(!is.list(data))stop("Data are not stored in list mode or a matrix")
if(p!=length(data)){
print("The total number of groups, based on the specified levels, is")
print(p)
print("The number of groups stored in x is")
print(length(data))
print("Warning: These two values are not equal")
}
if(p!=length(grp))stop("Apparently a subset of the groups was specified that does not match the total number of groups indicated by the values for J and K.")
tmeans<-0
h<-length(data[[grp[1]]])
v<-matrix(0,p,p)
for (i in 1:p)tmeans[i]<-mean(data[[grp[i]]],tr=tr,na.rm=TRUE)
v<-covmtrim(data,tr=tr)
ij<-matrix(c(rep(1,J)),1,J)
ik<-matrix(c(rep(1,K)),1,K)
jm1<-J-1
cj<-diag(1,jm1,J)
for (i in 1:jm1)cj[i,i+1]<-0-1
km1<-K-1
ck<-diag(1,km1,K)
for (i in 1:km1)ck[i,i+1]<-0-1
#  Do test for factor A
cmat<-kron(cj,ik)  # Contrast matrix for factor A
#Qa<-johansp(cmat,tmeans,v,h,J,K)
Qa<-trimww.sub(cmat,tmeans,v,h,J,K)
#Qa.siglevel<-1-pf(Qa$teststat,J-1,999)
Qa.siglevel<-1-pf(Qa,J-1,999)
# Do test for factor B
cmat<-kron(ij,ck)  # Contrast matrix for factor B
#Qb<-johansp(cmat,tmeans,v,h,J,K)
Qb<-trimww.sub(cmat,tmeans,v,h,J,K)
Qb.siglevel<-1-pf(Qb,K-1,999)
# Do test for factor A by B interaction
cmat<-kron(cj,ck)  # Contrast matrix for factor A by B
#Qab<-johansp(cmat,tmeans,v,h,J,K)
Qab<-trimww.sub(cmat,tmeans,v,h,J,K)
Qab.siglevel<-1-pf(Qab,(J-1)*(K-1),999)
list(Qa=Qa,Qa.siglevel=Qa.siglevel,
Qb=Qb,Qb.siglevel=Qb.siglevel,
Qab=Qab,Qab.siglevel=Qab.siglevel)
}


Stein.LC<-function(x,delta,con,alpha=.05,power=.8,z=NULL,tr=.2,reps=100000,SEED=TRUE){
#
# For a collection of linear contrast coefficients,
# determine the total number of observations to achieve power >= the value
# specified by the argument power if the linear contrast is >= the value
# indicated by the argument
# delta
#
#
if(SEED)set.seed(2)
if(is.null(ncol(con)))N=Stein.LC1(x,delta=delta,con=con,alpha=alpha,power=power,tr=tr,
reps=reps,SEED=SEED)$N
else{
J=nrow(con)
NL=ncol(con)
N=matrix(NA,J,NL)
for(k in 1:NL)N[,k]=Stein.LC1(x,delta=delta,con[,k],alpha=alpha,power=power,tr=tr,
reps=reps,SEED=SEED)$N
}
list(con=con,N=N)
}




# ----------------------------------------------------------------------------

# Stein.LC1

# ----------------------------------------------------------------------------

Stein.LC1<-function(x,delta,con,alpha=.05,power=.8,z.sqrt=NULL,tr=0,reps=100000,SEED=TRUE){
#
# For a single set of linear contrast coefficients,
# determine the total number of observations to achieve power >= the value
# specified by the argument power when the linear contrast is >= the value
# indicated by the argument
# delta
#
#
if(SEED)set.seed(2)
x=elimna(x)
if(is.matrix(x))x=listm(x)
J=length(x)
n=lapply(x,length)
n=list2vec(n)
g=floor(tr*n)
df=n-2*g-1
sq=lapply(x,winsdN,tr=tr)
sq=list2vec(sq)
sq=sq^2
top=1-alpha/2
t2=1-power
if(is.null(z.sqrt)){
v=sum.T(df,reps=reps,con=con)
ta=qest(v,top)
q=qest(v,t2)
delta=abs(delta) # Same result if not done but this avoids getting negative sqrt(z)
z.sqrt=delta/(ta-q)
}
N=rep(0,J)
for(k in 1:J){
if(con[k]!=0)N[k]=max(floor(sq[k]/z.sqrt^2),n[k])
}
list(n=n,N=N,z.sqrt=z.sqrt)
}




# ----------------------------------------------------------------------------

# Stein.pairs

# ----------------------------------------------------------------------------

Stein.pairs<-function(x,delta,alpha=.05,power=.8,z.sqrt=NULL,tr=.2,reps=100000,SEED=TRUE){
#
# All pairwise comparisons among J independent groups
# Using available data, determine how many observations, if any, are needed to get power >= the
#  value indicated by the argument
# power, given that  the difference between the measures of location is
#  delta
#
if(is.matrix(x))x=listm(x)
J=length(x)
ic=0
ALL=(J^2-J)/2
output=matrix(NA,ALL,6)
for(j in 1:J){
for(k in 1:J){
if(j<k){
ic=ic+1
output[ic,1]=j
output[ic,2]=k
a=Stein2g(x[[j]],x[[k]],pow=pow,SEED=SEED,delta=delta,alpha=alpha,tr=tr,reps=reps,z.sqrt=z.sqrt)
output[ic,3:4]=a$n
output[ic,5:6]=a$N
z.sqrt=a$z.sqrt
}}}
dimnames(output)=list(NULL,c('Group','Group','n','n','N','N'))
output
}




# ----------------------------------------------------------------------------

# stein1

# ----------------------------------------------------------------------------

stein1<-function(x,del,alpha=.05,pow=.8,oneside=FALSE,n=NULL,VAR=NULL){
#
# Performs Stein's method on the data in x.
# In the event additional observations are required
# and can be obtained, use the R function stein2.
#
del<-abs(del)
if(is.null(n))n<-length(x)
if(is.null(VAR))VAR=var(x)
df<-n-1
if(!oneside)alpha<-alpha/2
d<-(del/(qt(pow,df)-qt(alpha,df)))^2
N<-max(c(n,floor(VAR/d)+1))
N
}




# ----------------------------------------------------------------------------

# stein1.tr

# ----------------------------------------------------------------------------

stein1.tr<-function(x,del,alpha=.05,pow=.8,tr=.2){
#
# Extension of Stein's method when performing all pairwise
# comparisons among J dependent groups.
#
# If x represents a single group, one-sample analysis is performed.
#
if(tr < 0 || tr >=.5)stop("Argument tr must be between 0 and .5")
if(is.matrix(x))m<-x
if(is.list(x))m<-matl(x)
if(!is.matrix(x) && !is.list(x))m<-matrix(x,ncol=1)
m<-elimna(m)
m<-as.matrix(m)
ntest<-1
n<-nrow(m)
J<-ncol(m)
if(ncol(m) > 1)ntest<-(J^2-J)/2
g<-floor(tr*nrow(m))
df<-n-2*g-1
t1<-qt(pow,df)
t2<-qt(alpha/(2*ntest),df)
dv<-(del/(t1-t2))^2
nvec<-NA
if(ntest > 1){
ic<-0
for (j in 1:ncol(m)){
for (jj in 1:ncol(m)){
if(j<jj){
ic<-ic+1
dif<-m[,j]-m[,jj]
nvec[ic]<-floor(winvar(dif,tr=tr)/dv)+1
}}}}
if(ntest == 1)nvec[1]<-floor(winvar(m[,1],tr=tr)/dv)+1
N<-max(c(n,nvec))
N
}




# ----------------------------------------------------------------------------

# stein2

# ----------------------------------------------------------------------------

stein2<-function(x1,x2,mu0=0,alpha=.05){
#
# Do second stage of Stein's method
# x1 contains first stage data
# x2 contains first stage data
# mu0 is the hypothesized value
#
n<-length(x1)
df<-n-1
N<-n+length(x2)
test<-sqrt(N)*(mean(c(x1,x2))-mu0)/sqrt(var(x1))
crit <- qt(1 - alpha/2, df)
low<- mean(c(x1,x2))-crit*sqrt(var(x1))
up<- mean(c(x1,x2))+crit*sqrt(var(x1))
sig<-2*(1-pt(test,df))
list(ci = c(low, up), siglevel =sig,mean=mean(c(x1,x2)),
teststat = test, crit =  crit, df = df)
}




# ----------------------------------------------------------------------------

# stein2.tr

# ----------------------------------------------------------------------------

stein2.tr<-function(x,y,alpha=.05,tr=.2,null.value=0){
#
# Extension of Stein's method when performing all pairwise
# comparisons among J dependent groups.
#
# If x represents a single group, one-sample analysis is performed.
#
if(tr < 0 || tr >=.5)stop("Argument tr must be between 0 and .5")
if(is.matrix(x))m<-x
if(is.list(x))m<-matl(x)
if(is.list(y))y<-matl(y)
if(!is.matrix(x) && !is.list(x))m<-matrix(x,ncol=1)
if(!is.matrix(y) && !is.list(y))y<-matrix(y,ncol=1)
m<-elimna(m)
m<-as.matrix(m)
g<-floor(tr*nrow(m))
df<-nrow(m)-2*g-1
m<-rbind(m,y)
ic<-0
ntest<-(ncol(m)^2-ncol(m))/2
if(ntest==0)ntest<-1
test<-matrix(NA,ncol=3,nrow=ntest)
for (j in 1:ncol(m)){
for (jj in 1:ncol(m)){
if(j<jj){
ic<-ic+1
dif<-m[,j]-m[,jj]
test[ic,1]<-j
test[ic,2]<-jj
test[ic,3]<-sqrt(nrow(m))*(1-2*tr)*(mean(dif,tr=tr,na.rm=TRUE)-null.value)/sqrt(winvar(dif,tr=tr))
}}}
crit<-qt(1-alpha/(2*ntest),df)
if(ntest == 1)
test<-sqrt(nrow(m))*(1-2*tr)*mean(m[,1],tr=tr,na.rm=TRUE)/sqrt(winvar(m[,1],tr=tr))
list(test.stat=test,crit.val=crit)
}




# ----------------------------------------------------------------------------

# Stein2g

# ----------------------------------------------------------------------------

Stein2g<-function(x,y=NULL,delta,alpha=.05,power=.8,z.sqrt=NULL,tr=.2,reps=100000,SEED=TRUE){
#
# For two independent groups,  a Stein-type two stage method for determining how many more
# observations, if any, are need to achieve power indicated by the argument
# pow given a difference in location
# delta
#
# if y=NULL
# x can be a matrix with 2 columns or have list mode with length 2
#
# Or
# x and y can be vector.
#
if(SEED){
if(is.null(z.sqrt))set.seed(2)
}
if(!is.null(y))x=list(x,y)
x=elimna(x)
if(is.matrix(x))x=listm(x)
J=length(x)
if(J!=2)stop('This function is for two groups only')
n=lapply(x,length)
n=list2vec(n)
g=floor(tr*n)
df=n-2*g-1
sq=lapply(x,winsdN,tr=tr)
sq=list2vec(sq)
sq=sq^2
top=1-alpha/2
t2=1-power
if(is.null(z.sqrt)){
v=sum.T(df,reps=reps,con=c(1,-1))
ta=qest(v,top)
q=qest(v,t2)
delta=abs(delta) # Same result if not done but this avoids getting negative sqrt(z)
z.sqrt=delta/(ta-q)
}
N=NA
for(k in 1:2){
N[k]=max(floor(sq[k]/z.sqrt^2),n[k])
}
list(n=n,N=N,z.sqrt=z.sqrt)
}





# ============================================================================

# Multicore Variants (4 functions)

# ============================================================================


# ----------------------------------------------------------------------------

# ESmainMCP

# ----------------------------------------------------------------------------

ESmainMCP<-function(J,K,x,tr=0.2,nboot=100,SEED=TRUE){
#
#  Compute explanatory measure of effect size for all main effects
#  in a two-way design. That is, for Factor A, compute it for all levels j < j'
#  For Factor B, compute it for all level k<k'
#
if(is.matrix(x))x=listm(x)
x=lapply(x,elimna)
con=con2way(J,K)
conA=con$conA
FA=matrix(NA,nrow=ncol(conA),ncol=3)
ic=0
for(jj in 1:J){
for(jjj in 1:J){
if(jj < jjj){
ic=ic+1
FA[ic,1]=jj
FA[ic,2]=jjj
}}}
for(j in 1:ncol(conA)){
flag1=(conA[,j]==1)
flagm1=(conA[,j]==-1)
x1=as.vector(matl(x[flag1]))
x2=as.vector(matl(x[flagm1]))
FA[j,3]=yuenv2(x1,x2,tr=tr,nboot=nboot,SEED=SEED)$Effect.Size
}
dimnames(FA)<-list(NULL,c("Level","Level","Effect.Size"))
conB=con$conB
FB=matrix(NA,nrow=ncol(conB),ncol=3)
ic=0
for(jj in 1:K){
for(jjj in 1:K){
if(jj < jjj){
ic=ic+1
FB[ic,1]=jj
FB[ic,2]=jjj
}}}
for(j in 1:ncol(conB)){
for(jj in 1:J){
for(jjj in 1:J){
if(jj < jjj){
}}}
flag1=(conB[,j]==1)
flagm1=(conB[,j]==-1)
x1=as.vector(matl(x[flag1]))
x2=as.vector(matl(x[flagm1]))
FB[j,3]=yuenv2(x1,x2,tr=tr,nboot=nboot,SEED=SEED)$Effect.Size
}
dimnames(FB)<-list(NULL,c("Level","Level","Effect.Size"))
list(Factor.A=FA,Factor.B=FB)
}




# ----------------------------------------------------------------------------

# MCDCOR

# ----------------------------------------------------------------------------

MCDCOR<-function(x){
#val<-cov.mcd(x)
val<-DetMCD(x)
val=cov2cor(val$cov)
val
}




# ----------------------------------------------------------------------------

# MCWB

# ----------------------------------------------------------------------------

MCWB<-function(x,tr=.2,alpha=.05,SEED=TRUE,REPS=5000,...){
#
# J independent groups
#  Multiple comparisons with the best based on trimmed means
#  Control FWE when all J have a common trimmed mean.
#
#
chk=0
if(is.matrix(x)||is.data.frame(x))x<-listm(x)
J=length(x)
for(j in 1:J)x[[j]]=elimna(x[[j]])
e=lapply(x,tmean,tr)
e=pool.a.list(e)
n=pool.a.list(lapply(x,length))
id=which(e==max(e))
CON=conCON(J,id)$conCON
pcrit=MCWB.crit(n=n,alpha=alpha,SEED=SEED,REPS=REPS,...)
a=lincon(x,con=CON,pr=FALSE)
numsig=sum(a$psihat[,5]<=pcrit)
list(n=a$n,tests=a$test,psihat=a$psihat,con=CON,
Best.Group=id,Est.=e,IND.p.values=a$psihat[,5],p.crit=pcrit,
num.sig=numsig)
}

MCWB.crit<-function(n,alpha,SEED=TRUE,REPS=5000,...){
if(SEED)set.seed(3)
J=length(n)
z=list()
REM=NA
for(i in 1:REPS){
for(j in 1:J)z[[j]]=rnorm(n[j])
e=lapply(z,tmean,tr)
e=pool.a.list(e)
id=which(e==max(e))
CON=conCON(J,id)$conCON
a=lincon(z,con=CON,pr=FALSE)
REM[i]=min(a$psihat[,5])
}
hd(REM,alpha)
}




# ----------------------------------------------------------------------------

# rm.marg.OMCI

# ----------------------------------------------------------------------------

rm.marg.OMCI<-function(x,y=NULL,locfun=onestep,nboot=1000,SEED=TRUE,alpha=.05,
null.val=0,MC=FALSE,...){
#
# Two dependent groups.
# Confidence interval for effect size that takes into account heteroscedasticity as well as the
# association between X and Y based on the marginal distributions, not the
# difference scores. For robust estimators, these two approaches generally give
# different results.
#
if(!is.null(y))x=cbind(x,y)
x=elimna(x)
if(SEED)set.seed(2)
e=rm.margOM.es(x)
n=nrow(x)
if(!MC){
v=NA
for(i in 1:nboot){
id=sample(n,replace=TRUE)
v[i]=rm.margOM.es(x[id,],locfun=locfun)
}
}
if(MC){
d=list()
for(j in 1:nboot){
id=sample(n,replace=TRUE)
d[[j]]=x[id,]
}
v=mclapply(d,rm.margOM.es,locfun=locfun)
v=matl(v)
}

v=sort(v)
ilow<-round((alpha/2) * nboot)
ihi<-nboot - ilow
ilow<-ilow+1
ci=v[ilow]
ci[2]=v[ihi]
pv=mean(v<null.val)
pv=2*min(pv,1-pv)
list(n=n,effect.size=e,ci=ci,p.value=pv)
}


Aband<-function(x,alpha=.05,plotit=TRUE,sm=TRUE,SEED=TRUE,nboot=500,grp=c(1:4),
xlab="X (First Factor)",ylab="Delta",crit=NA,print.all=FALSE,plot.op=FALSE){
#
# Apply the shift function when analyzing main effect in a
# 2 by 2 design.
#
# For variables x1, x2, x3 and x4,
# In effect, this function applies a shift function to the distributions
# d1=(x1+x2)/2 and d2=(x3+x4)/2
#  That is, focus on first factor.
#  For second factor, use Bband.
#
# grp indicates the groups to be compared. By default grp=c(1,2,3,4)
# meaning that the first level of factor A consists of groups 1 and 2
# and the 2nd level of factor A consists of groups 3 and 4.
# (So level 1 of factor B consists of groups 1 and 3
#
# print.all=F,
# returns number sig, meaning number of confidence intervals that do not
# contain zero,
# the critical value used as well as the KS test statistics.
# print.all=T reports all confidence intervals, the number of which can
# be large.
#
if(!is.list(x) && !is.matrix(x))stop("store data in list mode or a matrix")
if(SEED)set.seed(2)
if(is.matrix(x))x<-listm(x)
for(j in 1:length(x))x[[j]]=elimna(x[[j]])/2
if(length(grp)<4)stop("There must be at least 4 groups")
if(length(x)!=4)stop("The argument grp must have 4 values")
x<-x[grp]
n<-c(length(x[[1]]),length(x[[2]]),length(x[[3]]),length(x[[4]]))
# Approximate the critical value
#
vals<-NA
y<-list()
if(is.na(crit)){
print("Approximating critical value. Please wait.")
for(i in 1:nboot){
for(j in 1:4)
y[[j]]<-rnorm(n[j])
temp<-ks.test(outer(y[[1]],y[[2]],FUN="+"),outer(y[[3]],y[[4]],FUN="+"))
vals[i]<-temp[1]$statistic
}
vals<-sort(vals)
ic<-(1-alpha)*nboot
crit<-vals[ic]
}
if(plot.op){
plotit<-F
g2plot(v1,v2)
}
output<-sband(outer(x[[1]],x[[2]],FUN="+"),outer(x[[3]],x[[4]],FUN="+"),
plotit=plotit,crit=crit,flag=FALSE,sm=sm,xlab=xlab,ylab=ylab)
if(!print.all){
numsig<-output$numsig
ks.test.stat<-ks.test(outer(x[[1]],x[[2]],FUN="+"),
outer(x[[3]],x[[4]],FUN="+"))$statistic
output<-matrix(c(numsig,crit,ks.test.stat),ncol=1)
dimnames(output)<-list(c("number sig","critical value","KS test statistics"),
NULL)
}
output
}




# ----------------------------------------------------------------------------

# ABES.KS

# ----------------------------------------------------------------------------

ABES.KS<-function(J,K,x,tr=0.2){
#
#  Effect size for Factor A, ignoring B and
#  Factor B, ignoring A
  #
  #  A robust heteroscedastic analog of Cohen's d is used
  #
  if(is.data.frame(x))x=as.matrix(x)
  if(is.matrix(x))x=listm(x)
  JK=J*K
  mat=matrix(c(1:JK),nrow=J,byrow=TRUE)
  A=list()
  for(j in 1:J){
  id=mat[j,]
  z=pool.a.list(x[id])
    A[[j]]=z
  }

  B=list()
  for(k in 1:K){
    id=mat[,k]
    z=pool.a.list(x[id])
    B[[k]]=z
  }
  E1=KS.ANOVA.ES(A,tr=tr)
  E2=KS.ANOVA.ES(B,tr=tr)
list(A.Effect.Size=E1,B.Effect.Size=E2)
}

absfun<-function(y,na.rm=FALSE){
absfun<-sum(abs(y),na.rm=na.rm)
absfun
}

acbinomci<-function(x=sum(y),nn=length(y),y=NULL,n=NA,alpha=.05){
#
#  Compute a 1-alpha confidence interval for p, the probability of
#  success for a binomial distribution, using a generalization of the
#  Agresti-Coull  method that was studied by Brown, Cai DasGupta
#  (Annals of Statistics, 2002, 30, 160-201.)
#
#  y is a vector of 1s and 0s.
#  x is number of successes.
#
if(!is.null(y[1])){
y=elimna(y)
nn=length(y)
}
if(nn==1)stop("Something is wrong: number of observations is only 1")
n<-nn
cr=qnorm(1-alpha/2)
ntil=n+cr^2
ptil=(x+cr^2/2)/ntil
if(x!=n && x!=0){
lower=ptil-cr*sqrt(ptil*(1-ptil)/ntil)
upper=ptil+cr*sqrt(ptil*(1-ptil)/ntil)
}
if(x==0){  #Use Clopper-Pearson
lower<-0
upper<-1-alpha^(1/n)
}
if(x==1){
upper<-1-(alpha/2)^(1/n)
lower<-1-(1-alpha/2)^(1/n)
}
if(x==n-1){
lower<-(alpha/2)^(1/n)
upper<-(1-alpha/2)^(1/n)
}
if(x==n){
lower<-alpha^(1/n)
upper<-1
}
phat<-x/n
list(phat=phat,se=sqrt(ptil*(1-ptil)/ntil),ci=c(lower,upper),n=n)
}

acbinomciv2<-function(x=sum(y),nn=length(y),y=NULL,n=NA,alpha=.05,nullval=.5){
#  Compute a p-value when testing the hypothesis that the probability of
#  success for a binomial distribution is equal to
#  nullval, which defaults to .5
#  The Agresti-Coull method is used.
#
#  y is a vector of 1s and 0s.
#  Or can use the argument
#  x = the number of successes observed among
#  n=nn trials.
#
res=acbinomci(x=x,nn=nn,y=y,alpha=alpha)
ci=res$ci
alph<-c(1:99)/100
for(i in 1:99){
irem<-i
chkit<-acbinomci(x=x,nn=nn,y=y,alpha=alph[i])$ci
if(chkit[1]>nullval || chkit[2]<nullval)break
}
p.value<-irem/100
if(p.value<=.1){
iup<-(irem+1)/100
alph<-seq(.001,iup,.001)
for(i in 1:length(alph)){
p.value<-alph[i]
chkit<-acbinomci(x=x,nn=nn,y=y,alpha=alph[i])$ci
if(chkit[1]>nullval || chkit[2]<nullval)break
}}
if(p.value<=.001){
alph<-seq(.0001,.001,.0001)
for(i in 1:length(alph)){
p.value<-alph[i]
chkit<-acbinomci(x=x,nn=nn,y=y,alpha=alph[i])$ci
if(chkit[1]>nullval || chkit[2]<nullval)break
}}
list(n=nn,phat=res$phat,se=res$se,ci=res$ci,p.value=p.value)
}

acbinomcipv=acbinomciv2


adcom<-function(x,y,est=mean,tr=0,nboot=600,alpha=.05,fr=NA,
jv=NA,SEED=TRUE,...){
#
# Test the hypothesis that component
# jv
# is zero. That is, in a generalized additive model, test
# H_0: f_jv(X_jv) = 0.
# Use a variation of Stute et al. (1998, JASA, 93, 141-149).
# method, and running interval version of the backfitting
# algorithm
#
# if jv=NA, all components are tested.
#
# Current version allows only 0 or 20% trimming
#
x=as.matrix(x)
if(!is.matrix(x))stop("X values should be stored in a matrix")
if(ncol(x)==1)stop("There should be two or more predictors")
temp<-cbind(x,y)
p<-ncol(x)
p1<-p+1
temp<-elimna(temp)
x<-temp[,1:p]
x<-as.matrix(x)
y<-temp[,p1]
if(is.na(fr)){
if(tr==.2){
nval<-c(20,40,60,80,120,160)
fval<-c(1.2,1,.85,.75,.65,.65)
if(length(y)<=160)fr<-approx(nval,fval,length(y))$y
if(length(y)>160)fr<-.65
}
if(tr==0){
nval<-c(20,40,60,80,120,160)
fval<-c(.8,.7,.55,.5,.5,.5)
if(length(y)<=160)fr<-approx(nval,fval,length(y))$y
if(length(y)>160)fr<-.6
}
}
if(is.na(fr))stop("Span can be deteremined only for 0 or .2 trimming")
if(SEED)set.seed(2)
x<-as.matrix(x)
mflag<-matrix(NA,nrow=length(y),ncol=length(y))
for (j in 1:length(y)){
for (k in 1:length(y)){
mflag[j,k]<-(sum(x[j,]<=x[k,])==ncol(x))
}
}
if(!is.na(jv))prval<-jv
if(is.na(jv))prval<-c(1:ncol(x))
c.sum<-matrix(NA,nrow=length(prval),ncol=2)
dimnames(c.sum)<-list(NULL,c("d.stat","p.value"))
for(ip in 1:length(prval)){
flag<-rep(TRUE,ncol(x))
flag[prval[ip]]<-FALSE
yhat<-adrun(x[,flag],y,plotit=FALSE,fr=fr,pyhat=TRUE)
regres<-y-yhat
temp<-indt(x[,!flag],regres)
c.sum[ip,1]<-temp$dstat
c.sum[ip,2]<-temp$p.value.d
}
list(results=c.sum)
}




# ----------------------------------------------------------------------------

# adjboxout

# ----------------------------------------------------------------------------

adjboxout<-function(x){
#
# determine outliers using adjusted boxplot rule based on the
# medcouple
#
x=elimna(x)
n=length(x)
MC=mcskew(x)
val=idealf(x)
iqr=val$qu-val$ql
if(MC>=0){
bot=val$ql-1.5*exp(0-4*MC)*iqr
top=val$qu+1.5*exp(3*MC)*iqr
}
if(MC<0){
bot=val$ql-1.5*exp(0-3*MC)*iqr
top=val$qu+1.5*exp(4*MC)*iqr
}
flag=rep(F,length(x))
fl=(x<bot)
fu=(x>top)
flag[fl]=T
flag[fu]=T
vec<-c(1:n)
outid<-NULL
if(sum(flag)>0)outid<-vec[flag]
keep<-vec[!flag]
outval<-x[flag]
keep=x[!flag]
list(out.val=outval,out.id=outid,keep=keep,cl=bot,cu=top)
}

adpchk<-function(x,y,adfun=adrun,gfun=runm3d,xlab="Additive Fit",
ylab="Gen. Fit",plotfun=lplot,xout=FALSE,outfun=out,...){
#
# Compare adrun estimate to runm3d
#
x=as.matrix(x)
p=ncol(x)
p1=p+1
temp=elimna(cbind(x,y))
x=temp[,1:p]
y=temp[,p1]
if(xout){
flag<-outfun(x,...)$keep
x<-as.matrix(x)
x<-x[flag,]
y<-y[flag]
x<-as.matrix(x)
}
fit1<-adfun(x,y,pyhat=TRUE,plotit=FALSE,...)
if(is.list(fit1))fit1=fit1[[length(fit1)]]
fit2<-gfun(x,y,pyhat=TRUE,plotit=FALSE,pr=FALSE,...)
if(is.list(fit2))fit2=fit2[[length(fit2)]]
plotfun(fit1,fit2,xlab=xlab,ylab=ylab)
abline(0,1)
}




# ----------------------------------------------------------------------------

# adrun

# ----------------------------------------------------------------------------

adrun<-function(x,y,est=tmean,iter=10,pyhat=FALSE,plotit=TRUE,fr=1,xlab='X',
ylab='Y',zlab='',
theta=50,phi=25,expand=.5,scale=TRUE,zscale=TRUE,xout=FALSE,eout=xout,outfun=out,ticktype='simple',...){
#
# additive model based on running interval smoother
# and backfitting algorithm
#
m<-elimna(cbind(x,y))
if(xout){
flag<-outfun(x,plotit=FALSE)$keep
x=x[flag,]
y=y[flag]
}
x<-as.matrix(x)
p<-ncol(x)
if(p==1)val<-rungen(x[,1],y,est=est,pyhat=TRUE,plotit=plotit,fr=fr,
xlab=xlab,ylab=ylab,...)$output
if(p>1){
np<-p+1
x<-m[,1:p]
y<-m[,np]
fhat<-matrix(NA,ncol=p,nrow=length(y))
fhat.old<-matrix(NA,ncol=p,nrow=length(y))
res<-matrix(NA,ncol=np,nrow=length(y))
dif<-1
for(i in 1:p)
fhat.old[,i]<-rungenv2(x[,i],y,est=est,pyhat=TRUE,plotit=FALSE,fr=fr,...)
eval<-NA
for(it in 1:iter){
for(ip in 1:p){
res[,ip]<-y
for(ip2 in 1:p){
if(ip2 != ip)res[,ip]<-res[,ip]-fhat.old[,ip2]
}
fhat[,ip]<-rungenv2(x[,ip],res[,ip],est=est,pyhat=TRUE,plotit=FALSE,fr=fr,...)
}
eval[it]<-sum(abs(fhat/sqrt(sum(fhat^2))-fhat.old/sqrt(sum(fhat.old^2))))
if(it > 1){
itm<-it-1
dif<-abs(eval[it]-eval[itm])
}
fhat.old<-fhat
if(dif<.01)break
}
val<-apply(fhat,1,sum)
aval<-est(y-val,...)
val<-val+aval
if(plotit && p==2){
fitr<-val
iout<-c(1:length(fitr))
nm1<-length(fitr)-1
for(i in 1:nm1){
ip1<-i+1
for(k in ip1:length(fitr))if(sum(x[i,]==x[k,])==2)iout[k]<-0
}
fitr<-fitr[iout>=1] # Eliminate duplicate points in the x-y plane
#                 This is necessary when doing three dimensional plots
#                 with the R function interp
mkeep<-x[iout>=1,]
fitr<-interp(mkeep[,1],mkeep[,2],fitr)
persp(fitr,theta=theta,phi=phi,xlab=xlab,ylab=ylab,zlab=zlab,expand=expand,
scale=scale,ticktype=ticktype)
}}
if(!pyhat)val<-'Done'
val
}

adrunl<-function(x,y,est=tmean,iter=10,pyhat=FALSE,plotit=TRUE,fr=.8,
xlab="x1",ylab="x2",zlab="",theta=50,phi=25,expand=.5,scale=FALSE,
zscale=TRUE,xout=FALSE,outfun=out,ticktype="simple",...){
#
# additive model based on running interval smoother
# and backfitting algorithm
#
m<-elimna(cbind(x,y))
x<-as.matrix(x)
p<-ncol(x)
if(p==1)val<-lplot(x[,1],y,pyhat=TRUE,plotit=plotit,span=fr,pr=FALSE)$yhat.values
if(p>1){
np<-p+1
x<-m[,1:p]
y<-m[,np]
fhat<-matrix(NA,ncol=p,nrow=length(y))
fhat.old<-matrix(NA,ncol=p,nrow=length(y))
res<-matrix(NA,ncol=np,nrow=length(y))
dif<-1
for(i in 1:p)
fhat.old[,i]<-lplot(x[,i],y,pyhat=TRUE,plotit=FALSE,span=fr,pr=FALSE)$yhat.values
eval<-NA
for(it in 1:iter){
for(ip in 1:p){
res[,ip]<-y
for(ip2 in 1:p){
if(ip2 != ip)res[,ip]<-res[,ip]-fhat.old[,ip2]
}
fhat[,ip]<-lplot(x[,ip],res[,ip],pyhat=TRUE,plotit=FALSE,span=fr,pr=FALSE)$yhat.values
}
eval[it]<-sum(abs(fhat/sqrt(sum(fhat^2))-fhat.old/sqrt(sum(fhat.old^2))))
if(it > 1){
itm<-it-1
dif<-abs(eval[it]-eval[itm])
}
fhat.old<-fhat
if(dif<.01)break
}
val<-apply(fhat,1,sum)
aval<-est(y-val,...)
val<-val+aval
if(plotit && p==2){
fitr<-val
iout<-c(1:length(fitr))
nm1<-length(fitr)-1
for(i in 1:nm1){
ip1<-i+1
for(k in ip1:length(fitr))if(sum(x[i,]==x[k,])==2)iout[k]<-0
}
fitr<-fitr[iout>=1] # Eliminate duplicate points in the x-y plane
#                 This is necessary when doing three dimensional plots
#                 with the R function interp
mkeep<-x[iout>=1,]
fitr<-interp(mkeep[,1],mkeep[,2],fitr)
persp(fitr,theta=theta,phi=phi,xlab=xlab,ylab=ylab,zlab=zlab,expand=expand,
scale=scale,ticktype=ticktype)
}}
if(!pyhat)val<-"Done"
val
}




# ----------------------------------------------------------------------------

# adtest

# ----------------------------------------------------------------------------

adtest<-function(x,y,est=tmean,nboot=100,alpha=.05,fr=NA,xout=FALSE,outfun=out,SEED=TRUE,...){
#
# Test the hypothesis that the regression model is additive.
# Use a variation of Stute et al. (1998, JASA, 93, 141-149).
# method, and running interval version of the backfitting
# algorithm
#
x=as.matrix(x)
if(ncol(x)==1)stop("There should be two or more predictors")
temp<-cbind(x,y)
p<-ncol(x)
p1<-p+1
temp<-elimna(temp)
x<-temp[,1:p]
x<-as.matrix(x)
y<-temp[,p1]
if(xout){
keepit<-rep(TRUE,nrow(x))
flag<-outfun(x,plotit=FALSE,...)$out.id
keepit[flag]<-FALSE
x<-x[keepit,]
y<-y[keepit]
}
if(alpha<.05 && nboot<=100)warning("You used alpha<.05 and nboot<=100")
if(is.na(fr)){
fr<-.8
if(ncol(x)==2){
nval<-c(20,30,50,80,150)
fval<-c(0.40,0.36,0.18,0.15,0.09)
if(length(y)<=150)fr<-approx(nval,fval,length(y))$y
if(length(y)>150)fr<-7.57/length(y)+.05
}
}
if(SEED)set.seed(2)
x<-as.matrix(x)
mflag<-matrix(NA,nrow=length(y),ncol=length(y))
for (j in 1:length(y)){
for (k in 1:length(y)){
mflag[j,k]<-(sum(x[j,]<=x[k,])==ncol(x))
}
}
yhat<-adrun(x,y,est=est,plotit=FALSE,fr=fr,pyhat=TRUE)
regres<-y-yhat
print("Taking bootstrap samples, please wait.")
data<-matrix(runif(length(y)*nboot),nrow=nboot)
data<-sqrt(12)*(data-.5) # standardize the random numbers.
rvalb<-apply(data,1,adtests1,yhat,regres,mflag,x,fr)
# An n x nboot matrix of R values
rvalb<-rvalb/sqrt(length(y))
dstatb<-apply(abs(rvalb),2,max)
wstatb<-apply(rvalb^2,2,mean)
v<-c(rep(1,length(y)))
rval<-adtests1(v,yhat,regres,mflag,x,fr)
rval<-rval/sqrt(length(y))
dstat<-max(abs(rval))
wstat<-mean(rval^2)
p.value.d<-1-sum(dstat>=dstatb)/nboot
p.value.w<-1-sum(wstat>=wstatb)/nboot
list(dstat=dstat,wstat=wstat,p.value.d=p.value.d,p.value.w=p.value.w)
}

adtestl<-function(x,y,est=tmean,nboot=100,alpha=.05,fr=NA,SEED=TRUE,...){
#
# Test the hypothesis that the regression model is additive.
# Use a variation of Stute et al. (1998, JASA, 93, 141-149).
# method, and running interval version of the backfitting
# algorithm
#
if(!is.matrix(x))stop("X values should be stored in a matrix")
if(ncol(x)==1)stop("There should be two or more predictors")
temp<-cbind(x,y)
p<-ncol(x)
p1<-p+1
temp<-elimna(temp)
x<-temp[,1:p]
x<-as.matrix(x)
y<-temp[,p1]
if(alpha<.05 && nboot<=100)warning("You used alpha<.05 and nboot<=100")
if(is.na(fr)){
fr<-.8
if(ncol(x)==2){
nval<-c(20,30,50,80,150)
fval<-c(0.40,0.36,0.18,0.15,0.09)
if(length(y)<=150)fr<-approx(nval,fval,length(y))$y
if(length(y)>150)fr<-.09
}
}
if(SEED)set.seed(2)
x<-as.matrix(x)
mflag<-matrix(NA,nrow=length(y),ncol=length(y))
for (j in 1:length(y)){
for (k in 1:length(y)){
mflag[j,k]<-(sum(x[j,]<=x[k,])==ncol(x))
}
}
yhat<-adrunl(x,y,plotit=FALSE,fr=fr,pyhat=TRUE)
regres<-y-yhat
print("Taking bootstrap sample, please wait.")
data<-matrix(runif(length(y)*nboot),nrow=nboot)
data<-sqrt(12)*(data-.5) # standardize the random numbers.
rvalb<-apply(data,1,adtestls1,yhat,regres,mflag,x,fr)
# An n x nboot matrix of R values
rvalb<-rvalb/sqrt(length(y))
dstatb<-apply(abs(rvalb),2,max)
wstatb<-apply(rvalb^2,2,mean)
dstatb<-sort(dstatb)
wstatb<-sort(wstatb)
# compute test statistic
v<-c(rep(1,length(y)))
rval<-adtestls1(v,yhat,regres,mflag,x,fr)
rval<-rval/sqrt(length(y))
dstat<-max(abs(rval))
wstat<-mean(rval^2)
ib<-round(nboot*(1-alpha))
critd<-dstatb[ib]
critw<-wstatb[ib]
list(dstat=dstat,wstat=wstat,critd=critd,critw=critw)
}




# ----------------------------------------------------------------------------

# adtestls1

# ----------------------------------------------------------------------------

adtestls1<-function(vstar,yhat,res,mflag,x,fr){
ystar<-yhat+res*vstar
bres<-adrunl(x,ystar,fr=fr,pyhat=TRUE,plotit=FALSE)
bres<-ystar-bres
rval<-0
for (i in 1:nrow(x)){
rval[i]<-sum(bres[mflag[,i]])
}
rval
}




# ----------------------------------------------------------------------------

# adtests1

# ----------------------------------------------------------------------------

adtests1<-function(vstar,yhat,res,mflag,x,fr){
ystar<-yhat+res*vstar
bres<-adrun(x,ystar,fr=fr,pyhat=TRUE,plotit=FALSE)
bres<-ystar-bres
rval<-0
for (i in 1:nrow(x)){
rval[i]<-sum(bres[mflag[,i]])
}
rval
}




# ----------------------------------------------------------------------------

# adtestv2

# ----------------------------------------------------------------------------

adtestv2<-function(x,y,est=tmean,nboot=500,alpha=.05,fr=NA,xout=TRUE,outfun=outpro,com.pval=FALSE,SEED=TRUE,qval=.5,...){
#
# For two predictors, test the hypothesis that the regression model is additive. That is, there is no interaction.
#  In essence, for the model Y=g_1(X_1)+g_2(X_2)+g_3(X_1X_2), test H_0: g_3(X_1X_2)=0
#
# The method fits an additive model using running interval smoother and the backfitting
# algorithm and then tests the hypothesis that the median of X_1X_2, given the residuals,
# is a straight horizontal line.
#
if(ncol(x)!=2)stop("There should be two predictors")
temp<-cbind(x,y)
p<-ncol(x)
p1<-p+1
temp<-elimna(temp)
x<-temp[,1:p]
x<-as.matrix(x)
y<-temp[,p1]
if(xout){
keepit<-rep(TRUE,nrow(x))
flag<-outfun(x,plotit=FALSE,...)$out.id
keepit[flag]<-FALSE
x<-x[keepit,]
y<-y[keepit]
}
if(alpha<.05 && nboot<=100)warning("You used alpha<.05 and nboot<=100")
if(is.na(fr)){
fr<-.8
if(ncol(x)==2){
nval<-c(20,30,50,80,100,200,300,400)
fval<-c(0.40,0.36,0.3,0.25,0.23,.12,.08,.015)
if(length(y)<=400)fr<-approx(nval,fval,length(y))$y
if(length(y)>400)fr<-.01
}
}
if(SEED)set.seed(2)
x<-as.matrix(x)
mflag<-matrix(NA,nrow=length(y),ncol=length(y))
for (j in 1:length(y)){
for (k in 1:length(y)){
mflag[j,k]<-(sum(x[j,]<=x[k,])==ncol(x))
}
}
yhat<-adrun(x,y,est=est,plotit=FALSE,fr=fr,pyhat=TRUE)
regres<-y-yhat
test2=medind(regres,x[,1]*x[,2],qval=qval,nboot=nboot,com.pval=com.pval,SEED=SEED,alpha=alpha,
pr=TRUE,xout=xout,outfun=outfun,...)
test2
}




# ----------------------------------------------------------------------------

# akerdcdf

# ----------------------------------------------------------------------------

akerdcdf<-function(xx,hval=NA,aval=.5,op=1,fr=.8,pyhat=TRUE,pts=0,plotit=FALSE,
xlab="",ylab=""){
#
# Compute cumulative adaptive kernel density estimate
# for univariate data
# (See Silverman, 1986)
# By default (univiate case) determine P(X<=pts),
# pts=0 by default.
#
# op=1 Use expected frequency as initial estimate of the density
# op=2 Univariate case only
#      Use normal kernel to get initial estimate of the density
#
fval<-"Done"
if(is.matrix(xx)){
if(ncol(xx)>1)fval<-akerdmul(xx,pts=pts,hval=hval,aval=aval,fr=fr,pr=pyhat,plotit=plotit)
plotit<-F
}
if(is.matrix(xx) && ncol(xx)==1)xx<-xx[,1]
if(!is.matrix(xx)){
x<-sort(xx)
if(op==1){
m<-mad(x)
if(m==0){
temp<-idealf(x)
m<-(temp$qu-temp$ql)/(qnorm(.75)-qnorm(.25))
}
if(m==0)m<-sqrt(winvar(x)/.4129)
if(m==0)stop("All measures of dispersion are equal to 0")
fhat <- rdplot(x,pyhat=TRUE,plotit=FALSE,fr=fr)
if(m>0)fhat<-fhat/(2*fr*m)
}
if(op==2){
init<-density(xx)
fhat <- init$y
x<-init$x
}
n<-length(x)
if(is.na(hval)){
sig<-sqrt(var(x))
temp<-idealf(x)
iqr<-(temp$qu-temp$ql)/1.34
A<-min(c(sig,iqr))
if(A==0)A<-sqrt(winvar(x))/.64
hval<-1.06*A/length(x)^(.2)
# See Silverman, 1986, pp. 47-48
}
gm<-exp(mean(log(fhat[fhat>0])))
alam<-(fhat/gm)^(0-aval)
dhat<-NA
if(is.na(pts[1]))pts<-x
pts<-sort(pts)
for(j in 1:length(pts)){
temp<-(pts[j]-x)/(hval*alam)
sq5=0-sqrt(5)
epan=.75*(temp-.2*temp^3/3)/sqrt(5)-.75*(sq5-.2*sq5^3/3)/sqrt(5)
flag=(temp>=sqrt(5))
epan[flag]=1
flag=(temp<sq5)
epan[flag]=0
dhat[j]<-mean(epan)
}
if(plotit){
plot(pts,dhat,type="n",ylab=ylab,xlab=xlab)
lines(pts,dhat)
}
if(pyhat)fval<-dhat
}
fval
}




# ----------------------------------------------------------------------------

# akerdmul

# ----------------------------------------------------------------------------

akerdmul<-function(x,pts=NA,hval=NA,aval=.5,fr=.8,pr=FALSE,plotit=TRUE,theta=50,
phi=25,expand=.5,scale=FALSE,xlab="X",ylab="Y",zlab="",ticktype="simple"){
#
# Compute adaptive kernel density estimate
# for multivariate data
# (See Silverman, 1986)
#
#  Use expected frequency as initial estimate of the density
#
# hval is the span used by the kernel density estimator
# fr is the span used by the expected frequency curve
# pr=T, returns density estimates at pts
# ticktype="detailed" will create ticks as done in two-dimensional plot
#
if(is.na(pts[1]))pts<-x
if(ncol(x)!=ncol(pts))stop("Number of columns for x and pts do not match")
if(!is.matrix(x))stop("Data should be stored in a matrix")
fhat <- rdplot(x,pyhat=TRUE,plotit=FALSE,fr=fr)
n<-nrow(x)
d<-ncol(x)
pi<-gamma(.5)^2
cd<-c(2,pi)
if(d==2)A<-1.77
if(d==3)A<-2.78
if(d>2){
for(j in 3:d)cd[j]<-2*pi*cd[j-2]/n  # p. 76
}
if(d>3)A<-(8*d*(d+2)*(d+4)*(2*sqrt(pi))^d)/((2*d+1)*cd[d])  # p. 87
if(is.na(hval))hval<-A*(1/n)^(1/(d+4))  # Silverman, p. 86
svec<-NA
for(j in 1:d){
sig<-sqrt(var(x[,j]))
temp<-idealf(x[,j])
iqr<-(temp$qu-temp$ql)/1.34
A<-min(c(sig,iqr))
x[,j]<-x[,j]/A
svec[j]<-A
}
hval<-hval*sqrt(mean(svec^2)) # Silverman, p. 87
# Now do adaptive; see Silverman, 1986, p. 101
gm<-exp(mean(log(fhat[fhat>0])))
alam<-(fhat/gm)^(0-aval)
dhat<-NA
nn<-nrow(pts)
for(j in 1:nn){
temp1<-t(t(x)-pts[j,])/(hval*alam)
temp1<-temp1^2
temp1<-apply(temp1,1,FUN="sum")
temp<-.5*(d+2)*(1-temp1)/cd[d]
epan<-ifelse(temp1<1,temp,0) # Epanechnikov kernel, p. 76
dhat[j]<-mean(epan/(alam*hval)^d)
}
if(plotit && d==2){
fitr<-dhat
iout<-c(1:length(fitr))
nm1<-length(fitr)-1
for(i in 1:nm1){
ip1<-i+1
for(k in ip1:length(fitr))if(sum(x[i,]==x[k,])==2)iout[k]<-0
}
fitr<-fitr[iout>=1]
mkeep<-x[iout>=1,]
fit<-interp(mkeep[,1],mkeep[,2],fitr)
persp(fit,theta=theta,phi=phi,xlab=xlab,ylab=ylab,zlab=zlab,expand=expand,
scale=scale,ticktype=ticktype)
}
m<-"Done"
if(pr)m<-dhat
m
}
AKPmcp.ci<-function(x,tr=.2,alpha=0.05,SEED=TRUE,nboot=500,CI=TRUE,method='hoch'){
#
#  Estimate AKP effect size  when comparing all
#  pairs of groups in a one-way (independent) groups design
#
#  CI=TRUE: confidence intervals for the measure of effect size are computed.
#
if(is.matrix(x) || is.data.frame(x))x<-listm(x)
if(!is.list(x))stop('Data must be stored in a matrix or in list mode.')
J=length(x)
Jall=(J^2-J)/2
con1=con1way(J)
output=matrix(NA,nrow=Jall,ncol=7)
dimnames(output)=list(NULL,c('Group','Group','Effect.Size','low.ci','up.ci','p.value','p.adjust'))
ic=0
for(j in 1:J){
for(k in 1:J){
if(j<k){
ic=ic+1
output[ic,1]=j
output[ic,2]=k
if(!CI)output[ic,3]=akp.effect(x[[j]],x[[k]],tr=tr)
if(CI){
ci=akp.effect.ci(x[[j]],x[[k]],tr=tr,nboot=nboot,alpha=alpha,SEED=SEED)
output[ic,3]=ci$akp.effect
output[ic,4]=ci$ci[1]
output[ic,5]=ci$ci[2]
output[ic,6]=ci$p.value
}}}}
output[,7]=p.adjust(output[,6],method=method)
output
}

apdis<-function(m,est=sum,...){
#
# For bivariate data,
# compute distance between each pair
# of points and measure depth of a point
# in terms of its  distance to all
# other points
#
#  m is an n by 2 matrix
#  (In this version, ncol(m)=2 only, for general
#  case, use apgdis
#
m<-elimna(m)  # eliminate any missing values
disx<-outer(m[,1],m[,1],"-")
disy<-outer(m[,2],m[,2],"-")
temp<-sqrt(disx^2+disy^2)
dis<-apply(temp,1,est,...)
dis
temp2<-order(dis)
center<-m[temp2[1],]
list(center=center,distance=dis)
}




# ----------------------------------------------------------------------------

# apgdis

# ----------------------------------------------------------------------------

apgdis<-function(m,est=sum,se=TRUE,...){
#
# For multivariate data,
# compute distance between each pair
# of points and measure depth of a point
# in terms of its  distance to all
# other points
#
#  Using se=T ensures that ordering of distance
# will not change with a change in scale.
#
#  m is an n by p matrix
#
m<-elimna(m)  # eliminate any missing values
temp<-0
if(se){
for(j in 1:ncol(m))m[,j]<-(m[,j]-median(m[,j]))/mad(m[,j])
}
for(j in 1:ncol(m)){
disx<-outer(m[,j],m[,j],"-")
temp<-temp+disx^2
}
temp<-sqrt(temp)
dis<-apply(temp,1,est,...)
temp2<-order(dis)
center<-m[temp2[1],]
list(center=center,distance=dis)
}


attract<-function(X, Y, k = 5)
{
# Works in Splus but not in R.
# For simple linear regression: plots k elemental starts and
# their domains of attraction.  Calls conc2.
	l1coef <- l1fit(X, Y)$coef
	X <- as.matrix(X)
	nr <- dim(X)[1]
	nc <- dim(X)[2] + 1
	J <- 1:nc
	dom <- matrix(nrow = k, ncol = nc)
	par(mfrow = c(1, 2))
	plot(X, Y)
	title("a) 5 Elemental Starts")
	for(i in 1:k) {
## get J
		J <- sample(nr, nc)	## get bJ, the elem fit
		if(abs(X[J[1]] - X[J[2]]) < 1/100000000) {
			slope <- 0
		}
		else {
			slope <- (Y[J[1]] - Y[J[2]])/(X[J[1]] - X[J[2]])
		}
		int <- Y[J[1]] - slope * X[J[1]]
		fit <- c(int, slope)
		yhat <- X %*% fit[2:nc] + fit[1]
		lines(X, yhat)
	## get the domain of attraction for LTA concentration
		dom[i,  ] <- conc2(X, Y, start = fit)$coef
	}
	plot(X, Y)
	for(i in 1:k) {
		fit <- dom[i,  ]
		yhat <- X %*% fit[2:nc] + fit[1]
		lines(X, yhat)
	}
	title("b) The Corresponding Attractors")
}




# ----------------------------------------------------------------------------

# B.outbox

# ----------------------------------------------------------------------------

B.outbox<-function(x,mbox=FALSE,gval=NA,plotit=FALSE,STAND=FALSE){
#
#  Uses the method derived by
#  Walker, M. L.,   Dovoedo, Y. H.,   Chakraborti, S. \& Hilton, C. W. (2018).
#  An  Improved  Boxplot  for  Univariate  Data. {\em American Statistician, 72}, 348--353.
#
#
x<-x[!is.na(x)] # Remove missing values
if(plotit)boxplot(x)
n<-length(x)
temp<-idealf(x)
M=median(x)
Bc=(temp$qu+temp$ql-2*M)/(temp$qu-temp$ql)
if(is.na(gval))gval<-1.5
cl<-temp$ql-gval*(temp$qu-temp$ql)*((1-Bc)/(1+Bc))
cu<-temp$qu+gval*(temp$qu-temp$ql)*((1+Bc)/(1-Bc))
flag<-NA
outid<-NA
vec<-c(1:n)
for(i in 1:n){
flag[i]<-(x[i]< cl || x[i]> cu)
}
if(sum(flag)==0)outid<-NULL
if(sum(flag)>0)outid<-vec[flag]
keep<-vec[!flag]
outval<-x[flag]
n.out=sum(length(outid))
list(out.val=outval,out.id=outid,keep=keep,n=n,n.out=n.out,cl=cl,cu=cu)
}

b1way<-function(x,est=onestep,nboot=599,SEED=TRUE,...){
#
#   Test the hypothesis that J measures of location are equal
#   using the percentile bootstrap method.
#   By default, M-estimators are compared using 599 bootstrap samples.
#
#   The data are assumed to be stored in x in list mode.  Thus,
#   x[[1]] contains the data for the first group, x[[2]] the data
#   for the second group, etc. Length(x)=the number of groups = J, say.
#
#
if(is.data.frame(x))x=as.matrix(x)
if(is.matrix(x))x<-listm(x)
if(!is.list(x))stop("Data must be stored in list mode or a matrix.")
J<-length(x)
for(j in 1:J)x[[j]]=elimna(x[[j]])
nval<-vector("numeric",length(x))
gest<-vector("numeric",length(x))
if(SEED)set.seed(2) # set seed of random number generator so that
#             results can be duplicated.
bvec<-matrix(0,J,nboot)
#print("Taking bootstrap samples. Please wait.")
for(j in 1:J){
#print(paste("Working on group ",j))
nval[j]<-length(x[[j]])
gest[j]<-est(x[[j]])
xcen<-x[[j]]-est(x[[j]],...)
data<-matrix(sample(xcen,size=length(x[[j]])*nboot,replace=TRUE),nrow=nboot)
bvec[j,]<-apply(data,1,est,...) # A J by nboot matrix
#                     containing the bootstrap values of est.
}
teststat<-wsumsq(gest,nval)
testb<-apply(bvec,2,wsumsq,nval)
p.value<-1 - sum(teststat >= testb)/nboot
teststat<-wsumsq(gest,nval)
if(teststat == 0)p.value <- 1
list(teststat=teststat,p.value=p.value)
}


b2ci<-function(x,y,alpha=.05,nboot=2000,est=bivar,SEED=TRUE,...){
#
#   Compute a bootstrap confidence interval for the
#   the difference between any two parameters corresponding to
#   independent groups.
#   By default, biweight midvariances are compared.
#   Setting est=mean, for example, will result in a percentile
#   bootstrap confidence interval for the difference between means.
#   The default number of bootstrap samples is nboot=399
#
x<-x[!is.na(x)] # Remove any missing values in x
y<-y[!is.na(y)] # Remove any missing values in y
e1=est(x)
e2=est(y)
if(SEED)set.seed(2) # set seed of random number generator so that
#             results can be duplicated.
#print("Taking bootstrap samples. Please wait.")
datax<-matrix(sample(x,size=length(x)*nboot,replace=TRUE),nrow=nboot)
datay<-matrix(sample(y,size=length(y)*nboot,replace=TRUE),nrow=nboot)
bvecx<-apply(datax,1,est,...)
bvecy<-apply(datay,1,est,...)
bvec<-sort(bvecx-bvecy)
low <- round((alpha/2) * nboot) + 1
up <- nboot - low
temp <- sum(bvec < 0)/nboot + sum(bvec == 0)/(2 * nboot)
sig.level <- 2 * (min(temp, 1 - temp))
list(est1=e1,est2=e2,ratio=e1/e2,ci = c(bvec[low], bvec[up]), p.value = sig.level)
}




# ----------------------------------------------------------------------------

# Bagdist

# ----------------------------------------------------------------------------

Bagdist<-function(x,pts=NULL){
#
#  Compute  bagdistance.
#
# requires R package mrfDepth
library(mrfDepth)
d=bagdistance(x,pts)$bagdistance
d
}

Bband<-function(x,alpha=.05,plotit=TRUE,sm=TRUE,SEED=TRUE,nboot=500,grp=c(1:4),
xlab="X (First Level)",ylab="Delta",crit=NA,print.all=FALSE,plot.op=FALSE){
#
# Apply the shift function when analyzing main effect in a
# 2 by 2 design.
#
# For variables x1, x2, x3 and x4,
# In effect, this function applies a shift function to the distributions
# d1=(x1+x3)/2 and d2=(x2+x4)/2.
# That is, focus on main effects of Factor B.
#
# grp indicates the groups to be compared. By default grp=c(1,2,3,4)
# meaning that the first level of factor A consists of groups 1 and 2
# and the 2nd level of factor A consists of groups 3 and 4.
# (So level 1 of factor B consists of groups 1 and 3
#
# print.all=F,
# returns number sig, meaning number of confidence intervals that do not
# contain zero,
# the critical value used as well as the KS test statistics.
# print.all=T reports all confidence intervals, the number of which can
# be large.
#
if(!is.list(x) && !is.matrix(x))stop("store data in list mode or a matrix")
if(SEED)set.seed(2)
if(is.matrix(x))x<-listm(x)
for(j in 1:length(x))x[[j]]=elimna(x[[j]])/2
if(length(x)<4)stop("There must be at least 4 groups")
if(length(grp)!=4)stop("The argument grp must have 4 values")
x<-x[grp]
grp=c(1,3,2,4)
x<-x[grp]  # Arrange groups for main effects on factor B
n<-c(length(x[[1]]),length(x[[2]]),length(x[[3]]),length(x[[4]]))
# Approximate the critical value
#
vals<-NA
y<-list()
if(is.na(crit)){
print("Approximating critical value. Please wait.")
for(i in 1:nboot){
for(j in 1:4)
y[[j]]<-rnorm(n[j])
temp<-ks.test(outer(y[[1]],y[[2]],FUN="+"),outer(y[[3]],y[[4]],FUN="+"))
vals[i]<-temp[1]$statistic
}
vals<-sort(vals)
ic<-(1-alpha)*nboot
crit<-vals[ic]
}
if(plot.op){
plotit<-F
g2plot(v1,v2)
}
output<-sband(outer(x[[1]],x[[2]],FUN="+"),outer(x[[3]],x[[4]],FUN="+"),
plotit=plotit,crit=crit,flag=FALSE,sm=sm,xlab=xlab,ylab=ylab)
if(!print.all){
numsig<-output$numsig
ks.test.stat<-ks.test(outer(x[[1]],x[[2]],FUN="+"),
outer(x[[3]],x[[4]],FUN="+"))$statistic
output<-matrix(c(numsig,crit,ks.test.stat),ncol=1)
dimnames(output)<-list(c("number sig","critical value","KS test statistics"),
NULL)
}
output
}




# ----------------------------------------------------------------------------

# bbblinQS

# ----------------------------------------------------------------------------

bbblinQS<-function(J,K,L,x,locfun,nreps=100,SEED=TRUE,POOL=TRUE,pr=TRUE){
#
#  Compute quantile shift measure of effect size for all main effects and interactions.
#
#
        #   The data are assumed to be stored in x in list mode or in a matrix.
        #  If grp is unspecified, it is assumed x[[1]] contains the data
        #  for the first level of both factors: level 1,1.
        #  x[[2]] is assumed to contain the data for level 1 of the
        #  first factor and level 2 of the second factor: level 1,2
        #  x[[j+1]] is the data for level 2,1, etc.
        #  If the data are in wrong order, grp can be used to rearrange the
        #  groups. For example, for a two by two design, grp<-c(2,4,3,1)
        #  indicates that the second group corresponds to level 1,1;
        #  group 4 corresponds to level 1,2; group 3 is level 2,1;
        #  and group 1 is level 2,2.
        #
        #   Missing values are automatically removed.
        #
        JKL <- J * K*L
        if(is.matrix(x) || is.data.frame(x))
                x <- listm(x)
        if(!is.list(x))
                stop('Data must be stored in list mode or a matrix.')
        if(JKL != length(x))
                print('Warning: JKL does not match the number of groups.')
x=elimna(x)  # Remove missing values.
temp<-con3way(J,K,L)
conA<-temp$conA
conB<-temp$conB
conC<-temp$conC
conAB<-temp$conAB
conAC<-temp$conAC
conBC<-temp$conBC
conABC<-temp$conABC
Factor.A=list()
for(j in 1:ncol(conA)){
Factor.A[[j]]=linQS(x,con=conA[,j])
}
Factor.B=list()
for(k in 1:ncol(conB)){
Factor.B[[k]]=linQS(x,con=conB[,k])
}
Factor.C=list()
for(k in 1:ncol(conC)){
Factor.C[[k]]=linQS(x,con=conB[,k])
}
# Do interactions
Factor.AB=list()
for(l in 1:ncol(conAB)){
Factor.AB[[l]]=linQS(x,con=conAB[,l])
}
Factor.AC=list()
for(l in 1:ncol(conAC)){
Factor.AC[[l]]=linQS(x,con=conAC[,l])
}
Factor.BC=list()
for(l in 1:ncol(conBC)){
Factor.BC[[l]]=linQS(x,con=conBC[,l])
}
Factor.ABC=list()
for(l in 1:ncol(conABC)){
Factor.ABC[[l]]=linQS(x,con=conABC[,l])
}

if(pr){
print('Note: Under normality and homoscedasticity, Cohen d= 0, .2, .5, .8')
print('correspond approximately to Q.effect = 0.5, 0.55, 0.65 and 0.70, respectively')
}
list(Factor.A=Factor.A,Factor.B=Factor.C,Factor.C=Factor.C,Factor.AB=Factor.AB,
Factor.AC=Factor.AC,Factor.BC=Factor.BC,Factor.AB=Factor.ABC,contrast.coef=temp)
}


bbbtrim<-function(J,K,L,data,tr=.2,grp=c(1:p),alpha=.05,p=J*K*L,nboot=600,pr=FALSE){
#
#  Perform three-way anova, independent groups, based on trimmed means
#
#  That is, there are three factors with a total of JKL independent groups.
#
#  A bootstrap-t method is used to perform multiple comparisons
#  The variable data is assumed to contain the raw
#  data stored in list mode. data[[1]] contains the data
#  for the first level of all three factors: level 1,1,1.
#  data[[2]] is assumed to contain the data for level 1 of the
#  first two factors and level 2 of the third factor: level 1,1,2
#  data[[L]] is the data for level 1,1,L
#  data[[L+1]] is the data for level 1,2,1. data[[2L]] is level 1,2,L.
#  data[[KL+1]] is level 2,1,1, etc.
#
#  The default amount of trimming is tr=.2
#
#  It is assumed that data has length JKL, the total number of
#  groups being tested.
#
if(is.list(data))data=listm(elimna(matl(data)))
if(is.matrix(data))data=listm(elimna(data))
if(!is.list(data))stop("Data are not stored in list mode or a matrix")
if(p!=length(data)){
print("The total number of groups, based on the specified levels, is")
print(p)
print("The number of groups in data is")
print(length(data))
print("Warning: These two values are not equal")
}
x=data
temp=con3way(J,K,L)
conA<-temp$conA
conB<-temp$conB
conC<-temp$conC
conAB<-temp$conAB
conAC<-temp$conAC
conBC<-temp$conBC
conABC=temp$conABC
Factor.A<-linconb(x,con=conA,tr=tr,alpha=alpha,nboot=nboot,pr=pr)
Factor.B<-linconb(x,con=conB,tr=tr,alpha=alpha,nboot=nboot,pr=pr)
Factor.C<-linconb(x,con=conC,tr=tr,alpha=alpha,nboot=nboot,pr=pr)
Factor.AB<-linconb(x,con=conAB,tr=tr,alpha=alpha,nboot=nboot,pr=pr)
Factor.AC<-linconb(x,con=conAC,tr=tr,alpha=alpha,nboot=nboot,pr=pr)
Factor.BC<-linconb(x,con=conBC,tr=tr,alpha=alpha,nboot=nboot,pr=pr)
Factor.ABC<-linconb(x,con=conABC,tr=tr,alpha=alpha,nboot=nboot,pr=pr)
list(Factor.A=Factor.A,Factor.B=Factor.B,Factor.C=Factor.C,
Factor.AB=Factor.AB,Factor.AC=Factor.AC,Factor.BC=Factor.BC,
Factor.ABC=Factor.ABC,pr=pr)
}


bbiQS<-function(J,K,x,locfun=median,...){
#
#  Quantile shift  measure of effect size for interactions in a
#  between-by-between design
#
#  The R variable x is assumed to contain the raw
#  data stored in list mode or in a matrix.
#  If in list mode, x[[1]] contains the data
#  for the first level of both factors: level 1,1.
#  x[[2]] is assumed to contain the data for level 1 of the
#  first factor and level 2 of the second: level 1,2
#  x[[K]] is the data for level 1,K
#  x[[K+1]] is the data for level 2,1, x[[2K]] is level 2,K, etc.
#
#  If the data are in a matrix, column 1 is assumed to
#  correspond to x[[1]], column 2 to x[[2]], etc.
#
JK<-J*K
MJ<-(J^2-J)/2
MK<-(K^2-K)/2
MJMK<-MJ*MK
if(is.matrix(x) || is.data.frame(x))x=listm(x)
if(JK!=length(x))stop('Something is wrong. Expected ',JK,' groups but x contains ', length(x), ' groups instead.')
m=matrix(c(1:JK),nrow=J,byrow=TRUE)
output=matrix(NA,ncol=5,nrow=MJMK)
dimnames(output)<-list(NULL,c('A','A','B','B','Effect.Size'))
ic=0
for(j in 1:J){
for(jj in 1:J){
if(j<jj){
for(k in 1:K){
for(kk in 1:K){
if(k<kk){
ic<-ic+1
output[ic,1]<-j
output[ic,2]<-jj
output[ic,3]<-k
output[ic,4]<-kk
ID=c(m[j,k],m[j,kk],m[jj,k],m[jj,kk])
print(ID)
output[ic,5]<-interQS(x[ID],locfun=locfun,...)$Q.Effect
}}}}}}
output
}




# ----------------------------------------------------------------------------

# bblinQS

# ----------------------------------------------------------------------------

bblinQS<-function(J,K,x,locfun,nreps=100,SEED=TRUE,POOL=TRUE,pr=TRUE){
#
#  Compute quantile shift measure of effect size for all main effects and interactions.
#
#   To get an explanatory measure of effect size, use bbmcpEP
#
        #   The data are assumed to be stored in x in list mode or in a matrix.
        #  If grp is unspecified, it is assumed x[[1]] contains the data
        #  for the first level of both factors: level 1,1.
        #  x[[2]] is assumed to contain the data for level 1 of the
        #  first factor and level 2 of the second factor: level 1,2
        #  x[[j+1]] is the data for level 2,1, etc.
        #  If the data are in wrong order, grp can be used to rearrange the
        #  groups. For example, for a two by two design, grp<-c(2,4,3,1)
        #  indicates that the second group corresponds to level 1,1;
        #  group 4 corresponds to level 1,2; group 3 is level 2,1;
        #  and group 1 is level 2,2.
        #
        #   Missing values are automatically removed.
        #
        JK <- J * K
        if(is.matrix(x) || is.data.frame(x))
                x <- listm(x)
        if(!is.list(x))
                stop('Data must be stored in list mode or a matrix.')
        if(JK != length(x))
                print('Warning: JK does not match the number of groups.')
x=elimna(x)  # Remove missing values.
temp<-con2way(J,K)
conA<-temp$conA
conB<-temp$conB
conAB<-temp$conAB
Factor.A=list()
for(j in 1:ncol(conA)){
Factor.A[[j]]=linQS(x,con=conA[,j])
}
Factor.B=list()
for(k in 1:ncol(conB)){
Factor.B[[k]]=linQS(x,con=conB[,k])
}
# Do interactions
Factor.AB=list()
for(l in 1:ncol(conAB)){
Factor.AB[[l]]=linQS(x,con=conAB[,l])
}
if(pr){
print('The columns of conAB contain the contrast coefficients for the interactions.')
print('For example, the output in FactorAB[[1]] are the results based')
print('on the contrast coefficients in column 1')
print('which is the interaction for the first two rows and the first two columns')
print('  ')
print('Note: Under normality and homoscedasticity, Cohen d= 0, .2, .5, .8')
print('correspond approximately to Q.effect = 0.5, 0.55, 0.65 and 0.70, respectively')
}
list(Factor.A=Factor.A,Factor.B=Factor.B,Factor.AB=Factor.AB,conAB=conAB)
}




# ----------------------------------------------------------------------------

# bbmcpQS

# ----------------------------------------------------------------------------

bbmcpQS<-function(J,K,x,tr=.2,alpha=.05,grp=NA,op=FALSE,pr=TRUE,locfun=tmean,...){
#
#  Test all linear contrasts associated with
# main effects for Factor A and B and all interactions based on trimmed means
# By default,
# tr=.2, meaning 20% trimming is used.
#
#   bbmcpEP has an option for pooling over the levels of the factors.
#
        #   The data are assumed to be stored in x in list mode or in a matrix.
        #  If grp is unspecified, it is assumed x[[1]] contains the data
        #  for the first level of both factors: level 1,1.
        #  x[[2]] is assumed to contain the data for level 1 of the
        #  first factor and level 2 of the second factor: level 1,2
        #  x[[j+1]] is the data for level 2,1, etc.
        #  If the data are in wrong order, grp can be used to rearrange the
        #  groups. For example, for a two by two design, grp<-c(2,4,3,1)
        #  indicates that the second group corresponds to level 1,1;
        #  group 4 corresponds to level 1,2; group 3 is level 2,1;
        #  and group 1 is level 2,2.
        #
        #   Missing values are automatically removed.
        #
        JK <- J * K
        if(is.matrix(x))
                x <- listm(x)
        if(!is.na(grp[1])) {
                yy <- x
                x<-list()
                for(j in 1:length(grp))
                        x[[j]] <- yy[[grp[j]]]
        }
        if(!is.list(x))
                stop("Data must be stored in list mode or a matrix.")
        for(j in 1:JK) {
                xx <- x[[j]]
                x[[j]] <- xx[!is.na(xx)] # Remove missing values
        }
        #

        if(JK != length(x))
                warning("The number of groups does not match the number of contrast coefficients.")
for(j in 1:JK){
temp<-x[[j]]
temp<-temp[!is.na(temp)] # Remove missing values.
x[[j]]<-temp
}
        # Create the three contrast matrices
temp<-con2way(J,K)
conA<-temp$conA
conB<-temp$conB
conAB<-temp$conAB
if(!op){
Factor.A<-linconQS(x,con=conA,tr=tr,alpha=alpha,pr=pr,locfun=locfun,...)
Factor.B<-linconQS(x,con=conB,tr=tr,alpha=alpha,pr=FALSE,locfun=locfun,..)
Factor.AB<-linconQS(x,con=conAB,tr=tr,alpha=alpha,pr=FALSE,locfun=locfun,..)
}
All.Tests<-NA
if(op){
Factor.A<-NA
Factor.B<-NA
Factor.AB<-NA
con<-cbind(conA,conB,conAB)
All.Tests<-linconQS(x,con=con,tr=tr,alpha=alpha,locfun=locfun,..)
}
list(Factor.A=Factor.A,Factor.B=Factor.B,Factor.AB=Factor.AB,All.Tests=All.Tests,conA=conA,conB=conB,conAB=conAB)
}




# ----------------------------------------------------------------------------

# bbQS

# ----------------------------------------------------------------------------

bbQS<-function(J,K,x,locfun=median,pr=TRUE){
#
#  Compute quantile shift measure of effect size for all pairwise comparisons
#  in a between-by-between design plus interactions.
#
if(pr){
print('output: A[[1]] contains results for level 1 of Factor A;')
print(' all pairwise comparisons over Factor B')
print('A[[2]] contains results for level 2, etc.')
print(' ')
print('Note: Under normality and homoscedasticity, Cohen d= 0, .2, .5, .8')
print('correspond approximately to Q.effect = 0.5, 0.55, 0.65 and 0.70, respectively')
}
A=list()
B=list()
AB=list()
if(is.matrix(x) || is.data.frame(x))x<-listm(x)
JK=J*K
ID=matrix(c(1:JK),nrow=J,ncol=K,byrow=TRUE)
A=list()
for (j in 1:J)A[[j]]=bmcpQS(x[ID[j,]],locfun=locfun)
B=list()
for(k in 1:K)B[[k]]=bmcpQS(x[ID[,k]],locfun=locfun)
AB=bbiQS(J,K,x)
list(Factor.A=A,Factor.B=B,interactions=AB)
}




# ----------------------------------------------------------------------------

# bbw2list

# ----------------------------------------------------------------------------

bbw2list<-function(x,grp.col,lev.col,pr=TRUE){
#
#  for a between-by-between-by-within design
#  grp.col indicates the columns where values of the  levels of between factor
#  are stored.
#  lev.col indicates the columns where repeated measures are contained.
#  If, for example, there are data for three times, stored in columns
#  6, 8 and 11, set
#  lev.col=c(6,8,11)
#
#  Example:  Have a 3 x 4 x 2 design
#  values in columns 2 and 4 indicate the
#  levels of the two between factors.
#  column 3 contains time 1 data,
#  column 7 contains time 2 data
#  bbw2list(x,(c(2,4),c(3,7)) will store data in list mode that can be
#  used by bbwtrim and related functions
#
res=selbybbw(x,grp.col,lev.col,pr=pr)
res
}




# ----------------------------------------------------------------------------

# bbwmatna

# ----------------------------------------------------------------------------

bbwmatna<-function(J,K,L,x){
#
# data are assumed to be stored in a matrix
# for a between by within by within (three-way) anova.
# For the last factor, eliminate any missing values
# and then store the data in list mode.
#
y=list()
ad=L
ilow=1
iup=ad
ic=0
for(j in 1:J){
for(k in 1:K){
z=x[,ilow:iup]
d=elimna(z)
im=0
for(l in 1:L){
ic=ic+1
im=im+1
y[[ic]]=d[,im]
}
ilow=ilow+ad
iup=iup+ad
}}
y
}




# ----------------------------------------------------------------------------

# bbwna

# ----------------------------------------------------------------------------

bbwna<-function(J,K,L,x){
#
# x: data are assumed to be stored in list mode
# for a between by within by within (three-way) anova.
# For the last factor, eliminate any missing values.
#
y=list()
ad=L
ilow=1
iup=ad
ic=0
for(j in 1:J){
for(k in 1:K){
z=x[ilow:iup]
d=as.matrix(elimna(matl(z)))
im=0
ilow=ilow+ad
iup=iup+ad
for(l in 1:L){
ic=ic+1
im=im+1
y[[ic]]=d[,im]
}}
}
y
}
bwwtrim.sub<-function(cmat,vmean,vsqse,h,p){
#
#  The function computes  variation of Johansen's test statistic
#  used to test the hypothesis  C mu = 0 where
#  C is a k by p matrix of rank k and mu is a p by 1 matrix of
#  of unknown  trimmed means.
#  The argument cmat contains the matrix C.
#  vmean is a vector of length p containing the p trimmed means
#  vsqe is matrix containing the
#  estimated covariances among the trimmed means
#  h is  the sample size
#
yvec<-matrix(vmean,length(vmean),1)
test<-cmat%*%vsqse%*%t(cmat)
invc<-solve(test)
test<-t(yvec)%*%t(cmat)%*%invc%*%cmat%*%yvec
temp<-0
mtem<-vsqse%*%t(cmat)%*%invc%*%cmat
temp<-(sum(diag(mtem%*%mtem))+(sum(diag(mtem)))^2)/(h-1)
A<-.5*sum(temp)
cval<-nrow(cmat)+2*A-6*A/(nrow(cmat)+2)
test<-test/cval
test
}

bd1way<-function(x,est=tmean,nboot=599,alpha=.05,SEED=TRUE,misran=FALSE,na.rm=NULL,pr=TRUE,...){
#
#   Test the hypothesis of equal measures of location for J
#   dependent groups using a
#   percentile bootstrap method.
#   By default, a one-step M-estimator is used.
#   For example, bd1way(x,mean) would compare means
#
#   Data are assumed to be stored  in list mode or an n by J matrix.
#   misran=F means missing values do not occur at random, case wise deletion is used.
#   misran=T, all values will be used assuming missing values occur at random
#   OR set na.rm=F to use all of the data. na.rm=F means misran=T will be used.
#   In effect, specifying na.rm=T, for example, the argument misran is ignored.
#
if(!is.list(x) && !is.matrix(x))stop("Data must be stored in list mode or in an n by J matrix.")
if(pr)print('As of Oct, 2015, the default measure of location is a trimmed mean, not the one-step M-estimator')
if(is.list(x)){
m<-matrix(0,length(x[[1]]),length(x))
for (j in 1:length(x))m[,j]<-x[[j]]
}
if(is.matrix(x))m<-x
if(!is.null(na.rm))misran=!na.rm
if(!misran)m=elimna(m)
xcen<-m
locval=apply(m,2,est,na.rm=TRUE,...)
for (j in 1:ncol(m))xcen[,j]<-m[,j]-est(m[,j],na.rm=misran,...)
if(SEED)set.seed(2) # set seed of random number generator so that
#             results can be duplicated.
print("Taking bootstrap samples. Please wait.")
data<-matrix(sample(nrow(m),size=nrow(m)*nboot,replace=TRUE),nrow=nboot)
bvec<-vector("numeric")
bvec<-apply(data,1,bd1way1,xcen,est,misran=misran,...)
# A vector of  nboot test statistics.
icrit<-floor((1-alpha)*nboot+.5)
testv<-vector("numeric")
for (j in 1:ncol(m))testv[j]<-est(m[,j],na.rm=misran,...)
test<-(length(testv)-1)*var(testv)
pv=mean((test<bvec))
list(test=test,estimates=locval,p.value=pv)
}



bd1way1<-function(isub,xcen,est,misran,...){
#
#  Compute test statistic for bd1way
#
#  isub is a vector of length n,
#  a bootstrap sample from the sequence of integers
#  1, 2, 3, ..., n
#
#  xcen is an n by J matrix containing the input data
#
val<-vector("numeric")
for (j in 1:ncol(xcen))val[j]<-est(xcen[isub,j],na.rm=misran,...)
bd1way1<-(length(val)-1)*var(val)
bd1way1
}


bdiag<-function(nb,np,rho=0){
#
# Let p=nb*np
# Create a p by p block diagonal matrix with each
# np by np block having a correlation matrix with common
# correlation rho
# So nb is the number of blocks
#
p<-nb*np
m<-matrix(0,p,p)
mat<-matrix(rho,np,np)
diag(mat)<-1
ilow<-1-np
iup<-0
for(i in 1:nb){
ilow<-ilow+np
iup<-iup+np
m[ilow:iup,ilow:iup]<-mat
}
m
}

bdms1<-function(x,con){
# This function is used by bdm
#
# Pool all data and rank
pool<-x[[1]]
JK<-length(x)
for (j in 2:JK)pool<-c(pool,x[[j]])
N<-length(pool)
rval<-rank(pool)
rvec<-list()
up<-length(x[[1]])
rvec[[1]]<-rval[1:up]
rbar<-mean(rvec[[1]])
nvec<-length(rvec[[1]])
for(j in 2:JK){
down<-up+1
up<-down+length(x[[j]])-1
rvec[[j]]<-rval[down:up]
nvec[j]<-length(rvec[[j]])
rbar[j]<-mean(rvec[[j]])
}
phat<-(rbar-.5)/N
phat<-as.matrix(phat)
svec<-NA
for(j in 1:JK)svec[j]<-sum((rvec[[j]]-rbar[j])^2)/(nvec[j]-1)
svec<-svec/N^2
VN<-N*diag(svec/nvec)
top<-con[1,1]*sum(diag(VN))
Ftest<-N*(t(phat)%*%con%*%phat)/top
nu1<-con[1,1]^2*sum(diag(VN))^2/sum(diag(con%*%VN%*%con%*%VN))
lam<-diag(1/(nvec-1))
nu2<-sum(diag(VN))^2/sum(diag(VN%*%VN%*%lam))
sig<-1-pf(Ftest,nu1,nu2)
list(F=Ftest,nu1=nu1,nu2=nu2,q.hat=phat,p.value=sig)
}




# ----------------------------------------------------------------------------

# best.PB

# ----------------------------------------------------------------------------

best.PB<-function(x,est=tmean,nboot=1000,SEED=TRUE,...){
#
#  Take bootstrap samples
#  determine how often group is largest estimate
#  remains highest
#  Return a pseudo p-value
#  If sufficiently small, make a decision.
#
if(SEED)set.seed(2)
chk=0
if(is.matrix(x)||is.data.frame(x))x<-listm(x)
x=elimna(x)
e=pool.a.list(lapply(x,est,...))
id=which(e==max(e))
z=list()
J=length(x)
for(i in 1:nboot){
for(j in 1:J)z[[j]]=sample(x[[j]],replace=TRUE)
b=pool.a.list(lapply(z,est,...))
ichk=which(b==max(b))
if(id==ichk)chk=chk+1
}
n=pool.a.list(lapply(x,length))
pv=chk/nboot
pv=2*min(pv,1-pv)
list(n=n,Est=e,p.value=pv)
}




# ----------------------------------------------------------------------------

# bg2ci

# ----------------------------------------------------------------------------

bg2ci<-function(x, alpha = 0.05)
{
#gets BGse with middle n^0.8 cases for sample median and
#the corresponding robust  100 (1-alpha)% CI. This is optimal
#for estimating the SE but is not resistant.
	n <- length(x)
	up <- 1 - alpha/2
	med <- median(x)
	ln <- max(1,floor(n/2) - ceiling(0.5 * n^0.8))
	un <- n - ln
	rdf <- un - ln - 1
	cut <- qt(up, rdf)
	d <- sort(x)
	se2 <- (d[un] - d[ln])/(2 * n^0.3)
	rval <- cut * se2
	rlo2 <- med - rval
	rhi2 <- med + rval
	#got low and high endpoints of robust CI
	list(int = c(rlo2, rhi2), med = med, se2 = se2)
}




# ----------------------------------------------------------------------------

# bi2ac

# ----------------------------------------------------------------------------

bi2ac<-function(r1=sum(elimna(x)),n1=length(elimna(x)),r2=sum(elimna(y)),
n2=length(elimna(y)),
x=NA,y=NA,alpha=.05,null.value=0){
v1=acbinomci(x=r1,nn=n1)
v2=acbinomci(x=r2,nn=n2)
dif=v1$phat-v2$phat
sq.se=v1$se^2+v2$se^2
crit=qnorm(1-alpha/2)
test=(dif-null.value)/sqrt(sq.se)
ci=dif-crit*sqrt(sq.se)
ci[2]=dif+crit*sqrt(sq.se)
pv=2*(1-pnorm(abs(test)))
list(n1=n1,n2=n2,phat1=v1$phat,phat2=v2$phat,dif=dif,ci=ci,test.stat=test,p.value=pv)
}


lincon.bin.sub<-function(r,n,con=NULL,alpha=.05,null.value=0,x=NULL,binCI=acbinomci){
#
#  r: number of successes for J independent groups
#  n: corresponding sample sizes
#
#  Compute confidence interval for a linear combination of independent binomials
#  using:
# A note on confidence interval estimation for a linear function
# of binomial proportion.
#  Zou, G. Y., Huang, W. & Zheng, X (2009) CSDA, 53, 1080-1085
#
#  con: contrast coeffiients
#  if NULL, all pairwise comparisons are performed.
#
#  x: if not NULL, taken to be a matrix containing 0s and 1s, columns correspond to groups
#  r and n are computed using the data in  x
#
#  binCI defaults to Agresti--Coull
#  Other choices for binCI:
#  binomci:  Pratt's method
#  binomCP:  Clopper--Pearson
# kmsbinomci:  Kulinskaya et al
#  wilbinomci:  Wilson
#  binomLCO:  Schilling--Doi
#
if(!is.null(x)){
r=apply(x,2,sum)
n=rep(nrow(x),ncol(x))
}
J=length(r)
est=matrix(NA,nrow=J,ncol=3)
for(j in 1:J){
v=binCI(r[j],n[j],alpha=alpha)
est[j,]=c(v$phat,v$ci)
}
if(!is.null(con))con=as.matrix(con)
if(is.null(con))con=con.all.pairs(J)
NT=ncol(con)
L=NA
U=NA
EST=NA
for(k in 1:NT){
mat=cbind(con[,k]*est[,2],con[,k]*est[,3])
LM=apply(mat,1,min)
UM=apply(mat,1,max)
term1=sum(con[,k]*est[,1])
EST[k]=term1
term2=sqrt(sum((con[,k]*est[,1]-LM)^2))
term3=sqrt(sum((con[,k]*est[,1]-UM)^2))
L[k]=term1-term2
U[k]=term1+term3
}
CI=cbind(EST,L,U)
dimnames(CI)=list(NULL,c('Est','ci.low','ci.hi'))
list(p.hat=est[,1],CI=CI,con=con)
}


#' Compute Confidence Interval for Zou et al. Method (Internal)
#'
#' @description
#' Internal function that computes the confidence interval for the difference
#' in two binomial proportions using the Zou, Hao, and Zhang method.
#'
#' @param r1 Number of successes in group 1
#' @param n1 Sample size for group 1
#' @param r2 Number of successes in group 2
#' @param n2 Sample size for group 2
#' @param x Vector of 1s and 0s for group 1 (not used)
#' @param y Vector of 1s and 0s for group 2 (not used)
#' @param alpha Significance level (default 0.05)
#' @param method Method for computing individual binomial CIs (default 'AC')
#'
#' @details
#' This is the computational workhorse for \code{\link{binom2g.ZHZ}}.
#' It computes CIs for each proportion separately, then combines them
#' using the method from Zou et al. (2009).
#'
#' @return
#' Confidence interval as a vector c(lower, upper).
#'
#' @seealso \code{\link{binom2g.ZHZ}}
#'
#' @keywords internal
#' @export
binom2g.ZHZ.main<-function(r1=sum(elimna(x)),n1=length(elimna(x)),
r2=sum(elimna(y)),n2 = length(elimna(y)), x = NA, y = NA, alpha=.05,
method='AC'){
#
#  Compare two binomials using the method in Zou et al.	2009 CSDA.
#
ci1=binom.conf(r1,n1,method=method,alpha=alpha,pr=FALSE)
ci2=binom.conf(r2,n2,method=method,alpha=alpha,pr=FALSE)
ci=ci1$phat-ci2$phat-sqrt((ci1$phat-ci1$ci[1])^2+(ci2$ci[2]-ci2$phat)^2)
ci[2]=ci1$phat-ci2$phat+sqrt((ci1$ci[2]-ci1$phat)^2+(ci2$phat-ci2$ci[1])^2)
ci
}




# ----------------------------------------------------------------------------

# bi2CR

# ----------------------------------------------------------------------------

bi2CR<-function(k1,n1,k2,n2,alpha=.05,fin=100,xlab="p1",ylab="p2"){
#
# Have two independent binomials with probability of success p1 and p2
# plot 1-alpha confidence region for p1 and p2 using
# Sterne 2-dimensional CS method

# parameters:
#   k1,n1,k2,n2: the 2 samples (k1 out of n1, k2 out of n2)
#   alpha: 1-nominal level
#   fin: the sample space is scanned with step size 1/fin
#   plt: controls plotting of the confidence set

# value: a data frame consisting of columns x,y,indik, where
#   x and y are coordinates of points in the
#  parameter space (with resolution 1/fin)
#   indik=1 if the parameter pair (x,y) pair belongs to the CS, =0 if not
# This function is based on a slight modification of code
# written by Jeno Reiczigel, Budapest, Hungary
# E-mail: reiczigel.jeno@aotk.szie.hu
# Homepage: www.univet.hu/users/jreiczig

# versions:
# version 18.04.2008: first version
# version 31.10.2010: plotting is optional
nom=1-alpha
x=rep(0:fin/fin,(fin+1))
y=rep(0:fin/fin,rep((fin+1),(fin+1)))
indik=rep(0,(fin+1)^2)
#
# CS calculation
#
for (i1 in 0:fin){
 p1=i1/fin
 pr1=dbinom(0:n1,n1,p1)
 for (i2 in 0:fin){
  p2=i2/fin
  pr2=dbinom(0:n2,n2,p2)
  jpr=rep(pr1,n2+1)*rep(pr2,rep(n1+1,n2+1))
  obs=dbinom(k1,n1,p1)*dbinom(k2,n2,p2)
  sumpr=sum(jpr[jpr>obs])
  if (sumpr<nom) {k=(i1+1)+(fin+1)*i2 ; indik[k]=indik[k]+1}
 }
}
# CS plotting
  plot(x[indik==1],y[indik==1],xlim=c(0,1),ylim=c(0,1),col="green",pch=15,
xlab=xlab,ylab=ylab)
  points(k1/n1,k2/n2,pch=19)
}




# ----------------------------------------------------------------------------

# bivar

# ----------------------------------------------------------------------------

bivar<-function(x){
# compute biweight midvariance of x
m<-median(x)
u<-abs((x-m)/(9*qnorm(.75)*mad(x)))
av<-ifelse(u<1,1,0)
top<-length(x)*sum(av*(x-m)^2*(1-u^2)^4)
bot<-sum(av*(1-u^2)*(1-5*u^2))
bi<-top/bot^2
bi
}




# ----------------------------------------------------------------------------

# block.diag

# ----------------------------------------------------------------------------

block.diag<-function(mat){
#
# mat is assumed to have list mode with
# mat[[1]]...mat[[p]] each having n-by-n matrices
#
# Create a np-by-np block diagonal matrix
#
# So p is the number of blocks
#
if(!is.list(mat))stop("mat should have list mode")
np<-length(mat)*ncol(mat[[1]])
m<-matrix(0,np,np)
n=nrow(mat[[1]])
p=length(mat)
ilow<-1-n
iup<-0
for(i in 1:p){
ilow<-ilow+n
iup<-iup+n
m[ilow:iup,ilow:iup]<-mat[[i]]
}
m
}

difQpciMC<-function(x,y=NULL,q=seq(5,40,5)/100,xlab='Quantile',ylab='Group 1 minus Group 2',plotit=TRUE,
SEED=TRUE,alpha=.05,nboot=1000){
#
#  Plot that provides perspective on the degree a distribution is symmetric about zero.
#  This function plots the sum of q and 1-q quantiles. A 1-alpha confidence interval for the sum is indicated by a +
#  If the distributions are symmetric
#  the plot should be approximately a horizontal line. If in addition the median
#  of the difference scores is zero, the horizontal line will intersect the y-axis at zero.
#
#  Similar to difQplot, only plots fewer quantiles by default and returns p-values for
#  each quantile indicated by the argument q.
#
#  FWE is controlled via Hochberg's method, which was used to determine critical
#  p-values based on the argument
#  alpha.
#
#  Can alter the quantiles compared via the argument
#  q
#  q must be less than .5
#
x=as.matrix(x)
if(is.null(y))dif=x
if(ncol(x)>2)stop("Should be at most two groups")
if(ncol(x)==2)dif=x[,1]-x[,2]
if(!is.null(y))dif=x-y
dif=elimna(dif)
dif=as.matrix(dif)
nv=length(dif)
output=matrix(NA,ncol=8,nrow=length(q))
dimnames(output)=list(NULL,c('quantile','Est_q','Est_1.minus.q','SUM','ci.low','ci.up','p_crit','p-value'))
for(i in 1:length(q)){
test=DqdifMC(dif,q=q[i],plotit=FALSE,nboot=nboot,SEED=SEED)
output[i,1]=q[i]
output[i,2]=test$est.q
output[i,3]=test$est.1.minus.q
output[i,8]=test$p.value
output[i,5]=test$conf.interval[1]
output[i,6]=test$conf.interval[2]
}
temp=order(output[,8],decreasing=TRUE)
zvec=alpha/c(1:length(q))
output[temp,7]=zvec
output <- data.frame(output)
output$signif=rep('YES',nrow(output))
for(i in 1:nrow(output)){
if(output[temp[i],8]>output[temp[i],7])output$signif[temp[i]]='NO'
if(output[temp[i],8]<=output[temp[i],7])break
}
output[,4]=output[,2]+output[,3]
if(plotit){
plot(rep(q,3),c(output[,4],output[,5],output[,6]),type='n',xlab=xlab,ylab=ylab)
points(q,output[,6],pch='+')
points(q,output[,5],pch='+')
points(q,output[,4],pch='*')
}
list(n=nv,output=output)
}




# ----------------------------------------------------------------------------

# bmpmul

# ----------------------------------------------------------------------------

bmpmul<-function(x,alpha=.05){
#
#  Perform Brunner-Munzel method for all pairs of J independent groups.
#
#  The familywise type I error probability is controlled by using
#  a critical value from the Studentized maximum modulus distribution.
#
#  The data are assumed to be stored in $x$ in list mode
#  or in a matrix having J columns.
#
#  Missing values are automatically removed.
#
#  The default value for alpha is .05. Any other value results in using
#  alpha=.01.
#
if(is.matrix(x))x<-listm(x)
if(!is.list(x))stop("Data must be stored in a matrix or in list mode.")
J<-length(x)
CC<-(J^2-J)/2
test<-matrix(NA,CC,7)
for(j in 1:J){
xx<-!is.na(x[[j]])
val<-x[[j]]
x[[j]]<-val[xx]  # Remove missing values
}
dimnames(test)<-list(NULL,c("Group","Group","P.hat","ci.lower","ci.upper","df","p.value"))
jcom<-0
for (j in 1:J){
for (k in 1:J){
if (j < k){
temp<-bmp(x[[j]],x[[k]],alpha)
crit<-0-smmcrit(temp$df,CC)
if(alpha!=.05)crit<-0-smmcrit01(temp$df,CC)
temp<-bmp(x[[j]],x[[k]],crit=crit)
jcom<-jcom+1
test[jcom,1]<-j
test[jcom,2]<-k
test[jcom,3]<-temp$phat
test[jcom,4]<-temp$ci.p[1]
test[jcom,5]<-temp$ci.p[2]
test[jcom,6]<-temp$df
test[jcom,7]<-temp$p.value
}}}
list(test=test)
}
box1way<-function(x,tr=.2,grp=c(1:length(x))){
#
#  A heteroscedastic one-way ANOVA for trimmed means
#  using a generalization of Box's method.
#
#  The data are assumed to be stored in $x$ in list mode.
#  Length(x) is assumed to correspond to the total number of groups.
#  By default, the null hypothesis is that all groups have a common mean.
#  To compare a subset of the groups, use grp to indicate which
#  groups are to be compared. For example, if you type the
#  command grp<-c(1,3,4), and then execute this function, groups
#  1, 3, and 4 will be compared with the remaining groups ignored.
#
#  Missing values are automatically removed.
#
J<-length(grp)  # The number of groups to be compared
print("The number of groups to be compared is")
print(J)
h<-vector("numeric",J)
w<-vector("numeric",J)
xbar<-vector("numeric",J)
svec<-vector("numeric",J)
for(j in 1:J){
xx<-!is.na(x[[j]])
val<-x[[j]]
x[[j]]<-val[xx]  # Remove missing values
h[j]<-length(x[[grp[j]]])-2*floor(tr*length(x[[grp[j]]]))
   # h is the number of observations in the jth group after trimming.
svec[j]<-((length(x[[grp[j]]])-1)*winvar(x[[grp[j]]],tr))/(h[j]-1)
xbar[j]<-mean(x[[grp[j]]],tr)
}
xtil<-sum(h*xbar)/sum(h)
fval<-h/sum(h)
TEST<-sum(h*(xbar-xtil)^2)/sum((1-fval)*svec)
nu1<-sum((1-fval)*svec)
nu1<-nu1^2/((sum(svec*fval))^2+sum(svec^2*(1-2*fval)))
nu2<-(sum((1-fval)*svec))^2/sum(svec^2*(1-fval)^2/(h-1))
sig<-1-pf(TEST,nu1,nu2)
list(TEST=TEST,nu1=nu1,nu2=nu2,p.value=sig)
}



boxdif<-function(x,names){
#
#  For J dependent groups, compute all pairwise differences and then
#  create boxplots for all pairs of groups.
#
if(is.null(dim(x)))stop('x should be a matrix or data frame')
ic=0
J=ncol(x)
n=nrow(x)
N=(J^2-J)/2
ic=0
dif=matrix(NA,nrow=n,ncol=N)
for(j in 1:J){
for(k in 1:J){
if(j<k){
ic=ic+1
dif[,ic]=x[,j]-x[,k]
}}}
boxplot(dif,names=names)
}
bprm<-function(x,y=NULL,grp=NA){
#
# Perform Brunner-Puri within groups rank-based ANOVA
#
# x can be a matrix with columns corresponding to groups
# or it can have list mode.
#
# For computational details, see Brunner, B., Domhof, S.  and Langer, F. (2002,
#  section 7.2.2, Nonparametric Analysis of Longitudinal Data in
#  Factorial Designs)
#
if(is.list(x))x<-matl(x)
if(!is.null(y[1]))x=cbind(x,y)
x<-elimna(x)
if(is.na(grp[1]))grp <- c(1:ncol(x))
if(!is.matrix(x))stop("Data are not stored in a matrix or in list mode.")
K<-length(grp) # The number of groups.
Jb<-matrix(1,K,K)
Ib<-diag(1,K)
Pb<-Ib-Jb/K
y<-matrix(rank(x),ncol=ncol(x)) #ranks of pooled data
ybar<-apply(y,2,mean) # average of ranks
N<-ncol(x)*nrow(x)
vhat<-var(y)/N^2
test<-nrow(x)*sum((ybar-(N+1)/2)^2)/N^2
trval<-sum(diag(Pb%*%vhat))
test<-test/trval # See Brunner, Domhof and Langer, p. 98, eq. 7.12
nu1<-trval^2/sum(diag(Pb%*%vhat%*%Pb%*%vhat))
sig.level<-1-pf(test,nu1,1000000)
list(test.stat=test,nu1=nu1,p.value=sig.level)
}



effectg.sub<-function(x,y,locfun=tmean,varfun=winvarN,...){
#
#  Compute a robust-heteroscedastic measure of effect size
#  based on the measure of location indicated by the argument
#  locfun, and the measure of scatter indicated by
#  varfun.
#
#  This subfunction is for the equal sample size case and is called by
#   effectg when sample sizes are not equal.
#
#  varfun defaults to winvarN, the Winsorized variance rescaled so that
#  it estimates the population variance under normality.
#
x<-x[!is.na(x)]  # Remove any missing values in x
y<-y[!is.na(y)]  # Remove any missing values in y
m1=locfun(x,...)
m2=locfun(y,...)
top=var(c(m1,m2))
pts=c(x,y)
#
bot=varfun(pts,...)
#
e.pow=top/bot
list(Var.Explained=e.pow,Effect.Size=sqrt(e.pow))
}




# ----------------------------------------------------------------------------

# bptd

# ----------------------------------------------------------------------------

bptd<-function(x,tr=.2,alpha=.05,con=0,nboot=599){
#
#   Using the percentile t bootstrap method,
#   compute a .95 confidence interval for all linear contasts
#   specified by con, a J by C matrix, where  C is the number of
#   contrasts to be tested, and the columns of con are the
#   contrast coefficients.
#
#   If con is not specified, all pairwise comparisons are performed.
#
#   The trimmed means of dependent groups are being compared.
#   By default, 20% trimming is used with B=599 bootstrap samples.
#
#   x can be an n by J matrix or it can have list mode
#
if(is.data.frame(x))x=as.matrix(x)
if(!is.list(x) && !is.matrix(x))stop("Data must be stored in a matrix or in list mode.")
if(is.list(x)){
if(is.matrix(con)){
if(length(x)!=nrow(con))stop("The number of rows in con is not equal to the number of groups.")
               }}
if(is.list(x)){
# put the data in an n by J matrix
mat<-matrix(0,length(x[[1]]),length(x))
for (j in 1:length(x))mat[,j]<-x[[j]]
}
if(is.matrix(x))mat=x
J<-ncol(mat)
Jm<-J-1
if(sum(con^2)==0){
d<-(J^2-J)/2
con<-matrix(0,J,d)
id<-0
for (j in 1:Jm){
jp<-j+1
for (k in jp:J){
id<-id+1
con[j,id]<-1
con[k,id]<-0-1
}}}
if(is.matrix(x)){
if(ncol(x)!=nrow(con))stop("The number of rows in con is not equal to the number of groups.")
mat<-x
}
if(sum(is.na(mat)>=1))stop("Missing values are not allowed.")
J<-ncol(mat)
connum<-ncol(con)
bvec<-matrix(0,connum,nboot)
set.seed(2) # set seed of random number generator so that
#             results can be duplicated.
# data is an nboot by n matrix
xcen<-matrix(0,nrow(mat),ncol(mat)) #An n by J matrix
xbars<-matrix(0,nboot,ncol(mat))
psihat<-matrix(0,connum,nboot)
print("Taking bootstrap samples. Please wait.")
data<-matrix(sample(nrow(xcen),size=nrow(mat)*nboot,replace=TRUE),nrow=nboot)
for (j in 1:J){
xcen[,j]<-mat[,j]-mean(mat[,j],tr) #Center data
xbars[,j]<-apply(data,1,bptdmean,xcen[,j],tr)
}
for (ic in 1:connum){
paste("Working on contrast number",ic)
bvec[ic,]<-apply(data,1,bptdsub,xcen,tr,con[,ic])
# bvec is a connum by nboot matrix containing the bootstrap sq standard error
psihat[ic,]<-apply(xbars,1,bptdpsi,con[,ic])
}
bvec<-psihat/sqrt(bvec)  #bvec now contains bootstrap test statistics
bvec<-abs(bvec)  #Doing two-sided confidence intervals
icrit<-round((1-alpha)*nboot)
critvec<-apply(bvec,2,max)
critvec<-sort(critvec)
crit<-critvec[icrit]
psihat<-matrix(0,connum,4)
dimnames(psihat)<-list(NULL,c("con.num","psihat","ci.lower","ci.upper"))
test<-matrix(NA,connum,3)
dimnames(test)<-list(NULL,c("con.num","test","se"))
isub<-c(1:nrow(mat))
tmeans<-apply(mat,2,mean,trim=tr)
sqse<-1
psi<-1
for (ic in 1:ncol(con)){
sqse[ic]<-bptdsub(isub,mat,tr,con[,ic])
psi[ic]<-sum(con[,ic]*tmeans)
psihat[ic,1]<-ic
psihat[ic,2]<-psi[ic]
psihat[ic,3]<-psi[ic]-crit*sqrt(sqse[ic])
psihat[ic,4]<-psi[ic]+crit*sqrt(sqse[ic])
test[ic,1]<-ic
test[ic,2]<-psi[ic]/sqrt(sqse[ic])
test[ic,3]<-sqrt(sqse[ic])
}
list(test=test,psihat=psihat,crit=crit,con=con)
}

bptdpsi<-function(x,con){
# Used by bptd to compute bootstrap psihat values
#
bptdpsi<-sum(con*x)
bptdpsi
}




# ----------------------------------------------------------------------------

# bptdsub

# ----------------------------------------------------------------------------

bptdsub<-function(isub,x,tr,con){
#
#  Compute test statistic for trimmed means
#  when comparing dependent groups.
#  By default, 20% trimmed means are used.
#  isub is a vector of length n,
#  a bootstrap sample from the sequence of integers
#  1, 2, 3, ..., n
#  con is a J by c matrix. The cth column contains
#  a vector of contrast coefficients.
#
#  This function is used by bptd.
#
h1 <- nrow(x) - 2 * floor(tr * nrow(x))
se<-0
for(j in 1:ncol(x)){
for(k in 1:ncol(x)){
djk<-(nrow(x) - 1) * wincor(x[isub,j],x[isub,k], tr)$cov
se<-se+con[j]*con[k]*djk
}
}
se/(h1*(h1-1))
}




# ----------------------------------------------------------------------------

# bsqrm

# ----------------------------------------------------------------------------

bsqrm<-function(x,y,alpha=0.05,bend=1.28){
#
#  Computes Bsqrm test statistic. This test statistic is from Ozdemir (2012)
#  "mestse" was used as the standard error of one-step M-estimator and
#  "mad" was used as a measure of scale.
#
x<-x[!is.na(x)]  # Remove any missing values in x
y<-y[!is.na(y)]  # Remove any missing values in y
zc<-qnorm(alpha/2)
x2<-(x-median(x))/mad(x)
y2<-(y-median(y))/mad(y)
C<-length(x[abs(x2)>bend])
D<-length(y[abs(y2)>bend])
e<-c(C,D)
alist<-list(x,y)
f<-(sapply(alist,length))-e
s=sapply(alist,mestse)^2
wden=sum(1/s)
w=(1/s)/wden
yplus<-sum(w*(sapply(alist,onestep)))
tt<-((sapply(alist,onestep))-yplus)/sqrt(s)
v<-(f-1)
z<-((4*v^2)+(5*((2*(zc^2))+3)/24))/((4*v^2)+v+(((4*(zc^2))+9)/12))*sqrt(v)*(sqrt(log(1+(tt^2/v))))
teststat<-sum(z^2)
list(teststat=teststat)
}




# ----------------------------------------------------------------------------

# bsqrmbt

# ----------------------------------------------------------------------------

bsqrmbt<-function(x,y,alpha=.05,bend=1.28,nboot=599,SEED=TRUE){
#
#  Goal: Test hypothesis that two independent groups have
#  equal population M-measures of location.
#  A bootstrap-t method is used.
#   The method used was derived by F. Ozdemir
#
if(SEED)set.seed(2) # set seed of random number generator so that
#             results can be duplicated.
T<-bsqrm(x,y,alpha,bend)$teststat
TT<-0
bsqrmbt<-numeric(2)
xone<-onestep(x,bend=bend)
yone<-onestep(y,bend=bend)
for(j in 1:nboot)
       {
       xx<-(sample(x,length(x),replace=TRUE)-xone)
       yy<-(sample(y,length(y),replace=TRUE)-yone)
       TT[j]<-bsqrm(xx,yy,alpha,bend)$teststat
       }
TT<-sort(TT)
bott<-round(alpha*nboot)+1
bsqrmbt<-TT[nboot-bott]
pv=mean(T<=TT)
list(critval=bsqrmbt,teststat=TRUE,p.value=pv)
}

M2gbt=bsqrmbt

bw.2by2.int.es<-function(x,CI=FALSE){
#
# Form difference scores and compute several measures of effect size
#
# if CI=TRUE, compute confidence intervals
#
if(is.matrix(x) || is.data.frame(x))x=listm(x)
d1=x[[1]]-x[[2]]
d2=x[[3]]-x[[4]]
if(!CI)a=ES.summary(d1,d2)
else
a=ES.summary.CI(d1,d2)
a
}




# ----------------------------------------------------------------------------

# bw.int.es

# ----------------------------------------------------------------------------

bw.int.es<-function(J,K,x,method='KMS',tr=.2,SEED=TRUE,nboot=2000,CI=FALSE){
#
#  All  Interactions based on difference scores
#
#. Choices for method: 'EP','QS','QStr','AKP','WMW','KMS'
#
if(is.matrix(x)  || is.data.frame(x))x=listm(x)
con=con2way(J,K)$conAB
num=ncol(con)
if(CI){
CON=matrix(NA,nrow=num,ncol=8)
dimnames(CON)=list(NULL,c('Con.num','n1','n2','Est.','ci.low','ci.up','p.value','p.adjusted'))
}
if(!CI){
CON=matrix(NA,nrow=num,ncol=2)
dimnames(CON)=list(NULL,c('Con.num','Est.'))
}

for(j in 1:ncol(con)){
id=which(con[,j]!=0)
dat=x[id]
d1=dat[[1]]-dat[[2]]
d2=dat[[3]]-dat[[4]]
if(!CI){
temp=ESfun(d1,d2)
CON[j,1]=j
CON[j,2]=temp
}
else{
temp=ESfun.CI(d1,d2)
CON[j,1]=j
a=ESfun.CI(d1,d2)
CON[j,2]=a$n1
CON[j,3]=a$n2
CON[j,4]=a$effect.size
CON[j,5]=a$ci[1]
CON[j,6]=a$ci[2]
CON[j,7]=a$p.value
}
}
if(CI)
CON[,8]=p.adjust(CON[,7],method='hoch')
list(CON=CON,con=con)
}




# ----------------------------------------------------------------------------

# bw2list

# ----------------------------------------------------------------------------

bw2list<-function(x,between.col,within.col,grp.col=between.col,lev.col=within.col,pr=TRUE){
#
#  for a between by within design
#  grp.col is column indicating levels of between factor.
#  lev.col indicates the columns where repeated measures are contained
#
#  Example:  column 2 contains information on levels of between factor
#  have a 3 by 2 design, column 3 contains time 1 data,
#  column 7 contains time 2
#  bw2list(x,2,c(3,7)) will store data in list mode that can be
#  used by rmanova and related functions
#
res=selbybw(x,grp.col,lev.col)
if(pr){
print("Levels for between factor:")
print(unique(x[,grp.col]))
}
res$x
}




# ----------------------------------------------------------------------------

# bwiDIF

# ----------------------------------------------------------------------------

bwiDIF<-function(J,K,x,JK=J*K,grp=c(1:JK),alpha=.05,SEED=TRUE){
#
#  Same as bwimcp only use a Patel type approach
#
# Multiple comparisons for interactions
# in a split-plot design.
# The analysis is done by taking difference scores
# among all pairs of dependent groups and
# determining which of
# these differences differ across levels of Factor A
# using trimmed means.
#
#  FWE is controlled via Hochberg's method
# To adjusted p-values, use the function p.adjust
#
# For MOM or M-estimators, use spmcpi which uses a bootstrap method
#
#  The R variable x is assumed to contain the raw
#  data stored in list mode or in a matrix.
#  If in list mode, x[[1]] contains the data
#  for the first level of both factors: level 1,1.
#  x[[2]] is assumed to contain the data for level 1 of the
#  first factor and level 2 of the second: level 1,2
#  x[[K]] is the data for level 1,K
#  x[[K+1]] is the data for level 2,1, x[[2K]] is level 2,K, etc.
#
#  If the data are in a matrix, column 1 is assumed to
#  correspond to x[[1]], column 2 to x[[2]], etc.
#
#  When in list mode x is assumed to have length JK, the total number of
#  groups being tested, but a subset of the data can be analyzed
#  using grp
#
       if(is.matrix(x)) {
                y <- list()
                for(j in 1:ncol(x))
                        y[[j]] <- x[, j]
                x <- y
}

JK<-J*K
if(JK!=length(x))stop('Something is wrong. Expected ',JK,' groups but x contains ', length(x), 'groups instead.')
MJ<-(J^2-J)/2
MK<-(K^2-K)/2
JMK<-J*MK
MJMK<-MJ*MK
Jm<-J-1
data<-list()
for(j in 1:length(x)){
data[[j]]<-x[[grp[j]]] # Now have the groups in proper order.
}
x<-data
output<-matrix(0,MJMK,9)
dimnames(output)<-list(NULL,c('A','A','B','B','p.hat','p.value','ci.low','ci.up','p.adjust'))
jp<-1-K
kv<-0
kv2<-0
test<-NA
for(j in 1:J){
jp<-jp+K
xmat<-matrix(NA,ncol=K,nrow=length(x[[jp]]))
for(k in 1:K){
kv<-kv+1
xmat[,k]<-x[[kv]]
}
xmat<-elimna(xmat)
for(k in 1:K){
kv2<-kv2+1
x[[kv2]]<-xmat[,k]
}}
m<-matrix(c(1:JK),J,K,byrow=TRUE)
ic<-0
for(j in 1:J){
for(jj in 1:J){
if(j<jj){
for(k in 1:K){
for(kk in 1:K){
if(k<kk){
ic<-ic+1
output[ic,1]<-j
output[ic,2]<-jj
output[ic,3]<-k
output[ic,4]<-kk
x1<-x[[m[j,k]]]-x[[m[j,kk]]]
x2<-x[[m[jj,k]]]-x[[m[jj,kk]]]
temp<-cidv2(x1,x2)
output[ic,5]<-temp$p.hat
test[ic]<-temp$p.value
output[ic,6]<-test[ic]
output[ic,7:8]<-temp$p.ci
}}}}}}
output[,9]=p.adjust(output[,6],method='hoch')
output
}



bwiQS<-function(J,K,x,locfun=median,...){
#
#  Quantile shift  measure of effect size for interactions in a
#  between-by-within design
#
#  The R variable x is assumed to contain the raw
#  data stored in list mode or in a matrix.
#  If in list mode, x[[1]] contains the data
#  for the first level of both factors: level 1,1.
#  x[[2]] is assumed to contain the data for level 1 of the
#  first factor and level 2 of the second: level 1,2
#  x[[K]] is the data for level 1,K
#  x[[K+1]] is the data for level 2,1, x[[2K]] is level 2,K, etc.
#
#  If the data are in a matrix, column 1 is assumed to
#  correspond to x[[1]], column 2 to x[[2]], etc.
#
JK<-J*K
MJ<-(J^2-J)/2
MK<-(K^2-K)/2
MJMK<-MJ*MK
if(is.matrix(x) || is.data.frame(x))x=listm(x)
if(JK!=length(x))stop('Something is wrong. Expected ',JK,' groups but x contains ', length(x), ' groups instead.')
m=matrix(c(1:JK),nrow=J,byrow=TRUE)
output=matrix(NA,ncol=5,nrow=MJMK)
dimnames(output)<-list(NULL,c('A','A','B','B','Q.Effect'))
ic=0
for(j in 1:J){
for(jj in 1:J){
if(j<jj){
for(k in 1:K){
for(kk in 1:K){
if(k<kk){
ic<-ic+1
output[ic,1]<-j
output[ic,2]<-jj
output[ic,3]<-k
output[ic,4]<-kk
x1<-x[[m[j,k]]]-x[[m[j,kk]]]
x2<-x[[m[jj,k]]]-x[[m[jj,kk]]]
output[ic,5]<-shiftes(x1,x2,locfun=locfun,...)$Q.Effect
}}}}}}
output
}




# ----------------------------------------------------------------------------

# bwmarpb

# ----------------------------------------------------------------------------

bwmarpb<-function(J,K,x,est=hd,JK=J*K,grp=c(1:JK),nboot=599,
SEED=TRUE,na.rm=TRUE,...){
#
#  Multiple comparisons using
#  a percentile bootstrap for performing a split-plot design
#  using marginal measures of location
#  By default, 20% trimming is used with B=599 bootstrap samples.
#
#  The R variable x is assumed to contain the raw
#  data stored in list mode or in a matrix or a data frame
#  If in list mode, x[[1]] contains the data
#  for the first level of both factors: level 1,1.
#  x[[2]] is assumed to contain the data for level 1 of the
#  first factor and level 2 of the second: level 1,2
#  x[[K]] is the data for level 1,K
#  x[[K+1]] is the data for level 2,1, x[[2K]] is level 2,K, etc.
#
#  If the data are in a matrix, column 1 is assumed to
#  correspond to x[[1]], column 2 to x[[2]], etc.
#
#  When in list mode x is assumed to have length JK, the total number of
#  groups being tested, but a subset of the data can be analyzed
#  using grp
#
if(is.matrix(x))
{
if(ncol(x)!=JK)print("WARNING: number of groups is not equal to JK")
}
if(is.list(x)){
if(length(x)!=JK)print("WARNING: number of groups is not equal to JK")
}
if(is.data.frame(x)){
if(ncol(x)!=JK)print("WARNING: number of groups is not equal to JK")
}
if(SEED)set.seed(2)
if(is.data.frame(x) || is.matrix(x)) {
y <- list()
ik=0
il=c(1:K)-K
for(j in 1:J){
il=il+K
zz=x[,il]
if(na.rm)zz=elimna(zz)
for(k in 1:K){
ik=ik+1
y[[ik]]=zz[,k]
}}
                x <- y
}

data<-list()
for(j in 1:length(x)){
data[[j]]<-x[[grp[j]]] # Now have the groups in proper order.
}
x<-data

#
con=con2way(J,K)
estA=psihat(x,est,con=con$conA,...)
estB=psihat(x,est,con=con$conB,...)
estAB=psihat(x,est,con=con$conAB,...)
set.seed(2) # set seed of random number generator so that
#             results can be duplicated.
# Next determine the n_j values
nvec<-NA
jp<-1-K
for(j in 1:J){
jp<-jp+K
nvec[j]<-length(x[[j]])
}
blist<-list()
testmatA<-matrix(NA,ncol=ncol(con$conA),nrow=nboot)
testmatB<-matrix(NA,ncol=ncol(con$conB),nrow=nboot)
testmatAB<-matrix(NA,ncol=ncol(con$conAB),nrow=nboot)
for(iboot in 1:nboot){
iv<-0
for(j in 1:J){
temp<-sample(nvec[j],replace = T)
for(k in 1:K){
iv<-iv+1
tempx<-x[[iv]]
blist[[iv]]<-tempx[temp]
}}
#
# Now do all linear contrasts on bootstrap samples
testmatA[iboot,]<-psihat(blist,est,con=con$conA,...)
testmatB[iboot,]<-psihat(blist,est,con=con$conB,...)
testmatAB[iboot,]<-psihat(blist,est,con=con$conAB,...)
}
pbA=NA
pbB=NA
pbAB=NA
for(j in 1:ncol(con$conA))pbA[j]=mean(testmatA[,j]<0)+.5*mean(testmatA[,j]==0)
for(j in 1:ncol(con$conB))pbB[j]=mean(testmatB[,j]<0)+.5*mean(testmatB[,j]==0)
for(j in 1:ncol(con$conAB))pbAB[j]=mean(testmatAB[,j]<0)+.5*mean(testmatAB[,j]==0)
for(j in 1:ncol(con$conA))pbA[j]=2*min(c(pbA[j],1-pbA[j]))
for(j in 1:ncol(con$conB))pbB[j]=2*min(c(pbB[j],1-pbB[j]))
for(j in 1:ncol(con$conAB))pbAB[j]=2*min(c(pbAB[j],1-pbAB[j]))
p.valueA=pbA
p.valueB=pbB
p.valueAB=pbAB
pbA=cbind(estA,p.valueA)
pbB=cbind(estB,p.valueB)
pbAB=cbind(estAB,p.valueAB)
list(FacA=pbA,FacB=pbB,p.FacAB=pbAB,conA=con$conA,conB=con$conB,conAB=con$conAB)
}

BWPHmcp<-function(J,K, x, method='KMS'){
#
# For a between-by-within design:
# Check for interactions by comparing binomials
# That is, use a Patel--Hoel approach.
#
#  KMS is the Kulinskaya et al. method. Other options:
#  'ZHZ'
#  'SK'
#
#
p<-J*K
connum<-(J^2-J)*(K^2-K)/4
if(is.matrix(x))x<-listm(x)
if(!is.list(x))stop('Data must be stored in a matrix or in list mode')
imap<-matrix(c(1:p),J,K,byrow=TRUE)
outm<-matrix(NA,ncol=10,nrow=connum)
outsk<-matrix(NA,ncol=8,nrow=connum)
dimnames(outsk)<-list(NULL,c('Fac.A','Fac.A','Fac.B','Fac.B','p1','p2','p.value','p.adj'))
dimnames(outm)<-list(NULL,c('Fac.A','Fac.A','Fac.B','Fac.B','p1','p2','ci.low','ci.up','p.value','p.adj'))
ic<-0
for (j in 1:J){
for (jj in 1:J){
if(j<jj){
for (k in 1:K){
for (kk in 1:K){
if(k<kk){
dif<-x[[imap[j,k]]]-x[[imap[j,kk]]]
dif<-dif[dif!=0]
dif1<-(dif<0)
dif<-(x[[imap[jj,k]]]-x[[imap[jj,kk]]])
dif<-dif[dif!=0]
dif2<-(dif<0)
ic<-ic+1
outm[ic,1]<-j
outm[ic,2]<-jj
outm[ic,3]<-k
outm[ic,4]<-kk
temp<-binom2g(x=dif1,y=dif2,alpha=alpha,method=method)
if(method!='SK'){
outm[ic,5]<-temp$p1
outm[ic,6]<-temp$p2
outm[ic,7]<-temp$ci[1]
outm[ic,8]<-temp$ci[2]
outm[ic,9]<-temp$p.value }
else{
outsk[ic,1]<-j
outsk[ic,2]<-jj
outsk[ic,3]<-k
outsk[ic,4]<-kk
outsk[ic,5]<-temp$p1
outsk[ic,6]<-temp$p2
outsk[ic,7]<-temp$p.value
}
}}}}}}

if(method!='SK'){
outm[,10]=p.adjust(outm[,9],method='hochberg')
res=outm
}
else{
 outsk[,8]=p.adjust(outsk[,7],method='hochberg')
res=outsk
}
res
}

bwQS<-function(J,K,x,locfun=median,pr=TRUE){
#
#  Compute quantile shift measure of effect size for all pairwise comparisons
#  in a between-by-within design plus interactions.
#
if(pr){
print('output: A[[1]] contains results for level 1 of Factor A;')
print(' all pairwise comparisons over Factor B')
print('A[[2]] contains results for level 2, etc.')
print(' ')
print('Note: Under normality and homoscedasticity, Cohen d= 0, .2, .5, .8')
print('correspond approximately to Q.effect = 0.5, 0.55, 0.65 and 0.70, respectively')
}
A=list()
B=list()
AB=list()
if(is.matrix(x) || is.data.frame(x))x<-listm(x)
JK=J*K
ID=matrix(c(1:JK),nrow=J,ncol=K,byrow=TRUE)
A=list()
for (j in 1:J)A[[j]]=wmcpQS(x[ID[j,]],locfun=locfun)
B=list()
for(k in 1:K)B[[k]]=bmcpQS(x[ID[,k]],locfun=locfun)
AB=bwiQS(J,K,x)
list(Factor.A=A,Factor.B=B,interactions=AB)
}




# ----------------------------------------------------------------------------

# bwquantile

# ----------------------------------------------------------------------------

bwquantile<-function(M1,M2,alpha=.05,nboot=1000,SEED=TRUE,q=.5,...){
#
#  M1 and M2 are assumed to be matrices with two columns
#  They are random samples from some bivariate distribution from
#   two independent groups
#
#  For example,
# have two dependent groups, e.g., same subjects under two conditions,
#  Have two independent groups, e.g., male and female
#
#  Consider difference between males and females at condition 1, estimate difference between quantiles
#  Under condition 2, does this difference differ from the difference under condition 1?
#
#  q indicates the quantile to be used
#
#
#  REQUIRES WRS PACKAGE OR THE FUNCTIONS IN RALLFUN-V38
#
M1=elimna(M1)
M2=elimna(M2)
n1=nrow(M1)
n2=nrow(M2)
e1=apply(M1,2,hd,q)
e2=apply(M2,2,hd,q)
dif1=e1[1]-e2[1]
dif2=e1[2]-e2[2]
dif=dif1-dif2
DIF=NA
for(i in 1:nboot){
id1=sample(n1,replace=TRUE)
id2=sample(n2,replace=TRUE)
B1=apply(M1[id1,],2,hd,q)
B2=apply(M2[id2,],2,hd,q)
DIF[i]=B1[1]-B2[1]-B1[2]+B2[2]
}
DIF=sort(DIF)
pv=mean(DIF<0)
pv=2*min(pv,1-pv)
ilow<-round((alpha/2) * nboot)
ihi<-nboot - ilow
ilow<-ilow+1
ci=DIF[ilow]
ci[2]=DIF[ihi]
list(p.value=pv,ci=ci)
}




# ----------------------------------------------------------------------------

# bwrank

# ----------------------------------------------------------------------------

bwrank<-function(J,K,x,grp=c(1:p),p=J*K){
#
# Between by within rank-based ANOVA
# That is, have a J by K design with J independent levels and K dependent
# measures
#
# x can be a matrix with columns corresponding to groups
# or it can have list mode.
#
#
if(is.data.frame(x))data=as.matrix(x)
if(is.matrix(x))x<-listm(x)
x=x[grp]
xx<-list()
nvec<-NA
alldat<-NA
klow<-1-K
kup<-0
iall=0
for (j in 1:J){
klow<-klow+K
kup<-kup+K
mtemp=elimna(matl(x[klow:kup]))
for(k in 1:K){
iall=iall+1
xx[[iall]]=mtemp[,k]
}}
for(j in 1:p){
alldat<-c(alldat,xx[[j]])
nvec[j]<-length(xx[[j]])
}
#
#  Check sample sizes
#
nmat<-matrix(nvec,J,K,byrow=TRUE)
for(j in 1:J){
if(var(nmat[j,]) !=0){
warning("Number of observations among dependent groups for level",paste(j)," of Factor A are unequal")
print("Matrix of sample sizes:")
print(nmat)
}}
if(sum(is.na(alldat[2:length(alldat)])>0))stop("Missing values not allowed")
rval<-rank(alldat[2:length(alldat)])
rdd<-mean(rval) # R bar ...
xr<-list()
il<-1-nvec[1]
iu<-0
for(j in 1:p){
il<-il+nvec[j]
iu<-iu+nvec[j]
xr[[j]]<-rval[il:iu]
}
v<-matrix(0,p,p)
Ja<-matrix(1,J,J)
Ia<-diag(1,J)
Pa<-Ia-Ja/J
Jb<-matrix(1,K,K)
Ib<-diag(1,K)
Pb<-Ib-Jb/K
cona<-kron(Pa,Ib)
conb<-kron(Ia,Pb)
conab<-kron(Pa,Pb)
for(k in 1:K){
temp<-x[[k]]
bigm<-matrix(temp,ncol=1)
jk<-k
for (j in 2:J){
jk<-jk+K
tempc<-matrix(x[[jk]],ncol=1)
bigm<-rbind(bigm,tempc)
temp<-c(temp,x[[jk]])
}}
N<-length(temp)
rbbd<-NA
for(k in 1:K){
bigm<-xr[[k]]
jk<-k
for (j in 2:J){
jk<-jk+K
bigm<-c(bigm,xr[[jk]])
}}
rbjk<-matrix(NA,nrow=J,ncol=K) #R_.jk
ic<-0
for (j in 1:J){
for(k in 1:K){
ic<-ic+1
rbjk[j,k]<-mean(xr[[ic]]) # R bar_.jk
}}
for(k in 1:K)rbbd[k]<-mean(rbjk[,k])
rbj<-1 # R_.j.
sigv<-0
njsam<-0 # n_j
icc<-1-K
ivec<-c(1:K)-K
for (j in 1:J){
icc<-icc+K
ivec<-ivec+K
temp<-xr[ivec[1]:ivec[K]]
temp<-matl(temp)
tempv<-apply(temp,1,mean)
njsam[j]<-nvec[icc]
rbj[j]<-mean(rbjk[j,])
sigv[j]<-var(tempv) # var of R bar_ij.
}
nv<-sum(njsam)
phat<-(rbjk-.5)/(nv*K)
sv2<-sum(sigv/njsam)
uv<-sum((sigv/njsam)^2)
dv<-sum((sigv/njsam)^2/(njsam-1))
testA<-J*var(rbj)/sv2
klow<-1-K
kup<-0
sv<-matrix(0,nrow=K,ncol=K)
rvk<-NA
for(j in 1:J){
klow<-klow+K
kup<-kup+K
sel<-c(klow:kup)
m<-matl(xr[klow:kup])
m<-elimna(m)
xx<-listm(m)
xx<-listm(m)
vsub<-nv*var(m)/(nv*K*nv*K*njsam[j])
v[sel,sel]<-vsub
sv<-sv+vsub
}
sv<-sv/J^2
testB<-nv/(nv*K*nv*K*sum(diag(Pb%*%sv)))*sum((rbbd-mean(rbbd))^2)
testAB<-0
for (j in 1:J){
for (k in 1:K){
testAB<-testAB+(rbjk[j,k]-rbj[j]-rbbd[k]+rdd)^2
}}
testAB<-nv*testAB/(nv*K*nv*K*sum(diag(conab%*%v)))
nu1B<-(sum(diag(Pb%*%sv)))^2/sum((diag(Pb%*%sv%*%Pb%*%sv)))
nu1A<-(J-1)^2/(1+J*(J-2)*uv/sv2^2)
nu2A<-sv2^2/dv
nu1AB<-(sum(diag(conab%*%v)))^2/sum(diag(conab%*%v%*%conab%*%v))
sig.A<-1-pf(testA,nu1A,nu2A)
sig.B<-1-pf(testB,nu1B,1000000)
sig.AB<-1-pf(testAB,nu1AB,1000000)
list(test.A=testA,p.value.A=sig.A,test.B=testB,p.value.B=sig.B,test.AB=testAB,
p.value.AB=sig.AB,avg.ranks=rbjk,rel.effects=phat)
}




# ----------------------------------------------------------------------------

# bwtrimbt

# ----------------------------------------------------------------------------

bwwmatna<-function(J,K,L,x){
#
# data are assumed to be stored in a matrix
# for a between by within by within (three-way) anova,
# for the last two factors, eliminate any missing values
# and then store the data in list mode.
#
if(is.data.frame(x))x=as.matrix(x)
y=list()
ad=K*L
ilow=1
iup=ad
ic=0
for(j in 1:J){
z=x[,ilow:iup]
d=elimna(z)
im=0
for(k in 1:K){
for(l in 1:L){
ic=ic+1
im=im+1
y[[ic]]=d[,im]
}}
ilow=ilow+ad
iup=iup+ad
}
y
}




# ----------------------------------------------------------------------------

# bwwna

# ----------------------------------------------------------------------------

bwwna<-function(J,K,L,x){
#
# data are assumed to be stored in list mode
# for a between by within by within (three-way) anova,
# for the last two factors, eliminate any missing values.
#
if(is.data.frame(x))x=as.matrix(x)
y=list()
ad=K*L
ilow=1
iup=ad
ic=0
for(j in 1:J){
z=x[ilow:iup]
d=elimna(matl(z))
#print(d)
im=0
for(k in 1:K){
for(l in 1:L){
ic=ic+1
im=im+1
y[[ic]]=d[,im]
}}
ilow=ilow+ad
iup=iup+ad
}
y
}
calwork <- function(a,b,ai,bi,ab,eps)
{
	if (abs(a-b) < 2.0*eps)
	{
		if (ai+bi==ab)
		{
			cwork=0
		}
		else
		{
			if (ai+bi<ab)
			{
				cwork=1
			}
			else
			{
				cwork=0-1
			}
		}
	}
	else
	{
		cwork=(a+b)/(a-b)
	}
	cwork
}


cat.dat.ci<-function(x,alpha=.05){
#
# x is assumed to be discrete with a relatively	small
# sample space.
# For each oberved value, x, compute a confidence interval
# for the probability that x occurs
#
x=elimna(x)
n=length(x)
v=unique(x)
v=sort(v)
N=length(v)
M=matrix(NA,nrow=N,ncol=4)
for(i in 1:N){
M[i,1]=v[i]
z=sum(x==v[i])
a=binom.conf(z,n,pr=FALSE)
M[i,2]=a$phat
M[i,3:4]=a$ci
}
dimnames(M)=list(NULL,c('x','Est.','ci.low','ci.up'))
list(output=M)
}

smvar.DO<-function(x,est=winsd,nboot=1000,SEED=TRUE,pr=TRUE,...){
#
#  For J independent groups.
#  Determine whether it is reasonable to
#  decide which group has smallest robust  measure of variation
#
#  Default is the Winsorized standard deviation
#
if(is.matrix(x)||is.data.frame(x))x<-listm(x)
J=length(x)
e=lapply(x,est,...)
e=pool.a.list(e)
id=which(e==min(e))
id=id[1]
e=lapply(x,est,...)
e=pool.a.list(e)
pv=NA
CON=conCON(J,id)$conCON
a=linconpb(x,con=CON,est=est,nboot=nboot,SEED=SEED,...)$output[,3]
pv=max(a)
list(Group.Smallest=id,Est.=e,p.value=pv)
}




# ----------------------------------------------------------------------------

# cav

# ----------------------------------------------------------------------------

cav<-function(alpha = 0.01, k = 5)
{
#gets n(asy var) for the alpha trimmed mean
#and T_(A,n)(k) if errors are Cauchy(0,1)
	z <- tan(pi * (alpha - 0.5))
	val <- (z - atan(z))/((1 - 2 * alpha) * atan(z))
	ntmav <- val + (2 * alpha * (tan(pi * (alpha - 0.5)))^2)/(1 - 2 * alpha
		)^2
	zj <- k
	alphaj <- 0.5 + atan( - k)/pi
	alphaj <- ceiling(100 * alphaj)/100
	zj <- tan(pi * (alphaj - 0.5))
	val <- (zj - atan(zj))/((1 - 2 * alphaj) * atan(zj))
	natmav <- val + (2 * alphaj * (tan(pi * (alphaj - 0.5)))^2)/(1 - 2 *
		alphaj)^2
	return(ntmav, natmav)
}




# ----------------------------------------------------------------------------

# cci

# ----------------------------------------------------------------------------

cci<-function(x, alpha = 0.05)
{
#gets classical  100 (1-alpha)% CI
#defaults are alpha = .05
	n <- length(x)
	up <- 1 - alpha/2
	mn <- mean(x)
	v <- var(x)
	se <- sqrt(v/n)
	val <- qt(up, n - 1) * se
	lo <- mn - val
	hi <- mn + val
	list(int = c(lo, hi), mean = mn, se = se)
}




# ----------------------------------------------------------------------------

# cell.com

# ----------------------------------------------------------------------------

cell.com<-function(x,i=1,j=2,alpha=.05,AUTO=TRUE,method='AC',data=NULL){
#
# For a multinomial distribution, compuate a confidence interval
#  for p_i-p_j, the difference between the probabilities asscoiated with cells i and j
#
# x= cell frequencies
#
if(!is.null(data))x=splot(data)$frequencies
n=sum(x)
c1=binom.conf(x[i],n,AUTO=AUTO,method=method,alpha=alpha,pr=FALSE)$ci
c2=binom.conf(x[j],n,AUTO=AUTO,method=method,alpha=alpha,pr=FALSE)$ci
p1=x[i]/n
p2=x[j]/n
COR=0-sqrt(p1*p1/((1-p1)*(1-p2)))
T1=(p1-c1[1])^2+(p2-c2[2])^2-2*COR*(p1-c1[1])*(c2[2]-p2)
T2=(p1-c1[2])^2+(p2-c2[1])^2-2*COR*(c1[2]-p1)*(p2-c2[1])
T2=max(c(0,T2))
T1=max(c(0,T1))
L=p1-p2-sqrt(T1)
U=p1-p2+sqrt(T2)
list(cells.compared=c(i,j),dif=p1-p2,Estimates=x/n,ci=c(L,U))
}


cell.com.pv<-function(x,i=1,j=2,method='AC',data=NULL){
#
# For a multinomial distribution, compute a confidence interval
#  for p_i-p_j, the difference between the probabilities asscoiated with cells i and j
#
# x= cell frequencies
#
if(!is.null(data))x=splot(data)$frequencies
n=sum(x)
p1=x[i]/n
p2=x[j]/n
COR=0-sqrt(p1*p1/((1-p1)*(1-p2)))
a=seq(.001,.1,.001)
a=c(a,seq(.1,.99,.01))
a=rev(a)

if(x[i]==x[j])pv=1
else{
for(k in 1:length(a)){
c2=acbinomci(x[j],n,alpha=a[k])$ci
c1=acbinomci(x[i],n,alpha=a[k])$ci
T1=(p1-c1[1])^2+(p2-c2[2])^2-2*COR*(p1-c1[1])*(c2[2]-p2)
T2=(p1-c1[2])^2+(p2-c2[1])^2-2*COR*(c1[2]-p1)*(p2-c2[1])
T2=max(c(0,T2))
T1=max(c(0,T1))
L=p1-p2-sqrt(T1)
U=p1-p2+sqrt(T2)
pv=a[k]
if(sign(L*U)<0)break
}}
if(n<=35){
if(x[i]==x[j])pvnew=1
else{
pv.up=pv+.1
anew=seq(pv,pv.up,.01)
for(k in 1:length(anew)){
c1=binom.conf(x[i],n,AUTO=TRUE,method=method,alpha=anew[k],pr=FALSE)$ci
c2=binom.conf(x[j],n,AUTO=TRUE,method=method,alpha=anew[k],pr=FALSE)$ci
T1=(p1-c1[1])^2+(p2-c2[2])^2-2*COR*(p1-c1[1])*(c2[2]-p2)
T2=(p1-c1[2])^2+(p2-c2[1])^2-2*COR*(c1[2]-p1)*(p2-c2[1])
T2=max(c(0,T2))
T1=max(c(0,T1))
L=p1-p2-sqrt(T1)
U=p1-p2+sqrt(T2)
pvnew=anew[k]
if(sign(L*U)>0)break
}}
pv=pvnew
}
pv
}




# ----------------------------------------------------------------------------

# cgci

# ----------------------------------------------------------------------------

cgci<-function(x, alpha = 0.05, ks = 3.5)
{
#gets T_S,n with a coarse grid
# and the corresponding robust  100 (1-alpha)% CI
	n <- length(x)
	up <- 1 - alpha/2
	med <- median(x)
	madd <- mad(x, constant = 1)
	d <- sort(x)	##get robust T_S,n CI
	lo <- sum(x < (med - ks * madd))
	hi <- sum(x > (med + ks * madd))
	tp <- max(hi, lo)/n
	if(tp == 0)
		tp <- 0
	if(tp > 0 && tp <= 0.01)
		tp <- 0.01
	if(tp > 0.01 && tp <= 0.1)
		tp <- 0.1
	if(tp > 0.1 && tp <= 0.25)
		tp <- 0.25
	if(tp > 0.25 && tp <= 0.4)
		tp <- 0.4
	if(tp > 0.4)
		tp <- 0.49
	tstmn <- mean(x, trim = tp)
	#have obtained the two stage trimmed mean
	ln <- floor(n * tp)
	un <- n - ln
	if(ln > 0) {
		d[1:ln] <- d[(ln + 1)]
		d[(un + 1):n] <- d[un]
	}
	den <- ((un - ln)/n)^2
	swv <- var(d)/den
	#got the scaled Winsorized variance
	rdf <- un - ln - 1
	rval <- qt(up, rdf) * sqrt(swv/n)
	tslo <- tstmn - rval
	tshi <- tstmn + rval
	##got low and high endpoints of robust T_S,n CI
	list(int = c(tslo, tshi), tp = tp)
}


cltv<-
function(gam = 0.5)
{
# Gets asy var for lts(h) and lta(h)at Cauchy C(0,1)
# where h/n -> gam.
	k <- tan((pi * gam)/2)
	num <- 2 * k - pi * gam
	den <- pi * (gam - (2 * k)/(pi * (1 + k^2)))^2
	ltsv <- num/den
	num <- gam
	den <- 4 * (1/pi - 1/(pi * (1 + k^2)))^2
	ltav <- num/den
	return(ltsv, ltav)
}

cmba2<-
function(x, csteps = 5, ii = 1)
{
# gets the covmba estimator using 98, 95, 90, 80, 70, 60 and 50% trimming
	n <- dim(x)[1]
	p <- dim(x)[2]
	mds <- matrix(nrow = n, ncol = 8, 0)	##get the DGK estimator
	covs <- var(x)
	mns <- apply(x, 2, mean)
	cmd <- sqrt(mahalanobis(x, mns, covs))	## concentrate
	for(i in 1:csteps) {
		md2 <- mahalanobis(x, mns, covs)
		medd2 <- median(md2)
		mns <- apply(x[md2 <= medd2,  ], 2, mean)
		covs <- var(x[md2 <= medd2,  ])
	}
	mds[, 8] <- sqrt(mahalanobis(x, mns, covs))
	covb <- covs
	mnb <- mns	##get the square root of det(covb)
	critb <- prod(diag(chol(covb)))	##get the resistant estimator
	covv <- diag(p)
	med <- apply(x, 2, median)
	md2 <- mahalanobis(x, center = med, covv)
	smd2 <- sort(md2)
	val <- p + 3
	tem <- 1:7
	tem[1] <- smd2[val + floor(0.02 * n)]
	tem[2] <- smd2[val + floor(0.05 * n)]
	tem[3] <- smd2[val + floor(0.1 * n)]
	tem[4] <- smd2[val + floor(0.2 * n)]
	tem[5] <- smd2[val + floor(0.3 * n)]
	tem[6] <- smd2[val + floor(0.4 * n)]
	tem[7] <- median(md2)
	medd2 <- tem[7]
	for(j in ii:7) {
## get the start
		val2 <- tem[j]
		mns <- apply(x[md2 <= val2,  ], 2, mean)
		covs <- var(x[md2 <= val2,  ])	## concentrate
		for(i in 1:csteps) {
			md2 <- mahalanobis(x, mns, covs)
			medd2 <- median(md2)
			mns <- apply(x[md2 <= medd2,  ], 2, mean)
			covs <- var(x[md2 <= medd2,  ])
		}
		mds[, j] <- sqrt(mahalanobis(x, mns, covs))
		plot(cmd, mds[, j])
		identify(cmd, mds[, j])
		crit <- prod(diag(chol(covs)))
		if(crit < critb) {
			critb <- crit
			covb <- covs
			mnb <- mns
		}
	}
	pairs(mds)	##scale for better performance at MVN
	rd2 <- mahalanobis(x, mnb, covb)
	const <- median(rd2)/(qchisq(0.5, p))
	covb <- const * covb
	list(center = mnb, cov = covb, mds = mds)
}

conc2<-
function(x, y, start = l1fit(x, y)$coef)
{   #Finds that LTA attractor of the start.
	nc <- dim(x)[2] + 1
	res <- y - (x %*% start[2:nc] + start[1])
	ares <- abs(res)
	cov <- ceiling(length(y)/2)
	m <- sort(ares, partial = cov)[cov]
	old <- sum(ares[ares <= m])
	new <- old - 1
	ct <- 0
	while(new < old) {
		ct <- ct + 1
		start <- l1fit(x[ares <= m,  ], y[ares <=
			m])$coef
		res <- y - (x %*% start[2:nc] + start[1
			])
		ares <- abs(res)
		m <- sort(ares, partial = cov)[cov]
		new <- sum(ares[ares <= m])	#print(old)
		if(new < old) {
			old <- new
			new <- new - 1
		}
	}
	list(coef = start, ct = ct)
}

concmv<-
function(n = 100, csteps = 5, gam = 0.4, outliers = TRUE, start = 2)
{
#Shows how concentration works when p = 2.
# Use start = 1 for DGK, start = 2 for MBA sphere, start = 3 for MBA MAD
	p <- 2	#A <- cbind(c(1, 0.9), c(0.9, 1))
	x <- matrix(rnorm(n * p), ncol = p, nrow = n)	#A <- diag(sqrt(1:p))
#if(outliers == T) {
# val <- floor(gam * n)
# tem <- 10 + 0 * 1:p
# x[1:val,  ] <- x[1:val,  ] + tem
#}
#x <- x %*% A
	A <- cbind(c(1, 0.4), c(0.4, 1))
	B <- cbind(c(0.5, 0), c(0, 0.5))
	if(outliers == T) {
		val <- floor(gam * n)
		x[(val + 1):n,  ] <- x[(val + 1):n,  ] %*% A
		x[1:val,  ] <- x[1:val,  ] %*% B
		x[1:val, 1] <- x[1:val, 1] + 0
		x[1:val, 2] <- x[1:val, 2] + 6
	}
	else {
		x <- x %*% A
	}
	if(start == 1) {
		covs <- var(x)
		mns <- apply(x, 2, mean)
	}
	if(start == 2) {
		covv <- diag(p)
		med <- apply(x, 2, median)
		md2 <- mahalanobis(x, center = med, covv)
		medd2 <- median(md2)	## get the start
		mns <- apply(x[md2 <= medd2,  ], 2, mean)
		covs <- var(x[md2 <= medd2,  ])
	}
	if(start >= 2) {
		tem <- apply(x, 2, mad)^2
		covv <- diag(tem)
		med <- apply(x, 2, median)
		md2 <- mahalanobis(x, center = med, covv)
		medd2 <- median(md2)	## get the start
		mns <- apply(x[md2 <= medd2,  ], 2, mean)
		covs <- var(x[md2 <= medd2,  ])
	}
## concentrate
	for(i in 1:csteps) {
		md2 <- mahalanobis(x, mns, covs)
		medd2 <- median(md2)
		mns <- apply(x[md2 <= medd2,  ], 2, mean)
		covs <- var(x[md2 <= medd2,  ])
		plot(x[, 1], x[, 2])
		points(x[md2 <= medd2, 1], x[md2 <= medd2, 2], pch = 15)
		identify(x[, 1], x[, 2])
	}
}

concsim<-
function(n = 100, p = 2, steps = 5, gam = 0.4, runs = 20)
{
# This Splus function is used to determine when the DD
# plot separates outliers from non-outliers for various starts.
	A <- sqrt(diag(1:p))
	mbact <- 0
	fmcdct <- 0
	mbct <- 0
	madct <- 0
	dgkct <- 0
	for(i in 1:runs) {
		x <- matrix(rnorm(n * p), ncol = p, nrow = n)
	## outliers have mean (10, 10 sqrt(2), ..., 10 sqrt(p))^T
		val <- floor(gam * n)
		tem <- 10 + 0 * 1:p
		x[1:val,  ] <- x[1:val,  ] + tem
		x <- x %*% A	#MBA
		out <- covmba(x, csteps = steps)
		center <- out$center
		cov <- out$cov
		rd2 <- mahalanobis(x, center, cov)
		if(min(rd2[1:val]) > max(rd2[(val + 1):n]))
mbact <- mbact + 1
	#DGK
		covs <- var(x)
		mns <- apply(x, 2, mean)	## concentrate
		for(i in 1:steps) {
			md2 <- mahalanobis(x, mns, covs)
			medd2 <- median(md2)
			mns <- apply(x[md2 <= medd2,  ], 2, mean)
			covs <- var(x[md2 <= medd2,  ])
		}
		rd2 <- mahalanobis(x, mns, covs)
		if(min(rd2[1:val]) > max(rd2[(val + 1):n])) dgkct <- dgkct + 1
	#Median Ball start
		covv <- diag(p)
		med <- apply(x, 2, median)
		md2 <- mahalanobis(x, center = med, covv)
		medd2 <- median(md2)	## get the start
		mns <- apply(x[md2 <= medd2,  ], 2, mean)
		covs <- var(x[md2 <= medd2,  ])	## concentrate
		for(i in 1:steps) {
			md2 <- mahalanobis(x, mns, covs)
			medd2 <- median(md2)
			mns <- apply(x[md2 <= medd2,  ], 2, mean)
			covs <- var(x[md2 <= medd2,  ])
		}
		rd2 <- mahalanobis(x, mns, covs)
		if(min(rd2[1:val]) > max(rd2[(val + 1):n])) mbct <- mbct + 1
	#MAD start
		tem <- apply(x, 2, mad)^2
		covv <- diag(tem)
		md2 <- mahalanobis(x, center = med, covv)
		medd2 <- median(md2)	## get the start
		mns <- apply(x[md2 <= medd2,  ], 2, mean)
		covs <- var(x[md2 <= medd2,  ])	## concentrate
		for(i in 1:steps) {
			md2 <- mahalanobis(x, mns, covs)
			medd2 <- median(md2)
			mns <- apply(x[md2 <= medd2,  ], 2, mean)
			covs <- var(x[md2 <= medd2,  ])
		}
		rd2 <- mahalanobis(x, mns, covs)
		if(min(rd2[1:val]) > max(rd2[(val + 1):n])) madct <- madct + 1
	#FMCD
		out <- cov.mcd(x)
		center <- out$center
		cov <- out$cov
		rd2 <- mahalanobis(x, center, cov)
		if(min(rd2[1:val]) > max(rd2[(val + 1):n]))
			fmcdct <- fmcdct + 1
	}
	list(mbact = mbact, fmcdct = fmcdct, dgkct = dgkct, mbct = mbct, madct
		 = madct)
}

corrsim<-
function(n = 100, p = 3, eps = 0.4, nruns = 100, type = 1)
{
#For R, first type "library(lqs)" before using this function
# This function generates 100 n by p matrices x.
# The output is the 100 sample correlations between the MDi and RDi
# RDi uses covmba for type = 1, rmba for type = 2, cov.mcd for type = 3
# mahalanobis gives squared Maha distances
	corrs <- 1:nruns
	for(i in 1:nruns) {
		wt <- 0 * (1:n)
		x <- matrix(rnorm(n * p), ncol = p, nrow = n)
	#The following 3 commands make x elliptically contoured.
#zu <- runif(n)
#x[zu < eps,] <- x[zu < eps,]*5
#x <- x^2
# To make marginals of x lognormal, use
#x <- exp(x)
		center <- apply(x, 2, mean)
		cov <- var(x)
		md2 <- mahalanobis(x, center, cov)
		if(type == 1) {
			out <- covmba(x)
		}
		if(type == 2) {
			out <- rmba(x)
		}
		if(type == 3) {
			out <- cov.mcd(x)
		}
		center <- out$center
		cov <- out$cov
		rd2 <- mahalanobis(x, center, cov)
	# need square roots for the usual distances
		md <- sqrt(md2)
		rd <- sqrt(rd2)
		const <- sqrt(qchisq(0.5, p))/median(rd)
		rd <- const * rd
	# wt[rd < sqrt(qchisq(0.975, p))] <- 1
#  corrs[i] <- cor(md[wt > 0], rd[wt > 0])}
		corrs[i] <- cor(md, rd)
	}
	cmean <- mean(corrs)
	cmin <- min(corrs)
	clt95 <- sum(corrs < 0.95)
	clt80 <- sum(corrs < 0.8)
	list(cmean = cmean, cmin = cmin, clt95 = clt95, clt80 = clt80,
		corrs = corrs)
}


covdgk<-
function(x, csteps = 10)
{
#computes the scaled DGK multivariate estimator
	p <- dim(x)[2]
	covs <- var(x)
	mns <- apply(x, 2, mean)	## concentrate
	for(i in 1:csteps) {
		md2 <- mahalanobis(x, mns, covs)
		medd2 <- median(md2)
		mns <- apply(x[md2 <= medd2,  ], 2,
			mean)
		covs <- var(x[md2 <= medd2,  ])
	}
##scale for consistency at MVN
	rd2 <- mahalanobis(x, mns, covs)
	const <- median(rd2)/(qchisq(0.5, p))
	covs <- const * covs
	list(center = mns, cov = covs)
}

cgen.bt <- function(n,p,r,alpha,asymp=FALSE){
#   find constants c1 and M that gives a specified breakdown r
#   and rejection point alpha
if (asymp == FALSE){if (r > (n-p)/(2*n) ) r <- (n-p)/(2*n)}
# maximum achievable breakdown
#
#   if rejection is not achievable, use c1=0 and best rejection
#
    limvec <- rejpt.bt.lim(p,r)
    if (1-limvec[2] <= alpha)
    {
        c1 <- 0
        M <- sqrt(qchisq(1-alpha,p))
    }
    else
    {
    c1.plus.M <- sqrt(qchisq(1-alpha,p))
    M <- sqrt(p)
    c1 <- c1.plus.M - M
    iter <- 1
    crit <- 100
    eps <- 1e-5
    while ((crit > eps)&(iter<100))
    {
        deps <- 1e-4
        M.old <- M
        c1.old <- c1
        er <- erho.bt(p,c1,M)
        fc <- er - r*(M^2/2+c1*(5*c1+16*M)/30)
        fcc1 <- (erho.bt(p,c1+deps,M)-er)/deps
        fcM  <- (erho.bt(p,c1,M+deps)-er)/deps
        fcp <- fcM - fcc1 - r*(M-(5*c1+16*M)/30+c1*9/30)
        M <- M - fc/fcp
        if (M >= c1.plus.M ){M <- (M.old + c1.plus.M)/2}
        c1 <- c1.plus.M - M
#        if (M-c1 < 0)  M <- c1.old+(M.old-c1.old)/2
        crit <- abs(fc)
        iter <- iter+1
    }
    }
list(c1=c1,M=M,r1=r)
}




# ----------------------------------------------------------------------------

# chbin2num

# ----------------------------------------------------------------------------

chbin2num<-function(x){
#
# Make sure x is binary and numeric
#
n=length(x)
v=rep(NA,n)
y=elimna(x)
chk=unique(y)
if(length(chk)!=2)stop('Should have binary data after NA removed')
M=max(chk)
id=x==M
v[id]=1
M=min(chk)
id=x==M
v[id]=0
v
}

chi.int.p <- function(p,a,c1)
  return( exp(lgamma((p+a)/2)-lgamma(p/2))*2^{a/2}*dchisq(c1^2,p+a)*2*c1 )




# ----------------------------------------------------------------------------

# chi.int2

# ----------------------------------------------------------------------------

chi.int2.p <- function(p,a,c1)
  return( -exp(lgamma((p+a)/2)-lgamma(p/2))*2^{a/2}*dchisq(c1^2,p+a)*2*c1 )




# ----------------------------------------------------------------------------

# chi.test.ind

# ----------------------------------------------------------------------------

chi.test.ind<-function(x){
#
# x is a matrix with r rows and c columns
# Goal: perform the chi-squared test of independence.
#
if(!is.matrix(x))stop("x should be a matrix")
n=sum(x)
phat=x/n
r=rowSums(x)
cc=colSums(x)
df=(nrow(x)-1)*(ncol(x)-1)
ex=outer(r,cc,"*")
val=sum(n*(x-ex/n)^2/ex)
pv=1-pchisq(val,df)
list(test.stat=val,p.value=pv)
}

chk4binary<-function(x){
#
#  x is a matrix or is a data frame
#  Determine which if my columns have binary data
#
if(is.null(x))stop('x should be a matrix or data frame')
p=ncol(x)
v=apply(x,2,unique)
L=lapply(v,length)
id=which(L==2)
id
}

 bin2binary.IV<-function(x,id1=1,id2=2){
#
# Have two binary IVs in col iid1 and id2. Values assumed to be 0 and 1
# Sort the data into four groups corresponding to
#  0,0  1 0  0,1 and 1,1
# Store results in list mode.
#
# Result:  for col1 id2 of  x
#              v[[1]] contains data corresponding to 0 0
#               v[[2]]  contains data corresponding to 0 1.
#  For col id1 ifx
#              v[[3]] contains data corresponding to 0 0
#               v[[4]  contains data corresponding to 0 1.
#
v=list()
v[[1]]=binmat2v(x,c(id1,id2),c(0,0),c(0,0))
v[[2]]=binmat2v(x,c(id1,id2),c(0,0),c(1,1))
v[[3]]=binmat2v(x,c(id1,id2),c(1,1),c(0,0))
v[[4]]=binmat2v(x,c(id1,id2),c(1,1),c(1,1))
v
}




# ----------------------------------------------------------------------------

# ci2bin

# ----------------------------------------------------------------------------

ci2bin<-function(r1=sum(x),n1=length(x),r2=sum(y),n2=length(y),x=NA,y=NA,alpha=0.05){
#
# Compute a confidence interval for the
# difference between probability of success
# for two independent binomials
#
# r1=number of successes in group 1
# n1=number of observations in group 1
#
cr<-qchisq(1-alpha,1)
p1<-r1/n1
p2<-r2/n2
a<-p1+p2
b<-p1-p2
u<-.25*(1/n1+1/n2)
v<-.25*(1/n1-1/n2)
V<-u*((2-a)*a-b^2)+2*v*(1-a)*b
A<-sqrt(cr*(V+cr*u^2*(2-a)*a+cr*v^2*(1-a)^2))
B<-(b+cr*v*(1-a))/(1+cr*u)
ci<-NA
ci[1]<-B-A/(1+cr*u)
ci[2]<-B+A/(1+cr*u)
list(ci=ci)
}
cjMAT<-function(J){
L=(J^2-J)/2
cj=matrix(0,nrow=L,ncol=J)
ic=0
for(j in 1:J){
for(k in 1:J){
if(j<k){
ic=ic+1
cj[ic,j]=1
cj[ic,k]=-1
}}}
cj
}

Ckappa<-function (x,fleiss=FALSE,w = NULL){
#
#  compute Cohen's kappa
#  if fleiss=T, compute weighted kappa with Fleiss weights if w=NULL
#  if fleiss=F, w=.5^|i-j| is used.
#  if argument w contains weights, they are used
#
if(!is.matrix(x))stop("x should be a square matrix")
if(ncol(x)!=nrow(x))stop("x should be a square matrix")
    p <- dim(x)[2]
    x <- as.matrix(x)
    tot <- sum(x)
    x <- x/tot
    rs <- rowSums(x)
    cs <- colSums(x)
    prob <- rs %*% t(cs)
    po <- sum(diag(x))
    pc <- sum(diag(prob))
    kappa <- (po - pc)/(1 - pc)
    if (is.null(w)) {
v=outer(c(1:p),c(1:p),"-")
w=outer(c(1:p),c(1:p),"-")
if(fleiss)w=1-w^2/(p-1)^2
if(!fleiss)w=.5^abs(w)
}
    weighted.prob <- w * prob
    weighted.obser <- w * x
    wpo <- sum(weighted.obser)
    wpc <- sum(weighted.prob)
    wkappa <- (wpo - wpc)/(1 - wpc)
    return(list(kappa = kappa, weighted.kappa = wkappa))
}




# ----------------------------------------------------------------------------

# clnorm

# ----------------------------------------------------------------------------

clnorm<-function(n,epsilon=.1,k=10){
#
# generate n observations from a contaminated lognormal
# distribution
#
#  Using default values, median is approximately 1.14 and 20% trimmed mean is 1.33
if(epsilon>1)stop('epsilon must be less than or equal to 1')
if(epsilon<0)stop('epsilon must be greater than or equal to 0')
if(k<=0)stop('k must be greater than 0')
val<-rlnorm(n)
uval<-runif(n)
flag<-(uval<=1-epsilon)
val[!flag]<-k*val[!flag]
val
}
cnorm<-function(n,epsilon=.1,k=10){
#
# generate n observations from a contaminated normal
# distribution
# probability 1-epsilon from a standard normal
# probability epsilon from normal with mean 0 and standard deviation k
#
if(epsilon>1)stop("epsilon must be less than or equal to 1")
if(epsilon<0)stop("epsilon must be greater than or equal to 0")
if(k<=0)stop("k must be greater than 0")
val<-rnorm(n)
uval<-runif(n)
flag<-(uval<=1-epsilon)
val[!flag]<-k*val[!flag]
val
}
cobs2g<-function(x1,y1,x2,y2,xlab="X",ylab="Y",qval=.5,xout=FALSE,outfun=out,...){
#
# Plot two regression lines, estimated via COBS
# (quantile regression using B-splines)
#
if(xout){
flag<-outfun(x1,...)$keep
x1<-x1[flag]
y1<-y1[flag]
flag<-outfun(x2,...)$keep
x2<-x2[flag]
y2<-y2[flag]
}
library(cobs)
xy=elimna(cbind(x1,y1))
x1=xy[,1]
xord=order(x1)
x1=x1[xord]
y1=xy[xord,2]
xy=elimna(cbind(x2,y2))
x2=xy[,1]
xord=order(x2)
x2=x2[xord]
y2=xy[xord,2]
plot(c(x1,x2),c(y1,y2),type="n",xlab=xlab,ylab=ylab)
temp1=cobs(x1,y1,print.mesg=FALSE,print.warn=FALSE,tau=qval)
temp2=cobs(x2,y2,print.mesg=FALSE,print.warn=FALSE,tau=qval)
points(x1,y1)
points(x2,y2,pch="+")
lines(x1,temp1$fitted)
lines(x2,temp2$fitted,lty=2)
}




# ----------------------------------------------------------------------------

# coefalpha

# ----------------------------------------------------------------------------

coefalpha<-function(x){
library(psych)
x=elimna(x)
res=alpha(x)
res
}


cohen2xi<-function(delta){
xi=sqrt((2*delta^2)/(4+delta^2))
xi
}




# ----------------------------------------------------------------------------

# com2gfun

# ----------------------------------------------------------------------------

com2gfun<-function(x,y,est=tmean,tr=.2,alpha=.05,SEED=TRUE,nboot=2000,method=c('Y','PB','CID','BM')){
#
#
# Y=Yuen
# PB=Percentile bootstrap
# CID= Cliff's
# BM = Brunner--Munzel
#
type=match.arg(method)
switch(type,
Y=yuen(x,y,tr=tr,alpha=alpha),
PB=pb2gen(x,y,est=est,alpha=alpha,SEED=SEED,nboot=nboot),
CID=cidv2(x,y,alpha=alpha),
BM=bmp(x,y,alpha=alpha))
}

comdsd.mcp<-function(x,method='holm',tr=0){
#
#  Compare the variances of J depenent variables.
#  Perform all pairwise comparisons using the HC4 extension of the Morgan-Pitman test
#
if(is.null(dim(x)))stop('x should be a matrix or data frame')
J=ncol(x)
CC=(J^2-J)/2
output<-matrix(0,CC,9)
dimnames(output)<-list(NULL,c('V1','V2','Var.1','Var.2','n','SD.1','SD.2','p.value','Adj.p.value'))
ic=0
for (j in 1:J){
for (k in 1:J){
if (j < k){
ic=ic+1
a=comdvar(x[,j],x[,k])
output[ic,1]=j
output[ic,2]=k
output[ic,3]=a$est1
output[ic,4]=a$est2
output[ic,5]=length(x[,j])
output[ic,6]=sqrt(a$est1)
output[ic,7]=sqrt(a$est2)
output[ic,8]=a$p.value
}}}
output[,9]=p.adjust(output[,8],method=method)
output
}




# ----------------------------------------------------------------------------

# comdvar

# ----------------------------------------------------------------------------

comdvar<-function(x,y,alpha=.05){
#
# Test the hypothesis that two dependent variables have equal variances.
# A heteroscedastic version of the Morgan-Pitman test is used.
# (The HC4 estimator is used to deal with heteroscedasticity)
#
xy=elimna(cbind(x,y))
est1=var(xy[,1])
est2=var(xy[,2])
pv=pcorhc4(xy[,1]-xy[,2],xy[,1]+xy[,2],alpha=alpha)
list(p.value=pv$p.value, est1=est1, est2=est2,test.stat=pv$test.stat)
}




# ----------------------------------------------------------------------------

# comdvar.astig

# ----------------------------------------------------------------------------

comdvar.astig<-function(m1,m2,method='holm'){
#
#  This function is designed specifically for dealing with astigmatism.
#
#  m= matrix or data frame, four columns
#
# compare variances of m[,1] vs m[,3] as well as m[,2] vs m[,4]
#
output=matrix(NA,2,5)
a=comdvar(m1[,1], m2[,1])
output[1,1]=a$est1
output[1,2]=a$est2
output[1,3]=a$est1/a$est2
output[1,4]=a$p.value
a=comdvar(m1[,2], m2[,2])
output[2,1]=a$est1
output[2,2]=a$est2
output[2,3]=a$est1/a$est2
output[2,4]=a$p.value
output[,5]=p.adjust(output[,4],method=method)
dimnames(output)=list(NULL,c('VAR 1','VAR 2','Ratio','p.value','p.adjusted') )
output
}




# ----------------------------------------------------------------------------

# comdvar.mcp

# ----------------------------------------------------------------------------

comdvar.mcp<-function(x,method='hoch'){
#
#  Compare the variances of J depenent variables.
#  Perform all pairwise comparisons using the HC4 extension of the Morgan-Pitman test
#
if(is.null(dim(x)))stop('x should be a matrix or data frame')
J=ncol(x)
CC=(J^2-J)/2
output<-matrix(0,CC,7)
dimnames(output)<-list(NULL,c('Var','Var','Est. 1','Est 2','Dif','p.value','Adj.p.value'))
ic=0
for (j in 1:J){
for (k in 1:J){
if (j < k){
ic=ic+1
a=comdvar(x[,j],x[,k])
output[ic,1]=j
output[ic,2]=k
output[ic,3]=a$est1
output[ic,4]=a$est2
output[ic,5]=a$est1-a$est2
output[ic,6]=a$p.value
}}}
output[,7]=p.adjust(output[,6],method=method)
output
}




# ----------------------------------------------------------------------------

# comsvm.best

# ----------------------------------------------------------------------------

comsvm.best<-function(x1,x2,MISS=FALSE,TABLE=FALSE,cost=1){
#
# try all possible kernels and report the best
#
# Example:  k(x,y) = exp( -||x-y||^2) is the Radial  kernel
#           k(x,y) = tanh(ax_i^Tx_i+r) is sigmoid kernel, a and r are parameters
#           k(x,y) = (ax_i^Tx_i+r)^d  polynomial
#
x1=as.matrix(x1)
x2=as.matrix(x2)
kvals=c('radial','linear','polynomial','sigmoid')
pcd.est=NA
for(j in 1:4)pcd.est[j]=comdepthsvm(x1,x2,kernel=kvals[j],cost=cost)$est.prob
id=which(pcd.est==max(pcd.est))
fin=comdepthsvm(x1,x2,MISS=MISS,kernel=kvals[id],cost=cost,TABLE=TABLE)
list(best.kernel=kvals[id],est.prob=fin$est.prob,
miss.class.vectors=fin$miss.class.vectors,TABLE=fin$TABLE)
}




# ----------------------------------------------------------------------------

# comsvm.bestv2

# ----------------------------------------------------------------------------

comsvm.bestv2<-function(x1,x2,MISS=FALSE,TABLE=FALSE,cost=1){
#
# try all possible kernels and report the best
#
# Example:  k(x,y) = exp( -||x-y||^2) is the Radial  kernel
#           k(x,y) = tanh(ax_i^Tx_i+r) is sigmoid kernel, a and r are parameters
#           k(x,y) = (ax_i^Tx_i+r)^d  polynomial
#
# if(length(cost)==1)stop('Use comsvm.best when there is only one value for cost')
x1=as.matrix(x1)
x2=as.matrix(x2)
kvals=c('radial','linear','polynomial','sigmoid')
idcost=1
idk=1
flag=TRUE
best=comdepthsvm(x1,x2,kernel=kvals[1],cost=cost[1])$est.prob
for(k in 1:length(cost)){
for(j in 1:4){
new=comdepthsvm(x1,x2,kernel=kvals[j],cost=cost[k])$est.prob
if(new>best){
best=new
idk=j
idcost=k
}}}
# Now check to see whether cost matters for the kernel that is being used
temp=NA
for(k in 1:length(cost))temp[k]=comdepthsvm(x1,x2,kernel=kvals[idk],cost=cost[k])$est.prob
if(var(temp)==0)flag=FALSE
fin=comdepthsvm(x1,x2,MISS=MISS,best.kernel=kvals[idk],best.cost=cost[idc],TABLE=TABLE)
list(best.kernel=kvals[idk],best.cost=cost[idcost],est.prob=fin$est.prob,cost.matters=flag,
miss.class.vectors=fin$miss.class.vectors,TABLE=fin$TABLE)
}




# ----------------------------------------------------------------------------

# comvar2

# ----------------------------------------------------------------------------

comvar2<-function(x,y,nboot=1000,SEED=TRUE){
#
#  Compare the variances of two independent groups.
#
x<-x[!is.na(x)]  # Remove missing values in x
y<-y[!is.na(y)]  # Remove missing values in y
# set seed of random number generator so that
# results can be duplicated.
est1=var(x)
est2=var(y)
sig<-est1-est2
if(SEED)set.seed(2)
nmin<-min(length(x),length(y))
datax<-matrix(sample(x,size=nmin*nboot,replace=TRUE),nrow=nboot)
datay<-matrix(sample(y,size=nmin*nboot,replace=TRUE),nrow=nboot)
v1<-apply(datax,1,FUN=var)
v2<-apply(datay,1,FUN=var)
boot<-v1-v2
boot<-sort(boot)
        ilow <- 15
        ihi <- 584
        if(nmin < 250) {
                ilow <- 13
                ihi <- 586
        }
        if(nmin < 180) {
                ilow <- 10
                ihi <- 589
        }
        if(nmin < 80) {
                ilow <- 7
                ihi <- 592
        }
        if(nmin < 40) {
                ilow <- 6
                ihi <- 593
        }
ilow<-round((ilow/599)*nboot)
ihi<-round((ihi/599)*nboot)
ci<-c(boot[ilow+1],boot[ihi])
list(n=c(length(x),length(y)),ci=ci,est.1=est1,est.2=est2,vardif=sig,ratio=est1/est2)
}


con2by2A<-function(J,K){
#
# For J by K design, for every two rows and two columns,
# create contrast coefficients for main effect for Factor A.
#
JK=J*K
Ja<-(J^2-J)/2
Ka<-(K^2-K)/2
conAB<-matrix(0,nrow=JK,ncol=Ka*Ja)
ic<-0
for(j in 1:J){
for(jj in 1:J){
if(j < jj){
for(k in 1:K){
for(kk in 1:K){
if(k<kk){
ic<-ic+1
mat<-matrix(0,nrow=J,ncol=K)
mat[j,k]<-1
mat[j,kk]<-1
mat[jj,k]<-0-1
mat[jj,kk]<-0-1
}
conAB[,ic]<-t(mat)
}}}}}
list(conA=conAB)
}




# ----------------------------------------------------------------------------

# con2by2B

# ----------------------------------------------------------------------------

con2by2B<-function(J,K){
#
# For J by K design, for every two rows and two columns,
# create contrast coefficients for main effect for Factor B.
#
JK=J*K
Ja<-(J^2-J)/2
Ka<-(K^2-K)/2
conAB<-matrix(0,nrow=JK,ncol=Ka*Ja)
ic<-0
for(j in 1:J){
for(jj in 1:J){
if(j < jj){
for(k in 1:K){
for(kk in 1:K){
if(k<kk){
ic<-ic+1
mat<-matrix(0,nrow=J,ncol=K)
mat[j,k]<-1
mat[j,kk]<-0-1
mat[jj,k]<-1
mat[jj,kk]<-0-1
}
conAB[,ic]<-t(mat)
}}}}}
list(conB=conAB)
}




# ----------------------------------------------------------------------------

# conCON

# ----------------------------------------------------------------------------

conCON<-function(J,conG=1){
#
#  Create contrast coefficients for comparisons to a controll
#
#  J = number of groups including the control group.
#  conG = the group that is the control. By default, assume group 1 is the control
#
Jm1=J-1
A=matrix(rep(1,Jm1^2+Jm1),nrow=J)
A[-conG,]=-1*diag(Jm1)
if(conG>1)A=-1*A
list(conCON=A)
}

contab<-function(dat,alpha=.05){
# dat is a 2-by-2 contingency table (matrix)
# Goal: compare the marginal probability of the first variable (e.g. Time 1)
# to the marginal probability of the second variable (e.g. Time 2).
# Issue: do the probabilities change from time 1 to time 2.
#
phat=dat
n=sum(phat)
phat=phat/n
p1.=phat[1,1]+phat[1,2]
p.1=phat[1,1]+phat[2,1]
del=p1.-p.1
sigsq=p1.*(1-p1.)+p.1*(1-p.1)-2*(phat[1,1]*phat[2,2]-phat[1,2]*phat[2,1])
sig=sqrt(sigsq/n)
test=abs(del)/sig
pv=2*(1-pnorm(test))
ci=del-qnorm(1-alpha/2)*sig
ci[2]=del+qnorm(1-alpha/2)*sig
list(s.e.=sig,delta=del,CI=ci,p.value=pv)
}




# ----------------------------------------------------------------------------

# COR.PAIR

# ----------------------------------------------------------------------------

COR.PAIR<-function(x,y,method=c('WIN','PB','skip','mve','mcd','Ken','Spear','BIC'),skip.cor=pcor,tr=.2,...){
#
# For the bivariat case, compute a correlation
#
# WIN: Winsorized
# PB:  Percentage Bend
# skip: Skipped correlation based on projection-type outlier detection method
# mve:  minimum volume ellipsoid
# mcd:  minimum covariance determinant
# Ken: Kendall's tau
# Spear: Spearman's rho
# BIC:  biweight correlation.
#
x=cbind(x,y)
type=match.arg(method)
switch(type,
    WIN=list(cor=winall(m=x,tr=tr)$cor[1,2]),
    PB=list(cor=pball(m=x,...)$pbcorm[1,2]),
    skip=list(cor=scor(x=x,corfun=skip.cor)$cor),
    mve=list(cor=MVECOR(x=x)[1,2]),
    mcd=list(cor=MCDCOR(x=x)[1,2]),
    Ken=list(cor=tauall(m=x)$taum[1,2]),
    Spear=list(cor=spear(x=x)$cor[1,2]),
    BIC=list(cor=bicovm(x)$mcor[1,2]),
    )
}

COR.ROB<-function(x,method=c('WIN','PB','skip','mve','mcd','Ken','Spear','BIC'),tr=.2,...){
#
#
# WIN: Winsorized
# PB:  Percentage Bend
# skip: Skipped correlation based on projection-type outlier detection method
# mve:  minimum volume ellipsoid
# mcd:  minimum covariance determinant
# Ken: Kendall's tau
# Spear: Spearman's rho
# BIC:  biweight correlation.
#
type=match.arg(method)
switch(type,
    WIN=winall(m=x,tr=tr)$cor,
    PB=pball(m=x,...)$pbcorm,
    skip=scorall(x=x,...),
    mve=MVECOR(x=x),
    mcd=MCDCOR(x=x),
    Ken=tauall(m=x)$taum,
    Spear=spear(x=x)$cor,
    BIC=bicovm(x)$mcor,
    )
}




# ----------------------------------------------------------------------------

# cor2xy

# ----------------------------------------------------------------------------

cor2xy<-function(x,y,corfun=spear,...){
 est1=corfun(x[,1],y,...)$cor
 est2=corfun(x[,2],y,...)$cor
 list(cor=c(est1,est2))
 }
regciCV2G.sub<-function(xy,regfun,null.value=0,pts=NULL,npts=100,...){
 pv=regYci2Gv2(xy[,1],xy[,2],xy[,3],xy[,4],SEED=FALSE,regfun=regfun,null.value=null.value,plotit=FALSE,
npts=npts,pts=pts,...)$output[,5]
 min(pv)
}

# To avoid nested calls, need:

corbsub<-function(isub,x,y,corfun,...){
#
#  Compute correlation for x[isub] and y[isub]
#  isub is a vector of length n,
#  a bootstrap sample from the sequence of integers
#  1, 2, 3, ..., n
#
#  This function is used by other functions when computing
#  bootstrap estimates.
#
#  corfun is some correlation function already stored in R
#
corbsub<-corfun(x[isub],y[isub],...)$cor
corbsub
}



cumrelf<-function(x,y=NA,xlab='X',ylab='CUM REL FREQ',pr.freq=FALSE){
#
#  plot the cumulative relative frequencies for 1 or more groups
#
#  x can be a matrix, columns corresponding to groups, or x
#  x can have list mode.
#  y=NA, if data are stored in y, it is assumes there two groups
#  with data for the second group stored in y
#
xu=NULL
cf=NULL
if(!is.na(y[1])){
xx=list()
xx[[1]]=x
xx[[2]]=y
x=xx
}
if(is.matrix(x) || is.data.frame(x))x=listm(x)
if(length(x)==1)stop('This function is designed for two or more groups')
x=elimna(x)
for(j in 1:length(x)){
z=splot(x[[j]],plotit=FALSE)
xu=c(xu,sort(unique(x[[j]])))
cf=c(cf,cumsum(z$frequencies)/length(x[[j]]))
}
plot(xu,cf,,type='n',xlab=xlab,ylab=ylab)
for(j in 1:length(x)){
z=splot(x[[j]],plotit=FALSE)
if(pr.freq)print(z)
lines(sort(unique(x[[j]])),cumsum(z$frequencies)/length(x[[j]]),lty=j)
}
}
cumrelfT<-function(x,y,pts=NULL,q=c(.1,.25,.5,.75,.9),xlab='X',ylab='CUM REL FREQ',plotit=TRUE,
op=1){
#
#  Compare the cumulative relative frequencies for 2 independent groups
#  based on the values in pts.
#
#  x and y are vectors.
#
#  op=1 use twobinom
#  op=2 usd bi2KMS

x=elimna(x)
y=elimna(y)
if(is.null(pts)){
for(i in 1:length(q))pts[i]=qest(x,q[i])
}
output=matrix(NA,nrow=length(pts),ncol=5)
for(j in 1:length(pts)){
flag1=x<=pts[j]
flag2=y<=pts[j]
temp=NULL
if(op==1)temp=twobinom(x=flag1,y=flag2)
if(op==2)temp=bi2KMSv2(x=flag1,y=flag2)
if(is.null(temp))stop('op should be equal to 1 or 2')
output[j,2]=temp$p1
output[j,3]=temp$p2
output[j,4]=temp$est.dif
output[j,5]=temp$p.value
}
output[,1]=pts
if(plotit){
m=list()
m[[1]]=x
m[[2]]=y
cumrelf(m,xlab=xlab,ylab=ylab)
}
#output[,6]=p.adjust(output[,5],method='hoch') # can beat this adjusted p-value
#dimnames(output)=list(NULL,c('pts','est.p1','est.p2','est.dif','p.value','p.adjusted'))
dimnames(output)=list(NULL,c('pts','est.p1','est.p2','est.dif','p.value'))
output
}
D.akp.effect<-function(x,y=NULL,null.value=0,tr=.2){
#
# Computes the robust effect size for one-sample case using
# a simple modification of
# Algina, Keselman, Penfield Pcyh Methods, 2005, 317-328
#
#  When comparing two dependent groups, data for the second group can be stored in
#  the second argument y. The function then computes the difference scores x-y
#
if(!is.null(y))x=x-y
x<-elimna(x)
s1sq=winvar(x,tr=tr)
cterm=1
if(tr>0)cterm=area(dnormvar,qnorm(tr),qnorm(1-tr))+2*(qnorm(tr)^2)*tr
cterm=sqrt(cterm)
dval<-cterm*(tmean(x,tr=tr)-null.value)/sqrt(s1sq)
dval
}


D.akp.effect.ci<-function(x,y=NULL,null.value=0,alpha=.05,tr=.2,nboot=1000,SEED=TRUE){
#
# Computes the robust effect size for one-sample case using
# a simple modification of
# Algina, Keselman, Penfield Pcyh Methods, 2005, 317-328
#
#  When comparing two dependent groups, data for the second group can be stored in
#  the second argument y. The function then computes the difference scores x-y
if(SEED)set.seed(2)
if(!is.null(y))x=x-y
x<-elimna(x)
a=D.akp.effect(x=x,tr=tr,null.value=null.value)
v=NA
for(i in 1:nboot){
X=sample(x,replace=TRUE)
v[i]=D.akp.effect(X,tr=tr,null.value=null.value)
}
v=sort(v)
ilow<-round((alpha/2) * nboot)
ihi<-nboot - ilow
ilow<-ilow+1
ci=v[ilow]
ci[2]=v[ihi]
pv=mean(v<0)+.5*mean(v==0)
pv=2*min(pv,1-pv)
list(Effect.Size=a,ci=ci,p.value=pv)
}




# ----------------------------------------------------------------------------

# dat2dif

# ----------------------------------------------------------------------------

dat2dif<-function(x){
#
# x is assumed to be a matrix or data frame with at least 2 columns
#
#  For J dependent groups, compute all pairwise differences and return the results
#
#
if(is.null(dim(x)))stop('x should be a matrix or data frame')
ic=0
J=ncol(x)
n=nrow(x)
N=(J^2-J)/2
ic=0
dif=matrix(NA,nrow=n,ncol=N)
for(j in 1:J){
for(k in 1:J){
if(j<k){
ic=ic+1
dif[,ic]=x[,j]-x[,k]
}}}
dif
}


dat2form<-function(x,alpha=.05){
#
#  Perform Fisher's LSD method
# x is assumed to be a matrix, or data frame, or to have list mode
#
if(is.matrix(x) || is.data.frame(x))x=listm(x)
n=lapply(x,length)
J=length(x)
g=NULL
X=NULL
for(j in 1:J){
g=c(g,rep(j,n[j]))
X=c(X,x[[j]])
}
g=as.factor(g)
list(x=X,g=g)
}




# ----------------------------------------------------------------------------

# dbetabin

# ----------------------------------------------------------------------------

dbetabin<-function(x,n,r,s){
#
# probability function for the beta-binomial distribution
#
v=lgamma(n+1)+lgamma(r+x)+lgamma(n+s-x)+lgamma(r+s)-lgamma(x+1)-lgamma(n-x+1)-lgamma(r+s+n)-lgamma(r)-lgamma(s)
v=exp(v)
v
}

regci.inter<-
function(x,y, regfun = tsreg, ID=c(1,2),nboot = 599, alpha = 0.05,method='PRO',
SEED = TRUE,LABELS=FALSE,not.out=NULL,CI=NULL,MC=FALSE,
    pr = TRUE, xout = FALSE, outfun = outpro, ...){

# A function for dealing with the usual regression interaction
# model where the product of  two independent variables is used.
#
#. ID indicates the two columns for which a product term is used.
# The function rearranges the data so that the first two columns are the two
# the will be used with their product.
#
# CI : columns ignored when checking for outliers and when using method='DUM'
#
#. When checking for leverage points, argument
#  method indicate the method used;
#PRO=outpro(x,plotit=plotit),         # projection method
#    PRO.R=outpro.depth(x),   #projection method   random, lower execution time vs outpro
#    BLP=outblp(x,y,regfun=regfun,plotit=FALSE),       # regression method
#    DUM=out.dummy(x,y,outfun=outpro.depth,id=id),   #   Detect outliers ignoring  col indicated by #argument id
#    MCD=outDETMCD(x,plotit=plotit),      use   robust analog of Mahalanobis distance
 #   BOX=outbox(x) boxplot (univariate only).
#
if(pr){
print('Assume the first two columns of x are to be used for an interaction. ')
print('If not the case, use  argument ID to indicate the appropriate columns')
}
p=ncol(x)
p11=p+1
p22=p+2
p1=p+3
if(is.null(p))stop('Should have two or more independent variables')
xy<-cbind(x,y)
xy<-elimna(xy)
x<-xy[,1:p]
y<-xy[,p11]
xsub=x[,-ID]
xID=x[,ID]
x=cbind(xID,xsub)
if(xout){
if(identical(method,'DUM')){
if(is.null(CI))stop('When using DUM, must specify the columns to be ignored via CI')
}
KE=out.methods(x,y,method=method,id=CI)$keep
x=x[KE,]
y=y[KE]
}
xx=cbind(x[,1:2],x[,1]*x[,2])
if(p>2)xx=cbind(x,x[,3:p])
vlabs=c('Intercept','x1','x2','x1*x2')
if(p>2){
p4=p+2
for(j in 5:p4)vlabs[j]=paste('x',j-1)
if(LABELS)vlabs[5:p4]=labels(x)[[2]][3:p]
}
clabs=c('ci.low','ci.up','Estimate','S.E.','p-value','p.adj')
if(!MC)a=regci(xx,y,regfun = regfun, nboot = nboot, alpha =alpha, SEED = SEED, pr =pr,...)
if(MC)a=regciMC(xx,y,regfun = regfun, nboot = nboot, alpha =alpha, SEED = SEED, pr =pr,...)
output=a$regci
dimnames(output)=list(vlabs,clabs)
list(output=output,n=a$n,n.keep=a$n.keep)
}

ddep<-function(x,est=onestep,alpha=.05,grp=NA,nboot=500,plotit=TRUE,SEED=TRUE,pr=TRUE,WT=TRUE,...){
#
#   Do ANOVA on dependent groups
#   using the partially centered method plus
#   depth of zero among  bootstrap values.
#
#   An improved version of ddep that better handles heteroscedasticity
#   (A weighted grand mean is used in this version.)
#
#   The data are assumed to be stored in x in list mode
#   or in a matrix. In the first case
#   x[[1]] contains the data for the first group, x[[2]] the data
#   for the second group, etc. Length(x)=the number of groups = J.
#   If stored in a matrix, columns correspond to groups.
#
#   grp is used to specify some subset of the groups, if desired.
#   By default, all J groups are used.
#
#   The default number of bootstrap samples is nboot=2000
#
if(pr)print("Warning: Might not be level robust if the number of groups is relatively large and n is small")
if(pr)print("Currently seems that rmmismcp is preferable")
if(is.list(x)){
nv<-NA
for(j in 1:length(x))nv[j]<-length(x[[j]])
if(var(nv) !=0){
stop("The groups are stored in list mode and appear to have different sample sizes")
}
temp<-matrix(NA,ncol=length(x),nrow=nv[1])
for(j in 1:length(x))temp[,j]<-x[[j]]
x<-temp
}
J<-ncol(x)
if(!is.na(grp[1])){ #Select the groups of interest
J<-length(grp)
for(j in 1:J)temp[,j]<-x[,grp[j]]
x<-temp
}
x<-elimna(x) # Remove any rows with missing values.
bvec<-matrix(0,ncol=J,nrow=nboot)
hval<-vector("numeric",J)
if(SEED)set.seed(2) # set seed of random number generator so that
#             results can be duplicated.
print("Taking bootstrap samples. Please wait.")
n<-nrow(x)
totv<-apply(x,2,est,na.rm=TRUE,...)
data<-matrix(sample(n,size=n*nboot,replace=TRUE),nrow=nboot)
for(ib in 1:nboot)bvec[ib,]<-apply(x[data[ib,],],2,est,na.rm=TRUE,...) #nboot by J matrix
if(!WT){
gv<-rep(mean(totv),J) #Grand mean
#m1<-rbind(bvec,gv)
}
bplus<-nboot+1
center<-totv
cmat<-var(bvec)
if(WT){
wt=1/diag(cmat)
ut=sum(wt)
gv<-rep(sum(wt*totv)/ut,J) #Grand mean
}
m1<-rbind(bvec,gv)
discen<-mahalanobis(m1,totv,cmat)
#print("Bootstrap complete; computing significance level")
if(plotit && ncol(x)==2){
plot(bvec,xlab="Group 1",ylab="Group 2")
temp.dis<-order(discen[1:nboot])
ic<-round((1-alpha)*nboot)
xx<-bvec[temp.dis[1:ic],]
xord<-order(xx[,1])
xx<-xx[xord,]
temp<-chull(xx)
lines(xx[temp,])
lines(xx[c(temp[1],temp[length(temp)]),])
abline(0,1)
}
sig.level<-sum(discen[bplus]<=discen)/bplus
list(p.value=sig.level,center=totv,grand.mean=gv)
}




# ----------------------------------------------------------------------------

# ddeptr

# ----------------------------------------------------------------------------

ddeptr<-function(x,na.rm=TRUE,alpha=.05,grp=NA,nboot=500,plotit=TRUE,SEED=TRUE,op=FALSE,tr=.2,...){
#
#   Do ANOVA on dependent groups
#   using the partially centered method plus
#   depth of zero among  bootstrap values.
#
#  The method is like the method used by the R function ddep,
#  but a weighted estimate of the grand mean is used.
#  This helps deal the heteroscedasticity among the marginal distributions.
#
#   The data are assumed to be stored in x in list mode
#   or in a matrix. In the first case
#   x[[1]] contains the data for the first group, x[[2]] the data
#   for the second group, etc. Length(x)=the number of groups = J.
#   If stored in a matrix, columns correspond to groups.
#
#  trimmed means are compared
#
#   grp is used to specify some subset of the groups, if desired.
#   By default, all J groups are used.
#
#   The default number of bootstrap samples is nboot=500
#
#   na.rm=T, all rows of data with missing values are removed.
#   na.rm=F will use all of the data assuming missing values occur at random
#
if(is.list(x)){
nv<-NA
for(j in 1:length(x))nv[j]<-length(x[[j]])
if(var(nv) !=0){
stop("The groups are stored in list mode and appear to have different sample sizes")
}
temp<-matrix(NA,ncol=length(x),nrow=nv[1])
for(j in 1:length(x))temp[,j]<-x[[j]]
x<-temp
}
J<-ncol(x)
if(!is.na(grp[1])){ #Select the groups of interest
J<-length(grp)
for(j in 1:J)temp[,j]<-x[,grp[j]]
x<-temp
}
if(na.rm)x<-elimna(x) # Remove any rows with missing values.
bvec<-matrix(0,ncol=J,nrow=nboot)
hval<-vector("numeric",J)
if(SEED)set.seed(2) # set seed of random number generator so that
#             results can be duplicated.
if(op)print("Taking bootstrap samples. Please wait.")
n<-nrow(x)
wt=apply(x,2,trimse,...)
wt=1/wt^2
ut=sum(wt)
totv<-apply(x,2,tmean,na.rm=TRUE,...)
gv<-rep(sum(wt*totv)/ut,J) #Weighted grand mean
data<-matrix(sample(n,size=n*nboot,replace=TRUE),nrow=nboot)
for(ib in 1:nboot)bvec[ib,]<-apply(x[data[ib,],],2,tmean,na.rm=TRUE,...) #nboot by J matrix
bplus<-nboot+1
m1<-rbind(bvec,gv)
center<-totv
cmat<-var(bvec)
discen<-mahalanobis(m1,totv,cmat)
if(op)print("Bootstrap complete; computing significance level")
if(plotit && ncol(x)==2){
plot(bvec,xlab="Group 1",ylab="Group 2")
temp.dis<-order(discen[1:nboot])
ic<-round((1-alpha)*nboot)
xx<-bvec[temp.dis[1:ic],]
xord<-order(xx[,1])
xx<-xx[xord,]
temp<-chull(xx)
lines(xx[temp,])
lines(xx[c(temp[1],temp[length(temp)]),])
abline(0,1)
}
sig.level<-sum(discen[bplus]<=discen)/bplus
list(p.value=sig.level,center=totv,weighted.grand.mean=gv[1])
}


dec2by2.A<-function(J,K,x,alpha = 0.05, q = c(1:9)/10, nboot = 1000, SEED = TRUE,
    method = "BH"){
#
# For every relevant interaction, compare multiple quantiles
#
if(is.matrix(x))x=listm(x)
INT=list()
JK=J*K
CO=con2by2A(J,K)$conA
for( j in 1:ncol(CO)){
id=which(CO[,j]!=0)
X=x[id]
INT[[j]]=decinter(X,alpha=alpha,q=q,nboot=nboot,SEED=SEED,method=method)
}
list(A=INT,con=CO)
}




# ----------------------------------------------------------------------------

# dec2by2.B

# ----------------------------------------------------------------------------

dec2by2.B<-function(J,K,x,alpha = 0.05, q = c(1:9)/10, nboot = 1000, SEED = TRUE,
    method = "BH"){
#
# For every relevant interaction, compare multiple quantiles
#
if(is.matrix(x))x=listm(x)
INT=list()
JK=J*K
CO=con2by2B(J,K)$conB
for( j in 1:ncol(CO)){
id=which(CO[,j]!=0)
X=x[id]
INT[[j]]=decinter(X,alpha=alpha,q=q,nboot=nboot,SEED=SEED,method=method)
}
list(B=INT,con=CO)
}




# ----------------------------------------------------------------------------

# dec2way.mcp

# ----------------------------------------------------------------------------

dec2way.mcp<-function(J,K,x,method=c('Int','A.main','B.main','A2by2','B2by2'),alpha=.05,
q=c(1:9)/10,nboot=1000,SEED=TRUE){
type=match.arg(method)
switch(type,
Int=decJKinter(J,K,x,alpha =alpha, q = q, nboot = nboot, SEED = SEED),
A.main=decJK.Amain(J,K,x,alpha =alpha, q = q, nboot = nboot, SEED = SEED),
B.main=decJK.Bmain(J,K,x,alpha =alpha, q = q, nboot = nboot, SEED = SEED),
A2by2=dec2by2.A(J,K,x=x,alpha =alpha, q = q, nboot = nboot, SEED = SEED),
B2by2=dec2by2.A(J,K,x=x,alpha =alpha, q = q, nboot = nboot, SEED = SEED))
}


deciles<-function(x,HD=TRUE,type=7){
#
#  Estimate the deciles for the data in vector x
#  HD=TRUE: use the Harrell-Davis estimate of the q quantile
#   HD=FALSE:use R function quantile
#
x=elimna(x)
if(HD){
xs<-sort(x)
n<-length(x)
vecx<-seq(along=x)
xq<-0
for (i in 1:9){
q<-i/10
m1<-(n+1)*q
m2<-(n+1)*(1-q)
wx<-pbeta(vecx/n,m1,m2)-pbeta((vecx-1)/n,m1,m2)  # W sub i values
xq[i]<-sum(wx*xs)
}}
if(!HD){
pts=seq(.1,.9,.1)
xq=quantile(x,probs=pts,type=type)
}
xq
}




# ----------------------------------------------------------------------------

# decinter

# ----------------------------------------------------------------------------

decinter<-function(x,alpha=.05,q=c(1:9)/10,nboot=1000,SEED=TRUE,method='BH'){
#
# By default, use  all deciles when dealing with interactions in a 2-by-2 design.
# The quantiles used can be altered via the argument q
#
if(SEED)set.seed(2)
if(is.matrix(x))x=listm(x)
x=elimna(x)
bv1=matrix(NA,nrow=9,ncol=nboot)
bv2=matrix(NA,nrow=9,ncol=nboot)
bv3=matrix(NA,nrow=9,ncol=nboot)
bv4=matrix(NA,nrow=9,ncol=nboot)
data<-matrix(sample(x[[1]],size=length(x[[1]])*nboot,replace=TRUE),nrow=nboot)
bv1=apply(data,1,hdmq,q=q)
data<-matrix(sample(x[[2]],size=length(x[[2]])*nboot,replace=TRUE),nrow=nboot)
bv2=apply(data,1,hdmq,q=q)
data<-matrix(sample(x[[3]],size=length(x[[3]])*nboot,replace=TRUE),nrow=nboot)
bv3=apply(data,1,hdmq,q=q)
data<-matrix(sample(x[[4]],size=length(x[[4]])*nboot,replace=TRUE),nrow=nboot)
bv4=apply(data,1,hdmq,q=q)
be=bv1-bv2-bv3+bv4
pv=NA
nq=length(q)
ilow<-round((alpha/2) * nboot)
ihi<-nboot - ilow
ilow<-ilow+1
vs=sort(be)
cilow=NA
ciup=NA
for(i in 1:nq){
pv[i]=mean(be[i,]<0)
pv[i]=2*min(pv[i],1-pv[i])
bes=sort(be[i,])
cilow[i]=bes[ilow]
ciup[i]=bes[ihi]
}
output=matrix(NA,nrow=nq,ncol=8)
dimnames(output)=list(NULL,c('Quant','Est.Lev 1','Est.Lev 2','Dif','ci.low','ci.up','p-value','p.adj'))
output[,1]=q
e=lapply(x,hdmq,q=q)
est=e[[1]]-e[[2]]-e[[3]]+e[[4]]
output[,2]=e[[1]]-e[[2]]
output[,3]=e[[3]]-e[[4]]
output[,4]=est
output[,5]=cilow
output[,6]=ciup
output[,7]=pv
output[,8]=p.adjust(pv,method=method)
output
}

ancsm.es<-
function(x1,y1,x2,y2,method='KMS',pts=NA,est=tmean,
fr1=1,fr2=1,nboot=NA,nmin=12,alpha=.05,xout=FALSE,
outfun=outpro,plotit=TRUE,LP=TRUE,xlab='X',ylab='Y',pch1='*',pch2='+',...){
#
# Compare two independent  groups using
#  a percentile bootstrap combined with a running interval
# smooth and some robust measure of effect size:
#
#Choices for method:
# 'EP','QS','QStr','AKP','WMW','KMS'
#
#  Assume data are in x1 y1 x2 and y2
#  Comparisons are made at the design points contained in the vector
#  pts
#
flag.est=FALSE
if(identical(est,onestep))flag.est=TRUE
if(flag.est)LP=FALSE   # Get an error when using onestep in conjunction with LP=T
if(identical(est,mom))flag.est=TRUE
xy1=elimna(cbind(x1,y1))
x1=xy1[,1]
y1=xy1[,2]
xy2=elimna(cbind(x2,y2))
x2=xy2[,1]
y2=xy2[,2]
if(xout){
flag<-outfun(x1,...)$keep
x1<-x1[flag]
y1<-y1[flag]
flag<-outfun(x2,...)$keep
x2<-x2[flag]
y2<-y2[flag]
}
npt<-5
gv1<-vector('list')
if(is.na(pts[1])){
output=matrix(NA,5,5)
dimnames(output)=list(NULL,c('pts','Effect.Size','ci.low','ci.up','p.value'))
output[,1]=c(1:5)
isub<-c(1:5)  # Initialize isub
test<-c(1:5)
xorder<-order(x1)
y1<-y1[xorder]
x1<-x1[xorder]
xorder<-order(x2)
y2<-y2[xorder]
x2<-x2[xorder]
n1<-1
n2<-1
vecn<-1
for(i in 1:length(x1))n1[i]<-length(y1[near(x1,x1[i],fr1)])
for(i in 1:length(x1))n2[i]<-length(y2[near(x2,x1[i],fr2)])
for(i in 1:length(x1))vecn[i]<-min(n1[i],n2[i])
sub<-c(1:length(x1))
isub[1]<-min(sub[vecn>=nmin])
isub[5]<-max(sub[vecn>=nmin])
isub[3]<-floor((isub[1]+isub[5])/2)
isub[2]<-floor((isub[1]+isub[3])/2)
isub[4]<-floor((isub[3]+isub[5])/2)
mat<-matrix(NA,5,3)
dimnames(mat)<-list(NULL,c('X','n1','n2'))
for (i in 1:5){
j<-i+5
temp1<-y1[near(x1,x1[isub[i]],fr1)]
temp2<-y2[near(x2,x1[isub[i]],fr2)]
temp1<-temp1[!is.na(temp1)]
temp2<-temp2[!is.na(temp2)]
mat[i,1]<-x1[isub[i]]
mat[i,2]<-length(temp1)
mat[i,3]<-length(temp2)
test=ESfun.CI(temp1,temp2,method=method)
if(method=='KMS')output[i,2]=test$effect.size
if(method=='QS')output[i,2]=test$effect.size
if(method=='QStr')output[i,2]=test$effect.size
if(method=='WMW'){
output[i,2]=test$p.hat
test$ci[1]=test$p.ci[1]
test$ci[2]=test$p.ci[2]
}
if(method=='AKP')output[i,2]=test$akp.effect
if(method=='EP')output[i,2]=test$Effect.Size
output[i,3]=test$ci[1]
output[i,4]=test$ci[2]
if(method!='EP')output[i,5]=test$p.value
}}
#
if(!is.na(pts[1])){
npt<-length(pts)
output=matrix(NA,npt,5)
output[,1]=c(1:npt)
dimnames(output)=list(NULL,c('pts','Effect.Size','ci.low','ci.up','p.value'))
n1<-1
n2<-1
vecn<-1
for(i in 1:length(pts)){
n1[i]<-length(y1[near(x1,pts[i],fr1)])
n2[i]<-length(y2[near(x2,pts[i],fr2)])
}
mat<-matrix(NA,length(pts),3)
dimnames(mat)<-list(NULL,c('X','n1','n2'))
gv<-vector('list',2*length(pts))
for (i in 1:length(pts)){
j<-i+npt
temp1<-y1[near(x1,pts[i],fr1)]
temp2<-y2[near(x2,pts[i],fr2)]
temp1<-temp1[!is.na(temp1)]
temp2<-temp2[!is.na(temp2)]
test=ESfun.CI(temp1,temp2,method=method)
output[i,2]=test$effect.size
output[i,3]=test$ci[1]
output[i,4]=test$ci[2]
mat[i,1]<-pts[i]
if(length(temp1)<=5)paste('Warning, there are',length(temp1),' points corresponding to the design point X=',pts[i])
if(length(temp2)<=5)paste('Warning, there are',length(temp2),' points corresponding to the design point X=',pts[i])
mat[i,2]<-length(temp1)
mat[i,3]<-length(temp2)
#gv1[[i]]<-temp1
#gv1[[j]]<-temp2
test=ESfun.CI(temp1,temp2,method=method)
output[i,2]=test$effect.size
output[i,3]=test$ci[1]
output[i,4]=test$ci[2]
output[i,5]=test$p.value
}
}
if(plotit){
runmean2g(x1,y1,x2,y2,fr=fr1,est=est,LP=LP,xlab=xlab,ylab=ylab,pch1=pch1,pch2=pch2,...)
}
list(mat=mat,output=output)
}

ancsm.es<-
function(x1,y1,x2,y2,ES='KMS',npt=8,est=tmean,method='BH',
fr1=1,fr2=1,nboot=NA,nmin=12,alpha=.05,xout=FALSE,SEED=TRUE,
outfun=outpro,plotit=TRUE,LP=FALSE,xlab='X',ylab='Effect Size',pch1='*',pch2='+',...){
#
# Compare two independent  groups using
#  a percentile bootstrap combined with a running interval
# smooth and some robust measure of effect size:
#
# This is done for npt covariate values, default is npt=8
#
#. FWE is controlled based on the argument method, default is FDR
#. (Bejamini - Hochberg method).
# plotit=TRUE, plot estimates plus confidence intervals not adjusted to
# get simultaneous probability coverage.
#
# Choices for ES, the measure of effect size:
# 'KMS', 'EP','QS','QStr','AKP','WMW'
#
#  Assume data are in x1 y1 x2 and y2
#
flag.est=FALSE
if(identical(est,onestep))flag.est=TRUE
if(flag.est)LP=FALSE   # Get an error when using onestep in conjunction with LP=T
if(identical(est,mom))flag.est=TRUE
xy1=elimna(cbind(x1,y1))
x1=xy1[,1]
y1=xy1[,2]
xy2=elimna(cbind(x2,y2))
x2=xy2[,1]
y2=xy2[,2]
if(xout){
flag<-outfun(x1,...)$keep
x1<-x1[flag]
y1<-y1[flag]
flag<-outfun(x2,...)$keep
x2<-x2[flag]
y2<-y2[flag]
}
#
#
res1=ancova(x1,y1,x2,y2,pr=FALSE,plotit=FALSE,SEED=FALSE)$output
pts=seq(res1[1,1],res1[5,1],length.out=npt)
pts=unique(pts)
npt=length(pts)
output=matrix(NA,npt,6)
output[,1]=c(1:npt)
dimnames(output)=list(NULL,c('pts','Effect.Size','ci.low','ci.up','p.value','p.adj'))
n1<-1
n2<-1
vecn<-1
for(i in 1:length(pts)){
n1[i]<-length(y1[near(x1,pts[i],fr1)])
n2[i]<-length(y2[near(x2,pts[i],fr2)])
}
mat<-matrix(NA,length(pts),3)
dimnames(mat)<-list(NULL,c('X','n1','n2'))
gv<-vector('list',2*length(pts))
for (i in 1:length(pts)){
temp1<-y1[near(x1,pts[i],fr1)]
temp2<-y2[near(x2,pts[i],fr2)]
temp1<-temp1[!is.na(temp1)]
temp2<-temp2[!is.na(temp2)]
test=ESfun.CI(temp1,temp2,method=ES,SEED=SEED,alpha=alpha)
output[i,2]=test$effect.size
output[i,3]=test$ci[1]
output[i,4]=test$ci[2]
mat[i,1]<-pts[i]
if(length(temp1)<=5)paste('Warning, there are',length(temp1),' points corresponding to the design point X=',pts[i])
if(length(temp2)<=5)paste('Warning, there are',length(temp2),' points corresponding to the design point X=',pts[i])
mat[i,2]<-length(temp1)
mat[i,3]<-length(temp2)
test=ESfun.CI(temp1,temp2,method=ES,alpha=alpha,SEED=SEED)
output[i,2]=test$effect.size
output[i,3]=test$ci[1]
output[i,4]=test$ci[2]
output[i,5]=test$p.value
}
output[,6]=p.adjust(output[,5],method=method)
if(plotit){
#runmean2g(x1,y1,x2,y2,fr=fr1,est=est,LP=LP,xlab=xlab,ylab=ylab,pch1=pch1,pch2=pch2,...)
plot(c(pts,pts,pts),c(output[,2],output[,3],output[,4]),xlab=xlab,ylab=ylab,type='n')
points(pts,output[,3],pch='+')
points(pts,output[,4],pch='+')
points(pts,output[,2],pch='*')
}
list(mat=mat,output=output)
}


decJKinter<-function(J,K,x,alpha = 0.05, q = c(1:9)/10, nboot = 1000, SEED = TRUE,
    method = "BH"){
#
# For every relevant interaction, compare multiple quantiles
#
if(is.matrix(x))x=listm(x)
INT=list()
JK=J*K
CO=con2way(J,K)$conAB
for( j in 1:ncol(CO)){
id=which(CO[,j]!=0)
X=x[id]
INT[[j]]=decinter(X,alpha=alpha,q=q,nboot=nboot,SEED=SEED,method=method)
}
list(interactions=INT,con=CO)
}




# ----------------------------------------------------------------------------

# degrees.2.radians

# ----------------------------------------------------------------------------

degrees.2.radians<-function(d)d*pi/180




# ----------------------------------------------------------------------------

# dep.dif.fun

# ----------------------------------------------------------------------------

dep.dif.fun<-function(x,y,tr=.2,alpha=.05,AUTO=TRUE,PVSD=FALSE,nboot=2000,method=c('TR','TRPB','HDPB','MED','AD','SIGN')){
#
#
# For two dependent groups,
# compute confidence intervals based  on difference scores
#
#  TR: trimmed mean Tukey--McLaughlin
#  TRPB:  trimmed means percentile bootstrap
#  MED: median of the difference scores.
#.HDPB: median of the difference scores using Harrell--Davis and a  percentile bootstrap
#  AD: based on the median of the distribution of x-y, which can differ from the median of the difference scores.
#  SIGN:  P(X<Y), sign test, probability that for a random pair, the first is less than the second.
#
#
if(length(x)!=length(y))stop('x and y have different lengths')
type=match.arg(method)
switch(type,
  TR=trimci.dif(x,y,tr=tr,alpha=alpha),
   TRPB=trimpb(x,y,tr=tr,alpha=alpha,nboot=nboot,pr=FALSE,plotit=FALSE),
  MED=sintv2(x,y,alpha=alpha),
  HDPB=hdpb(x-y,alpha=alpha),
  AD= l2drmci(x,y,alpha=alpha,nboot=nboot),
  SIGN=signt(x,y,alpha=alpha,AUTO=AUTO,PVSD=PVSD),
)
}




# ----------------------------------------------------------------------------

# dep2.sign

# ----------------------------------------------------------------------------

dep2.sign<-function(x,y=NULL,LP=FALSE,CI=TRUE,xout=FALSE,outfun=outpro,plotit=TRUE,xlab='X',
ylab='P(X<Y)',pr.ci=FALSE,...){
#
# Given an x, estimate probability that y will be greater than x
#
if(!is.null(y))x=cbind(x,y)
x=elimna(x)
if(ncol(x)!=2)stop('Should have data for two groups only')
flag=x[,1]<x[,2]
if(!CI){
if(!LP)e=logreg(x[,1],flag,xout=xout,outfun=outfun,plotit=plotit,xlab=xlab,ylab=ylab)
if(LP)e=logSM(x[,1],flag,xout=xout,outfun=outfun,plotit=plotit,xlab=xlab,ylab=ylab,pr=FALSE)
}
if(CI){
if(!LP)e=logreg.P.ci(x[,1],flag,xout=xout,outfun=outfun,plotit=plotit,xlab=xlab,ylab=ylab)
if(LP)e=rplot.binCI(x[,1],flag,xout=xout,outfun=outfun,plotit=plotit,xlab=xlab,ylab=ylab)
}
if(!pr.ci)e='Done'
e
}




# ----------------------------------------------------------------------------

# depQS

# ----------------------------------------------------------------------------

depQS<-function(x,y=NULL,null.value=0,locfun=median,...){
#
#  Probabilistic measure of effect size: shift of the median
#  based on difference scores for two dependent groups.
#
if(!is.null(y))L=x-y
else L=x
L=elimna(L)
est=locfun(L,...)
if(est>=null.value)ef.sizeND=mean(L-est+null.value<=est)
ef.size=mean(L-est+null.value<=est)
if(est<null.value)ef.sizeND=mean(L-est+null.value>=est)
list(Q.effect=ef.size)
}




# ----------------------------------------------------------------------------

# depQSci

# ----------------------------------------------------------------------------

depQSci<-function(x,y=NULL,null.value=0,locfun=hd,alpha=.05,nboot=500,SEED=TRUE,...){
#
# confidence interval for the quantile shift measure of effect size.
#
if(SEED)set.seed(2)
if(!is.null(y)){
xy=elimna(cbind(x,y))
xy=xy[,1]-xy[,2]
}
if(is.null(y))xy=elimna(x)
n=length(xy)
v=NA
for(i in 1:nboot){
id=sample(c(1:n),replace=TRUE)
v[i]=depQS(xy[id],locfun=locfun,null.value=null.value)$Q.effect
}
#
# Due to the discreteness of the sampling distribution, use the Harrell--Davis estimator
#  when computing confidence intervals and p-values
#  The usual method has been commented out.
#
#v=sort(v)
#ilow<-round((alpha/2) * nboot)
#ihi<-nboot - ilow
#ilow<-ilow+1
#ci=v[ilow]
#ci[2]=v[ihi]
ci=hd(v,alpha/2)
ci[2]=hd(v,1-alpha/2)
pv=mean(v<0.5)+.5*mean(v==0.5)
pv=2*min(pv,1-pv)
est=depQS(xy,null.value=null.value)$Q.effect
list(Q.effect=est,ci=ci,p.value=pv)
}




# ----------------------------------------------------------------------------

# derpsiBY3

# ----------------------------------------------------------------------------

derpsiBY3 <- function(t,c3)
{
res=NULL
 for (i in 1:length(t))
{
if (t[i] <= c3)
 { res=rbind(res,0) }
else
{res=rbind(res,-exp(-sqrt(t[i]))/(2*sqrt(t[i]))) }
}
res
}




# ----------------------------------------------------------------------------

# DETS

# ----------------------------------------------------------------------------

DETS<-function(m){
#
# Robust multivariate location and scatter estimator derived by
# Hubert et al.	(2015)
#
library('rrcov')
a=CovSest(m,method='sdet')
list(center=a@center,cov=a@cov)
}



dfried<-function(m,plotit=TRUE,pop=0,fr=.8,v2=FALSE,op=FALSE){
#
# Compare dependent groups using halfspace depth of
# 0 relative to distribution of differences.
#
# When plotting differences scores:
# pop=1 Plot expected frequency curve
# pop=2 kernel density estimate
# pop=3 S+ kernel density estimate
# pop=4 boxplot
#
if(is.list(m))m<-matl(m)
if(!is.matrix(m))stop("m should be a matrix having at least 2 columns.")
m<-elimna(m)
K<-ncol(m)
n<-nrow(m)
if(n<=10 && !op)print("With n<=10, might want to use op=T")
J<-(K^2-K)/2
dcen<-cov.mcd(m)$center
center<-NA
pval<-matrix(NA,ncol=J,nrow=nrow(m))
zvec<-rep(0,J)
ic<-0
for(k in 1:K){
for(kk in 1:K){
if(k<kk){
ic<-ic+1
pval[,ic]<-m[,k]-m[,kk]
center[ic]<-dcen[k]-dcen[kk]
}}}
pval0<-rbind(pval,zvec)
if(ncol(pval)==1)temp<-unidepth(as.vector(pval0))
if(!v2){
if(ncol(pval)>1)temp<-fdepth(pval0,center=center)
}
if(v2){
if(ncol(pval)>1)temp<-fdepthv2(pval0)
}
big.dep<-max(temp)
if(op){
v3<-dmean(pval,tr=.5,dop=2)
v3<-t(as.matrix(v3))
big.dep<-max(max(temp),fdepthv2(pval0,v3))
}
phat<-temp[nrow(m)+1]/big.dep
# Determine critical value
if(K==2)crit<-0.95-1.46/n^.5
if(K==3)crit<-1.00-1.71/n^.5
if(K==4)crit<-1.06-1.77/n^.5
if(K==5)crit<-1.11-1.76/n^.5
if(K==6)crit<-1.41-1.62/n^.3
if(K==7)crit<-1.49-1.71/n^.3
if(K>=8)crit<-1.39-1.38/n^.3
crit<-min(c(crit,1))
if(plotit && ncol(pval)==1){
if(pop==0)akerd(pval,fr=fr)
if(pop==1)rdplot(pval,fr=fr)
if(pop==2)kdplot(pval)
if(pop==3)skerd(pval)
if(pop==4)boxplot(pval)
}
list(phat=phat,crit.val=crit)
}

spat.sub<-function(x,theta){
xx<-x
for(i in 1:ncol(x))xx[,i]<-x[,i]-theta[i]
xx<-xx^2
temp<-sqrt(apply(xx,1,sum))
val<-mean(temp)
val
}




# ----------------------------------------------------------------------------

# difQpci

# ----------------------------------------------------------------------------

difQpci<-function(x,y=NULL,q=seq(5,40,5)/100,xlab="Quantile",ylab="Group 1 minus Group 2",plotit=TRUE,alpha=.05,nboot=1000,SEED=TRUE,LINE=FALSE){
#
#  x can be a vector, in which case compare quantiels of distribution of data in x
#  x can be a matrix with 2 columns, in which case analysis is done on dif=x[,1]-x[,2]
#  y supplied, then do analysis of dif=x-y
#
#  Plot that provides perspective on the degree a distribution is symmetric about zero.
#  This function plots the sum of q and 1-q quantiles. A 1-alpha confidence interval for the sum is indicated by a +
#  If the distributions are symmetric
#  the plot should be approximately a horizontal line. If in addition the median
#  of the difference scores is zero, the horizontal line will intersect the y-axis at zero.
#
#  Similar to difQplot, only plots fewer quantiles by default and returns p-values for
#  each quantile indicated by the argument q.
#
#  FWE is controlled via Hochberg's method, which was used to determine critical
#  p-values based on the argument
#  alpha.
#
#  Can alter the quantiles compared via the argument
#  q
#  q must be less than .5
#
#  LINE=TRUE. When plotting, a line connecting the estimates will be included.
#
x=as.matrix(x)
if(is.null(y))dif=x
if(ncol(x)>2)stop("Should be at most two groups")
if(ncol(x)==2)dif=x[,1]-x[,2]
if(!is.null(y))dif=x-y
dif=elimna(dif)
nv=length(dif)
output=matrix(NA,ncol=8,nrow=length(q))
dimnames(output)=list(NULL,c("quantile","Est_q","Est_1.minus.q","SUM","ci.low","ci.up","p_crit","p-value"))
for(i in 1:length(q)){
test=Dqdif(dif,q=q[i],plotit=FALSE,nboot=nboot,SEED=SEED)
output[i,1]=q[i]
output[i,2]=test$est.q
output[i,3]=test$est.1.minus.q
output[i,8]=test$p.value
output[i,5]=test$conf.interval[1]
output[i,6]=test$conf.interval[2]
}
temp=order(output[,8],decreasing=TRUE)
zvec=alpha/c(1:length(q))
output[temp,7]=zvec
output <- data.frame(output)
output$signif=rep("YES",nrow(output))
for(i in 1:nrow(output)){
if(output[temp[i],8]>output[temp[i],7])output$signif[temp[i]]="NO"
if(output[temp[i],8]<=output[temp[i],7])break
}
output[,4]=output[,2]+output[,3]
if(plotit){
plot(rep(q,3),c(output[,4],output[,5],output[,6]),type="n",xlab=xlab,ylab=ylab)
points(q,output[,6],pch="+")
points(q,output[,5],pch="+")
points(q,output[,4],pch="*")
if(LINE)lines(q,output[,4],pch="*")
}
list(n=nv,output=output)
}




# ----------------------------------------------------------------------------

# disband

# ----------------------------------------------------------------------------

disband<-function(x,sm=TRUE,op=1,grp=c(1:4),xlab="First Group",
ylab="Delta"){
#
# A shift-type plot aimed at helping see any disordinal interactions
# in a  2 by 2 design.
#
#  x is assumed to be a matrix with columns corresponding to groups
#  or x and have list mode.
#
#  four groups are analyzed,
#
# grp indicates the groups to be compared. By default grp=c(1,2,3,4)
# meaning that the first four groups are used with the difference between
# the first two compared to the difference between the second two.
#
# For four variables stored in x,
# this function plots the shift function for the first two
# variables as well as the second two.
#
#  No disordinal interaction corresponds to the two shift functions being
#  identical. That is, the difference between the quantiles is always the same
#
#  When plotting, the median of x is marked with a + and the two
#  quaratiles are marked with o.
#
#  sm=T, shift function is smoothed using:
#  op!=1, running interval smoother,
#  otherwise use lowess.
#
if(is.matrix(x))x=listm(x)
if(length(grp)!=4)stop("The argument grp must have 4 values")
x=x[grp]
for(j in 1:4)x[[j]]=elimna(x[[j]])
pc<-NA
crit= 1.36 * sqrt((length(x[[1]]) + length(x[[2]]))/(length(x[[1]]) *
    length(x[[2]])))
remx=x
for(iloop in 1:2){
if(iloop==1){
x=remx[[1]]
y=remx[[2]]
}
if(iloop==2){
x=remx[[3]]
y=remx[[4]]
}
xsort<-sort(x)
ysort<-c(NA,sort(y))
l<-0
u<-0
ysort[length(y)+1+1]<-NA
for(ivec in 1:length(x))
{
isub<-max(0,ceiling(length(y)*(ivec/length(x)-crit)))
l[ivec]<-ysort[isub+1]-xsort[ivec]
isub<-min(length(y)+1,floor(length(y)*(ivec/length(x)+crit))+1)
u[ivec]<-ysort[isub+1]-xsort[ivec]
}
num<-length(l[l>0 & !is.na(l)])+length(u[u<0 & !is.na(u)])
qhat<-c(1:length(x))/length(x)
m<-matrix(c(qhat,l,u),length(x),3)
dimnames(m)<-list(NULL,c("qhat","lower","upper"))
xsort<-sort(x)
ysort<-sort(y)
del<-0
for (i in 1:length(x)){
ival<-round(length(y)*i/length(x))
if(ival<=0)ival<-1
if(ival>length(y))ival<-length(y)
del[i]<-ysort[ival]-xsort[i]
}
if(iloop==1){
allx<-c(xsort,xsort,xsort)
ally<-c(del,m[,2],m[,3])
}
if(iloop==2){
allx<-c(allx,xsort,xsort,xsort)
ally<-c(ally,del,m[,2],m[,3])
plot(allx,ally,type="n",ylab=ylab,xlab=xlab)
}
ik<-rep(F,length(xsort))
if(sm){
if(op==1){
ik<-duplicated(xsort)
del<-lowess(xsort,del)$y
}
if(op!=1)del<-runmean(xsort,del,pyhat=TRUE)
}
if(iloop==1){
xsort1=xsort[!ik]
del1=del[!ik]
}
if(iloop==2){
lines(xsort1,del1,lty=iloop)
lines(xsort[!ik],del[!ik],lty=iloop)
}}
done="Done"
done
}


disc2.chi.sq<-function(x,y,simulate.p.value=FALSE,B=2000){
#
# Test the hypothesis of identical discrete distributions
# using a chi-squared test and a simulated p.value
#
n1 = length(x)
n2 = length(y)
g = c(rep(1,n1), rep(2,n2))
d = c(x,y)
df = data.frame(d, g)
res=chisq.test(df$d, df$g, simulate.p.value=simulate.p.value, B=B)
list(X.squared=res[1]$statistic,p.value=res[3]$p.value)
}

disc2com=disc2.chi.sq

discstep<-function(x,nboot=500,alpha=.05,SEED=TRUE){
#
#  Step-down multiple comparison procedure for comparing
#  J independent discrete random variables.
#  The method is based on a generalization of the Storer--Kim method
#  comparing independent binomials; it can be sensitive to differences
#  not detected by measures of location.
#
#  x is a matrix with n rows and J columns
#  or it can have list mode
#
if(is.matrix(x) || is.data.frame(x))x=listm(x)
library(mc2d)
if(SEED)set.seed(2)
vals=lapply(x,unique)
vals=sort(elimna(list2vec(vals)))
K=length(unique(vals))
n=lapply(x,length)
n=list2vec(n)
J=length(x)
if(J==2)stop('For 2 groups use disc2com')
if(J>5)stop('Designed for 5 groups or less')
com=modgen(J)
ntest=length(com)
jp1=J+1
com=com[jp1:length(com)]
ntest=length(com)
mout=matrix(NA,nrow=ntest,ncol=3)
dimnames(mout)=list(NULL,c('Groups','p-value','p.crit'))
test=NULL
for(i in 1:ntest){
test[i]=discANOVA.sub(x[com[[i]]])$test #$
nmod=length(com[[i]])-1
temp=c(nmod:0)
mout[i,1]=sum(com[[i]]*10^temp)
}
mout[,3]=alpha
xx=list()
pv=NA
jm2=J-2
mout[,3]=alpha
TB=matrix(NA,nrow=nboot,ncol=ntest)
step1=discANOVA.sub(x)
C1=step1$C1
HT=NULL
for(i in 1:K)HT[i]=mean(C1[i,])
for(ib in 1:nboot){
xx=list()
for(j in 1:J){
temp=rmultinomial(n[j],1,HT)
xx[[j]]=which(temp[1,]==1)
for(i in 2:n[j])xx[[j]][i]=which(temp[i,]==1)
}
for(k in 1:ntest)TB[ib,k]=discANOVA.sub(xx[com[[k]]])$test #$
}
for(k in 1:ntest){
mout[k,2]=1-mean(test[k]>TB[,k])-.5*mean(test[k]==TB[,k])
pnum=length(com[[k]])
pe=1-(1-alpha)^(pnum/J)
if(length(com[[k]])<=jm2)mout[k,3]=pe
}
list(results=mout[nrow(mout):1,])
}

disker<-function(x,y,z=NA,plotit=FALSE,op=1){
#
#  Estimate apparent effect size
#  using probability of correct classification based on values in
#  first group.
#
#  A "CORRECT" classification is the event of deciding
#  that an observation
#  from the first group did indeed come from the first group based
#  on a kernel density estimate of the distributions.
#  The function returns the
#  proportion of correctly classified observations (phat).
#
#  The function also returns a vector of 0s and 1s (in zhat)
#  indicating  whether values in z would be
#  classified as coming from the first group.
#
# op=1, use Rosenblatt's shifted histogram version of kernel estimate
# op=2, use adaptive kernel estimate with initial estimate based
#       on expected frequency curve.
#
xsort<-sort(x)
ysort<-sort(y)
xhat<-0
yhat<-0
yyhat<-0
if(op==1){
for(i in 1:length(xsort))xhat[i]<-kerden(x,0,xsort[i])
for(i in 1:length(xsort))yhat[i]<-kerden(y,0,xsort[i])
}
if(op==2){
xhat<-akerd(x,pts=xsort,pyhat=TRUE,plotit=FALSE)
yhat<-akerd(y,pts=xsort,pyhat=TRUE,plotit=FALSE)
}
yhat[is.na(yhat)]<-0
if(plotit){
if(op==1){
for(i in 1:length(ysort))yyhat[i]<-kerden(y,0,ysort[i])
}
if(op==2)yyhat<-akerd(y,pts=ysort,plotit=FALSE,pyhat=TRUE)
plot(c(xsort,ysort),c(xhat,yyhat),type="n",xlab="",ylab="")
lines(xsort,xhat)
lines(ysort,yyhat)
}
#
# Compute apparent error
#
phat<-sum(xhat>yhat)/length(x)
zhat<-NA
if(!is.na(z[1])){
#
#  Make decisions for the data in z,
#  set zhat=1 if decide it came from
#  group 1.
#
zxhat<-0
zyhat<-0
zhat<-0
if(op==2){
zxhat<-akerd(x,pts=z,pyhat=TRUE,plotit=FALSE)
zyhat<-akerd(y,pts=z,pyhat=TRUE,plotit=FALSE)
}
for(i in 1:length(z)){
if(op==1){
zxhat[i]<-kerden(x,0,z[i])
zyhat[i]<-kerden(y,0,z[i])
}
zhat[i]<-1
if(is.na(zxhat[i]) || is.na(zyhat[i])){
# Missing values,
# data can't be used to make a decision,
# so make a random decision about whether a value
# came from first group.
arb<-runif(1)
zhat[i]<-1
if(arb < .5)zhat[i]<-0
}
else
if(zxhat[i]<zyhat[i])zhat[i]<-0
}
}
list(phat=phat,zhat=zhat)
#phat is the apparent probability  of a correct classification
}


disord.inter<-function(J,K,x,con=NULL,ROW=TRUE,tr=.2){
#
# Is there a disordinal interaction?
# con=linear contrast coefficients.
#
# ROW=TRUE: differences over second factor.
# That is, for any two levels of Factor	A, use difference in trimmend means
# for two levels of Factor B.
#
# ROW=FALSE: differences over first factor.
#
# output:
#  A matrix, the ith row contains results based on the ith column of con
#
if(is.data.frame(x))x=as.matrix(x)
if(is.matrix(x))x<-listm(x)
if(is.null(con))con=con2way(J,K)$conAB
output=matrix(NA,nrow=ncol(con),ncol=6)
dimnames(output)=list(NULL,c('est1.1','est1.2','p.value.1','est1.2','est2.2','p.value.2'))
for(j in 1:ncol(con)){
idpos=which(con[,j]==1)
idneg=which(con[,j]==-1)
if(ROW){
print(c(idpos[[1]],idneg[[1]],idpos[[2]],idneg[[2]]))
Y1=yuen(x[[idpos[1]]],x[[idneg[1]]],tr=tr)
Y2=yuen(x[[idpos[2]]],x[[idneg[2]]],tr=tr)
}
if(!ROW){
Y1=yuen(x[[idpos[1]]],x[[idneg[2]]],tr=tr)
Y2=yuen(x[[idpos[2]]],x[[idneg[1]]],tr=tr)
}
output[j,]=c(Y1$est.1,Y1$est.2,Y1$p.value,Y2$est.1,Y2$est.2,Y2$p.value)
}
list(con=con,results=output)
}

Dqdif<-function(x,y=NULL,q=.25,nboot=1000,plotit=TRUE,xlab="Group 1 - Group 2",SEED=TRUE,alpha=.05){
#
#  Compare two dependent groups by comparing the
#  q and 1-q quantiles of the difference scores
#
# q should be < .5
# if the groups do not differ, then the difference scores should be symmetric
# about zero.
# In particular, the sum of q and 1-q quantiles should be zero.
#
# q indicates the quantiles to be compared. By default, the .25 and .75 quantiles are used.
#
if(SEED)set.seed(2)
if(q>=.5)stop("q should be less than .5")
if(!is.null(y)){
xy=elimna(cbind(x,y))
dif=xy[,1]-xy[,2]
}
if(is.null(y))dif=elimna(x)
n=length(dif)
if(plotit)akerd(dif,xlab=xlab)
bvec=NA
data<-matrix(sample(n,size=n*nboot,replace=TRUE),nrow=nboot)
for(ib in 1:nboot){
bvec[ib]<-hd(dif[data[ib,]],q=q)+hd(dif[data[ib,]],q=1-q)
}
est1=hd(dif,q=q)
est2=hd(dif,q=1-q)
pv=mean(bvec<0)+.5*mean(bvec==0)
p=2*min(c(pv,1-pv))
low<-round((alpha/2)*nboot)+1
up<-nboot-low
sbvec=sort(bvec)
ci=sbvec[low]
ci[2]=sbvec[up]
list(est.q=est1,est.1.minus.q=est2,conf.interval=ci,p.value=p)
}

ecdf<-function(x,val){
#  compute empirical cdf for data in x evaluated at val
#  That is, estimate P(X <= val)
#
ecdf<-length(x[x<=val])/length(x)
ecdf
}




# ----------------------------------------------------------------------------

# ees.ci

# ----------------------------------------------------------------------------

ees.ci<-function(x,y,SEED=TRUE,nboot=400,tr=.2,alpha=.05,pr=TRUE){
#
# Compute a 1-alpha  confidence interval
# for a robust, heteroscedastic  measure of effect size
#
if(SEED)set.seed(2) # set seed of random number generator so that
#             results can be duplicated.
x=elimna(x)
y=elimna(y)
bvec=0
datax<-matrix(sample(x,size=length(x)*nboot,replace=TRUE),nrow=nboot)
datay<-matrix(sample(y,size=length(x)*nboot,replace=TRUE),nrow=nboot)
for(i in 1:nboot){
bvec[i]=yuenv2(datax[i,],datay[i,],tr=tr,SEED=FALSE)$Var.Explained
}
bvec<-sort(bvec)
crit<-alpha/2
icl<-round(crit*nboot)+1
icu<-nboot-icl
ci<-NA
ci[1]<-bvec[icl]
pchk=yuen(x,y,tr=tr)$p.value
if(pchk>alpha)ci[1]=0
ci[2]<-bvec[icu]
if(ci[1]<0)ci[1]=0
ci=sqrt(ci)
ci
}




# ----------------------------------------------------------------------------

# effectg

# ----------------------------------------------------------------------------

effectg<-function(x,y,locfun=tmean,varfun=winvarN,nboot=100,SEED=TRUE,...){
#
# Compute a robust heteroscedastic measure of effect size
#  (explanatory power) based on the measures of location and scale
# indicated by the arguments locfun and varfun, respectively
#
if(SEED)set.seed(2)
x<-x[!is.na(x)]  # Remove any missing values in x
y<-y[!is.na(y)]  # Remove any missing values in y
n1=length(x)
n2=length(y)
if(n1==n2){
temp=effectg.sub(x,y,locfun=locfun,varfun=varfun,...)
e.pow=temp$Var.Explained
}
if(n1!=n2){
N=min(c(n1,n2))
vals=0
for(i in 1:nboot)vals[i]=effectg.sub(sample(x,N),sample(y,N),
locfun=locfun,varfun=varfun,...)$Var.Explained
e.pow=mean(vals)
}
list(Explanatory.power=e.pow,Effect.Size=sqrt(e.pow))
}


elimna2g<-function(x,y){
#
#  Assume both are matrices or list mode
#
if(is.matrix(x)){
J=ncol(x)
if(J!=ncol(y))stop('x and y have different number of columns')
J1=J+1
J2=2*J
xy=elimna(cbind(x,y))
x=xy[,1:J]
y=xy[,J1:J2]
}
if(is.list(x)){
J=length(x)
J1=J+1
J2=2*J
if(J!=length(y))stop('x and y have different lengths')
xy=elimna(c(x,y))
x=xy[1:J]
y=xy[J1:J2]
}
list(x=x,y=y)
}




# ----------------------------------------------------------------------------

# ellipse

# ----------------------------------------------------------------------------

ellipse <- function(x, center = apply(x, 2, mean), cov = var(x), alph = 0.95)
{# Makes a covering interval. The x should have 2 columns.
	mu1 <- center[1]
	mu2 <- center[2]
	w <- solve(cov)
	w11 <- w[1, 1]
	w12 <- w[1, 2]
	w22 <- w[2, 2]
	tem <- x[, 2] - mu2
	y2 <- seq(min(tem), max(tem), length = 100)
	xc <- qchisq(alph, 2)
	el <- matrix(0, 2, 2)
	ind <- 0
	for(i in 1:100) {
		j1 <- (y2[i] * w12)^2
		j2 <- w11 * ((y2[i])^2 * w22 - xc)
	# print(i)
# print(j1 - j2)
		if((j1 - j2) >= 0) {
			ind <- ind + 2
			tem <- (y2[i] * w12)^2
			tem <- tem - w11 * ((y2[i])^2 *
				w22 - xc)
			tem <- sqrt(tem)
			term <- ( - y2[i] * w12 + tem)/
				w11
			el <- rbind(el, c((term + mu1), (
				y2[i] + mu2)))
			term <- ( - y2[i] * w12 - tem)/
				w11
			el <- rbind(el, c((term + mu1), (
				y2[i] + mu2)))
		}
	}
	el <- el[3:ind,  ]
	nn <- dim(x)[1]
	if((ind - 2) > nn) {
		tem <- sample((ind - 2), nn)
		el <- el[tem,  ]
	}
	xt <- cbind(x[, 1], el[, 1])
	yt <- cbind(x[, 2], el[, 2])
	matplot(xt, yt)
}

essp<-
function(x, Y, M = 50)
{
# Trimmed view or ESSP for M percent
# trimming. Allows visualization of g
# and crude estimation of c beta in models
# of the form y = g(x^T beta,e).
# Workstation need to activate a graphics
# device with command "X11()" or "motif()."
# R needs command "library(lqs)."
# Click on the right mouse button to finish.
# In R, highlight "stop."
	x <- as.matrix(x)
        tval <- M/100
	out <- cov.mcd(x)
	center <- out$center
	cov <- out$cov
	rd2 <- mahalanobis(x, center, cov)
	val <- quantile(rd2, (1 - tval))
	bhat <- lsfit(x[rd2 <= val,  ], Y[rd2 <= val])$
		coef
	ESP <- x %*% bhat[-1]
	plot(ESP, Y)
	identify(ESP, Y)
	return(bhat[-1])
}

ffL<-
function(x, y)
{
# for unix, use X11() to turn on the graphics device before using this function
# this function makes a FF lambda plot where the competing models are Y^L
	n <- length(y)
	rmat <- matrix(nrow = n, ncol = 5)
	rmat[, 1] <- y - lsfit(x, y)$resid
	ytem <- (y^(0.5) - 1)/0.5
	rmat[, 2] <- ytem - lsfit(x, ytem)$resid
	rmat[, 3] <- log(y) - lsfit(x, log(y))$resid
	ytem <- (y^(-0.5) - 1)/-0.5
	rmat[, 4] <- ytem - lsfit(x, ytem)$resid
	ytem <- (y^(-1) - 1)/-1
	rmat[, 5] <- ytem - lsfit(x, ytem)$resid
	pairs(rmat, labels = c("YHAT", "YHAT^(0.5)", "YHAT^(0)", "YHAT^(-0.5)",
		"YHAT^(-1)"))
	min(cor(rmat))
}




# ----------------------------------------------------------------------------

# elo

# ----------------------------------------------------------------------------

elo<-function(x,y,lev=TRUE,reg=TRUE,outfun=outpro,plotit=FALSE,SEED=TRUE){
#
#
#  lev=TRUE, remove points flagged as leverage points
#  reg=TRUE, remove points flagged as regression outliers
#  So  lev=TRUE and reg=TRUE removes both
#
#  For regression outliers, the Rousseeuw and van Zomeren (1990) method is used.
#  (See section 10.15.1 in Wilcox, 2017, Intro to Robust Estimation and
#      Hypothsis Testing)
#
#  outfun: the function used to check for leverage points.
#

a=reglev(x,y,plotit=plotit,SEED=SEED)
o=outfun(x,plotit=plotit)
L=NULL
B=NULL
if(lev){
if(length(o$out.id)>0)L=o$out.id
}
if(reg){
if(length(a$regout)>0)B=a$regout
}
e=unique(c(L,B))
n=length(y)
id=c(1:n)
keep=id[-e]
list(keep=keep,reg.out.id=L,leverage.id=B)
}


EPci<-function(x,y,tr=.2,alpha=.05,nboot=1000,SEED=TRUE){
#
# Explanatory power, two-sample case, CI.
#
X=list()
X[[1]]=x
X[[2]]=y
a=t1way.EXES.ci(X,tr=.2,alpha=alpha,nboot=1000,SEED=SEED)
a
}

epmod<-function(x,y,smfun=lplot,xout=FALSE,outfun=outpro,STAND=TRUE,...){
#
# Estimates explanatory power, via a smoother, for all possible
# subsets of the p predictors. Currently limited to p<=5
# By default, use lowess. (smfun=lplot)
#
x<-as.matrix(x)
d<-ncol(x)
p1<-d+1
temp<-elimna(cbind(x,y))
x<-temp[,1:d]
y<-temp[,p1]
x<-as.matrix(x)
if(xout){
m<-cbind(x,y)
flag<-outfun(x,plotit=FALSE,...)$keep
m<-m[flag,]
x<-m[,1:d]
y<-m[,p1]
}
x<-as.matrix(x)
model<-modgen(d)
mout<-matrix(NA,length(model),2,dimnames=list(NULL,c("Model",
"Explanatory power")))
for (imod in 1:length(model)){
mout[imod,1]<-imod
mout[imod,2]<-smfun(x[,model[[imod]]],y,plotit=FALSE,...)$Explanatory.power
}
list(models=model,Explanatory.power=mout)
}
ts2str.sub<-function(isub,x,y){
val1<-tsreg(x[isub],y[isub])$Explanatory.Power
val1
}




# ----------------------------------------------------------------------------

# epowv2

# ----------------------------------------------------------------------------

epowv2<-function(x,y,pcor=FALSE,regfun=tsreg,corfun=pbcor,varfun=pbvar,xout=FALSE,outfun=outpro,plotit=FALSE,...){
#
# Estimate the explanatory correlation between x and y
#
# It is assumed that x is a vector or a matrix having one column only
xx<-elimna(cbind(x,y)) # Remove rows with missing values
p1=ncol(xx)
p=p1-1
x<-xx[,1:p]
y<-xx[,p1]
x<-as.matrix(x)
if(xout){
flag<-outfun(x,plotit=plotit,...)$keep
x=x[flag,]
y=y[flag]
}
coef<-regfun(x,y)$coef
yhat<-x %*% coef[2:p1] + coef[1]
stre=NULL
temp=varfun(y)
e.pow=NULL
if(temp>0)e.pow<-varfun(yhat)/temp
if(e.pow>1)e.pow=corfun(y,yhat)$cor^2
list(Strength.Assoc=e.pow,Explanatory.Power=sqrt(e.pow))
}




# ----------------------------------------------------------------------------

# erho.bt

# ----------------------------------------------------------------------------

errfun<-function(yhat,y,error=sqfun){
#
#   Compute error terms for regpre
#
#    yhat is an n by nboot matrix
#    y is n by 1.
#
ymat<-matrix(y,nrow(yhat),ncol(yhat))
blob<-yhat-ymat
errfun<-error(blob)
errfun
}

ESfun.CI<-function(x,y,QSfun=median,method=c('KMS','EP','QS','QStr','AKP','WMW'),tr=.2,pr=TRUE,alpha=.05,
nboot=2000,SEED=TRUE){
type=match.arg(method)
switch(type,
    KMS=KMS.ci(x,y,alpha=alpha,nboot=nboot,SEED=SEED),
    EP=EPci(x,y,tr=tr,alpha=alpha,SEED=SEED,nboot=nboot),
    QS=shiftPBci(x,y,locfun=QSfun,alpha=alpha,nboot=nboot,SEED=SEED),
    QStr=shiftPBci(x,y,locfun=tmean,alpha=alpha,nboot=nboot,SEED=SEED),
    AKP=akp.effect.ci(x,y,tr=tr,alpha=alpha,nboot=nboot,SEED=SEED),
    WMW=cidv2(x,y))
}


esI<-function(x,tr=.2,nboot=100,SEED=TRUE){
#
# Explanatory measure of effect size for an interaction in
# a 2-by-2 ANOVA
#
#  Assume x is a mtrix with 4 columns or has list mode with length 4
#  Also assume interaction is for x_1-x_2 versus x_3-x_4
#
if(is.matrix(x)|| is.data.frame(x))x=listm(x)
es=yuenv2(outer(x[[1]],x[[2]],"-"),outer(x[[3]],x[[4]],"-"),
tr=tr,nboot=nboot,SEED=SEED)$Effect.Size
list(Effect.Size=es)
}




# ----------------------------------------------------------------------------

# esImcp

# ----------------------------------------------------------------------------

esImcp<-function(J,K,x,tr=0.2,nboot=100,SEED=TRUE){
#
#  Compute measure of effect size for all interactions in a J-by-K design
#  A robust, heteroscedastic measure of effect (explanatory measure of
#  effect size) is used.
#
if(is.matrix(x)|| is.data.frame(x))x=listm(x)
con=con2way(J,K)$conAB
es=NULL
for (j in 1:ncol(con)){
flag=(con[,j]!=0)
es[j]=esI(x[flag],tr=tr,nboot=nboot,SEED=SEED)$Effect.Size
}
list(Effect.Sizes=es,contrast.coef=con)
}




# ----------------------------------------------------------------------------

# ESmcp.CI

# ----------------------------------------------------------------------------

ESmcp.CI<-function(x,method='KMS',alpha=.05,nboot=2000,SEED=TRUE,pr=TRUE){
#
# All
# Choices for method:
# 'EP','QS','QStr','AKP','WMW','KMS'
#
#

if(is.data.frame(x))x=as.matrix(x)
if(SEED)set.seed(2)
if(is.matrix(x))x<-listm(x)
x=elimna(x)
n=lapply(x,length)
J<-length(x)
JALL=(J^2-J)/2
if(identical(method,'EP')){
if(pr)print('Note: A method for computing a p.value for EP is not yet available')
output=matrix(NA,JALL,5)
ic=0
for(j in 1:J){
for(k in 1:J){
if(j<k){
ic=ic+1
a=ESfun.CI(x[[j]],x[[k]],method=method,alpha=alpha,nboot=nboot,SEED=SEED)
a=pool.a.list(a)
output[ic,]=c(j,k,a)
}}}
dimnames(output)=list(NULL,c('Grp','Grp','Effect Size','ci.low','ci.up'))
}
if(!identical(method,'EP')){
output=matrix(NA,JALL,7)
ic=0
for(j in 1:J){
for(k in 1:J){
if(j<k){
ic=ic+1
a=ESfun.CI(x[[j]],x[[k]],method=method,alpha=alpha,nboot=nboot,SEED=SEED)
a=pool.a.list(a)
if(identical(method,'WMW'))a=a[c(1,2,7,8,9,6)]
a=c(j,k,a[3:6])
output[ic,1:6]=a
}}}
dimnames(output)=list(NULL,c('Grp','Grp','Effect Size','ci.low','ci.up','p.value','p.adjusted'))
output[,7]=p.adjust(output[,6],method='hoch')
}
n=pool.a.list(n)
list(n=n,output=output)
}




# ----------------------------------------------------------------------------

# ESprodis

# ----------------------------------------------------------------------------

ESprodis<-function(x,est=tmean,REP=10,DIF=FALSE,SEED=TRUE,...){
#
#  Independent groups.
#  Compute an effect size based on projection distances
#
if(SEED)set.seed(2)
if(is.matrix(x))x=listm(x)
J=length(x)
n=pool.a.list(lapply(x,length))
nmin=min(n)
V=var(n)
if(V==0)E=ESprodis.EQ(x,est=est,DIF=DIF,REP=REP,...)
if(V!=0){
E=NA
XS=list()
for(i in 1:REP){
for(j in 1:J)XS[[j]]=sample(x[[j]],nmin)

E[i]=ESprodis.EQ(XS,est=est,DIF=DIF,...)
}
E=mean(E)
}
E
}




# ----------------------------------------------------------------------------

# ESprodis.EQ

# ----------------------------------------------------------------------------

ESprodis.EQ<-function(x,est=tmean,REP=1,DIF=TRUE,iter=1,...){
#
#  Independent groups.
#  Compute an effect size based on projection distances
#  Equal sample sizes
#
if(is.matrix(x))x=listm(x)
J=length(x)
n=pool.a.list(lapply(x,length))
nord=order(n)
nmin=n[nord[1]]
XS=list()
E=NA
for(k in 1:REP){
for(j in 1:J)XS[[j]]=sample(x[[j]])
if(!DIF)E[k]=rmES.pro(XS,est=est,iter=iter,...)$effect.size
if(DIF)E[k]=rmES.dif.pro(XS,est=est,...)
}
E=mean(E)
E
}



estBetaParams <- function(mu, var) {
#
#  Estimate parameters of the beta distribution, r and s, given the mean and variance.
  alpha <- ((1 - mu) / var - 1 / mu) * mu ^ 2
  beta <- alpha * (1 / mu - 1)
list(r=alpha,s=beta)
}
fac2BBMlist<-function(x,grp.col,lev.col,pr=TRUE){
#
#  This function is useful when dealing with a two-way MANOVA
#  It takes data stored in x, a matrix or data frame,
#  and creates groups based on the data in the two columns
#  indicated by the argument
#
#  grp.col
#  lev.col indicates the columns where p-variate  are contained.
#
#  Example:
#   z=fac2BBMlist(plasma,c(2,3),c(7,8))
#   creates groups based on values in columns 2 (Factor A) and 3 (Factor B).
#  z[[1]] contains a matrix having two columns; the data are taken
# from columns 7 and 8 of plasma
#
res=selbybbw(x,grp.col,lev.col,pr=pr)
p=length(lev.col)
J=length(unique(x[,grp.col[1]]))
K=length(unique(x[,grp.col[2]]))
y=list()
ic=1-p
iu=0
jk=0
for(j in 1:J){
for(k in 1:K){
ic=ic+p
iu=iu+p
jk=jk+1
y[[jk]]=matl(res[ic:iu])
}}
y
}


fac2Mlist<-function(x,grp.col,lev.col,pr=TRUE){
#
#  sort and store data in a matrix or data frame into
#  groups, where the jth group
#  has p-variate data
#
#  grp.col is column indicating levels of between factor.
#  lev.col indicates the columns where repeated measures are contained
#
#  Example:  column 2 contains information on levels of between factor
#  have a 3 by 2 design, column 3 contains time 1 data,
#  column 7 contains time 2
#  fac2Mlist(x,2,c(3,7)) will store data in list mode, having length
#  2 (the number of levels), with each level containing a
#  matrix having two columns. The first column is based on values
#  in column 3 of the matrix x, and the second column is based on
#  data in column 7 of x.
#
res=selbybw(x,grp.col,lev.col)
if(pr){
print("Levels for between factor:")
print(sort(unique(x[,grp.col])))
}
res=res$x
p=length(lev.col)
J=length(unique(x[,grp.col]))
y=list()
ic=1-p
iu=0
for(j in 1:J){
ic=ic+p
iu=iu+p
y[[j]]=matl(res[ic:iu])
}
y
}




# ----------------------------------------------------------------------------

# FactorAnalysis

# ----------------------------------------------------------------------------

FactorAnalysis <- function(data, corr.matrix = FALSE, max.iteration = 50,
                            n.factors = 0, corr.type = "pearson") {
# Analyzes comparison data with known factorial structures
#
# Args:
#   data          : Matrix to store the simulated data.
#   corr.matrix   : Correlation matrix (default is FALSE)
#   max.iteration : Maximum number of iterations (scalar, default is 50).
#   n.factors     : Number of factors (scalar, default is 0).
#   corr.type     : Type of correlation (character, default is "pearson",
#                   user can also call "spearman").
#
# Returns:
#   $loadings : Factor loadings (vector, if one factor. matrix, if multiple
#               factors)
#   $factors  : Number of factors (scalar).
#
  data <- as.matrix(data)
  n.variables <- dim(data)[2]
  if (n.factors == 0) {
    n.factors <- n.variables
    determine <- TRUE
  } else {
    determine <- FALSE
  }
  if (!corr.matrix) {
    corr.matrix <- cor(data, method = corr.type)
  } else {
    corr.matrix <- data
  }
  criterion <- .001
  old.h2 <- rep(99, n.variables)
  h2 <- rep(0, n.variables)
  change <- 1
  iteration <- 0
  factor.loadings <- matrix(nrow = n.variables, ncol = n.factors)
  while ((change >= criterion) & (iteration < max.iteration)) {
    iteration <- iteration + 1
    eigenvalue <- eigen(corr.matrix)
    l <- sqrt(eigenvalue$values[1:n.factors])
    for (i in 1:n.factors)
      factor.loadings[, i] <- eigenvalue$vectors[, i] * l[i]
    for (i in 1:n.variables)
      h2[i] <- sum(factor.loadings[i, ] * factor.loadings[i, ])
    change <- max(abs(old.h2 - h2))
    old.h2 <- h2
    diag(corr.matrix) <- h2
  }
  if (determine) n.factors <- sum(eigenvalue$values > 1)
  return(list(loadings = factor.loadings[, 1:n.factors],
              factors = n.factors))
}




# ----------------------------------------------------------------------------

# fflynx

# ----------------------------------------------------------------------------

fflynx<-function(){
# R users need to type library(ts) and data(lynx)
Y <- log10(lynx)
FAR2 <- 1:114
FAR11 <- 1:114
FAR12 <- 1:114
SETAR272 <- 1:114
SETAR252 <- 1:114
for(i in 3:114){
FAR2[i ] <- 1.05 + 1.41*Y[i-1] -0.77*Y[i-2]}
for(i in 12:114){
FAR11[i ] <-  1.13*Y[i-1] -0.51*Y[i-2] + .23*Y[i-3] -0.29*Y[i-4]
 + .14*Y[i-5] -0.14*Y[i-6] + 0.08*Y[i-7] -0.04*Y[i-8]
 + .13*Y[i-9] + 0.19*Y[i-10] - .31*Y[i-11] }
for(i in 13:114){
FAR12[i ] <-  1.123 + 1.084*Y[i-1] -0.477*Y[i-2] + .265*Y[i-3] -0.218*Y[i-4]
 + .180*Y[i-9]  - .224*Y[i-12] }
for(i in 13:114){
if( Y[i-2] <= 3.116){
SETAR272[i ] <-  0.546  + 1.032*Y[i-1] -0.173*Y[i-2] + .171*Y[i-3] -0.431*Y[i-4]
 + .332*Y[i-5]  - .284*Y[i-6] + .210*Y[i-7]}
else {SETAR272[i ] <-  2.632  + 1.492*Y[i-1] -1.324*Y[i-2]}
}
for(i in 13:114){
if( Y[i-2] <= 3.05){
SETAR252[i ] <-  0.768  + 1.064*Y[i-1] -0.200*Y[i-2] + .164*Y[i-3] -0.428*Y[i-4]
 + .181*Y[i-5] }
else {SETAR252[i ] <-  2.254  + 1.474*Y[i-1] -1.202*Y[i-2]}
}
x <- cbind(Y,FAR2,FAR11,FAR12,SETAR272,SETAR252)
x <- x[13:114,]
print(cor(x))
pairs(x)
}


ffplot<-
function(x, y, nsamps = 7)
{
# For Unix, use X11() to turn on the graphics device before
# using this function. For R, first type library(lqs).
# Makes an FF plot with several resistant estimators.
# Need the program mbareg..
	n <- length(y)
	rmat <- matrix(nrow = n, ncol = 6)
	lsfit <- y - lsfit(x, y)$residuals
	print("got OLS")
	l1fit <- y - l1fit(x, y)$residuals
	print("got L1")
	almsfit <- y - lmsreg(x, y)$resid
	print("got ALMS")
	altsfit <- y - ltsreg(x, y)$residuals
	print("got ALTS")
	mbacoef <- mbareg(x, y, nsamp = nsamps)$coef
	MBAFIT <- mbacoef[1] + x %*% mbacoef[-1]
	print("got MBA")
	rmat[, 1] <- y
	rmat[, 2] <- lsfit
	rmat[, 3] <- l1fit
	rmat[, 4] <- almsfit
	rmat[, 5] <- altsfit
	rmat[, 6] <- MBAFIT
	pairs(rmat, labels = c("Y", "OLS Fit", "L1 Fit", "ALMS Fit",
	                       "ALTS Fit", "MBAREG Fit"))
}

ffplot2<-
function(x, y, nsamps = 7)
{
# For Unix, use X11() to turn on the graphics device before
# using this function. For R, first type library(lqs).
# Makes an FF plot with several resistiant estimators.
# Need the program mbareg.
	n <- length(y)
	rmat <- matrix(nrow = n, ncol = 5)
	lsfit <- y - lsfit(x, y)$residuals
	print("got OLS")
	almsfit <- y - lmsreg(x, y)$resid
	print("got ALMS")
	altsfit <- y - ltsreg(x, y)$residuals
	print("got ALTS")
	mbacoef <- mbareg(x, y, nsamp = nsamps)$coef
	MBAFIT <- mbacoef[1] + x %*% mbacoef[-1]
	print("got MBA")
	rmat[, 1] <- y
	rmat[, 2] <- lsfit
	rmat[, 3] <- almsfit
	rmat[, 4] <- altsfit
	rmat[, 5] <- MBAFIT
	pairs(rmat, labels = c("Y", "OLS Fit", "ALMS Fit", "ALTS Fit", "MBAREG Fit"))
}




# ----------------------------------------------------------------------------

# FisherLSD

# ----------------------------------------------------------------------------

FisherLSD<-function(x,alpha=.05){
#
#  Perform Fisher's LSD method
# x is assumed to be a matrix, or data frame, or to have list mode
#
if(is.matrix(x) || is.data.frame(x))x=listm(x)
n=lapply(x,length)
J=length(x)
g=NULL
X=NULL
for(j in 1:J){
g=c(g,rep(j,n[j]))
X=c(X,x[[j]])
}
FT=anova1(x)
res=NULL
if(FT$p.value<=alpha)res=pairwise.t.test(X,g,p.adjust.method='none')
list(ANOVA_F_p.value=FT$p.value,LSD=res)
}




# ----------------------------------------------------------------------------

# freq5

# ----------------------------------------------------------------------------

freq5<-function(x1,x2=NULL,x3=NULL,x4=NULL,x5= NULL,xlab="X",ylab="Rel. Freq."){
#
# Compute relative frequencies associated with the sample space for up to five variables.
#
#
temp2=NULL
temp3=NULL
temp4=NULL
temp5=NULL
freqx2=NULL
freqx3=NULL
freqx4=NULL
freqx5=NULL

x1<-x1[!is.na(x1)]
temp1<-sort(unique(x1))
freqx1<-NA
for(i in 1:length(temp1)){
freqx1[i]<-sum(x1==temp1[i])
}
freqx1<-freqx1/length(x1)
N=1

if(!is.null(x2)){
N=2
x2<-x2[!is.na(x2)]
temp2<-sort(unique(x2))
freqx2<-NA
for(i in 1:length(temp2)){
freqx2[i]<-sum(x2==temp2[i])
}
freqx2<-freqx2/length(x2)
}

if(!is.null(x3)){
N=3
x3<-x3[!is.na(x3)]
temp3<-sort(unique(x3))
freqx3<-NA
for(i in 1:length(temp3)){
freqx3[i]<-sum(x3==temp3[i])
}
freqx3<-freqx3/length(x3)
}

if(!is.null(x4)){
N=4
x4<-x4[!is.na(x4)]
temp4<-sort(unique(x4))
freqx4<-NA
for(i in 1:length(temp4)){
freqx4[i]<-sum(x4==temp4[i])
}
freqx4<-freqx4/length(x4)
}

if(!is.null(x5)){
N=5
x5<-x5[!is.na(x5)]
temp5<-sort(unique(x5))
freqx5<-NA
for(i in 1:length(temp5)){
freqx5[i]<-sum(x5==temp5[i])
}
freqx5<-freqx5/length(x5)
}

v=list()
v[[1]]=cbind(temp1,freqx1)
v[[2]]=cbind(temp2,freqx2)
v[[3]]=cbind(temp3,freqx3)
v[[4]]=cbind(temp4,freqx4)
v[[5]]=cbind(temp5,freqx5)
for(j in 1:N)dimnames(v[[j]])=list(NULL,c('Value','Rel. Freq'))
v
}

splotg2=splotg5
#s2plot=splotg5   #Used in earlier versions.




# ----------------------------------------------------------------------------

# func.out

# ----------------------------------------------------------------------------

func.out<-function(x,xlab='Time',ylab=' '){
#
#  A spaghetti plot for functional data that indicates outliers with a dashed line
#  x is a matrix with n rows and p columns
#
#  It is assumed that the function is measured at times 1, 2, ..., p
#
x=elimna(t(x)) # colums with missing data are automatically removed
x=t(x)
p=ncol(x)
n=nrow(x)
plot(c(1:p),seq(min(x),max(x),length.out=p),type='n',xlab=xlab,ylab=ylab)
flag=func.plot(x,plotit=FALSE)$outpoint
chk=c(1:n)
flag2=chk
nsub=length(flag)
if(nsub>0)flag2=chk[-flag]
for(j in 1:length(flag2))lines(c(1:p),x[flag2[j],])
if(nsub>0)for(j in 1:nsub)lines(c(1:p),x[flag[j],],lty=2)
}

fysim<-function( runs = 20)
{
# 20 FY plots for simulated AR(2) time series data
fycorr <- 1:runs
for(i in 1: runs){
Y <- ardata()$arts
out <- ar.yw(Y)
Yts <- Y[10:200]
FIT <- Yts - out$resid[10:200]
plot(FIT,Yts)
abline(0,1)
fycorr[i] <- cor(FIT,Yts)
}
list(fycorr=fycorr)
}

gamper<-
function(h, k=500)
{
	n <- 10000
	c <- 5000
	gam0 <- min((n - c)/n, (1 - (1 - 0.2^(1/k))^(1/
		h))) * 100
	print(gam0)
}

gamper2<-
function(p, k = 500)
{
##estimates the amount of contamination fmcd can tolerate
	n <- 10000
	c <- 5000
	h <- p + 1
	gam0 <- min((n - c)/n, (1 - (1 - 0.2^(1/k))^(1/h))) * 100
	print(gam0)
}




# ----------------------------------------------------------------------------

# GBY3Fs

# ----------------------------------------------------------------------------

GBY3Fs <- function(s,c3)
{
  Fs= exp(-(log(1+exp(-abs(s)))+abs(s)*(s<0)))
  resGinf=exp(0.25)*sqrt(pi)*(pnorm(sqrt(2)*(0.5+sqrt(-log(Fs))))-1)
  resGinf=(resGinf+(Fs*exp(-sqrt(-log(Fs)))))*as.numeric(s <= -log(exp(c3)-1))
  resGsup=((Fs*exp(-sqrt(c3)))+(exp(0.25)*sqrt(pi)*(pnorm(sqrt(2)*(0.5+sqrt(c3)))-1)))*as.numeric(s > -log(exp(c3)-1))
  return(resGinf+resGsup)
}




# ----------------------------------------------------------------------------

# GBY3Fsm

# ----------------------------------------------------------------------------

GBY3Fsm <- function(s,c3)
{
  Fsm=exp(-(log(1+exp(-abs(s)))+abs(s)*(s>0)))
  resGinf=exp(0.25)*sqrt(pi)*(pnorm(sqrt(2)*(0.5+sqrt(-log(Fsm))))-1)
  resGinf=(resGinf+(Fsm*exp(-sqrt(-log(Fsm)))))*as.numeric(s >= log(exp(c3)-1))
  resGsup=((Fsm*exp(-sqrt(c3)))+(exp(0.25)*sqrt(pi)*(pnorm(sqrt(2)*(0.5+sqrt(c3)))-1)))*as.numeric(s < log(exp(c3)-1))
  return(resGinf+resGsup)
}




# ----------------------------------------------------------------------------

# gengh

# ----------------------------------------------------------------------------

gengh <- function(n.cases = 1000, n.variables = 2,
                  g = 0, h = 0, rho = 0,
                  corr.type = "pearson"){

# Reference
# Ruscio, J. & Kaczetow, W. (2008)
# Simulating Multivariate Nonnormal Data Using an Iterative Algorithm.
# Multivariate Behav Res, 43, 355-381.
# https://www.ncbi.nlm.nih.gov/pubmed/26741201

if(is.null(dim(rho))) { # create correlation matrix or already provided?
    target.corr <- diag(n.variables)
    target.corr[upper.tri(target.corr)] <- rho
    target.corr[lower.tri(target.corr)] <- rho
    } else if(dim(rho)[1] == n.variables & dim(rho)[2] == n.variables) {
    target.corr <- rho
}

# Change default to number of variables, as suggested by John Ruscio
# n.factors <- 0 # Number of factors (scalar)
n.factors <- n.variables # Number of factors (scalar)
max.trials <- 5 # Maximum number of trials (scalar)
initial.multiplier <- 1 # Value of initial multiplier (scalar)

# generate g-and-h data
if(length(g)==1) g <- rep(g, n.variables)
if(length(h)==1) h <- rep(h, n.variables)
distributions <- matrix(NA, nrow = n.cases, ncol = n.variables)
  for (V in 1:n.variables){
    distributions[,V] <- sort(ghdist(n.cases, g=g[V], h=h[V]))
  }

data <- matrix(0, nrow = n.cases, ncol = n.variables)
iteration <- 0
best.rmsr <- 1
trials.without.improvement <- 0
intermediate.corr <- target.corr

# If number of latent factors was not specified, determine it
if (n.factors == 0){
  Eigenvalues.Observed <- eigen(intermediate.corr)$values
  Eigenvalues.Random <- matrix(0, nrow = 100, ncol = n.variables)
  Random.Data <- matrix(0, nrow = n.cases, ncol = n.variables)
  for (i in 1:100){
    for (j in 1:n.variables){
      Random.Data[,j] <- sample(distributions[,j], size = n.cases, replace = TRUE)
    }
      Eigenvalues.Random[i,] <- eigen(cor(Random.Data))$values
  }
  Eigenvalues.Random <- apply(Eigenvalues.Random, 2, mean) # calculate mean eigenvalue for each factor
  n.factors <- max(1, sum(Eigenvalues.Observed > Eigenvalues.Random))
}

shared.comp <- matrix(rnorm(n.cases * n.factors, 0, 1), nrow = n.cases,
                      ncol = n.factors)
unique.comp <- matrix(rnorm(n.cases * n.variables, 0, 1), nrow = n.cases,
                      ncol = n.variables)
shared.load <- matrix(0, nrow = n.variables, ncol = n.factors)
unique.load <- matrix(0, nrow = n.variables, ncol = 1)
while (trials.without.improvement < max.trials) {
  iteration <- iteration + 1
  factor.analysis <- FactorAnalysis(intermediate.corr, corr.matrix = TRUE,
                                    max.iteration = 50, n.factors, corr.type)
  if (n.factors == 1) {
    shared.load[, 1] <- factor.analysis$loadings
  } else {
    for (i in 1:n.factors)
      shared.load[, i] <- factor.analysis$loadings[, i]
  }
  shared.load[shared.load > 1] <- 1
  shared.load[shared.load < -1] <- -1
  if (shared.load[1, 1] < 0)
    shared.load <- shared.load * -1
  for (i in 1:n.variables)
    if (sum(shared.load[i, ] * shared.load[i, ]) < 1) {
      unique.load[i, 1] <- (1 - sum(shared.load[i, ] * shared.load[i, ]))
    } else {
      unique.load[i, 1] <- 0
    }
  unique.load <- sqrt(unique.load)
  for (i in 1:n.variables)
    data[, i] <- (shared.comp %*% t(shared.load))[, i] + unique.comp[, i] *
    unique.load[i, 1]
  for (i in 1:n.variables) {
    data <- data[sort.list(data[, i]), ]
    data[, i] <- distributions[, i]
  }
  reproduced.corr <- cor(data, method = corr.type)
  residual.corr <- target.corr - reproduced.corr
  rmsr <- sqrt(sum(residual.corr[lower.tri(residual.corr)] *
                     residual.corr[lower.tri(residual.corr)]) /
                 (.5 * (n.variables * n.variables - n.variables)))
  if (rmsr < best.rmsr) {
    best.rmsr <- rmsr
    best.corr <- intermediate.corr
    best.res <- residual.corr
    intermediate.corr <- intermediate.corr + initial.multiplier *
      residual.corr
    trials.without.improvement <- 0
  } else {
    trials.without.improvement <- trials.without.improvement + 1
    current.multiplier <- initial.multiplier *
      .5 ^ trials.without.improvement
    intermediate.corr <- best.corr + current.multiplier * best.res
  }
}

factor.analysis <- FactorAnalysis(best.corr, corr.matrix = TRUE,
                                  max.iteration = 50, n.factors,
                                  corr.type)
if (n.factors == 1) {
  shared.load[, 1] <- factor.analysis$loadings
} else {
  for (i in 1:n.factors)
    shared.load[, i] <- factor.analysis$loadings[, i]
}
shared.load[shared.load > 1] <- 1
shared.load[shared.load < -1] <- -1
if (shared.load[1, 1] < 0)
  shared.load <- shared.load * -1
for (i in 1:n.variables)
  if (sum(shared.load[i, ] * shared.load[i, ]) < 1) {
    unique.load[i, 1] <- (1 - sum(shared.load[i, ] * shared.load[i, ]))
  } else {
    unique.load[i, 1] <- 0
  }
unique.load <- sqrt(unique.load)
for (i in 1:n.variables)
  data[, i] <- (shared.comp %*% t(shared.load))[, i] + unique.comp[, i] *
  unique.load[i, 1]
data <- apply(data, 2, scale) # standardizes each variable in the matrix
for (i in 1:n.variables) {
  data <- data[sort.list(data[, i]), ]
  data[, i] <- distributions[, i]
}
data
}

################################################################################




# ----------------------------------------------------------------------------

# getBetaHdi

# ----------------------------------------------------------------------------

getBetaHdi <- function(a, b, width) {
eps <- 1e-9
if (a < 1 + eps & b < 1 + eps) # Degenerate case
return(c(NA, NA))
if (a < 1 + eps & b > 1) # Left border case
return(c(0, width))
if (a > 1 & b < 1 + eps) # Right border case
return(c(1 - width, 1))
if (width > 1 - eps)
return(c(0, 1))
# Middle case
mode <- (a - 1) / (a + b - 2)
pdf <- function(x) dbeta(x, a, b)
l <- uniroot(
f = function(x) pdf(x) - pdf(x + width),
lower = max(0, mode - width),
upper = min(mode, 1 - width),
tol = 1e-9
)$root
r <- l + width
return(c(l, r))
}




# ----------------------------------------------------------------------------

# ghmul

# ----------------------------------------------------------------------------

ghmul<-function(n,g=0,h=0,p=2,cmat=diag(rep(1,p)),SEED=FALSE){
#
# generate n observations from a p-variate dist
# based on the g and h dist.
#
# cmat is the correlation matrix
#
x<-rmulnorm(n,p,cmat,SEED=SEED)
for(j in 1:p){
if (g>0){
x[,j]<-(exp(g*x[,j])-1)*exp(h*x[,j]^2/2)/g
}
if(g==0)x[,j]<-x[,j]*exp(h*x[,j]^2/2)
}
x
}




# ----------------------------------------------------------------------------

# ghtransform

# ----------------------------------------------------------------------------

ghtransform<-function(x,g=0,h=0){
#
#  transform normal data in x to a g-and-h distribution
 if (g>0){
 ghdist<-(exp(g*x)-1)*exp(h*x^2/2)/g
 }
 if(g==0)ghdist<-x*exp(h*x^2/2)
 ghdist
 }

 plot.ghdist<-function(g=0,h=0,xlab='',ylab='f(x)'){
#
# plot density function of a g-and-h distribution
#
x=seq(-3,3,.05)
pf=dnorm(x)
xs=ghtransform(x,g=g,h=h)
plot(xs,pf,type='n',xlab=xlab,ylab=ylab)
lines(xs,pf)
}




# ----------------------------------------------------------------------------

# gk

# ----------------------------------------------------------------------------

gk <- function(x, y, ...)
{
  ((gk.sigmamu(x + y, ...))^2 - (gk.sigmamu(x - y, ...))^2) / 4.0
}




# ----------------------------------------------------------------------------

# gk.sigmamu

# ----------------------------------------------------------------------------

gk.sigmamu <- function(x, c1 = 4.5, c2 = 3.0, mu.too = FALSE, ...)
{
  n <- length(x)

  medx <- median(x)
  sigma0 <- median(abs(x - medx))
#  w <- (x - medx) / sigma0
#  w <- (1.0 - (w / c1)^2)^2
  #w[w < 0.0] <- 0.0
w <- abs(x - medx) / sigma0
w <- ifelse(w<=c1,(1.0 - (w / c1)^2)^2,0)
  mu <- sum(x * w) / sum(w)

  x <- (x - mu) / sigma0
  rho <- x^2
  rho[rho > c2^2] <- c2^2
  sigma2 <- sigma0^2 / n * sum(rho)

  if(mu.too)
    c(mu, sqrt(sigma2))
  else
    sqrt(sigma2)
}




# ----------------------------------------------------------------------------

# gkcor

# ----------------------------------------------------------------------------

gkcor<-function(x,y,varfun=tauvar,ccov=FALSE,...){
#
# Compute a correlation coefficient using the Gnanadesikan-Ketterning
# estimator.
#  ccov=T, computes covariance instead.
# (cf. Marrona & Zomar, 2002, Technometrics
#
val<-.25*(varfun(x+y,...)-varfun(x-y,...))
if(!ccov)val<-val/(sqrt(varfun(x,...))*sqrt(varfun(y,...)))
val
}
grit<-function(x,y,itest=1,sm.fun=rplot,nboot=500,alpha=.05,SEED=TRUE,
fr=1,plot.fun=rplot,plotit=TRUE,...){
#
# Fit a running interval smoother using projection distances
# excluding the predictor variable itest
# itest=1 by default, meaning that the goal is to test
# the hypothesis that the first variable does not contribute
# to the regression model
#
# Method fits a smooth using x_1, ..., x_p, excluding variabe itest
# Then x_itest and the resulting residuals are passed to indt
# Alternative choices for smooth include
# sm.fun=lplot, and if p>2, runpd
#
if(!is.matrix(x))stop("Should have two or more predictors stored in a matrix")
p<-ncol(x)
pp<-p+1
x<-elimna(cbind(x,y))
y<-x[,pp]
x<-x[,1:p]
flag<-rep(TRUE,ncol(x))
flag[itest]<-FALSE
temp<-sm.fun(x[,flag],y,plotit=FALSE,pyhat=TRUE,fr=fr)
res<-y-temp
test.it<-indt(x[,itest],res)
if(plotit)plot.fun(x[,itest],res,...)
test.it
}




# ----------------------------------------------------------------------------

# gskew

# ----------------------------------------------------------------------------

gskew<-function(g){
#
# skew and kurtosis of a g-and-h distribution when h=0
#
#
v1=sqrt(3*exp(2*g^2)+exp(3*g^2)-4)
v2=3*exp(2*g^2)+2*exp(3*g^2)+exp(4*g^2)-3  #Headrick has -6 not	-3, but	based on n=1000000, -3 works
list(skew=v1,kurtosis=v2)
}




# ----------------------------------------------------------------------------

# gvar

# ----------------------------------------------------------------------------

gvar<-function(m){
#
# Compute the generalized variance of a matrix m
#
m<-elimna(m)
temp<-var(m)
gvar<-prod(eigen(temp)$values)
gvar
}

gvar2g<-function(x,y,nboot=100,DF=TRUE,eop=1,est=skipcov,
alpha=.05,cop=3,op=1,MM=FALSE,SEED=TRUE,pr=FALSE,fast=FALSE,...){
#
# Two independent groups.
# Test hypothesis of equal generalized variances.
#
# DF=T, means skipcov with MM=F is used.
#
# That is, W-estimator based on a projection outlier detection method
# and Carling's method applied to projections.
# if equal sample sizes, adjusted critical value is used where appopriate
#
# DF=F
# no adjusted critical value is used and any robust measure of
# scatter can be used.
#
#  Choices for est include:
#  var
#  covmcd
#  covmve
#  skipcov with MM=F (boxplot) MM=T (MAD-MEDIAN), op=1 (MGV method)
#               op=2 (projection method for outliers)
#  covroc  Rocke's measure of scatter,
#
#   op, cop and eop, see skipcov
#   adjusted critical level should be used with
#   skipcov and alpha=.05 only.
#   fast=T, will use skipcov.for if it is available.
#
#    Function returns ratio of first estimate divided by second estimate
#
if(SEED)set.seed(2)
#if(!is.matrix(x))stop("x should be a matrix with ncol>1")
if(is.null(dim(x)))stop("x should be a matrix or data frame with ncol>1")
if(is.null(dim(y)))stop("y should be a matrix or data frame with ncol>1")
#if(!is.matrix(y))stop("y should be a matrix with ncol>1")
if(ncol(x)==1 || ncol(y)==1)stop("Only multivariate data are allowed")
n1<-nrow(x)
n2<-nrow(y)
adalpha<-NA
if(DF){
if(n1==n2 && alpha==.05){
p1<-ncol(x)
if(p1==2){
if(n1>=20)adalpha<-1.36/n1+.05
}
if(p1==3){
if(n1>=20)adalpha<-1.44/n+.05
}
if(p1==4){
if(n1>=40)adalpha<-2.47/n1+.05
}
if(p1==5){
if(n1>=40)adalpha<-3.43/n+.05
}
if(p1==6){
if(n1>=60)adalpha<-4.01/n1+.05
}}}
val<-NA
for(j in 1:nboot) {
                data1 <- sample(n1, size = n1, replace = T)
                data2 <- sample(n2, size = n2, replace = T)
if(!DF){
val[j]<-rgvar(as.matrix(x[data1,]),est=est,...)-
rgvar(as.matrix(y[data2,]),est=est,...)
}
if(DF){val[j]<-
if(!fast){
rgvar(as.matrix(x[data1,]),est=skipcov,op=op,outpro.cop=cop,MM=MM,...)-
rgvar(as.matrix(y[data2,]),est=skipcov,op=op,outpro.cop=cop,MM=MM,...)
}
if(fast){
rgvar(as.matrix(x[data1,]),est=skipcov.for,op=op,outpro.cop=cop,MM=MM,...)-
rgvar(as.matrix(y[data2,]),est=skipcov.for,op=op,outpro.cop=cop,MM=MM,...)
}
if(pr)print(c(j,val[j]))
}}
p.value<-sum(val<0)/nboot
p.value<-2*min(p.value,1-p.value)
est1=rgvar(x,est=est)
est2=rgvar(y,est=est)
list(p.value=p.value,adjusted.crit.level=adalpha,ratio.of.estimates=est1/est2,n1=n1,n2=n2)
}




# ----------------------------------------------------------------------------

# gvarg

# ----------------------------------------------------------------------------

gvarg<-function(m,var.fun=cov.mba,...){
#
# Compute the generalized variance of a matrix m
# It is assumed that var.fun returns a covariance matrix only
#
# (Some functions return a covariance matrix in list mode: $cov
# These functions do not work here.)
#
# other possible choices for var.fun:
# skipcov
# tbscov
# covout
# covogk
# mgvcov
# mvecov
# mcdcov
#
m<-elimna(m)
m<-as.matrix(m)
temp<-var.fun(m,...)
gvar<-prod(eigen(temp)$values)
gvar
}




# ----------------------------------------------------------------------------

# H.lasso

# ----------------------------------------------------------------------------

H.lasso<- function(x,y,lambda.lasso.try=NULL,k=1.5, STAND=TRUE, xout=FALSE,outfun=outpro,...){
#
# A slight modification of code supplied by Jung et al. (2016)
#
#
X=x
Y=y

library(glmnet)
X=as.matrix(X)
p1<-ncol(X)+1
p<-ncol(X)
xy<-cbind(X,Y)
xy<-elimna(xy)
X<-xy[,1:p]
Y<-xy[,p1]
if(STAND)X=standm(X)
if(is.null(lambda.lasso.try))lambda.lasso.try=seq(0.01,0.6,length.out=100)
library(glmnet)
if(xout){
X<-as.matrix(X)
flag<-outfun(X,plotit=FALSE,...)$keep
X<-X[flag,]
Y<-Y[flag]
X<-as.matrix(X)
n.keep=nrow(X)
}
n<-length(Y)
Y.orgn<- Y
model.for.cv<- cv.glmnet(X, Y, family='gaussian',lambda=lambda.lasso.try)
lambda.lasso.opt<- model.for.cv$lambda.min
model.est<- glmnet(X,Y,family='gaussian',lambda=lambda.lasso.opt)
fit.lasso<- predict(model.est,X,s=lambda.lasso.opt)
res.lasso<- Y-fit.lasso
sigma.init<- mad(Y-fit.lasso)
beta.pre<- c(model.est$a0,as.numeric(model.est$beta))
Y.old<- Y
tol = 10
n.iter <- 0
while(tol>1e-4 & n.iter<100)
{
Y.new<- fit.lasso + winsorized(res.lasso,a=k, sigma=sigma.init)
model.for.cv<- cv.glmnet(X,Y.new, family='gaussian',lambda=lambda.lasso.try)
model.est<- glmnet(X,Y.new,family='gaussian',lambda=model.for.cv$lambda.min)
fit.lasso<- predict(model.est,X,s=model.for.cv$lambda.min)
res.lasso<- Y.new-fit.lasso
beta.post <- c(model.est$a0,as.numeric(model.est$beta))
tol<- sum((beta.pre-beta.post)^2)
n.iter<- n.iter+1
beta.pre<- beta.post
}
sigma.est<- mean((Y.new-cbind(rep(1,n),X)%*%beta.post)^2)
Y.fit<- cbind(rep(1,n),X)%*%beta.post
Y.res<- Y.new - Y.fit
#object<- list(coefficient=beta.post,fit=Y.fit, iter = n.iter, sigma.est = sigma.est,
list(coef=beta.post,fit=Y.fit, iter = n.iter, sigma.est = sigma.est,
lambda.lasso.opt = model.est$lambda, residuals = Y.res)
}

hard.rejection <- function(distances, p, beta = 0.9, ...)
{
  d0 <- qchisq(beta, p) * median(distances) / qchisq(0.5, p)
  weights <- double(length(distances))
  weights[distances <= d0] <- 1.0
  weights
}

hc4qtest<-function(x,y,k,nboot=500,SEED=TRUE){
#
# Test the hypothesis that a OLS slope is zero using HC4 wild bootstrap using quasi-t test.
# k is the index of coefficient being tested
#
if(SEED)set.seed(2)
x<-as.matrix(x)
# First, eliminate any rows of data with missing values.
temp <- cbind(x, y)
        temp <- elimna(temp)
        pval<-ncol(temp)-1
        x <- temp[,1:pval]
        y <- temp[, pval+1]
x<-as.matrix(x)
p<-ncol(x)
pp<-p+1
temp<-lsfit(x,y)
yhat<-mean(y)
res<-y-yhat
s<-lsfitNci4(x, y)$cov[-1, -1]
s<-as.matrix(s)
si<-s[k,k]
b<-temp$coef[2:pp]
qtest<-b[k]/sqrt(si)
data<-matrix(runif(length(y)*nboot),nrow=nboot)
data<-(data-.5)*sqrt(12) # standardize the random numbers.
rvalb<-apply(data,1,lsqtest4,yhat,res,x, k)
sum<-sum(abs(rvalb)>= abs(qtest[1]))
p.val<-sum/nboot
list(p.value=p.val)
}




# ----------------------------------------------------------------------------

# hc4test

# ----------------------------------------------------------------------------

hc4test<-function(x,y,pval=c(1:ncol(x)),xout=FALSE,outfun=outpro,pr=TRUE,plotit=FALSE,xlab="X",ylab="Y",...){
#
# Perform omnibus test using OLS and HC4 estimator
# That is, test the hypothesis that all of the slope parameters
# are equal to 0 in a manner that allows heteroscedasticity.
#
# recommended by Cribari-Neto (2004).
# Seems to work well with p=1 but can be unsatisfactory wit p>4 predictors,
# Unknown how large n must be when p>1
#
x<-as.matrix(x)
if(ncol(x)>1 && pr)print("WARNING: more than 1 predictor, olstest might be better")
if(nrow(x) != length(y))stop("Length of y does not match number of x values")
m<-cbind(x,y)
m<-elimna(m)
p=ncol(x)
p1=p+1
y<-m[,p1]
x=m[,1:p]
nrem=length(y)
n=length(y)
n.keep=n
x<-as.matrix(x)
if(xout){
if(identical(outfun,outblp))flag=outblp(x,y,plotit=FALSE)$keep
else
flag<-outfun(x,...)$keep
x<-as.matrix(x)
x<-x[flag,]
y<-y[flag]
n.keep=length(y)
x<-as.matrix(x)
}
n=n.keep
pvalp1<-pval+1
temp<-lsfit(x,y) # unrestricted
if(plotit){
if(p==1){
plot(x[,1],y,xlab=xlab,ylab=ylab)
abline(temp$coef)
}}
x<-cbind(rep(1,nrow(x)),x)
hval<-x%*%solve(t(x)%*%x)%*%t(x)
hval<-diag(hval)
hbar<-mean(hval)
delt<-cbind(rep(4,n),hval/hbar)
delt<-apply(delt,1,min)
aval<-(1-hval)^(0-delt)
x2<-x[,pvalp1]
pval<-0-pvalp1
x1<-x[,pval]
df<-length(pval)
x1<-as.matrix(x1)
imat<-diag(1,n)
M1<-imat-x1%*%solve(t(x1)%*%x1)%*%t(x1)
M<-imat-x%*%solve(t(x)%*%x)%*%t(x)
uval<-as.vector(M%*%y)
R2<-M1%*%x2
rtr<-solve(t(R2)%*%R2)
temp2<-aval*uval^2
S<-diag(aval*uval^2)
V<-n*rtr%*%t(R2)%*%S%*%R2%*%rtr
nvec<-as.matrix(temp$coef[pvalp1])
test<-n*t(nvec)%*%solve(V)%*%nvec
test<-test[1,1]
p.value<-1-pchisq(test,df)
list(n=nrem,n.keep=n.keep,test=test,p.value=p.value,coef=temp$coef)
}



hc4wmc<-function(x,y,nboot=599,k=2,grp=NA,con=0,SEED=TRUE,STOP=TRUE,...){
#
#   Test the hypothesis that J independent groups have identical slopes.
#   Using least squares regression
#   Data are stored in list mode or in a matrix.  In the first case
#   x[[1]] contains the data for the first group, x[[2]] the data
#   for the second group, etc. Length(x)=the number of groups = J.
#   If stored in a matrix, the columns of the matrix correspond
#   to groups.
#
#   Similarly, y[[1]] contains the data for the first group,
#   y[[2]] the data for the second groups, etc.
#
#   The argument grp can be used to analyze a subset of the groups
#   Example: grp=c(1,3,5) would compare groups 1, 3 and 5.
#
#   Missing values are allowed.
#
if(STOP)stop('Suggest ols1way. This function assumes equal n. To use anyway, set STOP=FALSE')
con<-as.matrix(con)
if(is.matrix(x))x<-listm(x)
if(!is.list(x))stop("Data must be stored in list mode or in matrix mode.")
if(is.matrix(y))y<-listm(y)
if(!is.list(y))stop("Data must be stored in list mode or in matrix mode.")
if(!is.na(sum(grp))){  # Only analyze specified groups.
xx<-list()
yy<-list()
for(i in 1:length(grp))
xx[[i]]<-x[[grp[i]]]
yy[[i]]<-y[[grp[i]]]
x<-xx
y<-yy
}
J<-length(x)
n<-length(x[[1]])
tempn<-0
slopes<-NA
covar<-NA
stemp<-NA
yhat<-numeric(J)
res<-matrix(,ncol=J, nrow=n)
for(j in 1:J){
temp<-cbind(x[[j]], y[[j]])
temp<-elimna(temp) # Remove missing values.
#n<-length(y[[j]])
tempn[j]<-length(temp)
x[[j]]<-temp[,1]
y[[j]]<-temp[,2]
tempx<-as.matrix(x[[j]])
tempy<-as.matrix(y[[j]])
#Getting yhat and residuals for wild bootstrap
yhat[j]<-mean(tempy)
res[,j]<-tempy-yhat[j]
#original Slope and SE
stemp<-lsfit(tempx, tempy)
slopes[j]<-stemp$coef[k] #Slopes for original data
covar[j]<-lsfitNci4(tempx, tempy)$cov[k,k] #original HC4 for coefficient(slope)
}
#
Jm<-J-1
#
# Determine contrast matrix
#
if(sum(con^2)==0){
ncon<-(J^2-J)/2
con<-matrix(0,J,ncon)
id<-0
for (j in 1:Jm){
jp<-j+1
for (h in jp:J){
id<-id+1
con[j,id]<-1
con[h,id]<-0-1
}}}
ncon<-ncol(con)
if(nrow(con)!=J){
stop("Something is wrong with con; the number of rows does not match the number of groups.")
}
#calculating original statistic
dif.slopes<-t(con)%*%slopes
o.se<-t(con^2)%*%covar
o.stat<-dif.slopes/sqrt(o.se) #original test statistics
#
om<-max(abs(o.stat)) #Max. absolute test statistics
#
if(SEED)set.seed(2) # set seed of random number generator so that
#             results can be duplicated.
#
data<-matrix(ifelse(rbinom(n*nboot*J,1,0.5)==1,-1,1),ncol=nboot*J) #discrete wild bootstrap sample
test<-numeric(nboot)
u<-rep(1, n)
c<-1
for (i in 1:nboot*J-J+1){
d<-data[,i:i+J-1]
ystar<-u%*%t(yhat)+res*d
ystar<-listm(ystar)
i<-i+J
test[c]<-mcslope(x,ystar, con, k)
#
c<-c+1
}
sum<-sum(test>= om)
p.val<-sum/nboot
list(p.value=p.val)
}




# ----------------------------------------------------------------------------

# hc4wtest

# ----------------------------------------------------------------------------

hc4wtest<-function(x,y,nboot=500,SEED=TRUE,RAD=TRUE,xout=FALSE,outfun=outpro,...){
#
# Test the hypothesis that all OLS slopes are zero
# using HC4 wild bootstrap using wald test.
#
# This function calls the functions
# olshc4 and
# lstest4
#
if(SEED)set.seed(2)
x<-as.matrix(x)
# First, eliminate any rows of data with missing values.
m<-elimna(cbind(x,y))
x<-as.matrix(x)
p<-ncol(x)
pp<-p+1
x<-m[,1:p]
y<-m[,pp]
if(xout){
flag<-outfun(x,...)$keep
x<-as.matrix(x)
x<-x[flag,]
y<-y[flag]
x<-as.matrix(x)
}
x<-as.matrix(x)
p<-ncol(x)
pp<-p+1
temp<-lsfit(x,y)
Rsq=ols(x,y)$R.squared
yhat<-mean(y)
res<-y-yhat
s<-olshc4(x, y)$cov[-1, -1]
si<-solve(s)
b<-temp$coef[2:pp]
wtest<-t(b)%*%si%*%b
if(RAD)data<-matrix(ifelse(rbinom(length(y)*nboot,1,0.5)==1,-1,1),nrow=nboot)
if(!RAD){
data<-matrix(runif(length(y)*nboot),nrow=nboot)
data<-(data-.5)*sqrt(12) # standardize the random numbers.
}
rvalb<-apply(data,1,lstest4,yhat,res,x)
sum<-sum(rvalb>= wtest[1,1])
p.val<-sum/nboot
list(p.value=p.val,R.squared=Rsq)
}




# ----------------------------------------------------------------------------

# hoch2.simp

# ----------------------------------------------------------------------------

hoch2.simp<-function(n,V,cil,tr,alpha=.05,con=NULL){
#
#
# Hochberg two-stage given n's sd's tr and alpha
# if tr>0, var. should contain winsorized variances
# raw data not provided
#
#V = variances or Winsorized variance if tr>0
# cil desired length of the confidence intervals
#
#
if(is.matrix(x))x<-listm(x)
J=length(n)
svec=V
tempn=n
tempt<-floor((1-2*tr)*tempn)
A<-sum(1/(tempt-1))
df<-J/A
if(is.null(con))con=con1way(J)
crit=qtukey(1-alpha,J,df)
avec<-NA
ncon=ncol(con)
for(i in 1:ncon){
temp<-con[,i]
avec[i]<-sum(temp[temp>0])
}
dvec<-(cil/(2*crit*avec))^2
d<-max(dvec)
n.vec<-NA
for(j in 1:J){
n.vec[j]<-max(tempn[j],floor(svec[j]/d)+1)
print(paste("Need an additional ", n.vec[j]-tempn[j],
" observations for group", j))
}
}




# ----------------------------------------------------------------------------

# hotel1

# ----------------------------------------------------------------------------

hotel1<-function(x,null.value=0,tr=0) {
#
# Perform a trimmed analog of Hotelling's (one-sample) T^2 test
# That is, for p-variate data, test the hypothesis that the p marginal
# trimmed means are equal to the value specified by
# the argument null.value
#
if (is.data.frame(x))
        x <- as.matrix(x)
x=elimna(x)
    if(!is.matrix(x))
        stop("'x' must be a numeric matrix or a data frame")
    n <- nrow(x)
    p <- ncol(x)
    mu=null.value
xbar=apply(x,2,mean,tr=tr)
    if(!is.numeric(mu) || ((lmu <- length(mu)) > 1 & lmu != p))
        stop("'null.value' must be a numeric vector of length ", p)
if(lmu == 1) mu <- rep(mu, p)
    xbar.mu <- xbar - mu
    V <- winall(x,tr=tr)$cov
h=n-2*floor(n*tr)
        k <- h / (n - 1) * (h - p) / p
        stat <- k * crossprod(xbar.mu, solve(V, xbar.mu))[1, ]
        pvalue <- 1 - pf(stat, p, h - p)
list(test.statistic = stat, degrees_of_freedom = c(p, h - p), p.value =
pvalue, estimate = xbar,
                null.value = mu)
}

 wwmcp<-function(J,K,x,tr=.2,alpha=.05,dif=TRUE,method='hoch'){
#
# Do all multiple comparisons for a within-by-within design
# using trimmed means
#
conM=con2way(J,K)
A=rmmcp(x,con=conM$conA,tr=tr,alpha=alpha,dif=dif)
 A$test[,4]=p.adjust(A$test[,3],method=method)
 dimnames(A$test)=list(NULL,c('con.num', 'test',   'p.value','adj.p.value',   'se'))
B=rmmcp(x,con=conM$conB,tr=tr,alpha=alpha,dif=dif)
 B$test[,4]=p.adjust(B$test[,3],method=method)
 dimnames(B$test)=list(NULL,c('con.num', 'test',   'p.value','adj.p.value',   'se'))
AB=rmmcp(x,con=conM$conAB,tr=tr,alpha=alpha,dif=dif)
  AB$test[,4]=p.adjust(AB$test[,3],method=method)
 dimnames(AB$test)=list(NULL,c('con.num', 'test',   'p.value','adj.p.value',   'se'))
list(Factor_A=A,Factor_B=B,Factor_AB=AB)
}

lindep.sub<-function(data,x,con=con,tr=tr){
con=as.matrix(con)
res=rmmcp(x[data,],con=con,tr=tr,dif=FALSE)$test[,2]
res
}

hotel1.tr<-function(x,null.value=0,tr=.2) {
#
# Perform a trimmed analog of Hotelling's (one-sample) T^2 test
# That is, for p-variate data, test the hypothesis that the p marginal
# trimmed means are equal to the value specified by
# the argument null.value
#
if (is.data.frame(x))
        x <- as.matrix(x)
x=elimna(x)
    if(!is.matrix(x))
        stop("'x' must be a numeric matrix or a data frame")
    n <- nrow(x)
    p <- ncol(x)
    mu=null.value
xbar=apply(x,2,mean,tr=tr)
    if(!is.numeric(mu) || ((lmu <- length(mu)) > 1 & lmu != p))
        stop("'null.value' must be a numeric vector of length ", p)
if(lmu == 1) mu <- rep(mu, p)
    xbar.mu <- xbar - mu
    V <- winall(x,tr=tr)$cov
h=n-2*floor(n*tr)
        k <- h / (n - 1) * (h - p) / p
        stat <- k * crossprod(xbar.mu, solve(V, xbar.mu))[1, ]
        pvalue <- 1 - pf(stat, p, h - p)
list(test.statistic = stat, degrees_of_freedom = c(p, h - p), p.value =
pvalue, estimate = xbar,
                null.value = mu)
}




# ----------------------------------------------------------------------------

# HQreg

# ----------------------------------------------------------------------------

HQreg<-function(x,y,alpha=1,xout=FALSE,method='huber',tau=.5,outfun=outpro,...){
#
#
#  Robust elastic net
# Yi, C. & Huang, J. (2016) Semismooth Newton coordinate descent algorithm for elastic-net penalized
# Huber loss regression and quantile regression.   (https://arxiv.org/abs/1509.02957)
# Journal of Computational and Graphical Statistics
# http://www.tandfonline.com/doi/full/10.1080/10618600.2016.1256816

#
library(hqreg)
x<-as.matrix(x)
xx<-cbind(x,y)
xx<-elimna(xx)
x<-xx[,1:ncol(x)]
x<-as.matrix(x)
y<-xx[,ncol(x)+1]
temp<-NA
x<-as.matrix(x)
if(xout){
x<-as.matrix(x)
flag<-outfun(x,plotit=plotit,...)$keep
x<-x[flag,]
y<-y[flag]
x<-as.matrix(x)
}
a=hqreg(x,y,method=method,alpha=alpha,tau=tau)$beta
list(coef=a[,100])
}




# ----------------------------------------------------------------------------

# iband

# ----------------------------------------------------------------------------

iband<-function(x,alpha=.05,q = c(0.1, 0.25, 0.5, 0.75, 0.9),  method='BH', SW=FALSE, plotit=FALSE,SEED=TRUE,nboot=500,grp=c(1:4),
xlab='X'){
#
# 2 by 2 design.
#
# For variables x1, x2, x3 and x4,
# This function compares the quantiles of the distributions
# d1=x1-x2 and d2=x3-x4
#
# SW=TRUE: switch rows and columns
#
if(SEED)set.seed(2)
if(is.matrix(x) || is.data.frame(x))x<-listm(x)
if(length(x)!=4)stop('Should be exactly 4 groups')
for(j in 1:length(x))x[[j]]=elimna(x[[j]])
if(SW)x=x[c(1,3,2,4)]
n<-c(length(x[[1]]),length(x[[2]]),length(x[[3]]),length(x[[4]]))
nq=length(q)
output=matrix(NA,nrow=length(q),ncol=8)
dimnames(output)=list(NULL,c('Quant','Est.Lev 1','Est.Lev 2','Dif','ci.low','ci.up','p-value','p.adj'))
output[,1]=q
for(j in 1:nq)output[j,2]=hd(outer(x[[1]],x[[2]],FUN='-'),q=q[j])
for(j in 1:nq)output[j,3]=hd(outer(x[[3]],x[[4]],FUN='-'),q=q[j])
output[,4]=output[,2]-output[,3]
e=lapply(q,iband.sub,x=x,nboot=nboot)
for(j in 1:nq)output[j,5]=e[[j]]$ci[1]
for(j in 1:nq)output[j,6]=e[[j]]$ci[2]
for(j in 1:nq)output[j,7]=e[[j]]$p.value
output[,8]=p.adjust(output[,7],method=method)
if(plotit){
g2plot(outer(x[[1]],x[[2]],FUN='-'),outer(x[[3]],x[[4]],FUN='-'),xlab=xlab)

}
output
}

iband.sub<-function(q,x,nboot=500,alpha=.05,SEED=FALSE){
#
#
#
if(SEED)set.seed(2)
if(is.matrix(x))x<-listm(x)
if(length(x)!=4)stop('There must be 4 groups')
for(j in 1:length(x))x[[j]]=elimna(x[[j]])
v1=NA
v2=NA
B=list()
for(i in 1:nboot){
for(j in 1:4)B[[j]]=sample(x[[j]],replace=TRUE)
v1[i]=hd(outer(B[[1]],B[[2]],FUN='-'),q=q)
v2[i]=hd(outer(B[[3]],B[[4]],FUN='-'),q=q)
}
p=mean(v1<v2)+.5*mean(v1==v2)
pv=2*min(p,1-p)
ilow<-round((alpha/2) * nboot)
ihi<-nboot - ilow
ilow<-ilow+1
vs=sort(v1-v2)
ci=vs[ilow]
ci[2]=vs[ihi]
list(ci=ci,p.value=pv)
}




# ----------------------------------------------------------------------------

# ID.sm.varPB

# ----------------------------------------------------------------------------

ID.sm.varPB<-function(x,var.fun=winvar,nboot=500,NARM=FALSE,na.rm=TRUE,SEED=TRUE,...){
#
#
#  Strategy: suppose group 2 has the lowest estimate.
#  Generate a  bootstrap sample and determine whether
#  the lowest bootstrap estimate corresponds to group 2.
#  Repeat nboot times and let P denote the proportion of times group 2 has the lowest estimate
#  Make a decision if this proportion is sufficiently high.
#  P yields a pseudo p-value
#
#
x=elimna(x)
J=ncol(x)
chk=0
if(is.list(x))x<-matl(x)
if(NARM)x=elimna(x)
e=apply(x,2,var.fun,...)
id=which(e==min(e))
n=nrow(x)
J=length(x)
for(i in 1:nboot){
isam=sample(n,replace=TRUE)
b=apply(x[isam,],2,var.fun,na.rm=na.rm,...)
ichk=which(b==min(b))
if(id==ichk)chk=chk+1
}
pv=chk/nboot
pv=2*min(pv,1-pv)
list(n=n,Est=e,p.value=pv)
}

#' Decision-Only Method for Selecting Best Binomial Group
#'
#' @description
#' Internal function to determine whether it is reasonable to decide which
#' group has the largest probability of success using a decision-only approach.
#'
#' @param x Vector of number of successes for each group
#' @param n Vector of sample sizes for each group
#'
#' @details
#' This function is used by \code{\link{bin.PMD.PCD}} as an alternative
#' to \code{\link{bin.best}}. It uses linear constraints to test whether
#' the group with the largest estimate is significantly larger than all others.
#'
#' @return
#' List with components:
#' \itemize{
#'   \item Est.: Vector of estimated proportions
#'   \item p.value: Maximum p-value from the constraint tests
#' }
#'
#' @seealso \code{\link{bin.PMD.PCD}}, \code{\link{lincon.bin}}
#'
#' @keywords internal
#' @export
bin.best.DO<-function(x,n){
#
#  Determine whether it is reasonable to
#  decide which group has largest probability of success
#
#  x= vector number of successes
#  n=sample sizes
#
chk=0
e=x/n
J=length(x)
id=which(e==max(e))[1]
CON=conCON(J,id)$conCON
a=lincon.bin(x,n,con=CON)
pv=max(a$CI[,4])
list(Est.=e,p.value=pv)
}


idb<-function(x,n){
#
#  Determine whether a  sequence of integers contains a 1, 2, ..., n.
#  Return idb[i]=1 if the value i is in x; 0 otherwise.
#  This function is used by regpre
#
m1<-matrix(0,n,n)
m1<-outer(c(1:n),x,"-")
m1<-ifelse(m1==0,1,0)
idb<-apply(m1,1,sum)
idb<-ifelse(idb>=1,0,1)
idb
}

idealfIQR<-function(x){
#
#  Compute the interquartile range using the ideal fourths.
x=elimna(x)
res=idealf(x)$qu-idealf(x)$ql
res
}




# ----------------------------------------------------------------------------

# idmatch

# ----------------------------------------------------------------------------

idmatch<-function(m1,m2,id.col1,id.col2=id.col1){
#
#  for the id data in column id.col of matrices m1 and m2
#  pull out data for which both m1 and m2 have matching id's
#  return the data in a matrix, M1 before data and M2, the matching data time 2.
#
flag=!is.na(m1[,id.col1])
m1=m1[flag,]  # eliminate any rows where ID is missing
flag=!is.na(m2[,id.col1])
m2=m2[flag,]
M1=NULL
#if(sum(duplicated(m1))>0)stop('Duplicate ids detected in m1')
#if(sum(duplicated(m2))>0)stop('Duplicate ids detected in m2')
#print(m1[,id.col1])
if(sum(duplicated(m1[,id.col1]))>0)stop('Duplicate ids detected in m1')
if(sum(duplicated(m2[,id.col2]))>0)stop('Duplicate ids detected in m2')
for(i in 1:nrow(m1)){
flag=duplicated(c(m1[i,id.col1],m2[,id.col2]))
if(sum(flag>0)){
if(is.data.frame(m1)){
if(!is.null(dim(M1)))M1=rbind(M1,as.data.frame(m1[i,]))
if(is.null(dim(M1)))M1=as.data.frame(m1[i,])
}
if(!is.data.frame(m1)){
if(!is.null(dim(M1)))M1=rbind(M1,m1[i,])
if(is.null(dim(M1)))M1=matrix(m1[i,],nrow=1)
}
}}
M2=NULL
for(i in 1:nrow(m2)){
flag=duplicated(c(m2[i,id.col2],m1[,id.col1]))
if(sum(flag>0)){
if(is.data.frame(m2)){
if(!is.null(dim(M2)))M2=rbind(M2,as.data.frame(m2[i,]))
if(is.null(dim(M2)))M2=as.data.frame(m2[i,])
}
if(!is.data.frame(m2)){
if(!is.null(dim(M2)))M2=rbind(M2,m2[i,])
if(is.null(dim(M2)))M2=matrix(m2[i,],nrow=1)
}
}}
#m=cbind(M1[,id.col1],M1[,-id.col1],M2[,-id.col2])
list(M1=M1,M2=M2)
}


idmatchv2<-function(m1,m2,id.col1,id.col2=id.col1){
#
#  Same as idmatch, but also return cases not matched
#
#  OUTPUT:
#  m combined data for which there are matching id's
#  m1.no  data in m1 for which there are no matching id's in m2
#  m2.no  data in m2 for which there are no matching id's in m1
#
flag=!is.na(m1[,id.col1])
m1=m1[flag,]  # eliminate any rows where ID is missing
flag=!is.na(m2[,id.col1])
m2=m2[flag,]
M1=NULL
idnm1=NULL
ic1=0
idnm2=NULL
ic2=0
if(sum(duplicated(m1))>0)stop('Duplicate ids detected in m1')
if(sum(duplicated(m2))>0)stop('Duplicate ids detected in m2')
for(i in 1:nrow(m1)){
flag=duplicated(c(m1[i,id.col1],m2[,id.col2]))
if(sum(flag)==0){
ic1=ic1+1
idnm1[ic1]=i
}
if(sum(flag>0)){
if(is.data.frame(m1)){
if(!is.null(dim(M1)))M1=rbind(M1,as.data.frame(m1[i,]))
if(is.null(dim(M1)))M1=as.data.frame(m1[i,])
}
if(!is.data.frame(m1)){
if(!is.null(dim(M1)))M1=rbind(M1,m1[i,])
if(is.null(dim(M1)))M1=matrix(m1[i,],nrow=1)
}
}}
M2=NULL
for(i in 1:nrow(m2)){
flag=duplicated(c(m2[i,id.col2],m1[,id.col1]))
if(sum(flag)==0){
ic2=ic2+1
idnm2[ic2]=i
}
if(sum(flag>0)){
if(is.data.frame(m2)){
if(!is.null(dim(M2)))M2=rbind(M2,as.data.frame(m2[i,]))
if(is.null(dim(M2)))M2=as.data.frame(m2[i,])
}
if(!is.data.frame(m2)){
if(!is.null(dim(M2)))M2=rbind(M2,m2[i,])
if(is.null(dim(M2)))M2=matrix(m2[i,],nrow=1)
}
}}
m=cbind(M1[,id.col1],M1[,-id.col1],M2[,-id.col2])
nc1=ncol(m2)-1
m1u=NULL
if(!is.null(idnm1))m1u=m1[idnm1,]
m2u=NULL
if(!is.null(idnm2))m2u=m2[idnm2,]
list(m=m,idnm1=idnm1,idnm2=idnm2,m1.no=m1u,m2.no=m2u)
}

idrange<-function(x,na.rm=FALSE){
#
# Compute the interquartile range based on the ideal fourths.
#
temp=idealf(x,na.rm=na.rm)
res=temp$qu-temp$ql
res
}
ifmest<-function(x,bend=1.28,op=2){
#
#   Estimate the influence function of an M-estimator, using
#   Huber's Psi, evaluated at x.
#
#   Data are in the vector x, bend is the percentage bend
#
#  op=2, use adaptive kernel estimator
#  otherwise use Rosenblatt's shifted histogram
#
tt<-mest(x,bend)  # Store M-estimate in tt
s<-mad(x)*qnorm(.75)
if(op==2){
val<-akerd(x,pts=tt,plotit=FALSE,pyhat=TRUE)
val1<-akerd(x,pts=tt-s,plotit=FALSE,pyhat=TRUE)
val2<-akerd(x,pts=tt+s,plotit=FALSE,pyhat=TRUE)
}
if(op!=2){
val<-kerden(x,0,tt)
val1<-kerden(x,0,tt-s)
val2<-kerden(x,0,tt+s)
}
ifmad<-sign(abs(x-tt)-s)-(val2-val1)*sign(x-tt)/val
ifmad<-ifmad/(2*.6745*(val2+val1))
y<-(x-tt)/mad(x)
n<-length(x)
b<-sum(y[abs(y)<=bend])/n
a<-hpsi(y,bend)*mad(x)-ifmad*b
ifmest<-a/(length(y[abs(y)<=bend])/n)
ifmest
}




# ----------------------------------------------------------------------------

# Ifun

# ----------------------------------------------------------------------------

Ifun<-function(x1,x2,x3,x4,bhat){
x=outer(x1,x2,FUN='-')
y=outer(x3,x4,FUN='-')
NV=length(x1)*length(x2)*length(x3)*length(x4)
v=(bmp(x,y)$phat-bhat)*NV
v
}




# ----------------------------------------------------------------------------

# in.interval

# ----------------------------------------------------------------------------

in.interval<-function(x,low,up){
#
#  flag values in the vector x between low and up
#
x=elimna(x)
n=length(x)
id=rep(FALSE,n)
flag1=x<=up
flag2=x>=low
flag=flag1*flag2
id[flag]=TRUE
id
}


IND.INT.DIF.ES<-function(J,K,x,...){
#
#  J independent groups.
#  Interaction: difference between two measures of effect size.
#  Each column of con will have four values != 0 the values will be 1 -1 -1 and
#  For each the first two such groups, compute measures of effect size. So the same for next two,
#  return the difference
#
#  Returns contrast coefficients followed by effect size measures for each column
#  of con.
#
fun=ES.summary
con=con2way(J,K)$conAB
if(is.matrix(x) || is.data.frame(x))x=listm(x)
CON=list()
J=length(x)
P=ncol(con)
ic=0
LAB2=c('Est 1','Est 2', 'NULL','Difference')
for(j in 1:P){
id=which(con[,j]!=0)
ic=ic+1
e1=fun(x[[id[1]]],  x[[id[2]]],...)
e2=fun(x[[id[3]]],  x[[id[4]]],...)
M=matrix(cbind(e1[,1],e2[,1],e1[,2],e1[,1]-e2[,1]),ncol=4)
dimnames(M)=list(names(e1[,1]),LAB2)
CON[[ic]]=M
}
list(con=con,effect.size.con.col=CON)
}




# ----------------------------------------------------------------------------

# indt

# ----------------------------------------------------------------------------

indt<-function(x,y,nboot=500,flag=1,SEED=TRUE,pr=TRUE){
#
# Test the hypothesis of independence between x and y by
# testing the hypothesis that the regression surface is a horizontal plane.
# Stute et al. (1998, JASA, 93, 141-149).
#
#  flag=1 gives Kolmogorov-Smirnov test statistic
#  flag=2 gives the Cramer-von Mises test statistic
#  flag=3 causes both test statistics to be reported.
#
#  tr=0 results in the Cramer-von Mises test statistic when flag=2
#      With tr>0, a trimmed version of the test statistic is used.
#
#  Modified Dec 2005.
#
tr=0
#if(tr<0)stop("Amount trimmed must be > 0")
#if(tr>.5)stop("Amount trimmed must be <=.5")
if(SEED)set.seed(2)
x<-as.matrix(x)
# First, eliminate any rows of data with missing values.
temp <- cbind(x, y)
        temp <- elimna(temp)
        pval<-ncol(temp)-1
        x <- temp[,1:pval]
        y <- temp[, pval+1]
x<-as.matrix(x)
mflag<-matrix(NA,nrow=length(y),ncol=length(y))
for (j in 1:length(y)){
for (k in 1:length(y)){
mflag[j,k]<-(sum(x[j,]<=x[k,])==ncol(x))
}
}
# ith row of mflag indicates which rows of the matrix x are less
# than or equal to ith row of x
#
yhat<-mean(y)
res<-y-yhat
if(pr)print("Taking bootstrap sample, please wait.")
data<-matrix(runif(length(y)*nboot),nrow=nboot)#
data<-(data-.5)*sqrt(12) # standardize the random numbers.
rvalb<-apply(data,1,regts1,yhat,res,mflag,x,tr)
# An n x nboot matrix of R values
rvalb<-rvalb/sqrt(length(y))
dstatb<-apply(abs(rvalb),2,max)
wstatb<-apply(rvalb^2,2,mean,tr=tr)
v<-c(rep(1,length(y)))
rval<-regts1(v,yhat,res,mflag,x,tr=0)
rval<-rval/sqrt(length(y))
dstat<-NA
wstat<-NA
critd<-NA
critw<-NA
p.vald<-NA
p.valw<-NA
if(flag==1 || flag==3){
dstat<-max(abs(rval))
p.vald<-1-sum(dstat>=dstatb)/nboot
}
if(flag==2 || flag==3){
wstat<-mean(rval^2,tr=tr)
p.valw<-1-sum(wstat>=wstatb)/nboot
}
list(dstat=dstat,wstat=wstat,p.value.d=p.vald,p.value.w=p.valw)
}


trimww.sub<-function(cmat,vmean,vsqse,h,J,K){
#
#  This function is used by trimww
#
#  The function performs a variation of Johansen's test of C mu = 0 for
#  a within by within design
#  C is a k by p matrix of rank k and mu is a p by 1 matrix of
#  of unknown  medians.
#  The argument cmat contains the matrix C.
#  vmean is a vector of length p containing the p medians
#  vsqe is matrix containing the
#  estimated covariances among the medians
#  h is  the sample size
#
p<-J*K
yvec<-matrix(vmean,length(vmean),1)
test<-cmat%*%vsqse%*%t(cmat)
invc<-solve(test)
test<-t(yvec)%*%t(cmat)%*%invc%*%cmat%*%yvec
temp<-0
mtem<-vsqse%*%t(cmat)%*%invc%*%cmat
temp<-(sum(diag(mtem%*%mtem))+(sum(diag(mtem)))^2)/(h-1)
A<-.5*sum(temp)
cval<-nrow(cmat)+2*A-6*A/(nrow(cmat)+2)
test<-test/cval
test
}




# ----------------------------------------------------------------------------

# indt0

# ----------------------------------------------------------------------------

indt0<-function(x,y,nboot=500,alpha=.05,flag=1,SEED=TRUE){
#
# Test the hypothesis that the regression plane
#   between x and y  is a flat horizontal plane with intercept 0
# The method is based on results in
# Stute et al. (1998, JASA, 93, 141-149).
#
#  flag=1 gives Kolmogorov-Smirnov test statistic
#  flag=2 gives the Cramer-von Mises test statistic
#  flag=3 causes both test statistics to be reported.
#
if(SEED)set.seed(2)
x<-as.matrix(x)
# First, eliminate any rows of data with missing values.
temp <- cbind(x, y)
        temp <- elimna(temp)
        pval<-ncol(temp)-1
        x <- temp[,1:pval]
        y <- temp[, pval+1]
x<-as.matrix(x)
mflag<-matrix(NA,nrow=length(y),ncol=length(y))
for (j in 1:length(y)){
for (k in 1:length(y)){
mflag[j,k]<-(sum(x[j,]<=x[k,])==ncol(x))
}
}
# ith row of mflag indicates which rows of the matrix x are less
# than or equal to ith row of x
#
yhat<-0
res<-y-yhat
print("Taking bootstrap sample, please wait.")
data<-matrix(runif(length(y)*nboot),nrow=nboot)
data<-(data-.5)*sqrt(12) # standardize the random numbers.
rvalb<-apply(data,1,indt0sub,yhat,res,mflag,x,tr)
# An n x nboot matrix of R values
rvalb<-rvalb/sqrt(length(y))
dstatb<-apply(abs(rvalb),2,max)
wstatb<-apply(rvalb^2,2,mean)
mstatb<-apply(abs(rvalb),2,median)
dstatb<-sort(dstatb)
wstatb<-sort(wstatb)
mstatb<-sort(mstatb)
# compute test statistic
v<-c(rep(1,length(y)))
rval<-indt0sub(v,yhat,res,mflag,x,tr)
rval<-rval/sqrt(length(y))
dstat<-NA
wstat<-NA
critd<-NA
critw<-NA
ib<-round(nboot*(1-alpha))
if(flag==1 || flag==3){
dstat<-max(abs(rval))
critd<-dstatb[ib]
}
if(flag==2 || flag==3){
wstat<-mean(rval^2)
critw<-wstatb[ib]
}
list(dstat=dstat,wstat=wstat,critd=critd,critw=critw)
}




# ----------------------------------------------------------------------------

# indt0sub

# ----------------------------------------------------------------------------

indt0sub<-function(vstar,yhat,res,mflag,x,tr){
bres<-res*vstar
rval<-0
for (i in 1:nrow(x)){
rval[i]<-sum(bres[mflag[,i]])
}
rval
}

indtall<-function(x,y=NULL,tr=0,nboot=500,SEED=TRUE){
#
# Test the hypothesis of independence for
#  1. all pairs of variables in matrix x, if y=NA, or
#  2. between each variable stored in the matrix x and y.
#  This is done by repeated  calls to indt
#
x<-as.matrix(x)
# First, eliminate any rows of data with missing values.
if(!is.null(y[1])){
temp <- cbind(x, y)
        temp <- elimna(temp)
        pval<-ncol(temp)-1
        x <- temp[,1:pval]
        y <- temp[, pval+1]
}
x<-as.matrix(x)
if(is.null(y[1])){
ntest<-(ncol(x)^2-ncol(x))/2
if(ntest==0)stop("Something is wrong. Does x have only one column?")
output<-matrix(NA,nrow=ntest,ncol=4)
dimnames(output)<-list(NULL,c("VAR","VAR","Test Stat.","p-value"))
x<-elimna(x)
ic<-0
for (j in 1:ncol(x)){
for (jj in 1:ncol(x)){
if(j<jj){
temp<-indt(x[,j],x[,jj],nboot=nboot,SEED=SEED)
ic<-ic+1
output[ic,1]<-j
output[ic,2]<-jj
output[ic,3]<-temp$dstat
output[ic,4]<-temp$p.value.d
}}}}
if(!is.null(y[1])){
ntest<-ncol(x)
output<-matrix(NA,nrow=ntest,ncol=3)
dimnames(output)<-list(NULL,c("VAR","Test Stat.","p-value"))
ic<-0
for (j in 1:ncol(x)){
temp<-indt(x[,j],y,nboot=nboot,SEED=SEED)
ic<-ic+1
output[ic,1]<-j
output[ic,2]<-temp$dstat
output[ic,3]<-temp$p.value.d
}}
list(output=output)
}





interQS<-function(x,nreps=200,locfun=median,SEED=TRUE,...){
#
# Quantile shift-type measure of effect size for an  interaction in a 2-by-2 design
#
if(is.matrix(x) || is.data.frame(x))x=listm(x)
nv=as.vector(matl(lapply(x,FUN='length')))
nmin=min(nv)
ef=NA
nt=prod(nv)
nmin=min(c(nmin,50))
M=matrix(NA,nrow=nmin,ncol=4)
if(nt>10^3){
if(SEED)set.seed(2)
for(i in 1:nreps){
for(j in 1:4)M[,j]=sample(x[[j]],nmin)
L1=outer(M[,1],M[,2],FUN='-')
L2=outer(M[,3],M[,4],FUN='-')
ef[i]=shiftes(L1,L2,locfun=locfun)$Q.Effect
}}
else{
L1=outer(x[[1]],x[[2]],FUN='-')
L2=outer(x[[3]],x[[4]],FUN='-')
ef=shiftes(L1,L2,locfun=locfun,...)$Q.effect
}
es=mean(ef)
list(Q.Effect=es)
}


IQR2g.W<-function(x,y,nboot=100,alpha=.05,SEED=TRUE){
#
# Wald-type test for comparing interquartile range of two independent groups.
#
sd1=bootse(x,est=IQRhd,nboot=nboot,SEED=SEED)
sd2=bootse(x,est=IQRhd,nboot=nboot,SEED=SEED)
e1=IQRhd(x)
e2=IQRhd(y)
se=sqrt(sd1^2+sd2^2)
W=(e1-e2)/se
crit=qnorm(1-alpha/2)
ci=(e1-e2)-crit*se
ci[2]=(e1-e2)+crit*se
pv=2*(1-pnorm(abs(W)))
list(Est.1=e1,Est.2=e2,Test.Stat=W,ci=ci,p.value=pv)
}

IQRstand<-function(x){
vals=idealf(x)
res=x/(vals$qu-vals$ql)
res
}




# ----------------------------------------------------------------------------

# JK.AB.KS.ES

# ----------------------------------------------------------------------------

JK.AB.KS.ES<-function(J,K,x){
#
# Independent groups
# Compute main effect sizes
#
JK=J*K
if(is.matrix(x)  || is.data.frame(x))x=listm(x)
mat=matrix(c(1:JK),nrow=J,byrow=TRUE)
A=list()
B=list()
for(j in 1:nrow(mat)){
id=mat[j,]
A[[j]]=KS.ANOVA.ES(x[id])
}
mat=t(mat)
for(j in 1:nrow(mat)){
id=mat[j,]
B[[j]]=KS.ANOVA.ES(x[id])
}
list(Fac.A=A,Fac.B=B)
}




# ----------------------------------------------------------------------------

# johan

# ----------------------------------------------------------------------------

johan<-function(cmat,vmean,vsqse,h,alpha=.05){
#
#  This function is used by other functions that come with this book,
#  and it can be used to test hypotheses not covered in the text.
#
#  The function performs Johansen's test of C mu = 0 for p independent groups,
#  where C is a k by p matrix of rank k and mu is a p by 1 matrix of
#  of unknown trimmed means.
#  The argument cmat contains the matrix C.
#  vmean is a vector of length p containing the p trimmed means
#  vsqe is a diagonal matrix containing the squared standard errors of the
#  the trimmed means in vmean.
#  h is a vector containing the effective sample sizes
#
yvec<-matrix(vmean,length(vmean),1)
if(!is.matrix(vsqse))vsqse<-diag(vsqse)
test<-cmat%*%vsqse%*%t(cmat)
invc<-solve(test)
test<-t(yvec)%*%t(cmat)%*%invc%*%cmat%*%yvec
R<-vsqse%*%t(cmat)%*%invc%*%cmat
A<-sum(diag((diag(R))^2/diag(h-1)))
df<-nrow(cmat)
crit<-qchisq(1-alpha,df)
crit<-crit+(crit/(2*df))*A*(1+3*crit/(df+2))
list(teststat=test[1],crit=crit[1],df=df)
}




# ----------------------------------------------------------------------------

# johansp

# ----------------------------------------------------------------------------

johansp<-function(cmat,vmean,vsqse,h,J,K){
#
#  This function is used by other functions that come with this book,
#  and it can be used to test hypotheses not covered in the text.
#
#  The function performs Johansen's test of C mu = 0 for
#  a split-plot design where the first factor has independent groups,
#  while the second factor is within subjects,
#  C is a k by p matrix of rank k and mu is a p by 1 matrix of
#  of unknown trimmed means.
#  The argument cmat contains the matrix C.
#  vmean is a vector of length p containing the p trimmed means
#  vsqe is a block diagonal matrix, the jth block being the
#  estimated covariances among the trimmed means
#  in the jth level of factor A,
#  the trimmed means are in vmean,
#  h is a vector of length J containing the effective sample sizes for
#  the jth level of factor A.
#
p<-J*K
yvec<-matrix(vmean,length(vmean),1)
test<-cmat%*%vsqse%*%t(cmat)
invc<-solve(test)
test<-t(yvec)%*%t(cmat)%*%invc%*%cmat%*%yvec
temp<-0
klim<-1-K
kup<-0
for (j in 1:J){
klim<-klim+K
kup<-kup+K
Q<-matrix(0,p,p) #  create Q sub j
for (k in klim:kup)Q[k,k]<-1
mtem<-vsqse%*%t(cmat)%*%invc%*%cmat%*%Q
temp[j]<-(sum(diag(mtem%*%mtem))+(sum(diag(mtem)))^2)/(h[j]-1)
}
A<-.5*sum(temp)
df1<-nrow(cmat)
df2<-nrow(cmat)*(nrow(cmat)+2)/(3*A)
cval<-nrow(cmat)+2*A-6*A/(nrow(cmat)+2)
test<-test/cval
sig<-1-pf(test,df1,df2)
list(teststat=test[1],p.value=sig)
}


kbcon<-function(x,con=0,tr=.2,alpha=.05,pr=TRUE){
#
#  A heteroscedastic test of d linear contrasts using trimmed means.
#  based on the Kaiser-Bowden method
#
#  The data are assumed to be stored in $x$ in list mode or in a matrix
#  Length(x) (or ncol(x))is assumed to correspond to the total number of groups
#  It is assumed all groups are independent.
#
#  con is a J by d matrix containing the contrast coefficients.
#  If con is not specified, all pairwise comparisons are made.
#
#  Missing values are automatically removed.
#
if(is.matrix(x))x<-listm(x)
if(!is.list(x))stop("Data must be stored in a matrix or in list mode.")
con<-as.matrix(con)
J<-length(x)
h<-vector("numeric",J)
w<-vector("numeric",J)
xbar<-vector("numeric",J)
for(j in 1:J){
xx<-!is.na(x[[j]])
val<-x[[j]]
x[[j]]<-val[xx]  # Remove missing values
h[j]<-length(x[[j]])-2*floor(tr*length(x[[j]]))
   # h is the number of observations in the jth group after trimming.
w[j]<-((length(x[[j]])-1)*winvar(x[[j]],tr))/(h[j]*(h[j]-1))
xbar[j]<-mean(x[[j]],tr)
}
if(sum(con^2)==0){
CC<-(J^2-J)/2
v1=J-1
psihat<-matrix(0,CC,6)
dimnames(psihat)<-list(NULL,c("Group","Group","psihat","ci.lower","ci.upper",
"p.value"))
test<-matrix(NA,CC,6)
dimnames(test)<-list(NULL,c("Group","Group","test","crit","se","df"))
jcom<-0
for (j in 1:J){
for (k in 1:J){
if (j < k){
jcom<-jcom+1
df<-(w[j]+w[k])^2/(w[j]^2/(h[j]-1)+w[k]^2/(h[k]-1))
term=sqrt((J-1)*(1+(J-2)/df))
test[jcom,3]<-((xbar[j]-xbar[k])/(term*sqrt(w[j]+w[k])))^2
sejk<-sqrt(w[j]+w[k])
test[jcom,5]<-sejk
psihat[jcom,1]<-j
psihat[jcom,2]<-k
test[jcom,1]<-j
test[jcom,2]<-k
psihat[jcom,3]<-(xbar[j]-xbar[k])
test[jcom,6]<-df
crit=qf(1-alpha,v1,df)
aterm=sqrt(crit)*term
test[jcom,4]<-qf(1-alpha,v1,df)
psihat[jcom,4]<-(xbar[j]-xbar[k])-aterm*sejk
psihat[jcom,5]<-(xbar[j]-xbar[k])+aterm*sejk
psihat[jcom,6]<-1-pf(test[jcom,3],v1,df)
}}}}
if(sum(con^2)>0){
if(nrow(con)!=length(x)){
stop("The number of groups does not match the number of contrast coefficients.")
}
v1=nrow(con)-1
psihat<-matrix(0,ncol(con),5)
dimnames(psihat)<-list(NULL,c("con.num","psihat","ci.lower","ci.upper",
"p.value"))
test<-matrix(0,ncol(con),5)
dimnames(test)<-list(NULL,c("con.num","test","crit","se","df"))
df<-0
L=nrow(con)
for (d in 1:ncol(con)){
psihat[d,1]<-d
psihat[d,2]<-sum(con[,d]*xbar)
sejk<-sqrt(sum(con[,d]^2*w))
test[d,1]<-d
df<-(sum(con[,d]^2*w))^2/sum(con[,d]^4*w^2/(h-1))
A=(L-1)*(1+(L-2)/df)
test[d,2]<-(sum(con[,d]*xbar)/(sqrt(A)*sejk))^2
crit=qf(1-alpha,v1,df)
test[d,3]<-crit
test[d,4]<-sejk
test[d,5]<-df
psihat[d,3]<-psihat[d,2]-sqrt(crit*A)*sejk
psihat[d,4]<-psihat[d,2]+sqrt(crit*A)*sejk
psihat[d,5]<-1-pf(test[d,2],v1,df)
}}
#
if(pr){
print("Note: confidence intervals are adjusted to control FWE")
print("But p-values are not adjusted to control FWE")
}
list(test=test,psihat=psihat)
}




# ----------------------------------------------------------------------------

# kercon

# ----------------------------------------------------------------------------

kercon<-function(x,y,pyhat=FALSE,cval=NA,plotit=TRUE,eout=FALSE,xout=FALSE,
outfun=out,iran=.05,xlab="X",ylab="Y",pch='.'){
#
# Compute conditional local weighted regression with Epanechnikov kernel
#
# cf. Fan, Annals of Statistics, 1993, 21, 196-217.
#
d<-ncol(x)
if(d!=2)stop("Argument x should have two columns only")
np1<-d+1
m<-elimna(cbind(x,y))
x<-m[,1:d]
y<-m[,np1]
yhat1<-NA
if(eout && xout)stop("Can't have both eout and xout=F")
if(eout){
flag<-outfun(m)$keep
m<-m[flag,]
}
if(xout){
flag<-outfun(x)$keep
m<-m[flag,]
}
x<-m[,1:d]
y<-m[,np1]
if(is.na(cval[1])){temp<-idealf(x[,2])
cval<-c(temp$ql,median(x[,2]),temp$qu)
}
xrem<-x
x2<-x[,2]
n<-nrow(x)
sig<-sqrt(var(x2))
temp<-idealf(x2)
iqr<-(temp$qu-temp$ql)/1.34
A1<-min(c(sig,iqr))
A<-1.77
hval<-A*(1/n)^(1/6)  # Silverman, 1986, p. 86
svec<-NA
for(j in 1:d){
sig<-sqrt(var(x[,j]))
temp<-idealf(x[,j])
iqr<-(temp$qu-temp$ql)/1.34
A<-min(c(sig,iqr))
svec[j]<-A
x[,j]<-x[,j]/A
}
hval<-hval*sqrt(mean(svec^2))
ilow<-round(iran*length(y))
iup<-round((1-iran)*length(y))
for(il in 1:length(cval)){
temp4<-NA
for(j in 1:nrow(x)){
temp4[j]<-((x2[j]-cval[il])/A1)^2
}
yhat<-NA
epan1<-ifelse(temp4<1,.75*(1-temp4),0) # Epanechnikov kernel for x2
for(j in 1:n){
yhat[j]<-NA
temp1<-cbind(x[,1]-x[j,1],x[,2]-cval[il]/A)/hval
temp1<-temp1^2
temp1<-apply(temp1,1,FUN="sum")
temp<-.5*(d+2)*(1-temp1)/gamma(.5)^2
epan<-ifelse(temp1<1,temp,0) # Epanechnikov kernel, for both x1 and x2
if(epan1[j]>0)epan[j]<-epan[j]/epan1[j]
if(epan1[j]==0)epan[j]<-0
chkit<-sum(epan!=0)
if(chkit >= np1){
vals<-lsfit(x[,1],y,wt=epan)$coef
yhat[j]<-x[j,1]*vals[2]+vals[1]
}}
if(plotit){
xorder<-order(xrem[,1])
if(il==1)plot(xrem[,1],y,xlab=xlab,ylab=ylab,pch=pch)
lines(xrem[xorder[ilow:iup],1],yhat[xorder[ilow:iup]],lty=il)
}}
m<-"Done"
if(pyhat)m<-yhat
m
}

kerden<-function(x,q=.5,xval=0){
#   Compute the kernel density estimator of the
#   probability density function evaluated at the qth quantile.
#
#   x contains vector of observations
#   q is the quantile of interest, the default is the median.
#   If you want to evaluate f hat at xval rather than at the
#   q th quantile, set q=0 and xval to desired value.
#
y<-sort(x)
n<-length(x)
temp<-idealf(x)
h<-1.2*(temp$qu-temp$ql)/n^(.2)
iq<-floor(q*n+.5)
qhat<-y[iq]
if (q==0) qhat<-xval
xph<-qhat+h
A<-length(y[y<=xph])
xmh<-qhat-h
B<-length(y[y<xmh])
fhat<-(A-B)/(2*n*h)
fhat
}

kerSORT<-function(x,xlab='',ylab='',pts=NA){
#
#  kernel density estimator using Silverman's rule of thumb
#
#
A=min(c(sd(x),idealfIQR(x)/1.34))
bw=1.06*A/n^.2
init=density(x,bw=bw,kernel='epanechnikov')
plot(init$x,init$y,xlab=xlab,ylab=ylab,type='n')
lines(init$x,init$y)
}

BD2=function(matrizDatos){
	n=dim(matrizDatos)[1]
	p=dim(matrizDatos)[2]
	cont=rep(0,n)
	for (i in 1:(n-1)){
		for (j in (i+1):n){
			cont=cont+estaEntre(c(i,j),matrizDatos)
		}
	}
	contg=(cont/combinat(n,2))
}

#indicator function
estaEntre=function(v,matrizDatos){
	n=dim(matrizDatos)[1]
	p=dim(matrizDatos)[2]
	Z=matrizDatos
	inf=t(apply(Z[v,],2,min))
	sup=t(apply(Z[v,],2,max))
	resultados=colSums((t(Z)<=t(sup)%*%rep(1,n))* (t(Z)>=t(inf)%*%rep(1,n)))==p
}

#combination
combinat=function(n,p){
	if (n<p){combinat=0}
	else {combinat=exp(lfactorial(n)-(lfactorial(p)+lfactorial(n-p)))}
}

#BD3
BD3=function(matrizDatos){
	n=dim(matrizDatos)[1]
	p=dim(matrizDatos)[2]
	cont=rep(0,n)
	for (i in 1:(n-2)){
		for (j in (i+1):(n-1)){
			for (k in (j+1):n){
				cont=cont+estaEntre(c(i,j,k),matrizDatos)
			}
		}
	}
	contg=(cont/combinat(n,3))
}

#MBD
MBD=function(matrizDatos){
	n=dim(matrizDatos)[1]
	p=dim(matrizDatos)[2]
	cont=rep(0,n)
	for (i in 1:(n-1)){
		for (j in (i+1):n){
			cont=cont+aprops(c(i,j),matrizDatos)
		}
	}
	contg=(cont/combinat(n,2))
}

#proportion function
aprops=function(v,matrizDatos){
	n=dim(matrizDatos)[1]
	p=dim(matrizDatos)[2]
  Z=matrizDatos
	inf=t(apply(Z[v,],2,min))
	sup=t(apply(Z[v,],2,max))
	resul=colSums((t(Z)<=t(sup)%*%rep(1,n))* (t(Z)>=t(inf)%*%rep(1,n)))
	resultado=(resul/p)
}




#function boxplot
#fit: p by n functional data matrix, n is the number of curves
#method: BD2, BD3, MBD
kms.effect<-function(x,y,tr=.2,DIF=NULL){
#
# Computes the robust effect size using a simple generalization of the method in
# Kulinskaya, E., Morgenthaler, S. & Staudte, R. (2008).
# Meta Analysis: A guide to calibrating and combining statistical evidence  p. 177

#Cohen d=.2, .5 .8 correspond to .1, .25 and .4') (KMS p. 180)

# DIF can be used to specify a difference between the trimmed means,
# useful when thinking about power 
# using Stein2g.
# Given a value for the difference that is of interest
# Stein2g determines sample size needed so that power is equal to some specified value for DIF.

x<-elimna(x)
y<-elimna(y)
n1<-length(x)
n2<-length(y)
N=n1+n2
q=n1/N
s1sq=winvar(x,tr=tr)
s2sq=winvar(y,tr=tr)
t1=tmean(x,tr=tr)
t2=tmean(y,tr=tr)
if(is.null(DIF))DIF=t1-t2
top=(1-q)*s1sq+q*s2sq
bot=q*(1-q)
sigsq=top/bot  #  Quantity in brackets KMS p. 176 eq 21.1
varrho=s2sq/s1sq
d1=(DIF)/sqrt(sigsq)
cterm=1
if(tr>0)cterm=area(dnormvar,qnorm(tr),qnorm(1-tr))+2*(qnorm(tr)^2)*tr
cterm=sqrt(cterm)
del=cterm*d1  #rescale for a normal distribution.
list(effect.size=del,Cohen.d.equiv=2*del)
}




# ----------------------------------------------------------------------------

# kmsbinomci

# ----------------------------------------------------------------------------

kmsbinomci<-function(x = sum(y), nn = length(y), y = NULL, n = NA, alpha = 0.05){
#
# Boinomial
# Confidence interval for the probability of success.
# Kulinskaya, E., Morgenthaler, S. & Staudte, R. (2008).
# Meta Analysis: A guide to calibrating and combining statistical evidence  p. 140
#
if(!is.null(y[1])){
y=elimna(y)
nn=length(y)
}
if(nn==1)stop('Something is wrong: number of observations is only 1')
n<-nn
cr=qnorm(1-alpha/2)
ntil=n+.75
ptil=(x+3/8)/ntil
crit=qnorm(1-alpha/2)
if(x!=n && x!=0){
term1=sin(asin(sqrt(ptil))-crit/(2*sqrt(n)))
term2=sin(asin(sqrt(ptil))+crit/(2*sqrt(n)))
lower=term1^2
upper=term2^2
}
if(x==0){  #Use Clopper-Pearson
lower<-0
upper<-1-alpha^(1/n)
}
if(x==1){
upper<-1-(alpha/2)^(1/n)
lower<-1-(1-alpha/2)^(1/n)
}
if(x==n-1){
lower<-(alpha/2)^(1/n)
upper<-(1-alpha/2)^(1/n)
}
if(x==n){
lower<-alpha^(1/n)
upper<-1
}
phat=x/n
list(phat=phat,se=sqrt(ptil*(1-ptil)/ntil),ci=c(lower,upper),n=n)
}



#' P-Value for Binomial Confidence Interval Methods
#'
#' @description
#' Compute p-value for testing whether the probability of success equals a
#' specified null value, using various binomial CI methods available in
#' \code{\link{binom.conf}}.
#'
#' @param x Number of successes (computed from y if y is provided)
#' @param nn Sample size (computed from y if y is provided)
#' @param y Vector of 1s and 0s (optional)
#' @param method Character string specifying CI method: 'AC' (Agresti-Coull,
#'   default), 'P' (Pratt), 'CP' (Clopper-Pearson), 'KMS' (Kulinskaya et al.),
#'   'WIL' (Wilson), or 'SD' (Schilling-Doi)
#' @param AUTO Logical; if TRUE (default), automatically use 'SD' method when n < 35
#' @param pr Logical; if TRUE, print note about sign test (default FALSE)
#' @param PVSD Logical; if TRUE, compute p-value when using SD method; if FALSE,
#'   skip p-value computation to avoid high execution time (default TRUE)
#' @param alpha Significance level for CI (default 0.05)
#' @param nullval Null hypothesis value for probability of success (default 0.5)
#' @param NOTE Logical; if TRUE (default), print note when PVSD=FALSE with SD method
#'
#' @details
#' Computes a p-value by finding the smallest alpha level at which the
#' confidence interval excludes the null value. Multiple methods are available:
#' \itemize{
#'   \item AC: Agresti-Coull (default, good general performance)
#'   \item P: Pratt's method
#'   \item CP: Clopper-Pearson (exact, conservative)
#'   \item KMS: Kulinskaya et al.
#'   \item WIL: Wilson
#'   \item SD: Schilling-Doi (recommended for n < 35)
#' }
#'
#' When AUTO=TRUE, the SD method is automatically used for small samples (n < 35).
#' Computing p-values with SD method can be slow; set PVSD=FALSE to skip.
#'
#' @return
#' List with components:
#' \itemize{
#'   \item n: Sample size
#'   \item p.hat: Estimated proportion
#'   \item ci: Confidence interval
#'   \item p.value: P-value (NULL if PVSD=FALSE with SD method)
#' }
#'
#' @seealso \code{\link{binom.conf}}, \code{\link{binomcipv}}
#'
#' @keywords binomial htest
#' @export
binom.conf.pv<-function(x = sum(y), nn = length(y),y=NULL,method='AC',AUTO=TRUE, pr=FALSE, PVSD=TRUE,alpha=.05,nullval=.5,NOTE=TRUE){
#
#   p-value for the methods available in binom.conf
# AC: Agresti--Coull
# P: Pratt's method
# CP: Clopper--Pearson
# KMS:  Kulinskaya et al. 2008, p. 140
#  WIL:  Wilson type CI. Included for completeness; was used in simulations  relevant to binom2g
#   SD: Schilling--Doi
#
# AUTO=TRUE, use SD if n<35
# PVSD=FALSE: no p-value when using SD to avoid possibly high execution time
# use p-value based on what method indicates. Default is AC, Agresti--Coull
#
if(pr) print('Note: To perform the sign test, use the the R function signt')
if(!PVSD & nn<35)AUTO=FALSE
if(AUTO){
if(nn<35)method='SD'
}
ci<-binom.conf(x,nn,alpha=alpha,method=method,AUTO=FALSE,pr=FALSE)
pv=NULL
if(method=='SD'){
if(!PVSD){
if(NOTE)print('To get a p-value when method=SD, set PVSD=TRUE, but execution time might be high')
}}
if(method!='SD' || PVSD){
alph<-c(1:99)/100
for(i in 1:99){
irem<-i
chkit<-binom.conf(x,nn,alpha=alph[i],method=method,AUTO=FALSE,pr=FALSE)$ci
if(chkit[[1]]>nullval || chkit[[2]]<nullval)break
}
p.value<-irem/100
if(p.value<=.01){
iup<-(irem+1)/100
alph<-seq(.001,iup,.001)
for(i in 1:length(alph)){
p.value<-alph[i]
chkit<-binom.conf(x,nn,alpha=alph[i],method=method,AUTO=FALSE,pr=FALSE)$ci
if(chkit[[1]]>nullval || chkit[[2]]<nullval)break
}}
pv=alph[i]
}
list(n=ci$n,p.hat=ci$phat,ci=ci$ci,p.value=pv)
}




# ----------------------------------------------------------------------------

# ks

# ----------------------------------------------------------------------------

ks<-function(x,y,w=FALSE,sig=TRUE,alpha=.05){
#  Compute the Kolmogorov-Smirnov test statistic
#
#  w=T computes the weighted version instead.
#
#  sig=T indicates that the exact  level is to be computed.
#  If there are ties, the reported Type I  error probability is exact when
#  using the unweighted test, but for the weighted test the reported
#  level is too high.
#
#  This function uses the functions ecdf, kstiesig, kssig and kswsig
#
#  This function returns the value of the test statistic, the approximate .05
#  critical value, and the exact level if sig=T.
#
#  Missing values are automatically removed
#
x<-x[!is.na(x)]
y<-y[!is.na(y)]
n1 <- length(x)
n2 <- length(y)
w<-as.logical(w)
sig<-as.logical(sig)
tie<-logical(1)
siglevel<-NA
z<-sort(c(x,y))  # Pool and sort the observations
tie=FALSE
chk=sum(duplicated(x,y))
if(chk>0)tie=TRUE
v<-1   # Initializes v
for (i in 1:length(z))v[i]<-abs(ecdf(x,z[i])-ecdf(y,z[i]))
ks<-max(v)
if(!tie)crit=ks.crit(n1=n1,n2=n2,alpha=alpha)
else crit=ksties.crit(x,y,alpha=alpha)
if(!w && sig && !tie)siglevel<-kssig(length(x),length(y),ks)
if(!w && sig && tie)siglevel<-kstiesig(x,y,ks)
if(w){
crit=ksw.crit(length(x),length(y),alpha=alpha)
for (i in 1:length(z)){
temp<-(length(x)*ecdf(x,z[i])+length(y)*ecdf(y,z[i]))/length(z)
temp<-temp*(1.-temp)
v[i]<-v[i]/sqrt(temp)
}
v<-v[!is.na(v)]
ks<-max(v)*sqrt(length(x)*length(y)/length(z))
if(sig)siglevel<-kswsig(length(x),length(y),ks)
if(tie && sig)
warning(paste("Ties were detected. The reported significance level of the
weighted Kolmogorov-Smirnov test statistic is not exact."))
}
list(test=ks,critval=crit,p.value=siglevel)
}

ksw.crit<-function(n1,n2,alpha=.05){
#
# Compute a critical value so that probability coverage is
# >= 1-alpha while being close as possible to 1-alpha
#
if(alpha>.1)stop('The function assumes alpha is at least .1')
crit=2.4
del=.05
pc=.12
while(pc>alpha){
crit=crit+.05
pc=kswsig(n1,n2,crit)
}
crit
}




# ----------------------------------------------------------------------------

# kslope

# ----------------------------------------------------------------------------

kslope<-function(x,y,pyhat=FALSE,pts=x){
#
# Estimate slope at points in pts using kernel method
#
# See Doksum et al. 1994, JASA, 89, 571-
#
m<-elimna(cbind(x,y))
x<-m[,1]
y<-m[,2]
n<-length(y)
sig<-sqrt(var(x))
temp<-idealf(x)
iqr<-(temp$qu-temp$ql)/1.34
A<-min(c(sig,iqr))
yhat<-NA
vval<-NA
vals<-NA
rhosq<-NA
for(k in 1:n){
temp1<-NA
for(j in 1:n){
temp1[j]<-((x[j]-x[k])/A)^2
}
epan<-ifelse(temp1<1,.75*(1-temp1),0) # Epanechnikov kernel, p. 76
chkit<-sum(epan!=0)
if(chkit >= 2){
temp4<-lsfit(x,y,wt=epan)
vals[k]<-temp4$coef[2]
}}
vals
}




# ----------------------------------------------------------------------------

# ksnorm.test

# ----------------------------------------------------------------------------

ksnorm.test<-function(z)ks.test(z,'pnorm',mean=mean(z),sd=sd(z)) #KS test for normality

ksolve.bt <- function(d,p,c1,M,b0){
#       find a constant k which satisfies the s-estimation constraint
#       for modified biweight
    k <- 1
    iter <- 1
    crit <- 100
    eps <- 1e-5
    while ((crit > eps)&(iter<100))
    {
        k.old <- k
        fk <- mean(rho.bt(d/k,c1,M))-b0
        fkp <- -mean(psi.bt(d/k,c1,M)*d/k^2)
        k <- k - fk/fkp
        if (k < k.old/2)  k <- k.old/2
        if (k > k.old*1.5) k <- k.old*1.5
        crit <- abs(fk)
        iter <- iter+1
    }
    return(k)
}




# ----------------------------------------------------------------------------

# kssig

# ----------------------------------------------------------------------------

kssig<-function(m,n,val){
#
#    Compute significance level of the  Kolmogorov-Smirnov test statistic
#    m=sample size of first group
#    n=sample size of second group
#    val=observed value of test statistic
#
cmat<-matrix(0,m+1,n+1)
umat<-matrix(0,m+1,n+1)
for (i in 0:m){
for (j in 0:n)cmat[i+1,j+1]<-abs(i/m-j/n)
}
cmat<-ifelse(cmat<=val,1e0,0e0)
for (i in 0:m){
for (j in 0:n)if(i*j==0)umat[i+1,j+1]<-cmat[i+1,j+1]
else umat[i+1,j+1]<-cmat[i+1,j+1]*(umat[i+1,j]+umat[i,j+1])
}
term<-lgamma(m+n+1)-lgamma(m+1)-lgamma(n+1)
kssig<-1.-umat[m+1,n+1]/exp(term)
kssig=max(0,kssig)
kssig
}




# ----------------------------------------------------------------------------

# kstiesig

# ----------------------------------------------------------------------------

kstiesig<-function(x,y,val){
#
#    Compute significance level of the  Kolmogorov-Smirnov test statistic
#    for the data in x and y.
#    This function allows ties among the  values.
#    val=observed value of test statistic
#
m<-length(x)
n<-length(y)
z<-c(x,y)
z<-sort(z)
cmat<-matrix(0,m+1,n+1)
umat<-matrix(0,m+1,n+1)
for (i in 0:m){
for (j in 0:n){
if(abs(i/m-j/n)<=val)cmat[i+1,j+1]<-1e0
k<-i+j
if(k > 0 && k<length(z) && z[k]==z[k+1])cmat[i+1,j+1]<-1
}
}
for (i in 0:m){
for (j in 0:n)if(i*j==0)umat[i+1,j+1]<-cmat[i+1,j+1]
else umat[i+1,j+1]<-cmat[i+1,j+1]*(umat[i+1,j]+umat[i,j+1])
}
term<-lgamma(m+n+1)-lgamma(m+1)-lgamma(n+1)
kstiesig<-1.-umat[m+1,n+1]/exp(term)
kstiesig
}

kswsig<-function(m,n,val){
#
#    Compute significance level of the weighted
#    Kolmogorov-Smirnov test statistic
#
#    m=sample size of first group
#    n=sample size of second group
#    val=observed value of test statistic
#
mpn<-m+n
cmat<-matrix(0,m+1,n+1)
umat<-matrix(0,m+1,n+1)
for (i in 1:m-1){
for (j in 1:n-1)cmat[i+1,j+1]<-abs(i/m-j/n)*sqrt(m*n/((i+j)*(1-(i+j)/mpn)))
}
cmat<-ifelse(cmat<=val,1,0)
for (i in 0:m){
for (j in 0:n)if(i*j==0)umat[i+1,j+1]<-cmat[i+1,j+1]
else umat[i+1,j+1]<-cmat[i+1,j+1]*(umat[i+1,j]+umat[i,j+1])
}
term<-lgamma(m+n+1)-lgamma(m+1)-lgamma(n+1)
kswsig<-1.-umat[m+1,n+1]/exp(term)
kswsig
}




# ----------------------------------------------------------------------------

# L.ties

# ----------------------------------------------------------------------------

L.ties<-function(x){
#
#  x is assumed to have list mode
#
# Goal: determine whether there are any tied values
#
a=FALSE
if(is.matrix(x))x=listm(x)
if(!is.list(x))stop('x should be a matrix or have list mode')
x=elimna(x)
J=length(x)
for(j in 1:J){
u=unique(x[[j]])
if(length(u)!=length(x[[j]]))a=TRUE
}
a
}

L1medcen <- function(X, tol = 1e-08, maxit = 200, m.init = apply(X, 2, median),
                     trace = FALSE)
{
  ## L1MEDIAN calculates the multivariate L1 median
  ## I/O: mX=L1median(X,tol);
  ##
  ## X  : the data matrix
  ## tol: the convergence criterium:
  ##      the iterative process stops when ||m_k - m_{k+1}|| < tol.
  ## maxit: maximum number of iterations
  ## init.m: starting value for m; typically coordinatewise median
  ##
  ## Ref: Hossjer and Croux (1995)
  ##  "Generalizing Univariate Signed Rank Statistics for Testing
  ##   and Estimating a Multivariate Location Parameter";
  ##   Non-parametric Statistics, 4, 293-308.
  ##
  ## Implemented by Kristel Joossens
  ## Many thanks to Martin Maechler for improving the program!

  ## slightly faster version of 'sweep(x, 2, m)':
  centr <- function(X,m) X - rep(m, each = n)
  ## computes objective function in m based on X and a:
  mrobj <- function(X,m) sum(sqrt(rowSums(centr(X,m)^2)))
  d <- dim(X); n <- d[1]; p <- d[2]
  m <- m.init
  if(!is.numeric(m) || length(m) != p)
      stop("'m.init' must be numeric of length p =", p)
  k <- 1
  if(trace) nstps <- 0
  while (k <= maxit) {
    mold <- m
    obj.old <- if(k == 1) mrobj(X,mold) else obj
    X. <- centr(X, m)
    Xnorms <- sqrt(rowSums(X. ^ 2))
    inorms <- order(Xnorms)
    dx <- Xnorms[inorms] # smallest first, i.e., 0's if there are
    X  <- X [inorms,]
    X. <- X.[inorms,]
    ## using 1/x weighting {MM: should this be generalized?}
    w <- ## (0 norm -> 0 weight) :
        if (all(dn0 <- dx != 0))  1/dx
        else c(rep.int(0, length(dx)- sum(dn0)), 1/dx[dn0])
    delta <- colSums(X. * rep(w,p)) / sum(w)
    nd <- sqrt(sum(delta^2))

    maxhalf <- if (nd < tol) 0 else ceiling(log2(nd/tol))
    m <- mold + delta    # computation of a new estimate
    ## If step 'delta' is too far, we try halving the stepsize
    nstep <- 0
    while ((obj <- mrobj(X, m)) >= obj.old && nstep <= maxhalf) {
      nstep <- nstep+1
      m <- mold + delta/(2^nstep)
    }
    if(trace) {
        if(trace >= 2)
            cat(sprintf("k=%3d obj=%19.12g m=(",k,obj),
                paste(formatC(m),collapse=","),
                ")", if(nstep) sprintf(" nstep=%2d halvings",nstep) else "",
                "\n", sep="")
        nstps[k] <- nstep
    }
    if (nstep > maxhalf) { ## step halving failed; keep old
        m <- mold
        ## warning("step halving failed in ", maxhalf, " steps")
        break
      }
    k <- k+1
  }
  if (k > maxit) warning("iterations did not converge in ", maxit, " steps")
  if(trace == 1)
      cat("needed", k, "iterations with a total of",
          sum(nstps), "stepsize halvings\n")
#  return(m)
list(center=m)
}

l2dci<-function(x,y,est=median,alpha=.05,nboot=2000,SEED=TRUE,pr=TRUE,...){
#
#   Compute a bootstrap confidence interval for a
#   measure of location associated with
#   the distribution of x-y, where x and y are possibly dependent.
#   est indicates which measure of location will be used
#
#   Function returns confidence interval, p-value and estimate
#   of square standard error of the estimator used.
#
x<-x[!is.na(x)] # Remove any missing values in x
y<-y[!is.na(y)] # Remove any missing values in y
if(SEED)set.seed(2) # set seed of random number generator so that
#             results can be duplicated.
if(pr)print("Taking bootstrap samples. Please wait.")
datax<-matrix(sample(x,size=length(x)*nboot,replace=TRUE),nrow=nboot)
datay<-matrix(sample(y,size=length(y)*nboot,replace=TRUE),nrow=nboot)
bvec<-NA
for(i in 1:nboot)bvec[i]<-loc2dif(datax[i,],datay[i,],est=est)
bvec<-sort(bvec)
low<-round((alpha/2)*nboot)+1
up<-nboot-low
temp<-sum(bvec<0)/nboot+sum(bvec==0)/(2*nboot)
sig.level<-2*(min(temp,1-temp))
se<-var(bvec)
list(ci=c(bvec[low],bvec[up]),p.value=sig.level,sq.se=se)
}




# ----------------------------------------------------------------------------

# l2drmci

# ----------------------------------------------------------------------------

l2drmci<-function(x,y=NULL,est=median,alpha=.05,nboot=2000,SEED=TRUE,pr=TRUE,
na.rm=TRUE,...){
#
#   Compute a bootstrap confidence interval for a
#   measure of location associated with
#   the distribution of x-y,
#   est indicates which measure of location will be used
#   x and y are possibly dependent
#
#   na.rm=F, assumes missing values occur at random and will  use
#   all of the data that is not missing.
#   na.rm=T eliminates any pair with one or both values are missing.
#
if(is.null(y[1])){
if(!is.matrix(x) & !is.data.frame(x))stop("With y missing, x should be a matrix")
}
if(!is.null(y[1])){
if(length(x)!=length(y))stop('Unequal sample sizes. Might use wmwpb instead')
x<-cbind(x,y)
}
if(ncol(x)!=2)stop("Should have bivariate data")
if(na.rm)x=elimna(x)
EST=loc2dif(x[,1],x[,2],est=est)
if(SEED)set.seed(2) # set seed of random number generator so that
#             results can be duplicated.
data<-matrix(sample(nrow(x),size=nrow(x)*nboot,replace=TRUE),nrow=nboot)
bvec<-NA
for(i in 1:nboot)bvec[i]<-loc2dif(x[data[i,],1],x[data[i,],2],est=est,na.rm=na.rm,...)
bvec<-sort(bvec)
low<-round((alpha/2)*nboot)+1
up<-nboot-low
temp<-sum(bvec<0)/nboot+sum(bvec==0)/(2*nboot)
sig.level<-2*(min(temp,1-temp))
list(Est.=EST,ci=c(bvec[low],bvec[up]),p.value=sig.level)
}


l2v<-function(x){
#
#  combine data in list mode to a single
#  vector
if(!is.list(x))stop("x does not have list mode")
y=NULL
for(i in 1:length(x))y=c(y,x[[i]])
y
}




# ----------------------------------------------------------------------------

# LAD.lasso

# ----------------------------------------------------------------------------

LAD.lasso<-function(x,y,lam=NULL,WARN=FALSE,xout=FALSE,outfun=outpro,STAND=TRUE){
#
#  LAD (weighted) lasso based on
# Wang et al. Journal of Business and Economic Statistics
#
x=as.matrix(x)
p=ncol(x)
p1=p+1
xy=cbind(x,y)
xy=elimna(xy)
if(STAND)xy=standm(xy)
x=xy[,1:p]
y=xy[,p1]

if(xout){
flag<-outfun(x,plotit=FALSE)$keep
xy=cbind(x,y)
x=xy[flag,1:p]
y=xy[flag,p1]
}
n=nrow(x)
if(p==1)stop('Should have two or more independent variables')
library(quantreg)
if(!WARN)options(warn=-1)
temp<-rq(y~x)
init<-temp[1]$coefficients
if(is.null(lam))lam=log(n)/(n*abs(init[2:p1]))
yy=c(y,rep(0,p))
M=diag(n*lam)
xx=rbind(x,M)
coef=rq(yy~xx)[1]$coefficients
if(!WARN)options(warn=0)
res=y-x%*%coef[2:p1]-coef[1]
rv=order(lam)
list(coef=coef,lambda=lam,slopes.rank=rv,residuals=res)
}


LADlasso.Z <- function(x , y , STAND = TRUE, grid = seq(log(0.01),log(1400),length.out=100),xout=FALSE,outfun=outpro,...){
#
#     Zheng, Q., Gallagher, C., and Kulasekera, K. (2016). Robust adaptive lasso for variable
#     selection. Communications in Statistics - Theory and Methods, 46(9):4642-4659.
#
#    The code used here is a slight modification of code supplied by Qi Zheng
#
       ####X = matrix, list, or dataframe of predictor values
       ####Y = vector or single-dimension matrix/list/dataframe of outcome values
       ####grid = grid of lambda(tuning parameter) values to check

       library(lars)
       library(quantreg)

       X=x
       Y=y
p=ncol(X)
p1=p+1
xy=elimna(cbind(X,Y))
X=xy[,1:p]
Y=xy[,p1]
 if(STAND)X=standm(X)
     if(xout){
X<-as.matrix(X)
flag<-outfun(X,plotit=FALSE,...)$keep
X<-X[flag,]
Y<-Y[flag]
X<-as.matrix(X)
n.keep=nrow(X)
}
       object1=lsa.linear(X,Y);
       adlasso=object1$coef;
       n=length(Y);
       grid=exp(grid);
       rqob=rq(Y~0+X);
       BIC=rep(0,100);
       weights=1/abs(rqob$coef);
       for ( i in 1:100){
              rqfit=rq.fit.lasso(X,Y,lambda=grid[i]*weights);
              betalad_tmp=rqfit$coef;
              betalad_tmp=betalad_tmp*(betalad_tmp>1e-8);
              mse=mean(abs(rqfit$resi));
              mdsize=length(which(betalad_tmp!=0));
              BIC[i]=log(mse)+mdsize*log(n)/n;
       }
       step=which.min(BIC);
       betalad=rq.fit.lasso(X,Y,lambda=grid[step]*weights)$coef;
       ladlasso=betalad*(betalad>1e-8)
       colnames(ladlasso) <- names(X)
   # add an intercept for convenience
   alpha<-median(Y-X%*%ladlasso)
   coef<-c(alpha,ladlasso)
res<-Y-X%*%ladlasso-alpha
    list(coef=coef,residuals=res)
}

RA.lasso=LADlasso.Z




# ----------------------------------------------------------------------------

# larsR

# ----------------------------------------------------------------------------

larsR<-function(x,y,type="lasso",xout=FALSE,outfun=out){
library(lars)
p=ncol(x)
p1=p+1
xy=elimna(cbind(x,y))
if(xout){
x<-xy[,1:p]
y<-xy[,p1]
m<-cbind(x,y)
flag<-outfun(x,plotit=FALSE)$keep
m<-m[flag,]
x<-m[,1:p]
y<-m[,p1]
xy=cbind(x,y)
}
result=lars(xy[,1:p],xy[,p1],type=type)
result
}

lband<-function(x,y=NULL,alpha=.05,plotit=TRUE,sm=TRUE,op=1,ylab='delta',CI=TRUE,
xlab='x (first group)'){
#
#  Compute a confidence band for the shift function.
#  Assuming two dependent groups are being compared
#
#  See Lombard (2005, Technometrics, 47, 364-369)
#
#  if y=NA, assume x is a matrix with two columns or it has list mode
#
#  If plotit=TRUE, a plot of the shift function is created, assuming that
#  the graphics window has already been activated.
#
#  sm=T, plot of shift function is smoothed using:
#  expected frequency curve if op!=1
#  otherwise use S+ function lowess is used.
#
#  This function removes all missing observations.
#
#  When plotting, the median of x is marked with a + and the two
#  quartiles are marked with o.
#
if(!is.null(y[1]))x<-cbind(x,y)
if(is.list(x))x=matl(x)
if(ncol(x)!=2)stop('Should have two groups only')
m<-elimna(x)
y<-m[,2]
x<-m[,1]
n<-length(x)
crit<-nelderv2(m,1,lband.fun2,alpha=alpha)
plotit<-as.logical(plotit)
xsort<-sort(x)
ysort<-sort(y)
l<-0
u<-0
ysort[0]<-NA
ysort[n+1]<-NA
lsub<-c(1:n)-floor(sqrt(2*n)*crit)
usub<-c(1:n)+floor(sqrt(2*n)*crit)
for(ivec in 1:n){
isub<-max(0,lsub[ivec])
l[ivec]<-NA
if(isub>0)l[ivec]<-ysort[isub]-xsort[ivec]
isub<-min(n+1,usub[ivec])
u[ivec]<-NA
if(isub <= n)u[ivec]<-ysort[isub]-xsort[ivec]
}
num<-length(l[l>0 & !is.na(l)])+length(u[u<0 & !is.na(u)])
qhat<-c(1:n)/n
m<-cbind(qhat,l,u)
dimnames(m)<-list(NULL,c('qhat','lower','upper'))
if(plotit){
xsort<-sort(x)
ysort<-sort(y)
del<-0
for (i in 1:n)del[i]<-ysort[i]-xsort[i]
xaxis<-c(xsort,xsort)
yaxis<-c(m[,1],m[,2])
allx<-c(xsort,xsort,xsort)
ally<-c(del,m[,2],m[,3])
temp2<-m[,2]
temp2<-temp2[!is.na(temp2)]
plot(allx,ally,type='n',ylab=ylab,xlab=xlab)
ik<-rep(F,length(xsort))
if(sm){
if(op==1){
ik<-duplicated(xsort)
del<-lowess(xsort,del)$y
}
if(op!=1)del<-runmean(xsort,del,pyhat=TRUE)
}
lines(xsort[!ik],del[!ik])
lines(xsort,m[,2],lty=2)
lines(xsort,m[,3],lty=2)
temp<-summary(x)
text(temp[3],min(temp2),'+')
text(temp[2],min(temp2),'o')
text(temp[5],min(temp2),'o')
}
id.sig.greater=NULL
id.sig.less.than=NULL
num<-length(l[l>0 & !is.na(l)])+length(u[u<0 & !is.na(u)])
id.sig.greater=which(l>0)
id.sig.less.than=which(u<0)

flag=is.na(m[,2])
m[flag,2]=-Inf
flag=is.na(m[,3])
m[flag,3]=Inf
q.greater=NULL
if(length(id.sig.greater)>0)q.greater=m[id.sig.greater,1]
q.less=NULL
if(length(id.sig.less.than)>0)q.less=m[id.sig.less.than,1]
if(!CI)m=NULL
list(m=m,crit=crit,numsig=num,q.sig.greater=q.greater,q.sig.less=q.less)
}

lband.fun<-function(x,y,crit){
#
#  function used to determine probability of type I error given crit
#
pi<-gamma(.5)^2
xr<-rank(x)
yr<-rank(y)
temp<-apply(cbind(xr,yr),1,max)
n<-length(x)
fj<-NA
for(i in 1:n)fj[i]<-sum(temp==i)
v1<-NA
for(j in 1:n)v1[j]<-(j-sum(fj[1:j]))/n
psi<-rep(0,n)
for(j in 1:n){
if(v1[j]>0)psi[j]<-crit*exp(0-crit^2/(2*v1[j]))/sqrt(2*pi*v1[j]^3)
}
res<-mean(fj*psi)
res
}




# ----------------------------------------------------------------------------

# lband.fun2

# ----------------------------------------------------------------------------

lband.fun2<-function(m,crit,alpha=.05){
x<-m[,1]
y<-m[,2]
val<-abs(alpha-lband.fun(x,y,crit))
val
}




# ----------------------------------------------------------------------------

# LCO.CI

# ----------------------------------------------------------------------------

LCO.CI <- function(n,level,dp)
{

  # For desired decimal place accuracy of dp, search on grid using (dp+1)
  # accuracy then round final results to dp accuracy.
  iter <- 10**(dp+1)

  p <- seq(0,.5,1/iter)


  ############################################################################
  # Create initial cpf with AC[l,u] endpoints by choosing coverage
  # probability from highest acceptance curve with minimal span.


  cpf.matrix <- matrix(NA,ncol=3,nrow=iter+1)
  colnames(cpf.matrix)<-c("p","low","upp")

  for (i in 1:(iter/2+1)){
    p <- (i-1)/iter

    bin <- dbinom(0:n,n,p)
    x   <- 0:n
    pmf <- cbind(x,bin)

    # Binomial probabilities ordered in descending sequence
    pmf <- pmf[order(-pmf[,2],pmf[,1]),]
    pmf <- data.frame(pmf)

    # Select the endpoints (l,u) such that AC[l,u] will
    # be at least equal to LEVEL. The cumulative sum of
    # the ordered pmf will identify when this occurs.
    m.row  <- min(which((cumsum(pmf[,2])>=level)==TRUE))
    low.val <-min(pmf[1:m.row,][,1])
    upp.val <-max(pmf[1:m.row,][,1])

    cpf.matrix[i,] <- c(p,low.val,upp.val)

    # cpf flip only for p != 0.5

    if (i != iter/2+1){
      n.p <- 1-p
      n.low <- n-upp.val
      n.upp <- n-low.val

      cpf.matrix[iter+2-i,] <- c(n.p,n.low,n.upp)
    }
  }


  ############################################################################
  # LCO Gap Fix
  # If the previous step yields any violations in monotonicity in l for
  # AC[l,u], this will cause a CI gap. Apply fix as described in Step 2 of
  # algorithm as described in paper.

  # For p < 0.5, monotonicity violation in l can be determined by using the
  # lagged difference in l. If the lagged difference is -1 a violation has
  # occurred. The NEXT lagged difference of +1 identifies the (l,u) pair to
  # substitute with. The range of p in violation would be from the lagged
  # difference of -1 to the point just before the NEXT lagged difference of
  # +1. Substitute the old (l,u) with updated (l,u) pair. Then make required
  # (l,u) substitutions for corresponding p > 0.5.

  # Note the initial difference is defined as 99 simply as a place holder.

  diff.l <- c(99,diff(cpf.matrix[,2],differences = 1))

  if (min(diff.l)==-1){

    for (i in which(diff.l==-1)){
      j <- min(which(diff.l==1)[which(diff.l==1)>i])
      new.low <- cpf.matrix[j,2]
      new.upp <- cpf.matrix[j,3]
      cpf.matrix[i:(j-1),2] <- new.low
      cpf.matrix[i:(j-1),3] <- new.upp
      }

    # cpf flip
    pointer.1 <- iter - (j - 1) + 2
    pointer.2 <- iter - i + 2

    cpf.matrix[pointer.1:pointer.2,2]<- n - new.upp
    cpf.matrix[pointer.1:pointer.2,3]<- n - new.low
  }


  ############################################################################
  # LCO CI Generation

  ci.matrix <-  matrix(NA,ncol=3,nrow=n+1)
  rownames(ci.matrix) <- c(rep("",nrow(ci.matrix)))
  colnames(ci.matrix) <- c("x","lower","upper")

  # n%%2 is n mod 2: if n%%2 == 1 then n is odd
  # n%/%2 is the integer part of the division: 5/2 = 2.5, so 5%/%2 = 2

  if (n%%2==1) x.limit <- n%/%2
  if (n%%2==0) x.limit <- n/2

  for (x in 0:x.limit)
  {
    num.row <- nrow(cpf.matrix[(cpf.matrix[,2]<=x & x<=cpf.matrix[,3]),])

    low.lim <-
      round(cpf.matrix[(cpf.matrix[,2]<=x & x<=cpf.matrix[,3]),][1,1],
            digits=dp)

    upp.lim <-
      round(cpf.matrix[(cpf.matrix[,2]<=x & x<=cpf.matrix[,3]),][num.row,1],
            digits=dp)

    ci.matrix[x+1,]<-c(x,low.lim,upp.lim)

    # Apply equivariance
    n.x <- n-x
    n.low.lim <- 1 - upp.lim
    n.upp.lim <- 1 - low.lim

    ci.matrix[n.x+1,]<-c(n.x,n.low.lim,n.upp.lim)
  }


  heading <- matrix(NA,ncol=1,nrow=1)

  heading[1,1] <-
    paste("LCO Confidence Intervals for n = ",n," and Level = ",level,sep="")

  rownames(heading) <- c("")
  colnames(heading) <- c("")

#  print(heading,quote=FALSE)

 # print(ci.matrix)
ci.matrix
}


##############################################################################
#    The function LCO.CI() generates the LCO confidence intervals            #
#    for X = 0, 1, ..., n  for a specified n and confidence level.           #
#                                                                            #
#    Example: To generate all LCO confidence intervals at n=20,              #
#             level=.90, and 3rd decimal place accuracy, use                 #
#                                                                            #
#    > LCO.CI(20,.90,3)                                                      #
##############################################################################




# ----------------------------------------------------------------------------

# LG.samp

# ----------------------------------------------------------------------------

LG.samp<-function(x,y,del=0,alpha=.05,beta=.2){
#
# estimate sample sizes for Yuen methods, trimming =.2
# using Luh-Guo method
# 
f1<-length(x)-2*floor(tr*length(x))
f2<-length(y)-2*floor(tr*length(y))
wx=winval(x)
wy=winval(y)
sqw1=sum((wx-mean(wx))^2)/(f1-1)
sqw2=sum((wy-mean(wy))^2)/(f2-1)
#print(sqw1)
#print(sqw2)
root1=sqrt(sqw1)
root2=sqrt(sqw2)
crit1=qnorm(1-alpha/2)
crit2=qnorm(1-beta)
tauw=sqrt(sqw2/sqw1)
F1=root1*(root1+root2)*(crit1+crit2)^2/del^2++crit1^2/(2*(1+tauw))
F2=root2*(root1+root2)*(crit1+crit2)^2/del^2++crit1^2*tauw/(2*(1+tauw))
N1=floor(F1/.6)+1
N2=floor(F2/.6)+1
c(N1,N2)
}

lincdm<-function(x,con=0,alpha=.05,q=.5,mop=FALSE,nboot=100,SEED=TRUE){
#
#  A heteroscedastic test of d linear contrasts among
#  dependent groups using medians.
#
#  The data are assumed to be stored in $x$ in list mode.
#  Length(x) is assumed to correspond to the total number of groups, J
#  It is assumed all groups are independent.
#
#  con is a J by d matrix containing the contrast coefficients that are used.
#  If con is not specified, all pairwise comparisons are made.
#
#  q is the quantile used to compare groups.
#  con contains contrast coefficients,
#  con=0 means all pairwise comparisons are used
#  mop=F, use single order statistic
#  mop=T, use usual sample median, even if q is not equal to .5
#  in conjunction with a bootstrap estimate of covariances among
#  the medians using
#  nboot samples.
#
#  Missing values are automatically removed.
#
#
if(mop && SEED)set.seed(2)
if(is.list(x)){
x<-matl(x)
x<-elimna(x)
}
if(is.matrix(x))x<-listm(x)
if(!is.list(x))stop("Data must be stored in a matrix or in list mode.")
con<-as.matrix(con)
J<-length(x)
h<-length(x[[1]])
w<-vector("numeric",J)
xbar<-vector("numeric",J)
for(j in 1:J){
if(!mop)xbar[j]<-qest(x[[j]],q=q)
if(mop)xbar[j]<-median(x[[j]])
}
if(sum(con^2)==0){
temp<-qdmcp(x,alpha=alpha,q=q,pr=FALSE)
test<-temp$test
psihat<-temp$psihat
num.sig<-temp$num.sig
}
if(sum(con^2)>0){
ncon<-ncol(con)
if(alpha==.05){
dvec<-c(.05,.025,.0169,.0127,.0102,.00851,.0073,.00639,.00568,.00511)
if(ncon > 10){
avec<-.05/c(11:ncon)
dvec<-c(dvec,avec)
}}
if(alpha==.01){
dvec<-c(.01,.005,.00334,.00251,.00201,.00167,.00143,.00126,.00112,.00101)
if(ncon > 10){
avec<-.01/c(11:ncon)
dvec<-c(dvec,avec)
}}
if(alpha != .05 && alpha != .01)dvec<-alpha/c(1:ncon)
if(nrow(con)!=length(x)){
stop("The number of groups does not match the number of contrast coefficients.")
}
psihat<-matrix(0,ncol(con),4)
dimnames(psihat)<-list(NULL,c("con.num","psihat","ci.lower","ci.upper"))
test<-matrix(0,ncol(con),5)
dimnames(test)<-list(NULL,c("con.num","test","p.value","crit.p.value","se"))
df<-length(x[[1]])-1
if(!mop)w<-covmmed(x,q=q)
if(mop)w<-bootcov(x,nboot=nboot,pr=FALSE)
for (d in 1:ncol(con)){
psihat[d,1]<-d
psihat[d,2]<-sum(con[,d]*xbar)
cvec<-as.matrix(con[,d])
sejk<-sqrt(t(cvec)%*%w%*%cvec)
test[d,1]<-d
test[d,2]<-sum(con[,d]*xbar)/sejk
test[d,3]<-2*(1-pt(abs(test[d,2]),df))
test[d,5]<-sejk
}
temp1<-test[,3]
temp2<-order(0-temp1)
zvec<-dvec[1:ncon]
test[temp2,4]<-zvec
psihat[,3]<-psihat[,2]-qt(1-test[,4]/2,df)*test[,5]
psihat[,4]<-psihat[,2]+qt(1-test[,4]/2,df)*test[,5]
num.sig<-sum(test[,3]<=test[,4])
}
list(test=test,psihat=psihat,num.sig=num.sig)
}
lincdtr<-function(x,con=0,alpha=.05,tr=.2){
#
#  A heteroscedastic test of d linear contrasts among
#  dependent groups using trimmed means.
#
#  The data are assumed to be stored in $x$ in list mode.
#  Length(x) is assumed to correspond to the total number of groups, J
#  It is assumed all groups are independent.
#
#  con is a J by d matrix containing the contrast coefficients that are used.
#  If con is not specified, all pairwise comparisons are made.
#
#  con contains contrast coefficients,
#  con=0 means all pairwise comparisons are used
#
#  Missing values are automatically removed.
#
#
if(is.list(x)){
x<-matl(x)
x<-elimna(x)
}
if(is.matrix(x))x<-listm(x)
if(!is.list(x))stop("Data must be stored in a matrix or in list mode.")
con<-as.matrix(con)
J<-length(x)
h<-length(x[[1]])
w<-vector("numeric",J)
xbar<-vector("numeric",J)
for(j in 1:J){
xbar[j]<-mean(x[[j]],tr=tr)
}
if(sum(con^2)==0){
temp<-rmmcp(x,alpha=alpha,tr=tr,dif=FALSE)
test<-temp$test
psihat<-temp$psihat
num.sig<-temp$num.sig
}
if(sum(con^2)>0){
ncon<-ncol(con)
if(alpha==.05){
dvec<-c(.05,.025,.0169,.0127,.0102,.00851,.0073,.00639,.00568,.00511)
if(ncon > 10){
avec<-.05/c(11:ncon)
dvec<-c(dvec,avec)
}}
if(alpha==.01){
dvec<-c(.01,.005,.00334,.00251,.00201,.00167,.00143,.00126,.00112,.00101)
if(ncon > 10){
avec<-.01/c(11:ncon)
dvec<-c(dvec,avec)
}}
if(alpha != .05 && alpha != .01)dvec<-alpha/c(1:ncon)
if(nrow(con)!=length(x)){
stop("The number of groups does not match the number of contrast coefficients.")
}
psihat<-matrix(0,ncol(con),4)
dimnames(psihat)<-list(NULL,c("con.num","psihat","ci.lower","ci.upper"))
test<-matrix(0,ncol(con),5)
dimnames(test)<-list(NULL,c("con.num","test","p.value","crit.p.value","se"))
df<-length(x[[1]])-1
w<-covmtrim(x,tr=tr)
for (d in 1:ncol(con)){
psihat[d,1]<-d
psihat[d,2]<-sum(con[,d]*xbar)
cvec<-as.matrix(con[,d])
sejk<-sqrt(t(cvec)%*%w%*%cvec)
test[d,1]<-d
test[d,2]<-sum(con[,d]*xbar)/sejk
test[d,3]<-2*(1-pt(abs(test[d,2]),df))
test[d,5]<-sejk
}
temp1<-test[,3]
temp2<-order(0-temp1)
zvec<-dvec[1:ncon]
test[temp2,4]<-zvec
psihat[,3]<-psihat[,2]-qt(1-test[,4]/2,df)*test[,5]
psihat[,4]<-psihat[,2]+qt(1-test[,4]/2,df)*test[,5]
num.sig<-sum(test[,3]<=test[,4])
}
list(test=test,psihat=psihat,num.sig=num.sig)
}




# ----------------------------------------------------------------------------

# lindm

# ----------------------------------------------------------------------------

lindm<-function(x,con=0,est=onestep,grp=0,alpha=.05,nboot=999,...){
#
#   Compute a 1-alpha confidence interval for a set of d linear contrasts
#   involving M-estimators associated with the marginal distributions
#   using a bootstrap method.
#   Dependent groups are assumed.
#
#   The data are assumed to be stored in x in list mode.  Thus,
#   x[[1]] contains the data for the first group, x[[2]] the data
#   for the second group, etc. Length(x)=the number of groups = J, say.
#
#   con is a J by d matrix containing the contrast coefficents of interest.
#   If unspecified, all pairwise comparisons are performed.
#   For example, con[,1]=c(1,1,-1,-1,0,0) and con[,2]=c(,1,-1,0,0,1,-1)
#   will test two contrasts: (1) the sum of the first two trimmed means is
#   equal to the sum of the second two, and (2) the difference between
#   the first two is equal to the difference between the trimmed means of
#   groups 5 and 6.
#
#   The default number of bootstrap samples was nboot=399 and is now 999
#
#   This function uses the function trimpartt written for this
#   book.
#
#
#
if(!is.list(x) && !is.matrix(x))stop("Data must be stored in a matrix or in list mode.")
if(is.list(x)){
if(sum(grp)==0)grp<-c(1:length(x))
# put the data in an n by J matrix
mat<-matrix(0,length(x[[1]]),length(grp))
for (j in 1:length(grp))mat[,j]<-x[[grp[j]]]
}
if(is.matrix(x)){
if(sum(grp)==0)grp<-c(1:ncol(x))
mat<-x[,grp]
}
mat<-elimna(mat)
J<-ncol(mat)
Jm<-J-1
d<-(J^2-J)/2
if(sum(con^2)==0){
con<-matrix(0,J,d)
id<-0
for (j in 1:Jm){
jp<-j+1
for (k in jp:J){
id<-id+1
con[j,id]<-1
con[k,id]<-0-1
}}}
if(nrow(con)!=ncol(mat))stop("The number of groups does not match the number of contrast coefficients.")
m1<-matrix(0,J,nboot)
m2<-1 # Initialize m2
mval<-1
set.seed(2) # set seed of random number generator so that
#             results can be duplicated.
print("Taking bootstrap samples. Please wait.")
data<-matrix(sample(nrow(mat),size=nrow(mat)*nboot,replace=TRUE),nrow=nboot)
#    data is B by n matrix
xcen<-matrix(0,nrow(mat),ncol(mat)) #An n by J matrix
for (j in 1:J){xcen[,j]<-mat[,j]-est(mat[,j],...) #Center data
mval[j]<-est(mat[,j],...)
}
for (j in 1:J)m1[j,]<-apply(data,1,lindmsub,xcen[,j],est,...) # A J by nboot matrix.
m2<-var(t(m1)) # A J by J covariance matrix corresponding to the nboot values.
boot<-matrix(0,ncol(con),nboot)
bot<-1
for (d in 1:ncol(con)){
top<-apply(m1,2,trimpartt,con[,d])
#            A vector of length nboot containing psi hat values
consq<-con[,d]^2
bot[d]<-trimpartt(diag(m2),consq)
for (j1 in 1:J){
for (j2 in 1:J){
if(j1<j2)bot[d]<-bot[d]+2*con[j1,d]*con[j2,d]*m2[j1,j2]
}}
boot[d,]<-abs(top)/sqrt(bot[d])
}
testb<-apply(boot,2,max)
ic<-round((1-alpha)*nboot)
testb<-sort(testb)
psihat<-matrix(0,ncol(con),5)
dimnames(psihat)<-list(NULL,c("con.num","psihat","ci.lower","ci.upper","se"))
for (d in 1:ncol(con)){
psihat[d,1]<-d
psihat[d,2]<-trimpartt(mval,con[,d])
psihat[d,3]<-psihat[d,2]-testb[ic]*sqrt(bot[d])
psihat[d,4]<-psihat[d,2]+testb[ic]*sqrt(bot[d])
psihat[d,5]<-sqrt(bot[d])
}
list(psihat=psihat,crit=testb[ic],con=con)
}


lindmsub<-function(isub,x,est,...){
#
# isub is a vector of length n containing integers between
# randomly sampled with replacement from 1,...,n.
#
# Used by lindm to convert an n by B matrix of bootstrap values,
# randomly sampled from 1, ..., n, with replacement, to a
# J by B matrix of measures of location.
#
#
lindmsub<-est(x[isub],...)
lindmsub
}




# ----------------------------------------------------------------------------

# lindQS

# ----------------------------------------------------------------------------

lindQS<-function(x,con,locfun=median,...){
#
#  For dependent variables X_1...X_J
# compute quantile shift measure of effect size for
#  Y_i=sum_j c_jX_j
#
x=elimna(x)
if(sum(con)!=0)stop('Contrast coefficients must sum to zero')
if(is.data.frame(x))x=as.matrix(x)
if(is.list(x))x<-matl(x)
J<-ncol(x)
if(length(con)!=J)stop('Length of con should equal number of groups')
L=linWMWMC.sub(x,con=con)
est=locfun(L)
p=linQS.sub2(L,locfun=locfun)
list(Q.effect=p,center=est)
}




# ----------------------------------------------------------------------------

# linEP

# ----------------------------------------------------------------------------

linEP<-function(x,con,locfun=tmean,tr=.2,nreps=200,SEED=TRUE){
#
#  Estimate exlanatory power for a linear contrast aimed at main effects
#
#  con = contrast coefficients
#  x is a matrix or has list mode.
#
#  When dealing with main effects. Could pool the data and use yuenv2.
#  Or could estimate distribution of the linear contrast, which is the
#  strategy here. Note: for interactions, this latter strategy is needed.
#
#  (Uses the function linWMWMC.sub)
#
if(sum(con)!=0)stop('Contrast coefficients must sum to zero')
if(SEED)set.seed(2)
if(is.data.frame(x))x=as.matrix(x)
if(is.matrix(x))x<-listm(x)
if(!is.list(x))stop('Data must be stored in a matrix or in list mode.')
J<-length(x)
if(length(con)!=J)stop('Length of con should be equal to the number of groups')
x=elimna(x)
nv=as.vector(matl(lapply(x,FUN='length')))
nmin=min(nv)
B=list()
M=matrix(NA,nrow=nmin,ncol=J)
for(i in 1:nreps){
for(j in 1:J)M[,j]=sample(x[[j]],nmin)
B[[i]]=M
}
if(length(con)==2)ef.size=yuenv2(x[[1]],x[[2]],tr=tr)$Effect.Size
else{
flag=con==1
con1=con
con1[!flag]=0
con2=abs(con)
con2[flag]=0
L1=lapply(B,linWMWMC.sub,con=con1)
L2=lapply(B,linWMWMC.sub,con=con2)
ef.size=NA
for(j in 1:length(L1))ef.size[j]=yuenv2(L1[[j]],L2[[j]],SEED=FALSE)$Effect.Size
}
list(Effect.Size=mean(ef.size))
}
linhat<-function(x,con,est=tmean,...){
#
# estimate all linear contrasts specified by con
#
psihat=0
xbar=llocv2(x,est=est,...)$center
for(i in 1:ncol(con))psihat[i]=sum(con[,i]*xbar)
psihat
}

linpairpb<-function(x,tr=.2,alpha=.05,nboot=NA,est=tmean,method='hoch',bhop=FALSE,SEED=TRUE,...){
#
#  Report results for all pairwise  comparsisons
#  in a format convenient when using other functions that use this function
#
if(is.matrix(x))x=listm(x)
J=length(x)
con=con.all.pairs(J)
if(bhop)method='BH'
a=linconpb(x,tr=tr,alpha=alpha,nboot=nboot,est=est,SEED=SEED,method=method,...)
r=nrow(a$output)
cc=ncol(a$output)
cp1=cc+3
mat=matrix(NA,nrow=r,ncol=cp1)
for(i in 1:r){
g=which(a$con[,i]!=0)
z=c(g,a$output[i,2:cc],est(x[[g[1]]],...),est(x[[g[2]]],...))
e1=est(x[[g[1]]],...)
e2=est(x[[g[2]]],...)
mat[i,]=c(g,a$output[i,2:cc],e1,e2)
}
num.sig<-sum(mat[,4]<=mat[,5])
dimnames(mat)=list(NULL,c('Group','Group','psihat','p.value','p.crit','ci.lower','ci.upper','Est .1','Est. 2','p.adjusted'))
list(output=mat,num.sig=num.sig)
}

linpbg<-function(x,con=0,alpha=.05,nboot=NA,est=mest,...){
#
#   Compute a 1-alpha confidence interval
#   for a set of d linear contrasts
#   involving trimmed means using the percentile bootstrap method.
#   Independent groups are assumed.
#
#   The data are assumed to be stored in x in list mode or in a matrix.
#   Thus,
#   x[[1]] contains the data for the first group, x[[2]] the data
#   for the second group, etc.
#   If x has list mode, length(x)=the number of groups = J, say.
#
#   Missing values are automatically removed.
#
#   con is a J by d matrix containing the
#   contrast coefficents of interest.
#   If unspecified, all pairwise comparisons are performed.
#   For example, con[,1]=c(1,1,-1,-1,0,0)
#   and con[,2]=c(,1,-1,0,0,1,-1)
#   will test two contrasts: (1) the sum of the first
#   two trimmed means is
#   equal to the sum of the second two,
#   and (2) the difference between
#   the first two is equal to the difference
#   between the trimmed means of
#   groups 5 and 6.
#
#
con<-as.matrix(con)
if(is.matrix(x))x<-listm(x)
if(!is.list(x))stop("Data must be stored in a matrix or in list mode.")
J<-length(x)
for(j in 1:J){
xx<-x[[j]]
x[[j]]<-xx[!is.na(xx)] # Remove any missing values.
}
Jm<-J-1
d<-(J^2-J)/2
if(sum(con^2)==0){
con<-matrix(0,J,d)
id<-0
for (j in 1:Jm){
jp<-j+1
for (k in jp:J){
id<-id+1
con[j,id]<-1 #If con not specified do all pairwise comparisons
con[k,id]<-0-1
}}}
if(nrow(con)!=length(x)){
stop("The number of groups does not match the number of contrast coefficients.")
}
if(is.na(nboot)){
nboot<-5000
if(ncol(con)<=4)nboot<-2000
}
m1<-matrix(0,nrow=J,ncol=nboot)
set.seed(2) # set seed of random number generator so that
#             results can be duplicated.
print("Taking bootstrap samples. Please wait.")
for(j in 1:J){
paste("Working on group ",j)
data<-matrix(sample(x[[j]],size=length(x[[j]])*nboot,replace=TRUE),nrow=nboot)
m1[j,]<-apply(data,1,est,...)
}
testb<-NA
boot<-matrix(0,ncol(con),nboot)
testvec<-NA
for (d in 1:ncol(con)){
boot[d,]<-apply(m1,2,trimpartt,con[,d])
# A vector of length nboot containing psi hat values
# and corresponding to the dth linear contrast
testb[d]<-sum((boot[d,]>0))/nboot
testvec[d]<-min(testb[d],1-testb[d])
}
#
#  Determine critical value
#
dd<-ncol(con)
if(alpha==.05){
if(dd==1)crit<-alpha/2
if(dd==2)crit<-.014
if(dd==3)crit<-.0085
if(dd==4)crit<-.007
if(dd==5)crit<-.006
if(dd==6)crit<-.0045
if(dd==10)crit<-.0023
if(dd==15)crit<-.0016
}
else{
crit<-alpha/(2*dd)
}
icl<-round(crit*nboot)
icu<-round((1-crit)*nboot)
psihat<-matrix(0,ncol(con),4)
test<-matrix(0,ncol(con),3)
dimnames(psihat)<-list(NULL,c("con.num","psihat","ci.lower","ci.upper"))
dimnames(test)<-list(NULL,c("con.num","test","crit.val"))
for (d in 1:ncol(con)){
test[d,1]<-d
psihat[d,1]<-d
testit<-lincon(x,con[,d],tr)
test[d,2]<-testvec[d]
temp<-sort(boot[d,])
psihat[d,3]<-temp[icl]
psihat[d,4]<-temp[icu]
psihat[d,2]<-testit$psihat[1,2]
test[d,3]<-crit
}
list(psihat=psihat,test=test,con=con)
}




# ----------------------------------------------------------------------------

# linplot

# ----------------------------------------------------------------------------

linplot<-function(x,con,nreps=200,SEED=TRUE,xlab='DV',ylab=''){
#
# Determine distribution of Y_i=sum_j c_jX_j
# and then plot the distribution
#
con=as.vector(con)
if(sum(con)!=0)stop('Contrast coefficients must sum to zero')
if(SEED)set.seed(2)
if(is.data.frame(x))x=as.matrix(x)
if(is.matrix(x))x<-listm(x)
if(!is.list(x))stop('Data must be stored in a matrix or in list mode.')
J<-length(x)
if(length(con)!=J)stop('Length of con should equal number of groups')
x=elimna(x)
L=NA
linv=NA
for(i in 1:nreps){
for(j in 1:J)linv[j]=sample(x[[j]],1)
L[i]=sum(con*linv)
}
akerd(L,xlab='X',ylab=ylab)
}

linQS<-function(x,con,locfun=median,nreps=200,SEED=TRUE){
#
# Determine distribution of Y_i=sum_j c_jX_j
# Then estimate quantile shift in location measure of effect size
# locfun, which defaults to the median.
#
if(sum(con)!=0)stop('Contrast coefficients must sum to zero')
if(SEED)set.seed(2)
if(is.data.frame(x))x=as.matrix(x)
if(is.matrix(x))x<-listm(x)
if(!is.list(x))stop('Data must be stored in a matrix or in list mode.')
J<-length(x)
if(length(con)!=J)stop('Length of con should equal number of groups')
x=elimna(x)
nv=as.vector(matl(lapply(x,FUN='length')))
nmin=min(nv)
est=NA
p=NA
B=list()
M=matrix(NA,nrow=nmin,ncol=J)
for(i in 1:nreps){
for(j in 1:J)M[,j]=sample(x[[j]],nmin)
B[[i]]=M
}
L=lapply(B,linWMWMC.sub,con=con)
est=lapply(L,locfun)
p=lapply(L,linQS.sub2,locfun=locfun)
est=as.vector(matl(est))
p=as.vector(matl(p))
list(Q.effect=mean(p),center=mean(est))
}




# ----------------------------------------------------------------------------

# linQS.sub2

# ----------------------------------------------------------------------------

linQS.sub2<-function(L,locfun=median){
phat=mean(L-locfun(L)<locfun(L))
phat
}




# ----------------------------------------------------------------------------

# linsign

# ----------------------------------------------------------------------------

linsign<-function(x,con,nreps=200,SEED=TRUE,nmax=10^8){
#
#  Estimate the	probability that a linear contrast is less than	zero
#
if(sum(con)!=0)stop('Contrast coefficients must sum to zero')
if(SEED)set.seed(2)
if(is.data.frame(x))x=as.matrix(x)
if(is.matrix(x))x<-listm(x)
nv=as.vector(matl(lapply(x,FUN='length')))
if(!is.list(x))stop('Data must be stored in a matrix or in list mode.')
J<-length(x)
nv=as.vector(matl(lapply(x,FUN='length')))
if(length(con)!=J)stop('Length of con should equal number of groups')
x=elimna(x)
B=list()
np=prod(nv)
nmin=min(nv)
if(np>nmax)nmin=min(c(nmin,100))
M=matrix(NA,nrow=nmin,ncol=J)
for(i in 1:nreps){
for(j in 1:J)M[,j]=sample(x[[j]],nmin)
B[[i]]=M
}
L=lapply(B,linWMWMC.sub,con=con)
ef.size=NA
for(j in 1:length(L))ef.size[j]=mean(L[[j]]<0)
ef=mean(ef.size)
ef
}

lintpb<-function(x,con=0,tr=.2,alpha=.05,nboot=NA){
#
#   Compute a 1-alpha confidence interval
#   for a set of d linear contrasts
#   involving trimmed means using the percentile bootstrap method.
#   Independent groups are assumed.
#
#   The data are assumed to be stored in x in list mode or in a matrix.
#   Thus,
#   x[[1]] contains the data for the first group, x[[2]] the data
#   for the second group, etc.
#   If x has list mode, length(x)=the number of groups = J, say.
#
#   Missing values are automatically removed.
#
#   con is a J by d matrix containing the
#   contrast coefficents of interest.
#   If unspecified, all pairwise comparisons are performed.
#   For example, con[,1]=c(1,1,-1,-1,0,0)
#   and con[,2]=c(,1,-1,0,0,1,-1)
#   will test two contrasts: (1) the sum of the first
#   two trimmed means is
#   equal to the sum of the second two,
#   and (2) the difference between
#   the first two is equal to the difference
#   between the trimmed means of
#   groups 5 and 6.
#
#
con<-as.matrix(con)
if(is.matrix(x))x<-listm(x)
if(!is.list(x))stop("Data must be stored in a matrix or in list mode.")
J<-length(x)
for(j in 1:J){
xx<-x[[j]]
xx[[j]]<-xx[!is.na(xx)] # Remove any missing values.
}
Jm<-J-1
d<-(J^2-J)/2
if(sum(con^2)==0){
con<-matrix(0,J,d)
id<-0
for (j in 1:Jm){
jp<-j+1
for (k in jp:J){
id<-id+1
con[j,id]<-1 #If con not specified do all pairwise comparisons
con[k,id]<-0-1
}}}
if(nrow(con)!=length(x)){
stop("The number of groups does not match the number of contrast coefficients.")
}
if(is.na(nboot)){
nboot<-5000
if(ncol(con)<=4)nboot<-2000
}
m1<-matrix(0,nrow=J,ncol=nboot)
set.seed(2) # set seed of random number generator so that
#             results can be duplicated.
print("Taking bootstrap samples. Please wait.")
for(j in 1:J){
paste("Working on group ",j)
data<-matrix(sample(x[[j]],size=length(x[[j]])*nboot,replace=TRUE),nrow=nboot)
m1[j,]<-apply(data,1,mean,tr)
}
testb<-NA
boot<-matrix(0,ncol(con),nboot)
testvec<-NA
for (d in 1:ncol(con)){
boot[d,]<-apply(m1,2,trimpartt,con[,d])
# A vector of length nboot containing psi hat values
# and corresponding to the dth linear contrast
testb[d]<-sum((boot[d,]>0))/nboot
testvec[d]<-min(testb[d],1-testb[d])
}
#
#  Determine critical value
#
dd<-ncol(con)
if(alpha==.05){
if(dd==1)crit<-alpha/2
if(dd==2)crit<-.014
if(dd==3)crit<-.0085
if(dd==4)crit<-.007
if(dd==5)crit<-.006
if(dd==6)crit<-.0045
if(dd==10)crit<-.0023
if(dd==15)crit<-.0016
}
else{
crit<-alpha/(2*dd)
}
icl<-round(crit*nboot)
icu<-round((1-crit)*nboot)
psihat<-matrix(0,ncol(con),4)
test<-matrix(0,ncol(con),3)
dimnames(psihat)<-list(NULL,c("con.num","psihat","ci.lower","ci.upper"))
dimnames(test)<-list(NULL,c("con.num","test","crit.val"))
for (d in 1:ncol(con)){
test[d,1]<-d
psihat[d,1]<-d
testit<-lincon(x,con[,d],tr)
test[d,2]<-testvec[d]
temp<-sort(boot[d,])
psihat[d,3]<-temp[icl]
psihat[d,4]<-temp[icu]
psihat[d,2]<-testit$psihat[1,2]
test[d,3]<-crit
}
list(psihat=psihat,test=test,con=con)
}

list.dif<-function(x1,x2){
#
# Form all differences
#
if(!is.list(x1))stop('Argument x1 should have list mode')
if(!is.list(x2))stop('Argument x2 should have list mode')
if(length(x1)!=length(x2))stop('x1 and x2 have different lengths')
dif=list()
for(j in 1:length(x1))dif[[j]]=x1[[j]]-x2[[j]]
dif
}




# ----------------------------------------------------------------------------

# list2matrix

# ----------------------------------------------------------------------------

list2matrix<-function(x){
#
# take data in list mode and store it in a matrix
#
J=length(x)
nval=NA
for(j in 1:J)nval[j]=length(x[[j]])
temp<-matrix(NA,ncol=J,nrow=max(nval))
for(j in 1:J)temp[1:nval[j],j]<-x[[j]]
temp
}




# ----------------------------------------------------------------------------

# list2vec

# ----------------------------------------------------------------------------

list2vec<-function(x){
if(!is.list(x))stop("x should have list mode")
res=as.vector(matl(x))
res
}




# ----------------------------------------------------------------------------

# listv2mat

# ----------------------------------------------------------------------------

listv2mat<-function(x){
#
# Each x[[]] has a vector of same length, p
# store	in a matrix with p columns
#
p=length(x[[1]])
n=length(x)
m=matrix(NA,nrow=n,ncol=p)
for(i in 1:n)m[i,]=x[[i]]
m
}


DqdifMC<-function(x,y=NULL,q=.25,nboot=1000,plotit=TRUE,xlab='Group 1 - Group 2',SEED=TRUE,alpha=.05){
#
#  Compare two dependent groups by comparing the
#  q and 1-q quantiles of the difference scores
#
# q should be < .5
# if the groups do not differ, then the difference scores should be symmetric
# about zero.
# In particular, the sum of q and 1-q quantiles should be zero.
#
# q indicates the quantiles to be compared. By default, the .25 and .75 quantiles are used.
#
if(SEED)set.seed(2)
if(q>=.5)stop('q should be less than .5')
if(!is.null(y)){
xy=elimna(cbind(x,y))
dif=xy[,1]-xy[,2]
}
if(is.null(y))dif=elimna(x)
x=as.matrix(x)
n=length(dif)
if(plotit)akerd(dif,xlab=xlab)
bvec=NA
data<-matrix(sample(n,size=n*nboot,replace=TRUE),nrow=nboot)
data=listm(t(data))
bvec<-mclapply(data,difQMC_sub,dif,q,mc.preschedule=TRUE)
bvec=matl(bvec)
est1=hd(dif,q=q)
est2=hd(dif,q=1-q)
pv=mean(bvec<0)+.5*mean(bvec==0)
p=2*min(c(pv,1-pv))
low<-round((alpha/2)*nboot)+1
up<-nboot-low
sbvec=sort(bvec)
ci=sbvec[low]
ci[2]=sbvec[up]
list(est.q=est1,est.1.minus.q=est2,conf.interval=ci,p.value=p)
}

difQMC_sub<-function(data,dif,q){
es=hd(dif[data],q)+hd(dif[data],1-q)
es
}


qcom.sub<-function(x,y,q,alpha=.05,nboot=2000,SEED=TRUE){
#
x<-x[!is.na(x)] # Remove any missing values in x
y<-y[!is.na(y)] # Remove any missing values in y
if(SEED)set.seed(2) # set seed of random number generator so that
#             results can be duplicated.
datax<-matrix(sample(x,size=length(x)*nboot,replace=TRUE),nrow=nboot)
datay<-matrix(sample(y,size=length(y)*nboot,replace=TRUE),nrow=nboot)
datax=listm(t(datax))
datay=listm(t(datay))
bvecx<-mclapply(datax,hd,q,mc.preschedule=TRUE)
bvecy<-mclapply(datay,hd,q,mc.preschedule=TRUE)
bvecx=as.vector(matl(bvecx))
bvecy=as.vector(matl(bvecy))
bvec<-sort(bvecx-bvecy)
low<-round((alpha/2)*nboot)+1
up<-nboot-low
temp<-sum(bvec<0)/nboot+sum(bvec==0)/(2*nboot)
sig.level<-2*(min(temp,1-temp))
se<-var(bvec)
list(est.1=hd(x,q),est.2=hd(y,q),ci=c(bvec[low],bvec[up]),p.value=sig.level,sq.se=se,n1=length(x),n2=length(y))
}



smeanMC<-function(m,cop=6,MM=FALSE,op=1,outfun=outogk,cov.fun=rmba,...){
#
# m is an n by p matrix
#
# Compute a multivariate skipped measure of location
#
# op=1:
# Eliminate outliers using a projection method
# That is, first determine center of data using:
# if op=1, a multi-core processor is used via the
# package multicore
#
# cop=1 Donoho-Gasko median,
# cop=2 MCD,
# cop=3 marginal medians.
#  cop=4 uses MVE center
#  cop=5 uses TBS
#  cop=6 uses rmba (Olive's median ball algorithm)
#
# For each point
# consider the line between it and the center,
# project all points onto this line, and
# check for outliers using
#
# MM=F, a boxplot rule.
# MM=T, rule based on MAD and median
#
# Repeat this for all points. A point is declared
# an outlier if for any projection it is an outlier
# using a modification of the usual boxplot rule.
#
# op=2 use mgv (function outmgv) method to eliminate outliers
# an outlier if for any projection it is an outlier
# using a modification of the usual boxplot rule.
#
# op=3 use outlier method indicated by outfun
#
# Eliminate any outliers and compute means
#  using remaining data.
#
m<-elimna(m)
if(op==1){
temp<-outproMC(m,plotit=FALSE,cop=cop,MM=MM)$keep
}
if(op==2)temp<-outmgv(m,plotit=FALSE,cov.fun=cov.fun)$keep
if(op==3)temp<-outfun(m,plotit=FALSE,...)$keep
val<-apply(m[temp,],2,mean)
val
}

 pb2genMC<-function(x,y,alpha=.05,nboot=2000,est=onestep,SEED=TRUE,pr=FALSE,...){
#
#   Compute a bootstrap confidence interval for the
#   the difference between any two parameters corresponding to
#   independent groups.
#   By default, M-estimators are compared.
#   Setting est=mean, for example, will result in a percentile
#   bootstrap confidence interval for the difference between means.
#   Setting est=onestep will compare M-estimators of location.
#   The default number of bootstrap samples is nboot=2000
#
x<-x[!is.na(x)] # Remove any missing values in x
y<-y[!is.na(y)] # Remove any missing values in y
if(SEED)set.seed(2) # set seed of random number generator so that
#             results can be duplicated.
if(pr)print("Taking bootstrap samples. Please wait.")
datax<-matrix(sample(x,size=length(x)*nboot,replace=TRUE),nrow=nboot)
datay<-matrix(sample(y,size=length(y)*nboot,replace=TRUE),nrow=nboot)
#
datax=t(datax)
datay=t(datay)
datax=listm(datax)
datay=listm(datay)
bvecx<-mclapply(datax,est,mc.preschedule=TRUE,...)
bvecy<-mclapply(datay,est,mc.preschedule=TRUE,...)
bvec=sort(matl(bvecx)-matl(bvecy))
low<-round((alpha/2)*nboot)+1
up<-nboot-low
temp<-sum(bvec<0)/nboot+sum(bvec==0)/(2*nboot)
sig.level<-2*(min(temp,1-temp))
se<-var(bvec)
list(est.1=est(x,...),est.2=est(y,...),ci=c(bvec[low],bvec[up]),p.value=sig.level,sq.se=se,n1=length(x),n2=length(y))
}

cbmhd_subMC<-function(data,cbmhd_subMC,x,y,q,q2,n1,n2){
np1=n1+1
nall=n1+n2
mb=outer(x[data[1:n1]],y[data[np1:nall]],"-")
est=hd(mb,q)+hd(mb,q2)
est
}

llrdata <- function(n = 100,  q=5)
{
# Generates data for loglinear regression.
#
	y <- 0 * 1:n
	beta <- 0 * 1:q
        beta[1:3] <- 1
        alpha <- -2.5
	x <- matrix(rnorm(n * q), nrow = n,
			ncol = q)
        x <- 0.5*x + 1
        SP <- alpha + x%*%beta
	y <- rpois(n,lambda=exp(SP))
        list(x=x,y=y)
}




# ----------------------------------------------------------------------------

# llressp

# ----------------------------------------------------------------------------

llressp <- function(x,y)
{
# Makes the ESSP for loglinear regression.
# Workstation: need to activate a graphics
# device with command "X11()" or "motif()."
#
#   If q is changed, change the formula in the glm statement.
	q <- 5
# change formula to x[,1]+ ... + x[,q] with q
	out <- glm(y ~ x[, 1] + x[, 2] + x[, 3] +
			x[, 4] + x[,5], family = poisson)
	ESP <- x %*% out$coef[-1] + out$coef[1]
        Y <- y
        plot(ESP,Y)
        abline(mean(y),0)
        fit <- y
        fit <- exp(ESP)
        indx <- sort.list(ESP)
        lines(ESP[indx],fit[indx])
        lines(lowess(ESP,y),type="s")
                        }

llrplot<-
function(x, y)
{
# Makes ESSP, the weighted forward response and residual plots for loglinear regression.
#
#   If q is changed, change the formula in the glm statement.
	q <- 5	# change formula to x[,1]+ ... + x[,q] with q
	out <- glm(y ~ x[, 1] + x[, 2] + x[, 3] + x[, 4] + x[, 5], family =
		poisson)
	ESP <- x %*% out$coef[-1] + out$coef[1]
	Y <- y
	par(mfrow = c(2, 2))
	plot(ESP, Y)
	abline(mean(y), 0)
	Ehat <- exp(ESP)
	indx <- sort.list(ESP)
	lines(ESP[indx], Ehat[indx])
	lines(lowess(ESP, y), type = "s")
	title("a) ESSP")
	Vhat <- (y - Ehat)^2
	plot(Ehat, Vhat)
	abline(0, 1)
	#abline(lsfit(Ehat, Vhat)$coef)
	title("b)")
	Z <- y
	Z[y < 1] <- Z[y < 1] + 0.5
	MWRES <- sqrt(Z) * (log(Z) - x %*% out$coef[-1] - out$coef[1])
	MWFIT <- sqrt(Z) * log(Z) - MWRES
	plot(MWFIT, sqrt(Z) * log(Z))
	abline(0, 1)
	#abline(lsfit(MWFIT, sqrt(Z) * log(Z))$coef)
	title("c) WFRP Based on MLE")
	plot(MWFIT, MWRES)
	title("d) WRP Based on MLE")
}

llrsim<-
function(n = 100, nruns = 1, type = 1)
{
# Runs llrpot 10 times on simulated LLR.
# Type = 1 for Poisson data, Type = 2 for negative binomial data
# Calls llrdata, oddata, llrplot.
	q <- 5
	for(i in 1:nruns) {
		if(type == 1)
			out <- llrdata(n, q)
		else out <- oddata(n, q)
		x <- out$x
		y <- out$y
		llrplot(x, y)	#identify(MWFIT, MWRES)
	}
}




# ----------------------------------------------------------------------------

# llrwtfrp

# ----------------------------------------------------------------------------

llrwtfrp <- function(x,y)
{
# Makes the weighted forward response and residual plots for loglinear regression.
# Workstation: need to activate a graphics
# device with command "X11()" or "motif()."

#
#   If q is changed, change the formula in the glm statement.
	q <- 5
# change formula to x[,1]+ ... + x[,q] with q
	out <- glm(y ~ x[, 1] + x[, 2] + x[, 3] +
			x[, 4] + x[,5], family = poisson)
	ESP <- x %*% out$coef[-1] + out$coef[1]
        Z <- y
        Z[y<1] <- Z[y<1] + 0.5
        out2<-lsfit(x,y=log(Z),wt=Z)
        #WRES <- sqrt(Z)*(log(Z) - x%*%out2$coef[-1] - out2$coef[1])
        WRES <- out2$res
        WFIT <- sqrt(Z)*log(Z) - WRES
        MWRES <- sqrt(Z)*(log(Z) - x%*%out$coef[-1] - out$coef[1])
        MWFIT <- sqrt(Z)*log(Z) - MWRES
        par(mfrow=c(2,2))
        plot(WFIT,sqrt(Z)*log(Z))
        abline(0,1)
        title("a) Weighted Forward Response Plot")
        plot(WFIT,WRES)
        title("b) Weighted Residual Plot")
        plot(MWFIT,sqrt(Z)*log(Z))
        abline(0,1)
        title("c) WFRP Based on MLE")
        plot(MWFIT,MWRES)
        title("d) WRP Based on MLE")
                                }

lmsviews<-
function(x, Y, ii = 1)
{
# Trimmed views using lmsreg for 90, 80, ... 0 percent
# trimming.   Allows visualization of  m
# and crudely estimation of  c beta in models
# of the form y = m(x^T beta) + e.
# Workstation: activate a graphics device
# with commands "X11()" or "motif()."
# R needs command "library(lqs)."
# Advance the view with the right mouse button and
# in R, highight "stop."
	x <- as.matrix(x)
	out <- cov.mcd(x)
	center <- out$center
	cov <- out$cov
	rd2 <- mahalanobis(x, center, cov)
	labs <- c("90%", "80%", "70%", "60%", "50%", "40%", "30%", "20%", "10%",
		"0%")
	tem <- seq(0.1, 1, 0.1)
	for(i in ii:10) {
		val <- quantile(rd2, tem[i])
		b <- lmsreg(x[rd2 <= val,  ], Y[rd2 <= val])$coef
		ESP <- x %*% b[-1]
		plot(ESP, Y)
		title(labs[i])
		identify(ESP, Y)
		print(b)
	}
}




# ----------------------------------------------------------------------------

# loc2dif.ci

# ----------------------------------------------------------------------------

loc2dif.ci<-function(x,y,est=median,alpha=.05,nboot=2000,SEED=TRUE){
#
# Confidence interval for the median of D=X-Y,
# where	X and Y	are independent
#
x=elimna(x)
y=elimna(y)
n1=length(x)
n2=length(y)
es=loc2dif(x,y)
FLAG=FALSE
cliff=cid(x,y,alpha=alpha)$ci.p
del1=WMW2med(x,y,cliff[1])
del2=WMW2med(x,y,cliff[2])
ci=loc2dif(x+del1,y,est=est)
ci[2]=loc2dif(x+del2,y,est=est)
if(var(cliff)==0)FLAG=TRUE
if(es<cliff[1])FLAG=TRUE
if(es>cliff[2])FLAG=TRUE
if(FLAG)ci=wmwpb(x,y,est=est,alpha=alpha,nboot=nboot,SEED=SEED,pr=FALSE)$ci
list(n1=n1,n2=n2,est=es,conf.int=ci)
}



logadr<-function(x,y,est=mean,iter=10,pyhat=FALSE,plotit=TRUE,fr=.8,xout=FALSE,eout=xout,
outfun=out,theta=50,phi=25,expand=.5,STAND=TRUE,ticktype="simple",scale=FALSE,...){
#
# additive model based on a variation of Copas' (1983) smooth
# for binary outcomes.
# (Use backfitting algorithm.)
#
m<-elimna(cbind(x,y))
x<-as.matrix(x)
p<-ncol(x)
p1<-p+1
y<-m[,p1]
x<-m[,1:p]
x<-as.matrix(x)
if(STAND){
for (ip in 1:p)x[,ip]<-(x[,ip]-mean(x[,ip]))/sqrt(var(x[,ip]))
}
if(xout){
keepit<-rep(TRUE,nrow(x))
flag<-outfun(x,plotit=FALSE)$out.id
keepit[flag]<-FALSE
x<-x[keepit,]
y<-y[keepit]
}
x<-as.matrix(x)
if(p==1)val<-logrsm(x[,1],y,pyhat=TRUE,plotit=plotit,fr=fr,...)$output
if(p>1){
np<-p+1
x<-m[,1:p]
y<-m[,np]
fhat<-matrix(NA,ncol=p,nrow=length(y))
fhat.old<-matrix(NA,ncol=p,nrow=length(y))
res<-matrix(NA,ncol=np,nrow=length(y))
dif<-1
for(i in 1:p)
fhat.old[,i]<-logrsm(x[,i],y,pyhat=TRUE,plotit=FALSE,fr=fr)$output
eval<-NA
for(it in 1:iter){
for(ip in 1:p){
res[,ip]<-y
for(ip2 in 1:p){
if(ip2 != ip)res[,ip]<-res[,ip]-fhat.old[,ip2]
}
fhat[,ip]=logrsm(x[,ip],y,pyhat=TRUE,plotit=FALSE,fr=fr)$output
}
eval[it]<-sum(abs(fhat/sqrt(sum(fhat^2))-fhat.old/sqrt(sum(fhat.old^2))))
if(it > 1){
itm<-it-1
dif<-abs(eval[it]-eval[itm])
}
fhat.old<-fhat
if(dif<.01)break
}
#print(fhat)
val<-apply(fhat,1,sum)
aval<-est(y-val,...)
val<-val+aval
flag=(val<0)
val[flag]=0
flag=(val>1)
val[flag]=1
if(plotit && p==2){
fitr<-val
iout<-c(1:length(fitr))
nm1<-length(fitr)-1
for(i in 1:nm1){
ip1<-i+1
for(k in ip1:length(fitr))if(sum(x[i,]==x[k,])==2)iout[k]<-0
}
fitr<-fitr[iout>=1] # Eliminate duplicate points in the x-y plane
#                 This is necessary when doing three dimensional plots
#                 with the R function interp
mkeep<-x[iout>=1,]
fitr<-interp(mkeep[,1],mkeep[,2],fitr)
persp(fitr,theta=theta,phi=phi,expand=expand,xlab="x1",ylab="x2",zlab="",
scale=scale,ticktype=ticktype)
}}
if(!pyhat)val<-"Done"
val
}




# ----------------------------------------------------------------------------

# logistic.lasso

# ----------------------------------------------------------------------------

logistic.lasso<-function(x,y,xout=FALSE,outfun=outpro){
#
#  A lasso version for estimating the parameters of  logistic regression via the R package glmnet
#
library(glmnet)
x<-as.matrix(x)
xx<-cbind(x,y)
xx<-elimna(xx)
x<-xx[,1:ncol(x)]
x<-as.matrix(x)
y<-xx[,ncol(x)+1]
temp<-NA
n=nrow(x)
n.keep=n
if(xout){
x<-as.matrix(x)
flag<-outfun(x,plotit=FALSE)$keep
x<-x[flag,]
y<-y[flag]
x<-as.matrix(x)
n.keep=nrow(x)
}
cv.lasso <- cv.glmnet(x, y, alpha = 1, family = 'binomial')
e=coef(cv.lasso, cv.lasso$lambda.min)
list(n=n,n.keep=n.keep,estimates=e)
}

logistic.LR<-function(x,y,xout=FALSE,outfun=outpro,ROB=FALSE,ADJ=TRUE,reps=5000,SEED=TRUE){
#
# Logistic regression:
#  Likelihood ratio test that all slope parameters are equal to zero.
#  ROB = True, initial estimate is based on the Bianco and Yohai (1996) estimator
#
x<-as.matrix(x)
xx<-cbind(x,y)
p1=ncol(xx)
p=p1-1
xx<-elimna(xx)
x<-xx[,1:ncol(x)]
x<-as.matrix(x)
y<-xx[,p1]
n=nrow(x)
n.keep=n
if(ADJ){
if(SEED)set.seed(2)
rem=NA
for(i in 1:reps){
xx=rmul(n,p=p)
yy=rbinom(n,1,.5)
rem[i]=logistic.LR.sub(xx,yy,n)
}
}
if(ROB)xout=FALSE  # ROB=T deals with leverage points.
if(xout){
x<-as.matrix(x)
flag<-outfun(x,plotit=FALSE,...)$keep
x<-x[flag,]
y<-y[flag]
x<-as.matrix(x)
n.keep=nrow(x)
}
xx=cbind(rep(n.keep,1),x)
e=logreg.pred(x,y,pts=x)
LF=sum(y*log(e))+sum((1-y)*log(1-e))
py=mean(y)
LN=sum(y*log(py))+sum((1-y)*log(1-py))
LR.test=-2*(LN-LF)
pv=1-pchisq(LR.test,p)
if(ADJ)pv=1-mean(pv<=rem)
pv
}
logistic.LR.sub<-function(x,y,n){
p=ncol(x)
xx=cbind(rep(n,1),x)
e=logreg.pred(x,y,pts=x)
LF=sum(y*log(e))+sum((1-y)*log(1-e))
py=mean(y)
LN=sum(y*log(py))+sum((1-y)*log(1-py))
LR.test=-2*(LN-LF)
pv=1-pchisq(LR.test,p)
pv
}




logIVcom<-function(x,y,IV1=1,IV2=2,nboot=500,xout=FALSE,outfun=outpro,SEED=TRUE,
val=NULL,...){
#
#  For binary dependent variables. Assumes the logistic regression model is true.
#
# compare strength of the association for two subsets of independent variables
# IV1 and IV2 indicate the two sets of independent variables to be compared
# Example: IV1=c(1,2), IV2=1 would compare the first two independent
# variables to the third.
#
#  If y is not binary, it has K possible values
#  val: determines the value that will be used.
#  Example: sort(unique(y))= 2, 4, 8
#  val=2, the function focuses on P(Y=4|X), the second possible
#  value in y
#  Default is to use the largest value, 8 in the example.
#
options(warn=-1)
x<-as.matrix(x)
p1<-ncol(x)+1
p<-ncol(x)
if(max(c(IV1,IV2))>p)stop('IV1 or IV2 has a value that exceeds the number of col. in x')
xy<-cbind(x,y)
xy<-elimna(xy)
x<-xy[,1:p]
y<-xy[,p1]
nrem=length(y)
if(xout){
m<-cbind(x,y)
flag<-outfun(x,plotit=FALSE,...)$keep
m<-m[flag,]
x<-m[,1:p]
y<-m[,p1]
}
nkeep=length(y)
temp<-sort(unique(y))
nv=length(temp)
if(is.null(val))idv=nv
y=y==temp[nv]
y=as.numeric(y)
est1=sd(logreg.pred(x[,IV1],y,x[,IV1]))
est2=sd(logreg.pred(x[,IV2],y,x[,IV2]))
nv=length(y)
x<-as.matrix(x)
if(SEED)set.seed(2) # set seed of random number generator so that
#             results can be duplicated.

data<-matrix(sample(length(y),size=length(y)*nboot,replace=TRUE),nrow=nboot)
data=listm(t(data))

bvec1<-mclapply(data,regIVbinv2_sub,x[,IV1],y,x[,IV1])
bvec2<-mclapply(data,regIVbinv2_sub,x[,IV2],y,x[,IV2])
bvec1=as.vector(matl(bvec1))
bvec2=as.vector(matl(bvec2))
# bvec1 and bvec2 are  nboot standard deviations based on bootstrap  predict prob(Y|X)
pv1=mean(bvec1<bvec2,na.rm=TRUE)  # P(v1 < v2)
pv2=1-pv1
pv=2*min(c(pv1,pv2))
options(warn=0)
list(n.keep=nkeep,est.1=est1,est.2=est2,p.value=pv)
}

lognormal.kurt<-function(mu=0,sigma=1){
 exp(4*sigma^2)+2*exp(3*sigma^2)+3*exp(2*sigma^2)-3
 }




# ----------------------------------------------------------------------------

# lognormal.skew

# ----------------------------------------------------------------------------

lognormal.skew<-function(mu=0,sigma=1){
 (exp(sigma^2)+2)*sqrt(exp(sigma^2)-1)
}




# ----------------------------------------------------------------------------

# lognormal.var

# ----------------------------------------------------------------------------

lognormal.var<-function(mu=0,sig.sq=1)exp(2*mu+sig.sq)*(exp(sig.sq)-1)

logrchk<-function(x,y, FUN=lplot, xout=FALSE,outfun=outpro,xlab='Smooth.Est',ylab='Logistic.Est',...){
#
# Compare  fit based on a logistic regression model  to the smooth logSM
#  FUN determines the method used.
# Example: FUN=qhomt would check for heteroscedasticity and plot the results.
#  FUN is assumed to be some function that includes the arguments xlab and ylab
#
xy=elimna(cbind(x,y))
p1=ncol(xy)
p=p1-1
x=xy[,1:p]
y=xy[,p1]
if(xout){
x<-as.matrix(x)
flag<-outfun(x,plotit=FALSE)$keep
x<-x[flag,]
y<-y[flag]
x<-as.matrix(x)
n.keep=nrow(x)
}
y1=logSMpred(x,y,x)
y2=logreg.pred(x,y,x)
a=FUN(y1,y2,xlab=xlab,ylab=ylab,...)
a
}




# ----------------------------------------------------------------------------

# logrsm

# ----------------------------------------------------------------------------

logrsm<-function(x,y,fr=1,plotit=TRUE,pyhat=FALSE,xlab="X",ylab="Y",STAND=TRUE,
xout=FALSE,outfun=outpro,LP=TRUE,...){
#
#  Do a smooth as described by Hosmer and Lemeshow, p. 85
#
#  Assuming there is only one predictor
#
# xout=T will remove outliers from among the x values and then fit
# the regression line.
#  Default: a mad-median rule is used.
#
# LP=TRUE smooth the initial smooth using LOESS
#
x<-as.matrix(x)
if(ncol(x)>1)stop("With more than one predictor, use logSM")
xy=elimna(cbind(x,y))
x=xy[,1:ncol(x)]
y=xy[,ncol(xy)]
x<-as.vector(x)
if(xout){
flag<-outfun(x,...)$keep
x<-x[flag]
y<-y[flag]
}
if(STAND)x<-(x-median(x))/mad(x)
m1<-outer(x,x,"-")^2
m2<-exp(-1*m1)*(sqrt(m1)<=fr)
m3<-matrix(y,length(y),length(y))*m2
yhat<-apply(m3,2,sum)/apply(m2,2,sum) #sum over rows for each column
if(plotit){
xor<-order(x)
plot(x,y,xlab=xlab,ylab=ylab)
if(!LP)lines(x[xor],yhat[xor])
if(LP){
Yhat=lplot(x[xor],yhat[xor],pyhat=TRUE,plotit=FALSE)$yhat.values
lines(x[xor],Yhat)
}
}
output<-"Done"
if(pyhat)output<-yhat
list(output=output)
}




# ----------------------------------------------------------------------------

# logSM

# ----------------------------------------------------------------------------

logSM<-function(x,y,pyhat=FALSE,plotit=TRUE,xlab="X",ylab="Pred.Prob",
zlab=" ",xout=FALSE,outfun=outpro,pr=TRUE,theta=50,phi=25,duplicate="error",LP=TRUE,Lspan=.75,
expand=.5,scale=TRUE,fr=2,ticktype="simple",...){
#
# A smoother designed specifically for binary outcomes
# LP=TRUE: With two independent variables, smooth the initial smooth using LOESS
#
#  fr is span
#  Lspan: when plotting the regression surface,
#  LP =TRUE
#  means  that the plot will be smoothed using LOESS
#  Lspan is the span used by LOESS
#
y=chbin2num(y)
x=as.matrix(x)
p=ncol(x)
p1=p+1
xx<-elimna(cbind(x,y))
x<-xx[,1:p]
y<-xx[,p1]
if(xout){
m<-cbind(x,y)
flag<-outfun(x,plotit=FALSE,...)$keep
m<-m[flag,]
x<-m[,1:p]
y<-m[,p1]
}
if(length(unique(y))>2)stop('y should be binary')
# Next convert y to 0 and 1s
n=length(y)
yy=rep(0,n)
y=as.vector(y)
flag=y==max(y)
yy[flag]=1
y=yy
x=as.matrix(x)
m=cov.mve(x)
flag<-(y==1)
phat<-NA
m1=matrix(NA,nrow=length(y),ncol=length(y))
for(i in 1:nrow(x))m1[,i]<-mahalanobis(x,x[i,],m$cov)
m2<-exp(-1*m1)*(sqrt(m1)<=fr)
m3<-matrix(y,length(y),length(y))*m2
phat=apply(m3,2,sum)/apply(m2,2,sum)
if(p==1){
if(plotit){
plot(x,y,xlab=xlab,ylab=ylab)
flag2<-order(x)
lines(x[flag2],phat[flag2])
}}
if(p==2){
if(plotit){
if(pr){
if(!scale)print("With dependence, suggest using scale=TRUE")
}
fitr<-phat
iout<-c(1:length(fitr))
nm1<-length(fitr)-1
for(i in 1:nm1){
ip1<-i+1
for(k in ip1:length(fitr))if(sum(x[i,]==x[k,])==2)iout[k]<-0
}
fitr<-fitr[iout>=1] # Eliminate duplicate points in the x-y plane
#                 This is necessary when doing three dimensional plots
#                 with the R function interp
if(LP){
fitr=lplot(x[iout>=1,],fitr,pyhat=TRUE,pr=FALSE,plotit=FALSE,span=Lspan)$yhat
fitr[fitr>1]=1
fitr[fitr<0]=0
}
mkeep<-x[iout>=1,]
fit<-interp(mkeep[,1],mkeep[,2],fitr,duplicate=duplicate)
persp(fit,theta=theta,phi=phi,expand=expand,zlim=c(0,1),
scale=scale,xlab=xlab,ylab=ylab,zlab=zlab,ticktype=ticktype)
}}
if(!pyhat)phat<-"Done"
phat
}

logSM.pts.CI<-function(x1,y1,x2,y2,pts=NULL,alpha=.05,method='BH',fr=2,npts=5,xlab='X',ylab='Est. Dif',xout=FALSE,SEED=TRUE,
outfun=out,plotit=TRUE,nboot=1000,...){
#
# Compare probability of success for two independent  groups given a value for some covariate. 
# A running-interval smoother is used coupled with the technique indicated by the argument  
#  bin.method. See R function binom2g
# 
# method: how to adjust the p-value to control FWE, see R function p.adjust
# default is Benjamini--Hochberg.  method='hoch' is Hochberg
#
# pts=NULL, use npts covariate values, default is 10, spread over the values in x1.
#     
# 
#
if(SEED)set.seed(2)
xy=elimna(cbind(x1,y1))
x1=xy[,1]
y1=xy[,2]
xy=elimna(cbind(x2,y2))
x2=xy[,1]
y2=xy[,2]
isub=0
if(xout){
flag<-outfun(x1,...)$keep
x1<-x1[flag]
y1<-y1[flag]
flag<-outfun(x2,...)$keep
x2<-x2[flag]
y2<-y2[flag]
}
n1=length(y1)
n2=length(y2)
xorder=order(x1)
y1<-y1[xorder]
x1<-x1[xorder]
xorder<-order(x2)
y2<-y2[xorder]
x2<-x2[xorder]
if(is.null(pts)){
n1<-1
n2<-1
N1=NA
N2=NA
vecn<-1
isub=0
for(i in 1:length(x1))n1[i]<-length(y1[near(x1,x1[i],fr1)])
for(i in 1:length(x1))n2[i]<-length(y2[near(x2,x1[i],fr2)])
for(i in 1:length(x1))vecn[i]<-min(n1[i],n2[i])
sub<-c(1:length(x1))
isub[1]<-min(sub[vecn>=nmin])
isub[2]<-max(sub[vecn>=nmin])
bot=x1[isub[1]]
top=x1[isub[2]]
pts=seq(bot,top,length.out=npts)
n1=length(y1)
n2=length(y2)
}
npts=length(pts)
bmat1=matrix(NA,nboot,npts)
bmat2=matrix(NA,nboot,npts)

for(j in 1:nboot){
n1sub=sample(n1,replace=TRUE)
n2sub=sample(n2,replace=TRUE)

bmat1[j,]=logSMpred(x1[n1sub],y1[n1sub],pts=pts,fr=fr)
bmat2[j,]=logSMpred(x2[n2sub],y2[n2sub],pts=pts,fr=fr)

}
dif=bmat1-bmat2
dif=elimna(dif)
nboot=nrow(dif)
pv=NA
for(k in 1:npts){
dif[,k]=sort(dif[,k])
pv[k]=mean(dif[,k]<0)+.5*mean(dif[,k]==0)
pv[k]=2*min(pv[k],1-pv[k])
}
crit<-alpha/2
icl<-round(crit*nboot)+1
icu<-nboot-icl
ci.low=dif[icl,]
ci.up=dif[icu,]
output=matrix(NA,npts,8)
dimnames(output)=list(NULL,c('pts','p1','p2','est.dif','ci.lower','ci.upper','p.value','p.adjusted'))
output[,1]=pts
output[,2]=logSMpred(x1,y1,pts=pts,fr=fr)
output[,3]=logSMpred(x2,y2,pts=pts,fr=fr)
output[,4]=output[,2]-output[,3]
output[,5]=ci.low
output[,6]=ci.up
output[,7]=pv
output[,8]=p.adjust(pv,method=method)
if(plotit){
plot(c(pts,pts,pts),c(output[,4],output[,5],output[,6]),type='n',xlab=xlab,ylab=ylab,ylim=c(-1,1))
points(pts,output[,4])
points(pts,output[,5],pch='+')
points(pts,output[,6],pch='+')
}
output
}


ancovampG.QSCE<-
function(x1,y1,x2,y2,fr1=1,fr2=1,method='hoch',npts=10,nboot=500,locfun=median,TUKEY=FALSE,
alpha=.05, pts=NULL,SEED=TRUE,cov.fun=skip.cov,
pr=FALSE,q=.5,plotit=FALSE,LP=FALSE,theta=50,xlab=' X1',ylab='X2 ',...){
#
#  ANCOVA:
#
#  This function generalizes the R function ancovamp
#  where the goal is to use the quantile shift measure of effect size 
# when there  is a control group and an  covariate.
#
# No parametric assumption is made about the form of
# the regression surface--a running interval smoother is used.
# Design points are chosen based on depth of points in x1 if pts=NULL
#  Assume data are in x1 y1 x2 and y2, can have more than one covariate
#
#  pts: a matrix of design points at which groups are compared
# If null, pick points based on depth
#
#  If plotit=TRUE, instead create a scatterplot of the points used in pts, the covariate values
#  and mark the significant ones with *
#
#  SEED=TRUE sets the seed for the random number generator
#       so that same result is always returned when
#        using a bootstrap method or when using cov.mve or cov.mcd
#
#   cov.fun: returns covariance matrix in $cov (e.g.
#   skipcov does not return it in $cov, but skip.cov does. So cov.mve could be used)
#
#   Returns:
#   designs points where comparisons were made.
#   n's used, p-values
#   adjusted p-values based on  a method specified by the argument 
#    method    Default is Hochberg's method for controlling FWE.
#  
#
#
x1=as.matrix(x1)
p=ncol(x1)
p1=p+1
m1=elimna(cbind(x1,y1))
x1=m1[,1:p]
y1=m1[,p1]
x2=as.matrix(x2)
p=ncol(x2)
p1=p+1
m2=elimna(cbind(x2,y2))
x2=m2[,1:p]
y2=m2[,p1]
N1=length(y1)
N2=length(y2)
#
if(N1>N2){
remy2=y2
remx2=x2
x2=x1
y2=y1
x1=remx2
y1=remy2
N1=length(y1)
N2=length(y2)
}
if(is.null(pts[1])){
x1<-as.matrix(x1)
if(TUKEY)d=fdepth(x1,plotit=FALSE)
else d=pdis(x1)
U=round(N1/npts)
id=seq(1,N1,U)
ior=order(d)
pts=x1[ior[id],]
}
pts<-as.matrix(pts)
n1<-1
n2<-1
vecn<-1
mval1<-cov.fun(x1)
mval2<-cov.fun(x2)
for(i in 1:nrow(pts)){
n1[i]<-length(y1[near3d(x1,pts[i,],fr1,mval1)])
n2[i]<-length(y2[near3d(x2,pts[i,],fr2,mval2)])
}
flag<-rep(TRUE,nrow(pts))
for(i in 1:nrow(pts))if(n1[i]<10 || n2[i]<10)flag[i]<-FALSE
flag=as.logical(flag)
pts<-pts[flag,]
if(sum(flag)==1)pts<-t(as.matrix(pts))
dd=NULL
if(sum(flag)==0){
print('No comparable design points found, might increase span.')
pts=NULL
mat=NULL
dd=NULL
}
pts=unique(pts)
if(sum(flag)>0){
mat<-matrix(NA,nrow(pts),8)
mat[,5]=0
dimnames(mat)<-list(NULL,c('n1','n2','p.value','Adj.p.value','Sig','Effect.size','ci.low','ci.hi'))
for (i in 1:nrow(pts)){
g1<-y1[near3d(x1,pts[i,],fr1,mval1)]
g2<-y2[near3d(x2,pts[i,],fr2,mval2)]
g1<-g1[!is.na(g1)]
g2<-g2[!is.na(g2)]
temp=qshiftpb(g1,g2,locfun=locfun,alpha=alpha,SEED=SEED,nboot=nboot)
mat[i,3]=temp$p.value
mat[i,1]<-length(g1)
mat[i,2]<-length(g2)
mat[i,6]=temp$est
if(length(g1)<=5)print(paste('Warning, there are',length(g1),' points corresponding to the design point X=',pts[i,]))
if(length(g2)<=5)print(paste('Warning, there are',length(g2),' points corresponding to the design point X=',pts[i,]))
mat[i,7]=temp$ci.low
mat[i,8]=temp$ci.up
}
mat[,4]=p.adjust(mat[,3],method=method)
flag=mat[,4]<=alpha
if(sum(flag)>0)mat[flag,5]=1
}
if(plotit){
plot(pts,xlab=xlab,ylab=ylab)
id=which(mat[,4]<=alpha)
if(sum(flag)==1)points(t(as.matrix(pts[id,])),pch='*')
if(sum(flag)>1)points(pts[id,],pch='*')
}
list(points=pts,results=mat,num.sig=sum(flag))
}

ancovampG.QS<-
function(x1,y1,x2,y2,fr1=1,fr2=1, locfun=median,method='hoch',npts=10,nboot=500,TUKEY=FALSE,
alpha=.05, pts=NULL,SEED=TRUE,cov.fun=skip.cov,
pr=FALSE,q=.5,plotit=FALSE,xlab=' X1',ylab='X2 ',...){
#
#  ANCOVA:
#
#  This function generalizes the R function ancovamp
#  where the goal is to use the  quantile shift (no control group)  measure of effect size
# When there is a control group, can use ancovampG.QSCE
#
# No parametric assumption is made about the form of
# the regression surface--a running interval smoother is used.
# Design points are chosen based on depth of points in x1 if pts=NULL
#  Assume data are in x1 y1 x2 and y2, can have more than one covariate
#
#  pts: a matrix of design points at which groups are compared, If NULL, design points are
# chosen automatically.

#  If plotit=TRUE, instead create a scatterplot of the points used in pts, the covariate values
#  and mark the significant ones with *
#
#  SEED=TRUE sets the seed for the random number generator
#       so that same result is always returned when
#        using a bootstrap method or when using cov.mve or cov.mcd
#
#   cov.fun: returns covariance matrix in $cov (e.g.
#   skipcov does not return it in $cov, but skip.cov does. So cov.mve could be used)
#
#   Returns:
#   designs points where comparisons were made.
#   n's used, p-values
#   adjusted p-values based on  a method specified by the argument 
#    method    Default is Benjaminn-Hochberg's method for controlling FWE
#  
#
#
x1=as.matrix(x1)
p=ncol(x1)
p1=p+1
m1=elimna(cbind(x1,y1))
x1=m1[,1:p]
y1=m1[,p1]
x2=as.matrix(x2)
p=ncol(x2)
p1=p+1
m2=elimna(cbind(x2,y2))
x2=m2[,1:p]
y2=m2[,p1]
N1=length(y1)
N2=length(y2)
#
#
if(N1>N2){
remy2=y2
remx2=x2
x2=x1
y2=y1
x1=remx2
y1=remy2
N1=length(y1)
N2=length(y2)
}
if(is.null(pts[1])){
x1<-as.matrix(x1)
X1=rbind(x1,x2)
if(TUKEY)d=fdepth(x1,plotit=FALSE)
else d=pdis(x1)

U=round(N1/npts)
id=seq(1,N1,U)
ior=order(d)
pts=X1[ior[id],]
}
pts<-as.matrix(pts)

n1<-1
n2<-1
vecn<-1
mval1<-cov.fun(x1)
mval2<-cov.fun(x2)
for(i in 1:nrow(pts)){
n1[i]<-length(y1[near3d(x1,pts[i,],fr1,mval1)])
n2[i]<-length(y2[near3d(x2,pts[i,],fr2,mval2)])
}
flag<-rep(TRUE,nrow(pts))
for(i in 1:nrow(pts))if(n1[i]<10 || n2[i]<10)flag[i]<-FALSE
flag=as.logical(flag)
pts<-pts[flag,]
if(sum(flag)==1)pts<-t(as.matrix(pts))
dd=NULL
if(sum(flag)==0){
print('No comparable design points found, might increase span.')
pts=NULL
mat=NULL
dd=NULL
}
pts=unique(pts)
if(sum(flag)>0){
mat<-matrix(NA,nrow(pts),8)
mat[,5]=0
dimnames(mat)<-list(NULL,c('n1','n2','p.value','Adj.p.value','Sig','Effect.size','ci.low','ci.hi'))
output=list()
for (i in 1:nrow(pts)){
g1<-y1[near3d(x1,pts[i,],fr1,mval1)]
g2<-y2[near3d(x2,pts[i,],fr2,mval2)]
g1<-g1[!is.na(g1)]
g2<-g2[!is.na(g2)]
temp=QSci(g1,g2,locfun=locfun,,alpha=alpha,SEED=SEED,nboot=nboot)
mat[i,3]=temp$p.value
output[[i]]=temp
mat[i,1]<-length(g1)
mat[i,2]<-length(g2)
mat[i,6]=temp$effect.size
if(length(g1)<=5)print(paste('Warning, there are',length(g1),' points corresponding to the design point X=',pts[i,]))
if(length(g2)<=5)print(paste('Warning, there are',length(g2),' points corresponding to the design point X=',pts[i,]))
mat[i,7]=temp$ci[1]
mat[i,8]=temp$ci[2]
}
npt=nrow(pts)
mat[,4]=p.adjust(mat[,3],method=method)
flag=mat[,4]<=alpha
if(sum(flag)>0)mat[flag,5]=1
}
if(plotit){
plot(pts,xlab=xlab,ylab=ylab)
id=which(mat[,4]<=alpha)
if(sum(flag)==1)points(t(as.matrix(pts[id,])),pch='*')
if(sum(flag)>1)points(pts[id,],pch='*')
}
list(points=pts,results=mat,num.sig=sum(flag))
}
ancovampG.KMS<-
function(x1,y1,x2,y2,fr1=1,fr2=1, tr=.2,method='hoch',npts=10,nboot=500,TUKEY=FALSE,
alpha=.05, pts=NULL,SEED=TRUE,cov.fun=skip.cov,
pr=FALSE,q=.5,plotit=FALSE,xlab=' X1',ylab='X2 ',...){
#
#  ANCOVA:
#
#  This function generalizes the R function ancovamp
#  where the goal is to use the KMS measure of effect size
#
# No parametric assumption is made about the form of
# the regression surface--a running interval smoother is used.
# Design points are chosen based on depth of points in x1 if pts=NULL
#  Assume data are in x1 y1 x2 and y2, can have more than one covariate
#
#  pts: a matrix of design points at which groups are compared
#
#  If plotit=TRUE, instead create a scatterplot of the points used in pts, the covariate values
#  and mark the significant ones with *
#
#  theta can be use to rotate the plot.
#
#  SEED=TRUE sets the seed for the random number generator
#       so that same result is always returned when
#        using a bootstrap method or when using cov.mve or cov.mcd
#
#   cov.fun: returns covariance matrix in $cov (e.g.
#   skipcov does not return it in $cov, but skip.cov does. So cov.mve could be used)
#
#   Returns:
#   designs points where comparisons were made.
#   n's used, p-values
#   adjusted p-values based on  a method specified by the argument 
#    method    Default is Benjaminn-Hochberg's method for controlling FWE
#  
#
#
x1=as.matrix(x1)
p=ncol(x1)
p1=p+1
m1=elimna(cbind(x1,y1))
x1=m1[,1:p]
y1=m1[,p1]
x2=as.matrix(x2)
p=ncol(x2)
p1=p+1
m2=elimna(cbind(x2,y2))
x2=m2[,1:p]
y2=m2[,p1]
N1=length(y1)
N2=length(y2)
#
if(N1>N2){
remy2=y2
remx2=x2
x2=x1
y2=y1
x1=remx2
y1=remy2
N1=length(y1)
N2=length(y2)
}
#
if(is.null(pts[1])){
x1<-as.matrix(x1)
if(TUKEY)d=fdepth(x1,plotit=FALSE)
else d=pdis(x1)
U=round(N1/npts)
id=seq(1,N1,U)
ior=order(d)
pts=x1[ior[id],]
}
pts<-as.matrix(pts)
n1<-1
n2<-1
vecn<-1
mval1<-cov.fun(x1)
mval2<-cov.fun(x2)
for(i in 1:nrow(pts)){
n1[i]<-length(y1[near3d(x1,pts[i,],fr1,mval1)])
n2[i]<-length(y2[near3d(x2,pts[i,],fr2,mval2)])
}
flag<-rep(TRUE,nrow(pts))
for(i in 1:nrow(pts))if(n1[i]<10 || n2[i]<10)flag[i]<-FALSE
flag=as.logical(flag)
pts<-pts[flag,]
if(sum(flag)==1)pts<-t(as.matrix(pts))
dd=NULL
if(sum(flag)==0){
print('No appropriate design points found, might increase span.')
pts=NULL
mat=NULL
dd=NULL
}
pts=unique(pts)
if(sum(flag)>0){
mat<-matrix(NA,nrow(pts),10)
mat[,5]=0
dimnames(mat)<-list(NULL,c('n1','n2','p.value','Adj.p.value','Sig','Effect.size','ci.low','ci.up','Est1','Est2'))
output=list()
for (i in 1:nrow(pts)){
g1<-y1[near3d(x1,pts[i,],fr1,mval1)]
g2<-y2[near3d(x2,pts[i,],fr2,mval2)]
g1<-g1[!is.na(g1)]
g2<-g2[!is.na(g2)]
temp=KMS.ci(g1,g2,tr=tr,alpha=alpha,SEED=SEED,nboot=nboot)
mat[i,3]=temp$p.value
output[[i]]=temp
mat[i,1]<-length(g1)
mat[i,2]<-length(g2)
mat[i,6]=temp$effect.size
mat[i,7]=temp$ci[1]
mat[i,8]=temp$ci[2]
mat[i,9]=mean(g1,tr=tr)
mat[i,10]=mean(g2,tr=tr)
if(length(g1)<=5)print(paste('Warning, there are',length(g1),' points corresponding to the design point X=',pts[i,]))
if(length(g2)<=5)print(paste('Warning, there are',length(g2),' points corresponding to the design point X=',pts[i,]))
}
mat[,4]=p.adjust(mat[,3],method=method)
flag=mat[,4]<=alpha
if(sum(flag)>0)mat[flag,5]=1
}
if(plotit){
plot(pts,xlab=xlab,ylab=ylab)
id=which(mat[,4]<=alpha)
if(sum(flag)==1)points(t(as.matrix(pts[id,])),pch='*')
if(sum(flag)>1)points(pts[id,],pch='*')
}
list(points=pts,results=mat,num.sig=sum(flag))
}




# ----------------------------------------------------------------------------

# logSM2g

# ----------------------------------------------------------------------------

logSM2g<-function(x1,y1,x2,y2,fr=2,xout=FALSE,outfun=outpro,xlab='X',ylab='Y'){
#
#  plot smooth for binary data for each of two groups
#
xord1=order(x1)
xord2=order(x2)
p1=logSMpred(x1[xord1],y1[xord1],x1[xord1],fr=fr,xout=xout,outfun=outfun)
p2=logSMpred(x2[xord2],y2[xord2],x2[xord2],fr=fr,xout=xout,outfun=outfun)
plot(c(x1,x2),c(p1,p2),type='n',ylim=c(0,1),xlab=xlab,ylab=ylab)
lines(x1[xord1],p1)
lines(x2[xord2],p2,lty=2)
}




# ----------------------------------------------------------------------------

# logSMpred

# ----------------------------------------------------------------------------

logSMpred<-function(x,y,pts=NULL,fr=2,LP=TRUE,xout=FALSE,outfun=outpro,...){
#
# A smoother designed specifically for binary outcomes
# LP=TRUE: With two independent variables, smooth the initial smooth using LOESS
# Return predicted probability of 1 for points in
# pts
# based on the data in x and y
#
#
x=as.matrix(x)
p=ncol(x)
p1=p+1
xx<-elimna(cbind(x,y))
x<-xx[,1:p]
y<-xx[,p1]

n=nrow(xx)
yy=rep(1,n)
vals=sort(unique(y))
if(length(vals)>2)stop('y should be binary')
flag=y==vals[2]
yy[!flag]=0
y=yy

if(xout){
m<-cbind(x,y)
flag<-outfun(x,plotit=FALSE,...)$keep
m<-m[flag,]
x<-m[,1:p]
y<-m[,p1]
}
x=as.matrix(x)
if(is.null(pts))pts=x
pts=as.matrix(pts)
m=covmve(x)
phat<-NA
m1=matrix(NA,nrow=nrow(pts),ncol=nrow(pts))
yhat=NULL
for(i in 1:nrow(pts)){
d<-mahalanobis(x,pts[i,],m$cov)
flag=sqrt(d)<=fr
w=flag*exp(-1*d)
yhat[i]=sum(w*y)/sum(w)
}
yhat
}




# ----------------------------------------------------------------------------

# long2g

# ----------------------------------------------------------------------------

long2g<-function(x,x.col,y.col,s.id,grp.id,regfun=tsreg,MAR=TRUE,tr=.2){
#
# x is a matrix or data frame.
#
# Longitudinal data, compare two groups, where the groups correspond to two
# values in column
# grp.id.
# The outcome (dependent) variable is assumed to be stored in
# the column indicated by the argument  y.col.
# Example, y.col=3 means the outcome variable of interest is in col. 3
# Predictors are stored in columns  indicated by
# x.col.
# s.id indicates column where subject's id is stored.
#
# Assuming data are stored as for example in the R variable
# Orthodont,
# which can be accessed via the command  library(nlme)
#
m=matsplit(x,grp.id)
g1=longreg(m$m1,x.col,y.col,s.id,regfun)$est.S
g2=longreg(m$m2,x.col,y.col,s.id,regfun)$est.S
res=list()
if(MAR){
for(iv in 1:ncol(g1))res[[iv]]=yuen(g1[,iv],g2[,iv],tr=tr)
}
if(!MAR)res=smean2(g1,g2)
res
}

long2mat<-function(x,Sid.col,dep.col){
#
# Have data in a matrix or data frame, x
# Sid.col indicates Subject's id
# Here, each subject has one or more rows of data
#
# Goal: store the data in a data frame where
# each row contains all of the data for an individual
# subject.
#
# dep.col indicates column of the outcome (dependent) variable
# This version assumed a single column of outcome values are to be
# rearranged.
#
if(length(dep.col)!=1)stop("Argument dep.col must have a single value")
if(is.null(dim(x)))stop("x must be a matrix or data frame")
Sid=unique(x[,Sid.col])
n=nrow(x)
nid=length(Sid)
flag=(x[,Sid.col]==Sid[1])
num.out=sum(flag)
res=matrix(NA,nrow=nid,ncol=num.out)
for(i in 1:nid){
flag=(x[,Sid.col]==Sid[i])
res[i,]=x[flag,dep.col]
}
res
}




# ----------------------------------------------------------------------------

# lpindt

# ----------------------------------------------------------------------------

lpindt<-function(x,y,nboot=500,xout=FALSE,outfun=out){
#
# Test the hypothesis of no association based on the fit obtained
# from lplot (Cleveland's LOESS)
#
m<-elimna(cbind(x,y))
x<-as.matrix(x)
p<-ncol(x)
pp<-p+1
x<-m[,1:p]
y<-m[,pp]
if(xout){
m<-cbind(x,y)
flag<-outfun(x,plotit=FALSE)$keep
m<-m[flag,]
x<-m[,1:p]
y<-m[,pp]
}
n=length(y)
data1<-matrix(sample(n,size=n*nboot,replace=TRUE),nrow=nboot)
data2<-matrix(sample(n,size=n*nboot,replace=TRUE),nrow=nboot)
val=NA
x=as.matrix(x)
for(i in 1:nboot){
val[i]=lplot(x[data1[i,],],y[data2[i,]],plotit=FALSE,pr=FALSE)$Strength.Assoc
}
val=sort(val)
est=lplot(x,y,plotit=FALSE,pr=FALSE)$Strength.Assoc
p.value=mean((est<val))
list(Explanatory.power=est,p.value=p.value)
}
lrdata <- function(n = 200,  type = 3)
{
# Generates data for logistic regression.
# If X|y=1 ~ N(mu_1,I) and X|Y=0 ~ N(0,I) then beta = mu_1 and alpha = -0.5 ||mu_1||^2.
#
#   If q is changed, change the formula in the glm statement.
	q <- 5
	y <- 0 * 1:n
	y[(n/2 + 1):n] <- y[(n/2 + 1):n] + 1
	beta <- 0 * 1:q
        if(type == 1) {
		beta[1] <- 1
                alpha <- -0.5
	}
	if(type == 2) {
		beta <- beta + 1
                alpha <- -q/2
	}
        if(type == 3) {
		beta[1:3] <-  1
                alpha <- -1.5
	}
        x <- matrix(rnorm(n * q), nrow = n,
			ncol = q)
	if(type == 1) {
		x[(n/2 + 1):n, 1] <- x[(n/2 + 1
				):n, 1] + 1
		}
	if(type == 2) {
		x[(n/2 + 1):n,  ] <- x[(n/2 + 1
				):n,  ] + 1
		}
        if(type == 3) {
		x[(n/2 + 1):n, 1:3 ] <- x[(n/2 + 1
				):n, 1:3 ] + 1
		}
        #X|y=0 ~ N(0, I) and X|y=1 ~ N(beta,I)
	# change formula to x[,1]+ ... + x[,q] with q
	out <- glm(y ~ x[, 1] + x[, 2] + x[, 3] +
			x[, 4] + x[,5], family = binomial)
	list(alpha = alpha, beta = beta, lrcoef = out$coef,x=x,y=y)
}




# ----------------------------------------------------------------------------

# lressp

# ----------------------------------------------------------------------------

lressp <- function(x,y,slices=10)
{
# Makes the ESSP for logistic regression.
# If X|y=1 ~ N(mu_1,I) and X|Y=0 ~ N(0,I) then beta = mu_1 and alpha = ||mu_1||^2.
# Workstation need to activate a graphics
# device with command "X11()" or "motif()."
# R needs command "library(lqs)."
# Advance the view with the right mouse button.
# In R, highlight "stop."
#
#   If q is changed, change the formula in the glm statement.
	q <- 5
# change formula to x[,1]+ ... + x[,q] with q
	out <- glm(y ~ x[, 1] + x[, 2] + x[, 3] +
			x[, 4] + x[,5], family = binomial)
	ESP <- x %*% out$coef[-1] + out$coef[1]
        Y <- y
        plot(ESP,Y)
        abline(mean(y),0)
        fit <- y
        fit <- exp(ESP)/(1 + exp(ESP))
      #  lines(sort(ESP),sort(fit))
        indx <- sort.list(ESP)
        lines(ESP[indx],fit[indx])
        fit2 <- fit
        n <- length(y)
        val <- as.integer(n/slices)
        for(i in 1: (slices-1)){
          fit2[((i-1)*val+1):(i*val)] <- mean(y[indx[((i-1)*val+1):(i*val)]])
        }
        fit2[((slices-1)*val+1):n] <- mean(y[indx[((slices-1)*val+1):n]])
# fit2 is already sorted in order corresponding to indx
        lines(ESP[indx],fit2)
#list(fit2=fit2,n=n,slices=slices,val=val)
                }


lsviews<-
function(x, Y, ii = 1)
{
# This function is the same as tvreg except that the untrimmed
# cases are highlighted. It compares the LS fits for 90, 80,
# ..., 0 percent trimming. Used to visualize g if y = g(beta^T x,e).
# Workstation: activate a graphics
# device with command "X11()" or "motif()."
# R needs command "library(lqs)."
# Advance the view with the right mouse button.
# In R, highlight ``stop."
	x <- as.matrix(x)
	out <- cov.mcd(x)
	center <- out$center
	cov <- out$cov
	rd2 <- mahalanobis(x, center, cov)
	labs <- c("90%", "80%", "70%", "60%", "50%", "40%", "30%", "20%", "10%",
		"0%")
	tem <- seq(0.1, 1, 0.1)
	for(i in ii:10) {
		val <- quantile(rd2, tem[i])
		bhat <- lsfit(x[rd2 <= val,  ], Y[rd2 <= val])$coef
		ESP <- bhat[1] + x %*% bhat[-1]
		plot(ESP, Y)
		points(ESP[rd2 <= val], Y[rd2 <= val], pch = 15, cex = 1.4)
		abline(0, 1)
		title(labs[i])
		identify(ESP, Y)
		print(bhat)
	}
}

maha<-
function(x)
{
# Generates the classical mahalanobis distances.
	center <- apply(x, 2, mean)
	cov <- var(x)
	return(sqrt(mahalanobis(x, center, cov)))
}

mbalata<-
function(x, y, k=6, nsamp = 7)
{
#gets the median ball fit with 7 centers, med resid crit, 7 ball sizes
        x <- as.matrix(x)
	n <- dim(x)[1]
	q <- dim(x)[2]
	# q + 1 is number of predictors including intercept
	vals <- c(q + 3 + floor(n/100), q + 3 + floor(n/40), q + 3 +
		floor(n/20), q + 3 + floor(n/10), q + 3 + floor(n/5), q +
		3 + floor(n/3), q + 3 + floor(n/2))
	covv <- diag(q)
	centers <- sample(n, nsamp)
	temp <- lsfit(x, y)
	mbaf <- temp$coef	## get LATA criterion
	res <- temp$residuals
	crit <- k^2*median(res^2)
	cn <- sum(res^2 <= crit)
	absres <- sort(abs(res))
	critf <- sum(absres[1:cn])	##
	for(i in 1:nsamp) {
		md2 <- mahalanobis(x, center = x[centers[i],  ], covv)
		smd2 <- sort(md2)
		for(j in 1:7) {
			temp <- lsfit(x[md2 <= smd2[vals[j]],  ], y[md2 <=
				smd2[vals[j]]])
	#Use OLS on rows with md2 <= cutoff = smd2[vals[j]]
			res <- y - temp$coef[1] - x %*% temp$coef[-1]
	## get LATA criterion
			crit <- k^2*median(res^2)
			cn <- sum(res^2 <= crit)
			absres <- sort(abs(res))
			crit <- sum(absres[1:cn])	##
			if(crit < critf) {
				critf <- crit
				mbaf <- temp$coef
			}
		}
	}
	list(coef = mbaf, critf = critf)
}

mbamv<-
function(x, y, nsamp = 7)
{
# This function is for simple linear regression. The
# highlighted boxes get weight 1. Click on right
# mouse button to advance plot. Only uses 50% trimming.
        x <- as.matrix(x)
	n <- dim(x)[1]
	q <- dim(x)[2]
	covv <- diag(q)
	centers <- sample(n, nsamp)
	for(i in 1:nsamp) {
		md2 <- mahalanobis(x, center = x[centers[i],  ], covv)
		med <- median(md2)
		plot(x, y)
		points(x[md2 < med], y[md2 < med], pch = 15)
                abline(lsfit(x[md2 < med],y[md2 < med]))
		identify(x, y)
	}
}

mbamv2<-
function(x, Y, nsamp = 7)
{
# This function is for multiple linear regression. The
# highlighted boxes get weight 1. Click on right
# mouse button to advance plot. Only uses 50% trimming.
        x <- as.matrix(x)
       	n <- dim(x)[1]
	q <- dim(x)[2]
	covv <- diag(q)
	centers <- sample(n, nsamp)
	for(i in 1:nsamp) {
		md2 <- mahalanobis(x, center = x[centers[i],  ], covv)
		med <- median(md2)
                if(q ==1){out <- lsfit(x[md2 < med],Y[md2 < med])}
                else{out <- lsfit(x[md2 < med,],Y[md2 < med])}
                FIT <- out$coef[1] + x%*%out$coef[-1]
		RES <- Y - FIT
                par(mfrow=c(2,1))
                plot(FIT,Y)
		points(FIT[md2 < med], Y[md2 < med], pch = 15)
                abline(0,1)
		identify(FIT, Y)
                plot(FIT,RES)
                points(FIT[md2 < med], RES[md2 < med], pch = 15)
                abline(0,0)
		identify(FIT, RES)
	}
}

mbareg<-
function(x, y, nsamp = 7)
{
#gets the mbareg fit with 7 centers, med resid crit, 7 ball sizes
        x <- as.matrix(x)
	n <- dim(x)[1]
	q <- dim(x)[2]	# q + 1 is number of predictors including intercept
	vals <- c(q + 3 + floor(n/100), q + 3 + floor(n/40), q + 3 + floor(n/20
		), q + 3 + floor(n/10), q + 3 + floor(n/5), q + 3 + floor(n/3),
		q + 3 + floor(n/2))
	covv <- diag(q)
	centers <- sample(n, nsamp)
	temp <- lsfit(x, y)
	mbaf <- temp$coef
	critf <- median(temp$residuals^2)
	for(i in 1:nsamp) {
		md2 <- mahalanobis(x, center = x[centers[i],  ], covv)
		smd2 <- sort(md2)
		for(j in 1:7) {
			temp <- lsfit(x[md2 <= smd2[vals[j]],  ], y[md2 <= smd2[
				vals[j]]])
	#Use OLS on rows with md2 <= cutoff = smd2[vals[j]]
			res <- y - temp$coef[1] - x %*% temp$coef[-1]
			crit <- median(res^2)
			if(crit < critf) {
				critf <- crit
				mbaf <- temp$coef
			}
		}
	}
	list(coef = mbaf, critf = critf)
}

med2ci<-
function(x, cc = 4, alpha = 0.05)
{
#gets ~ 50% trimmed mean se for sample median and the corresponding robust  100 (1-alpha)% CI
#defaults are alpha = .05, cc = 5 may be better than the default
	up <- 1 - alpha/2
	n <- length(x)
	med <- median(x)
	ln <- floor(n/2) - ceiling(sqrt(n/cc))
	un <- n - ln
	low <- ln + 1
	d <- sort(x)
	if(ln > 0) {
		d[1:ln] <- d[(low)]
		d[(un + 1):n] <- d[un]
	}
	den <- ((un - ln)/n)^2
	swv <- var(d)/den
	#got the scaled Winsorized variance
	rdf <- un - low
	rval <- qt(up, rdf) * sqrt(swv/n)
	rlo <- med - rval
	rhi <- med + rval
	list(int = c(rlo, rhi), med = med, swv = swv)
}

medci<-
function(x, alpha = 0.05)
{
#gets Bloch and Gastwirth SE for sample median and the corresponding resistant  100 (1-alpha)% CI
#defaults are alpha = .05
	n <- length(x)
	up <- 1 - alpha/2
	med <- median(x)
	ln <- floor(n/2) - ceiling(sqrt(n/4))
	un <- n - ln
	d <- sort(x)
	rdf <- un - ln - 1
	cut <- qt(up, rdf)
	sebg <- 0.5 * (d[un] - d[ln + 1])
	rval <- cut * sebg
	rlo <- med - rval
	rhi <- med + rval
	list(int = c(rlo, rhi), med = med, sebg = sebg)
}
lsa.linear<-function(x,y){
       require(lars)

       ## Least square approximation. This version Oct 19, 2006
       ## Reference Wang, H. and Leng, C. (2006) and Efron et al. (2004).
       ##
       ## Written by Chenlei Leng
       ##
       ## Input
       ## obj: lm/glm/coxph or other object
       ##
       ## Output
       ## beta.ols: the MLE estimate
       ## beta.bic: the LSA-BIC estimate
       ## beta.aic: the LSA-AIC estimate

       lsa <- function(obj)
       {
              intercept <- attr(obj$terms,'intercept')
              if(class(obj)[1]=='coxph') intercept <- 0
              n <- length(obj$residuals)
              Sigma <- vcov(obj)
              SI <- solve(Sigma)

              beta.ols <- coef(obj)

              l.fit <- lars.lsa(SI, beta.ols, intercept, n)

              t1 <- sort(l.fit$BIC, ind=T)

              t2 <- sort(l.fit$AIC, ind=T)

              beta <- l.fit$beta

              if(intercept) {
                     beta0 <- l.fit$beta0+beta.ols[1]
                     beta.bic <- c(beta0[t1$ix[1]],beta[t1$ix[1],])
                     beta.aic <- c(beta0[t2$ix[1]],beta[t2$ix[1],])
              }

              else {
                     beta0 <- l.fit$beta0
                     beta.bic <- beta[t1$ix[1],]
                     beta.aic <- beta[t2$ix[1],]
              }



              obj <- list(beta.ols=beta.ols, beta.bic=beta.bic,
                          beta.aic = beta.aic)
              obj
       }

       ###################################
       ## lars variant for LSA
       lars.lsa <- function (Sigma0, b0, intercept,  n,
                             type = c("lasso", "lar"),
                             eps = .Machine$double.eps,max.steps)
       {
              type <- match.arg(type)
              TYPE <- switch(type, lasso = "LASSO", lar = "LAR")

              n1 <- dim(Sigma0)[1]

              ## handle intercept
              if (intercept) {
                     a11 <- Sigma0[1,1]
                     a12 <- Sigma0[2:n1,1]
                     a22 <- Sigma0[2:n1,2:n1]
                     Sigma <- a22-outer(a12,a12)/a11
                     b <- b0[2:n1]
                     beta0 <- crossprod(a12,b)/a11
              }

              else {
                     Sigma <- Sigma0
                     b <- b0
              }

              Sigma <- diag(abs(b))%*%Sigma%*%diag(abs(b))
              b <- sign(b)

              nm <- dim(Sigma)
              m <- nm[2]
              im <- inactive <- seq(m)

              Cvec <- drop(t(b)%*%Sigma)
              ssy <- sum(Cvec*b)
              if (missing(max.steps))
                     max.steps <- 8 * m
              beta <- matrix(0, max.steps + 1, m)
              Gamrat <- NULL
              arc.length <- NULL
              R2 <- 1
              RSS <- ssy
              first.in <- integer(m)
              active <- NULL
              actions <- as.list(seq(max.steps))
              drops <- FALSE
              Sign <- NULL
              R <- NULL
              k <- 0
              ignores <- NULL

              while ((k < max.steps) & (length(active) < m)) {
                     action <- NULL
                     k <- k + 1
                     C <- Cvec[inactive]
                     Cmax <- max(abs(C))
                     if (!any(drops)) {
                            new <- abs(C) >= Cmax - eps
                            C <- C[!new]
                            new <- inactive[new]
                            for (inew in new) {
                                   R <- updateR(Sigma[inew, inew], R, drop(Sigma[inew, active]),
                                                Gram = TRUE,eps=eps)
                                   if(attr(R, "rank") == length(active)) {
                                          ##singularity; back out
                                          nR <- seq(length(active))
                                          R <- R[nR, nR, drop = FALSE]
                                          attr(R, "rank") <- length(active)
                                          ignores <- c(ignores, inew)
                                          action <- c(action,  - inew)
                                   }
                                   else {
                                          if(first.in[inew] == 0)
                                                 first.in[inew] <- k
                                          active <- c(active, inew)
                                          Sign <- c(Sign, sign(Cvec[inew]))
                                          action <- c(action, inew)
                                   }
                            }
                     }
                     else action <- -dropid
                     Gi1 <- backsolve(R, backsolvet(R, Sign))
                     dropouts <- NULL
                     A <- 1/sqrt(sum(Gi1 * Sign))
                     w <- A * Gi1
                     if (length(active) >= m) {
                            gamhat <- Cmax/A
                     }
                     else {
                            a <- drop(w %*% Sigma[active, -c(active,ignores), drop = FALSE])
                            gam <- c((Cmax - C)/(A - a), (Cmax + C)/(A + a))
                            gamhat <- min(gam[gam > eps], Cmax/A)
                     }
                     if (type == "lasso") {
                            dropid <- NULL
                            b1 <- beta[k, active]
                            z1 <- -b1/w
                            zmin <- min(z1[z1 > eps], gamhat)
                            # cat('zmin ',zmin, ' gamhat ',gamhat,'\n')
                            if (zmin < gamhat) {
                                   gamhat <- zmin
                                   drops <- z1 == zmin
                            }
                            else drops <- FALSE
                     }
                     beta[k + 1, ] <- beta[k, ]
                     beta[k + 1, active] <- beta[k + 1, active] + gamhat * w

                     Cvec <- Cvec - gamhat * Sigma[, active, drop = FALSE] %*% w
                     Gamrat <- c(Gamrat, gamhat/(Cmax/A))

                     arc.length <- c(arc.length, gamhat)
                     if (type == "lasso" && any(drops)) {
                            dropid <- seq(drops)[drops]
                            for (id in rev(dropid)) {
                                   R <- downdateR(R,id)
                            }
                            dropid <- active[drops]
                            beta[k + 1, dropid] <- 0
                            active <- active[!drops]
                            Sign <- Sign[!drops]
                     }

                     actions[[k]] <- action
                     inactive <- im[-c(active)]
              }
              beta <- beta[seq(k + 1), ]

              dff <- b-t(beta)

              RSS <- diag(t(dff)%*%Sigma%*%dff)

              if(intercept)
                     beta <- t(abs(b0[2:n1])*t(beta))
              else
                     beta <- t(abs(b0)*t(beta))

              if (intercept) {
               beta0 <- as.vector(beta0)-drop(t(a12)%*%t(beta))/a11
              }
              else {
                     beta0 <- rep(0,k+1)
              }
              dof <- apply(abs(beta)>eps,1,sum)
              BIC <- RSS+log(n)*dof
              AIC <- RSS+2*dof
              object <- list(AIC = AIC, BIC = BIC,
                             beta = beta, beta0 = beta0)
              object
       }

       ##This part is written by Hansheng Wang.
       vcov.rq <- function(object,...)
       {
              q=object$tau
              x=as.matrix(object$x)
              resid=object$residuals
              f0=density(resid,n=1,from=0,to=0)$y
              COV=q*(1-q)*solve(t(x)%*%x)/f0^2
              COV
       }

       # adaptive lasso for linear reg, tuning parameter by bic
       # calls software from Wang and Leng (2007, JASA).
       ok<-complete.cases(x,y)
       x<-x[ok,]                            # get rid of na's
       y<-y[ok]                             # since regsubsets can't handle na's
       m<-ncol(x)
       n<-nrow(x)
       as.matrix(x)->x
       lm(y~x)->out
       lsa(out)->out.lsa
       coeff<-out.lsa$beta.bic
       coeff2<-coeff[2:(m+1)]               # get rid of intercept
       pred<-x%*%coeff2+coeff[1]
       st<-sum(coeff2 !=0)                 # number nonzero
       mse<-sum((y-pred)^2)/(n-st-1)
       if(st>0) x.ind<-as.vector(which(coeff2 !=0)) else x.ind<-0
       return(list(fit=pred,st=st,mse=mse,x.ind=x.ind,coeff=coeff2,
                   intercept=coeff[1]))
}




# ----------------------------------------------------------------------------

# lscale

# ----------------------------------------------------------------------------

lscale<-function(x,m,q)
{
#
# Compute the L-scale as used by Marrona
# Technometrics, 2005, 47, 264-273
#
# so it is assumed that values in x have been centered
# (a measure of location has been subtracted from each value)
# and the results squared.
#
#  q is defined in Marrona. For principal components, want to reduce
#  to p dimensional data, q=ncol(x)-p
#
hval<-floor((length(x)+m-q+2)/2)
flag<-(x<0)
if(sum(flag)>0)stop("For lscale, all values must be nonnegative")
x<-sort(x)
val<-sum(x[1:hval])
val
}




# ----------------------------------------------------------------------------

# lsfitNci4

# ----------------------------------------------------------------------------

lsfitNci4<-function(x,y,alpha=.05){
#
# Compute confidence for least squares
# regression using heteroscedastic method
# recommended by Cribari-Neto (2004).
#
x<-as.matrix(x)
if(nrow(x) != length(y))stop("Length of y does not match number of x values")
m<-cbind(x,y)
m<-elimna(m)
y<-m[,ncol(x)+1]
temp<-lsfit(x,y)
x<-cbind(rep(1,nrow(x)),m[,1:ncol(x)])
xtx<-solve(t(x)%*%x)
h<-diag(x%*%xtx%*%t(x))
n<-length(h)
d<-(n*h)/sum(h)
for(i in 1:length(d)){
	d[i]<-min(4, d[i])
}
hc4<-xtx%*%t(x)%*%diag(temp$res^2/(1-h)^d)%*%x%*%xtx
df<-nrow(x)-ncol(x)
crit<-qt(1-alpha/2,df)
al<-ncol(x)
ci<-matrix(NA,nrow=al,ncol=3)
for(j in 1:al){
ci[j,1]<-j
ci[j,2]<-temp$coef[j]-crit*sqrt(hc4[j,j])
ci[j,3]<-temp$coef[j]+crit*sqrt(hc4[j,j])
}
list(ci=ci,stand.errors=sqrt(diag(hc4)), cov=hc4)
}




# ----------------------------------------------------------------------------

# lsqs2

# ----------------------------------------------------------------------------

lsqs2<-function(x,y,MD=FALSE,tr=.05,plotit=TRUE){
#  cf Liu and Singh, JASA 1993, 252-260
#
if(is.list(x))x<-matl(x)
if(is.list(y))y<-matl(y)
disyx<-NA # depth of y in x
disxy<-NA # depth of x in y
if(!is.matrix(x) && !is.matrix(y)){
x<-x[!is.na(x)]
y<-y[!is.na(y)]
#
tempxx<-NA
for(i in 1:length(x)){
tempxx[i]<-sum(x[i]<=x)/length(x)
if(tempxx[i]>.5)tempxx[i]<-1-tempxx[i]
}
for(i in 1:length(x)){
temp<-sum(x[i]<=y)/length(y)
if(temp>.5)temp<-1-temp
disxy[i]<-mean(temp>tempxx)
}
tempyy<-NA
for(i in 1:length(y)){
tempyy[i]<-sum(y[i]<=y)/length(y)
if(tempyy[i]>.5)tempyy[i]<-1-tempyy[i]
}
for(i in 1:length(y)){
temp<-sum(y[i]<=x)/length(x)
if(temp>.5)temp<-1-temp # depth of y_i in x
disyx[i]<-mean(temp>tempyy)
}
qhatxy<-mean(disyx)
qhatyx<-mean(disxy)
qhat<-(qhatxy+qhatyx)/2
}
if(is.matrix(x) && is.matrix(x)){
if(!MD){
if(ncol(x)!=2 || ncol(y)!=2){
# Use approximate depth
tempyy<-fdepth(y)
temp<-fdepth(y,x)
for(i in 1:nrow(x)){
disxy[i]<-mean(temp[i]>tempyy)
}
tempxx<-NA
tempxx<-fdepth(x)
temp<-fdepth(x,pts=y)
for(i in 1:nrow(y)){
disyx[i]<-mean(temp[i]>tempxx)
}}
if(ncol(x)==2 && ncol(y)==2){
if(plotit){
plot(rbind(x,y),type="n",xlab="Var 1",ylab="VAR 2")
points(x)
points(y,pch="o")
temp<-NA
for(i in 1:nrow(x)){
temp[i]<-depth(x[i,1],x[i,2],x)
}
flag<-(temp>=median(temp))
xx<-x[flag,]
xord<-order(xx[,1])
xx<-xx[xord,]
temp<-chull(xx)
xord<-order(xx[,1])
xx<-xx[xord,]
temp<-chull(xx)
lines(xx[temp,])
lines(xx[c(temp[1],temp[length(temp)]),])
temp<-NA
for(i in 1:nrow(y)){
temp[i]<-depth(y[i,1],y[i,2],y)
}
flag<-(temp>=median(temp))
xx<-y[flag,]
xord<-order(xx[,1])
xx<-xx[xord,]
temp<-chull(xx)
flag<-(temp>=median(temp))
xord<-order(xx[,1])
xx<-xx[xord,]
temp<-chull(xx)
lines(xx[temp,],lty=2)
lines(xx[c(temp[1],temp[length(temp)]),],lty=2)
}
tempyy<-NA
for(i in 1:nrow(y))tempyy[i]<-depth(y[i,1],y[i,2],y)
for(i in 1:nrow(x)){
temp<-depth(x[i,1],x[i,2],y)
disxy[i]<-mean(temp>tempyy)
}
tempxx<-NA
for(i in 1:nrow(x))tempxx[i]<-depth(x[i,1],x[i,2],x)
for(i in 1:nrow(y)){
temp<-depth(y[i,1],y[i,2],x)
disyx[i]<-mean(temp>tempxx)
}
}}
if(MD){
mx<-apply(x,2,median)
my<-apply(y,2,median)
vx<-apply(x,2,winval,tr=tr)-apply(x,2,mean,trim=tr)+mx
vx<-var(vx)
vy<-apply(y,2,winval,tr=tr)-apply(y,2,mean,trim=tr)+my
vy<-var(vy)
tempxx<-1/(1+mahalanobis(x,mx,vx))
tempyx<-1/(1+mahalanobis(y,mx,vx))
for(i in 1:nrow(y)){
disyx[i]<-mean(tempyx[i]>tempxx)
}
tempyy<-1/(1+mahalanobis(y,my,vy))
tempxy<-1/(1+mahalanobis(x,my,vy))
for(i in 1:nrow(x)){
disxy[i]<-mean(tempxy[i]>tempyy)
}
}
qhatxy<-sum(disxy)
qhatyx<-sum(disyx)
qhat<-(qhatxy+qhatyx)/(length(disxy)+length(disyx))
}
qhatyx<-mean(disyx)
qhatxy<-mean(disxy)
list(qhatxy,qhatyx,qhat)
}

lsqs3<-function(x,y,plotit=TRUE,cop=2,ap.dep=FALSE,v2=FALSE,pv=FALSE,SEED=TRUE,nboot=1000,ypch="o",xpch="+"){
#
#  Compute the typical depth of x in y,
#  Compute the typical depth of y in x,
#  use the maximum of the two typical depths
#  as a test statistic.
#  This method is designed to be sensitive to
#  shifts in location.
#
# Use Tukey's depth; bivariate case only.
#
# cop=2 use MCD location estimator when
# computing depth with function fdepth
# cop=3 uses medians
# cop=3 uses MVE
#
#  xpch="+" means when plotting the data, data from the first
#  group are indicated by a +
#  ypch="o" are data from the second group
#
if(is.list(x))x<-matl(x)
if(is.list(y))y<-matl(y)
x<-elimna(x)
y<-elimna(y)
x<-as.matrix(x)
y<-as.matrix(y)
nx=nrow(x)
ny=nrow(y)
if(ncol(x) != ncol(y))stop("Number of variables not equal")
disyx<-NA # depth of y in x
disxy<-NA # depth of x in y
#
if(ncol(x)==2){
if(plotit){
plot(rbind(x,y),type="n",xlab="VAR 1",ylab="VAR 2")
points(x,pch=xpch)
points(y,pch=ypch)
if(nrow(x)>50){
if(!ap.dep){
print("If execution time is high, might use ap.dep=FALSE")
}
if(!ap.dep)temp<-depth2(x,plotit=FALSE)
if(ap.dep)temp<-fdepth(x,plotit=FALSE,cop=cop)
}
if(!ap.dep)temp<-depth2(x,plotit=FALSE)
if(ap.dep)temp<-fdepth(x,plotit=FALSE,cop=cop)
flag<-(temp>=median(temp))
xx<-x[flag,]
xord<-order(xx[,1])
xx<-xx[xord,]
temp<-chull(xx)
xord<-order(xx[,1])
xx<-xx[xord,]
temp<-chull(xx)
lines(xx[temp,])
lines(xx[c(temp[1],temp[length(temp)]),])
if(ap.dep)temp<-fdepth(y,plotit=FALSE,cop=cop)
if(!ap.dep)temp<-depth2(y,plotit=FALSE)
if(!ap.dep)temp<-depth2(y,plotit=FALSE)
if(!ap.dep)temp<-fdepth(y,plotit=FALSE)
flag<-(temp>=median(temp))
xx<-y[flag,]
xord<-order(xx[,1])
xx<-xx[xord,]
temp<-chull(xx)
flag<-(temp>=median(temp))
xord<-order(xx[,1])
xx<-xx[xord,]
temp<-chull(xx)
lines(xx[temp,],lty=2)
lines(xx[c(temp[1],temp[length(temp)]),],lty=2)
}
tempyx<-NA
tempxy<-NA
if(ap.dep)tempyx<-fdepth(x,y,plotit=FALSE,cop=cop)
if(!ap.dep)tempyx<-depth2(x,y,plotit=FALSE)
if(ap.dep)tempxy<-fdepth(y,x,plotit=FALSE,cop=cop)
if(!ap.dep)tempxy<-depth2(y,x,plotit=FALSE)
}
if(ncol(x)==1){
tempyx<-unidepth(as.vector(x),as.vector(y))
tempxy<-unidepth(as.vector(y),as.vector(x))
}
if(ncol(x)>2){
if(!v2){
tempxy<-fdepth(y,x,plotit=FALSE,cop=cop)
tempyx<-fdepth(x,y,plotit=FALSE,cop=cop)
}
if(v2){
tempxy<-fdepthv2(y,x,plotit=FALSE)
tempyx<-fdepthv2(x,y,plotit=FALSE)
}}
qhatxy<-mean(tempxy)
qhatyx<-mean(tempyx)
qhat<-max(c(qhatxy,qhatyx))
n1<-nrow(x)
n2<-nrow(y)
nv<-(3*min(c(n1,n2))+max(c(n1,n2)))/4
if(ncol(x)==1)crit<-.2536-.4578/sqrt(nv)
if(ncol(x)==2)crit<-.1569-.3/sqrt(nv)
if(ncol(x)==3)crit<-.0861-.269/sqrt(nv)
if(ncol(x)==4)crit<-.054-.1568/sqrt(nv)
if(ncol(x)==5)crit<-.0367-.0968/sqrt(nv)
if(ncol(x)==6)crit<-.0262-.0565/sqrt(nv)
if(ncol(x)==7)crit<-.0174-.0916/sqrt(nv)
if(ncol(x)>7)crit<-.013
rej<-"Fail to reject"
if(qhat<=crit)rej<-"Reject"
testv=NULL
pval=NULL
if(pv){
if(SEED)set.seed(2)
rej="NULL"
for(i in 1:nboot)testv[i]=lsqs3.sub(rmul(n1,ncol(x)),rmul(n2,ncol(x)),cop=cop,ap.dep=ap.dep,v2=v2,)$test
pval=mean(qhat>=testv)
}
list(n1=nx,n2=ny,avg.depth.of.x.in.y=qhatxy,avg.depth.of.y.in.x=qhatyx,test=qhat,crit=crit,Decision=rej,p.value=pval)
}

# The next function is used to compute p-values for lsqs3; it  avoids lsqs3 calling itself.

lsqs3.sub<-function(x,y,plotit=FALSE,cop=2,ap.dep=FALSE,v2=FALSE,pv=FALSE,SEED=TRUE,nboot=1000,ypch="o",xpch="+"){
#
#  Compute the typical depth of x in y,
#  Compute the typical depth of y in x,
#  use the maximum of the two typical depths
#  as a test statistic.
#  This method is designed to be sensitive to
#  shifts in location.
#
# Use Tukey's depth; bivariate case only.
#
# cop=2 use MCD location estimator when
# computing depth with function fdepth
# cop=3 uses medians
# cop=3 uses MVE
#
#  xpch="+" means when plotting the data, data from the first
#  group are indicated by a +
#  ypch="o" are data from the second group
#
if(is.list(x))x<-matl(x)
if(is.list(y))y<-matl(y)
x<-elimna(x)
y<-elimna(y)
x<-as.matrix(x)
y<-as.matrix(y)
nx=nrow(x)
ny=nrow(y)
if(ncol(x) != ncol(y))stop("Number of variables not equal")
disyx<-NA # depth of y in x
disxy<-NA # depth of x in y
#
if(ncol(x)==2){
if(plotit){
plot(rbind(x,y),type="n",xlab="VAR 1",ylab="VAR 2")
points(x,pch=xpch)
points(y,pch=ypch)
if(nrow(x)>50){
if(!ap.dep){
print("If execution time is high, might use ap.dep=FALSE")
}
if(!ap.dep)temp<-depth2(x,plotit=FALSE)
if(ap.dep)temp<-fdepth(x,plotit=FALSE,cop=cop)
}
if(!ap.dep)temp<-depth2(x,plotit=FALSE)
if(ap.dep)temp<-fdepth(x,plotit=FALSE,cop=cop)
flag<-(temp>=median(temp))
xx<-x[flag,]
xord<-order(xx[,1])
xx<-xx[xord,]
temp<-chull(xx)
xord<-order(xx[,1])
xx<-xx[xord,]
temp<-chull(xx)
lines(xx[temp,])
lines(xx[c(temp[1],temp[length(temp)]),])
if(ap.dep)temp<-fdepth(y,plotit=FALSE,cop=cop)
if(!ap.dep)temp<-depth2(y,plotit=FALSE)
if(!ap.dep)temp<-depth2(y,plotit=FALSE)
if(!ap.dep)temp<-fdepth(y,plotit=FALSE)
flag<-(temp>=median(temp))
xx<-y[flag,]
xord<-order(xx[,1])
xx<-xx[xord,]
temp<-chull(xx)
flag<-(temp>=median(temp))
xord<-order(xx[,1])
xx<-xx[xord,]
temp<-chull(xx)
lines(xx[temp,],lty=2)
lines(xx[c(temp[1],temp[length(temp)]),],lty=2)
}
tempyx<-NA
tempxy<-NA
if(ap.dep)tempyx<-fdepth(x,y,plotit=FALSE,cop=cop)
if(!ap.dep)tempyx<-depth2(x,y,plotit=FALSE)
if(ap.dep)tempxy<-fdepth(y,x,plotit=FALSE,cop=cop)
tempxy<-depth2(y,x,plotit=FALSE)
}
if(ncol(x)==1){
tempyx<-unidepth(as.vector(x),as.vector(y))
tempxy<-unidepth(as.vector(y),as.vector(x))
}
if(ncol(x)>2){
if(!v2){
tempxy<-fdepth(y,x,plotit=FALSE,cop=cop)
tempyx<-fdepth(x,y,plotit=FALSE,cop=cop)
}
if(v2){
tempxy<-fdepthv2(y,x,plotit=FALSE)
tempyx<-fdepthv2(x,y,plotit=FALSE)
}}
qhatxy<-mean(tempxy)
qhatyx<-mean(tempyx)
qhat<-max(c(qhatxy,qhatyx))
n1<-nrow(x)
n2<-nrow(y)
nv<-(3*min(c(n1,n2))+max(c(n1,n2)))/4
if(ncol(x)==1)crit<-.2536-.4578/sqrt(nv)
if(ncol(x)==2)crit<-.1569-.3/sqrt(nv)
if(ncol(x)==3)crit<-.0861-.269/sqrt(nv)
if(ncol(x)==4)crit<-.054-.1568/sqrt(nv)
if(ncol(x)==5)crit<-.0367-.0968/sqrt(nv)
if(ncol(x)==6)crit<-.0262-.0565/sqrt(nv)
if(ncol(x)==7)crit<-.0174-.0916/sqrt(nv)
if(ncol(x)>7)crit<-.013
rej<-"Fail to reject"
if(qhat<=crit)rej<-"Reject"
testv=NULL
pval=NULL
if(pv){
if(SEED)set.seed(2)
rej="NULL"
for(i in 1:nboot)testv[i]=lsqs3.sub(rmul(n1,ncol(x)),rmul(n2,ncol(x)),cop=cop,ap.dep=ap.dep,v2=v2,)$test
pval=mean(qhat>=testv)
}
list(n1=nx,n2=ny,avg.depth.of.x.in.y=qhatxy,avg.depth.of.y.in.x=qhatyx,test=qhat,crit=crit,Decision=rej,p.value=pval)
}




# ----------------------------------------------------------------------------

# lsqtest4

# ----------------------------------------------------------------------------

lsqtest4<-function(vstar,yhat,res,x, k){
ystar <- yhat + res * vstar
p<-ncol(x)
pp<-p+1
vals<-lsfit(x,ystar)$coef[2:pp]
sa<-lsfitNci4(x, ystar)$cov[-1, -1]
sa<-as.matrix(sa)
sai<-sa[k,k]
test<-vals[k]/sqrt(sai)
test
}




# ----------------------------------------------------------------------------

# lstest4

# ----------------------------------------------------------------------------

lstest4<-function(vstar,yhat,res,x){
ystar <- yhat + res * vstar
p<-ncol(x)
pp<-p+1
vals<-t(as.matrix(lsfit(x,ystar)$coef[2:pp]))
sa<-lsfitNci4(x, ystar)$cov[-1, -1]
sai<-solve(sa)
test<-(vals)%*%sai%*%t(vals)
test<-test[1,1]
test
}




# ----------------------------------------------------------------------------

# LTS.EN

# ----------------------------------------------------------------------------

LTS.EN<-function(x,y,xout=FALSE,family='gaussian',alphas=NULL,lambdas=NULL,outfun=outpro,...){
#
#
#  Robust elastic net
# Yi, C. & Huang, J. (2016) Semismooth Newton coordinate descent algorithm for elastic-net penalized
# Huber loss regression and quantile regression.   (https://arxiv.org/abs/1509.02957)
# Journal of Computational and Graphical Statistics
# http://www.tandfonline.com/doi/full/10.1080/10618600.2016.1256816
#
#
library(enetLTS)
x<-as.matrix(x)
xx<-cbind(x,y)
xx<-elimna(xx)
x<-xx[,1:ncol(x)]
x<-as.matrix(x)
y<-xx[,ncol(x)+1]
temp<-NA
x<-as.matrix(x)
if(xout){
x<-as.matrix(x)
flag<-outfun(x,plotit=plotit,...)$keep
x<-x[flag,]
y<-y[flag]
x<-as.matrix(x)
}
if(!is.null(alphas) & !is.null(lambdas))a=enetLTS(x,y,alphas=alphas,lambdas=lambdas,family=family,plot=FALSE)
if(!is.null(alphas) & is.null(lambdas))a=enetLTS(x,y,alphas=alphas,family=family,plot=FALSE)
if(is.null(alphas) & is.null(lambdas))a=enetLTS(x,y,family=family,plot=FALSE)
list(coef=a[6]$raw.coefficients)
}

ltsR<-function(x,y,RES=FALSE,varfun=pbvar,corfun=pbcor){
#
xy=elimna(cbind(x,y))
p1=ncol(xy)
p=p1-1
x=xy[,1:p]
y=xy[,p1]
temp=ltsreg(x,y)$coef
x=as.matrix(x)
p=ncol(x)+1
res<-y-x%*%temp[2:p]-temp[1]
yhat<-y-res
if(!RES)res=NULL
e.pow<-varfun(yhat)/varfun(y)
if(is.na(e.pow))e.pow<-1
if(e.pow>=1)e.pow<-corfun(yhat,y)$cor^2
list(coef=temp,residuals=res,Explanatory.Power=e.pow,
Strength.Assoc=sqrt(e.pow))
}




# ----------------------------------------------------------------------------

# M1M2

# ----------------------------------------------------------------------------

M1M2<-function(m1,m2,test=yuen,alpha=.05,...){
#
# Goal: compare data in col 1 of m1 to col 1 data in m2,
# do this again for col 2, etc.
# control FWE via Hochberg  method
# alpha is the desired family wise error rate
# The argument test is assumed to be a function that returns a p-value stored in $p.value
#
vals=0
if(is.list(m1))m1=matl(m1)
if(is.list(m2))m2=matl(m2)
m1=as.matrix(m1)
m2=as.matrix(m2)
ntest=ncol(m1)
outp=matrix(0,ncol=2,nrow=ntest)
dimnames(outp)=list(NULL,c("p.value","crit.p.value"))
if(ncol(m1)!=ncol(m2))stop("m1 and m2 do not have the same number of columns")
for(i in 1:ncol(m1))vals[i]=test(m1[,i],m2[,i],...)$p.value
outp[,1]=vals
dvec=alpha/(c(1:ntest))
temp2<-order(0-vals)
zvec<-dvec
outp[temp2,2]=zvec
flag=(outp[,1]<=outp[,2])
dd=sum(outp[,1]<=outp[,2])
chk=c(1:ntest)
list(results=outp,number.sig=dd,sig.tests=chk[flag])
}




# ----------------------------------------------------------------------------

# m1way

# ----------------------------------------------------------------------------

m1way<-function(x,est=hd,nboot=599,SEED=TRUE,...){
#
#   Test the hypothesis that J measures of location are equal
#   using the percentile bootstrap method.
#   By default, medians are compared using 599 bootstrap samples.
#   and the Harrell-Davis Estimator. To use the usual sample median, set
#   est=median
#
#   The data are assumed to be stored in x in list mode.  Thus,
#   x[[1]] contains the data for the first group, x[[2]] the data
#   for the second group, etc. Length(x)=the number of groups = J, say.
#
#
if(is.matrix(x))x<-listm(x)
if(!is.list(x))stop("Data must be stored in list mode or a matrix.")
J<-length(x)
nval<-vector("numeric",length(x))
gest<-vector("numeric",length(x))
if(SEED)set.seed(2) # set seed of random number generator so that
#             results can be duplicated.
bvec<-matrix(0,J,nboot)
print("Taking bootstrap samples. Please wait.")
for(j in 1:J){
print(paste("Working on group ",j))
nval[j]<-length(x[[j]])
gest[j]<-est(x[[j]])
xcen<-x[[j]]-est(x[[j]],...)
data<-matrix(sample(xcen,size=length(x[[j]])*nboot,replace=TRUE),nrow=nboot)
bvec[j,]<-apply(data,1,est,...) # A J by nboot matrix
#                     containing the bootstrap values of est.
}
teststat<-wsumsq(gest,nval)
testb<-apply(bvec,2,wsumsq,nval)
p.value<-1 - sum(teststat >= testb)/nboot
teststat<-wsumsq(gest,nval)
list(teststat=teststat,p.value=p.value)
}

m2ci<-function(x,y,alpha=.05,nboot=1000,bend=1.28,os=FALSE){
#
#   Compute a bootstrap, .95 confidence interval for the
#   the difference between two independent
#   M-estimator of location based on Huber's Psi.
#   The default percentage bend is bend=1.28
#   The default number of bootstrap samples is nboot=399
#
#   By default, the fully iterated M-estimator is used. To use the
#   one-step M-estimator instead, set os=T
#
os<-as.logical(os)
x<-x[!is.na(x)] # Remove any missing values in x
y<-y[!is.na(y)] # Remove any missing values in y
if(length(x)<=19 || length(y)<=19)
warning(paste("The number of observations in at least one group
is less than 20. This function might fail due to division by zero,
which in turn causes an error in function hpsi having to do with
a missing value."))
set.seed(2) # set seed of random number generator so that
#             results can be duplicated.
print("Taking bootstrap samples. Please wait.")
datax<-matrix(sample(x,size=length(x)*nboot,replace=TRUE),nrow=nboot)
datay<-matrix(sample(y,size=length(y)*nboot,replace=TRUE),nrow=nboot)
if(!os){
bvecx<-apply(datax,1,mest,bend)
bvecy<-apply(datay,1,mest,bend)
}
if(os){
bvecx<-apply(datax,1,onestep,bend)
bvecy<-apply(datay,1,onestep,bend)
}
bvec<-sort(bvecx-bvecy)
test<-sum(bvec<0)/nboot+.5*sum(bvec==0)/nboot
pv=2*min(c(test,1-test))
low<-round((alpha/2)*nboot)
up<-round((1-alpha/2)*nboot)
se<-sqrt(var(bvec))
list(ci=c(bvec[low],bvec[up]),se=se,p.value=pv)
}




# ----------------------------------------------------------------------------

# m2way

# ----------------------------------------------------------------------------

m2way<-function(J,K,x,est=hd,alpha=.05,nboot=600,SEED=TRUE,grp=NA,pr=FALSE,...){
#
# Two-way ANOVA based on forming averages
#
#  By default
#  est=hd meaning that medians are used with the Harrell-Davis estimator.
#
#   The data are assumed to be stored in x in list mode or in a matrix.
#  If grp is unspecified, it is assumed x[[1]] contains the data
#  for the first level of both factors: level 1,1.
#  x[[2]] is assumed to contain the data for level 1 of the
#  first factor and level 2 of the second factor: level 1,2
#  x[[j+1]] is the data for level 2,1, etc.
#  If the data are in wrong order, grp can be used to rearrange the
#  groups. For example, for a two by two design, grp<-c(2,4,3,1)
#  indicates that the second group corresponds to level 1,1;
#  group 4 corresponds to level 1,2; group 3 is level 2,1;
#  and group 1 is level 2,2.
#
#   Missing values are automatically removed.
#
JK<-J*K
if(is.data.frame(x))x=as.matrix(x)
xcen<-list()
        if(is.matrix(x))
                x <- listm(x)
        if(!is.list(x))
                stop("Data must be stored in list mode or a matrix.")
        if(!is.na(grp[1])) {
                yy <- x
                for(j in 1:length(grp))
                        x[[j]] <- yy[[grp[j]]]
        }
for(j in 1:JK){
temp<-x[[j]]
temp<-temp[!is.na(temp)] # Remove missing values.
x[[j]]<-temp
}
xx<-list()
mloc<-NA
for(i in 1:JK){
xx[[i]]<-x[[i]]
mloc[i]<-est(xx[[i]],...)
xcen[[i]]<-xx[[i]]-mloc[i]
}
x<-xx
mat<-matrix(mloc,nrow=J,ncol=K,byrow=TRUE)
leva<-apply(mat,1,mean) # J averages over columns
levb<-apply(mat,2,mean)
gm<-mean(levb)
testa<-sum((leva-mean(leva))^2)
testb<-sum((levb-mean(levb))^2)
testab<-NA
tempab<-matrix(NA,nrow=J,ncol=K)
for(j in 1:J){
for(k in 1:K){
tempab[j,k]<-mat[j,k]-leva[j]-levb[k]+gm
}}
testab<-sum(tempab^2)
bvec<-matrix(NA,nrow=JK,ncol=nboot)
if(SEED)set.seed(2) # set seed of random number generator so that
#             results can be duplicated.
if(pr)print("Taking bootstrap samples. Please wait.")
for(j in 1:JK){
if(pr)print(paste("Working on group ",j))
data<-matrix(sample(xcen[[j]],size=length(xcen[[j]])*nboot,replace=TRUE),
nrow=nboot)
bvec[j,]<-apply(data,1,est,...) # JK by nboot matrix, jth row contains
#                          bootstrapped  estimates for jth group
}
boota<-NA
bootb<-NA
bootab<-NA
for(i in 1:nboot){
mat<-matrix(bvec[,i],nrow=J,ncol=K,byrow=TRUE)
leva<-apply(mat,1,mean) # J averages over columns
levb<-apply(mat,2,mean)
gm<-mean(mat)
boota[i]<-sum((leva-mean(leva))^2)
bootb[i]<-sum((levb-mean(levb))^2)
for(j in 1:J){
for(k in 1:K){
tempab[j,k]<-mat[j,k]-leva[j]-levb[k]+gm
}}
bootab[i]<-sum(tempab^2)}
pvala<-1-sum(testa>=boota)/nboot
pvalb<-1-sum(testb>=bootb)/nboot
pvalab<-1-sum(testab>=bootab)/nboot
list(p.value.A=pvala,p.value.B=pvalb,p.value.AB=pvalab,
test.A=testa,test.B=testb,
test.AB=testab,est.loc=matrix(mloc,nrow=J,ncol=K,byrow=TRUE))
}




# ----------------------------------------------------------------------------

# madsq

# ----------------------------------------------------------------------------

madsq<-function(x)mad(x)^2




# ----------------------------------------------------------------------------

# MADstand

# ----------------------------------------------------------------------------

MADstand<-function(x){
val=x/mad(x)
val
}
regtestMC<-function(x,y,regfun=tsreg,nboot=600,alpha=.05,plotit=TRUE,
grp=c(1:ncol(x)),nullvec=c(rep(0,length(grp))),xout=FALSE,outfun=outpro,SEED=TRUE,pr=TRUE,...){
#
#  Test the hypothesis that q of the p predictors are equal to
#  some specified constants. By default, the hypothesis is that all
#  p predictors have a coefficient equal to zero.
#  The method is based on a confidence ellipsoid.
#  The critical value is determined with the percentile bootstrap method
#  in conjunction with Mahalanobis distance.
#
x<-as.matrix(x)
p1<-ncol(x)+1
p<-ncol(x)
xy<-cbind(x,y)
xy<-elimna(xy)
x<-xy[,1:p]
y<-xy[,p1]
if(xout){
if(pr)print("Default for outfun is now outpro")
m<-cbind(x,y)
if(identical(outfun,outblp))flag=outblp(x,y,plotit=FALSE)$keep
else
flag<-outfun(x,plotit=FALSE,...)$keep
m<-m[flag,]
x<-m[,1:p]
y<-m[,p1]
}
x<-as.matrix(x)
if(length(grp)!=length(nullvec))stop("The arguments grp and nullvec must have the same length.")
if(SEED)set.seed(2) # set seed of random number generator so that
#             results can be duplicated.
data<-matrix(sample(length(y),size=length(y)*nboot,replace=TRUE),nrow=nboot)
data=listm(t(data))
# bvec<-apply(data,1,regboot,x,y,regfun) # A p+1 by nboot matrix. The first row
#                     contains the bootstrap intercepts, the second row
#                     contains the bootstrap values for first predictor, etc.
bvec=mclapply(data,regbootMC,x,y,regfun,mc.preschedule=TRUE) # list mode bvec[[1]]
#     contains estimate from first bootstrap sample, etc.
bvec=matl(bvec)
grp<-grp+1
est<-regfun(x,y)$coef
estsub<-est[grp]
bsub<-t(bvec[grp,])
if(length(grp)==1){
m1<-sum((bvec[grp,]-est)^2)/(length(y)-1)
dis<-(bsub-estsub)^2/m1
}
if(length(grp)>1){
mvec<-apply(bsub,2,FUN=mean)
m1<-var(t(t(bsub)-mvec+estsub))
dis<-mahalanobis(bsub,estsub,m1)
}
dis2<-order(dis)
dis<-sort(dis)
critn<-floor((1-alpha)*nboot)
crit<-dis[critn]
test<-mahalanobis(t(estsub),nullvec,m1)
sig.level<-1-sum(test>dis)/nboot
if(length(grp)==2 && plotit){
plot(bsub,xlab="Parameter 1",ylab="Parameter 2")
points(nullvec[1],nullvec[2],pch=0)
xx<-bsub[dis2[1:critn],]
xord<-order(xx[,1])
xx<-xx[xord,]
temp<-chull(xx)
lines(xx[temp,])
lines(xx[c(temp[1],temp[length(temp)]),])
}
list(test=test,crit=crit,p.value=sig.level,nullvec=nullvec,est=estsub)
}

man2pb<-function(x,y,alpha=.05,nboot=NA,crit=NA,SEED=TRUE){
#
#   Two-sample Behrens-Fisher problem.
#
#   For each of two independent groups,
#   have P measures for each subject. The goal is to compare the 20%
#   trimmed means of the first group to the trimmed means for the second;
#   this is done for each of the  P measures.
#
#   The percentile bootstrap method is used to
#   compute a .95, or .975, or .99 confidence interval.
#
#   Only 20% trimming is allowed.
#
#   x contains the data for the first group; it
#    can be an n by J matrix or it can have list mode.
#   y contains the data for the second group.
#
#   Vectors with missing values are eliminated from the analysis.
#
if(!is.list(x) && !is.matrix(x))stop("Data must be stored in a matrix or in list mode.")
if(!is.list(y) && !is.matrix(y))stop("Data must be stored in a matrix or in list mode.")
if(is.list(x)){
# put the data in an n by p matrix
matx<-matrix(0,length(x[[1]]),length(x))
for (j in 1:length(x))matx[,j]<-x[[j]]
}
if(is.list(y)){
# put the data in an n by p matrix
maty<-matrix(0,length(y[[1]]),length(y))
for (j in 1:length(y))maty[,j]<-y[[j]]
}
if(is.matrix(x)){
matx<-x
}
if(is.matrix(y)){
maty<-y
}
if(ncol(matx)!=ncol(maty))stop("The number of variables for group 1 is not equal to the number for group 2")
if(sum(is.na(matx)>=1))matx<-elimna(matx)
if(sum(is.na(maty)>=1))maty<-elimna(maty)
J<-ncol(matx)
connum<-ncol(matx)
if(is.na(nboot)){
if(ncol(matx)<=4)nboot<-2000
if(ncol(matx)>4)nboot<-5000
}
#
#  Determine critical value
#
if(ncol(matx)==2){
if(alpha==.05)crit<-.0125
if(alpha==.025)crit<-.0060
if(alpha==.01)crit<-.0015
}
if(ncol(matx)==3){
if(alpha==.05)crit<-.007
if(alpha==.025)crit<-.003
if(alpha==.01)crit<-.001
}
if(ncol(matx)==4){
if(alpha==.05)crit<-.0055
if(alpha==.025)crit<-.0020
if(alpha==.01)crit<-.0005
}
if(ncol(matx)==5){
if(alpha==.05)crit<-.0044
if(alpha==.025)crit<-.0016
if(alpha==.01)crit<-.0005
}
if(ncol(matx)==6){
if(alpha==.05)crit<-.0038
if(alpha==.025)crit<-.0018
if(alpha==.01)crit<-.0004
}
if(ncol(matx)==7){
if(alpha==.05)crit<-.0028
if(alpha==.025)crit<-.0010
if(alpha==.01)crit<-.0002
}
if(ncol(matx)==8){
if(alpha==.05)crit<-.0026
if(alpha==.025)crit<-.001
if(alpha==.01)crit<-.0002
}
if(ncol(matx)>8){
# Use an approximation of the critical value
if(alpha==.025)warning("Can't determine a critical value when alpha=.025 and the number of groups exceeds 8.")
nmin<-min(nrow(matx),nrow(maty))
if(alpha==.05){
if(nmin<100)wval<-smmcrit(60,ncol(matx))
if(nmin>=100)wval<-smmcrit(300,ncol(matx))
wval<-0-wval
crit<-pnorm(wval)
}
if(alpha==.01){
if(nmin<100)wval<-smmcrit01(60,ncol(matx))
if(nmin>=100)wval<-smmcrit01(300,ncol(matx))
wval<-0-wval
crit<-pnorm(wval)
}
}
if(is.na(crit))warning("Critical values can be determined for alpha=.05, .025 and .01 only")
icl<-ceiling(crit*nboot)
icu<-ceiling((1-crit)*nboot)
if(SEED)set.seed(2) # set seed of random number generator so that
#             results can be duplicated.
print("Taking bootstrap samples. Please wait.")
bootx<-bootdep(matx,tr=.2,nboot)
booty<-bootdep(maty,tr=.2,nboot)
        #
        # Now have an nboot by J matrix of bootstrap values.
        #
test<-1
for (j in 1:connum){
test[j]<-sum(bootx[,j]<booty[,j])/nboot
if(test[j]>.5)test[j]<-1-test[j]
}
output <- matrix(0, connum, 5)
        dimnames(output) <- list(NULL, c("variable #", "psihat", "p.value",
                "ci.lower", "ci.upper"))
        tmeanx <- apply(matx, 2, mean, trim = .2)
        tmeany <- apply(maty, 2, mean, trim = .2)
        psi <- 1
        for(ic in 1:connum) {
                output[ic, 2] <- tmeanx[ic]-tmeany[ic]
                output[ic, 1] <- ic
                output[ic, 3] <- test[ic]
                temp <- sort(bootx[,ic]-booty[,ic])
#print(length(temp))
                output[ic, 4] <- temp[icl]
                output[ic, 5] <- temp[icu]
        }
        list(output = output, crit.p.value = crit)
}


manES<-function(x1,x2,method=NULL,pro.p=0.8,nboot=100,...){
#
# Estimate  probability of a correct classification
#  for two independent groups having
# unknown multivariate distributions
#
# The function estimates misclassification rates using
# techniques indicated by the argument
# method.
# method=NULL means that methods 'KNN','DIS','DEP','SVM','RF','NN','PRO','LSM','GBT'
# are used See function CLASS.fun
#
# The lowest value is used as the
# estimate or a correct classification.
#
if(is.null(method))method=c('KNN','DIS','DEP','SVM','RF','NN','PRO','LSM','GBT')
if(method[1]=='ALL')method=NULL
a=class.error.com(x1,x2,method=method,pro.p=pro.p,nboot=nboot,...)
IOR=order(a$Error.rates[1,])
e=1-min(a$Error.rates[1,])
LAB=dimnames(a$Error.rates)[[2]][IOR[1]]
list(Method.Used=LAB,Prob.Correct.Decision=e)
}




# ----------------------------------------------------------------------------

# MARest

# ----------------------------------------------------------------------------

MARest<-function(x,kappa=.1){
#
# Compute Maronna multivariate measure of location and scatter
#
#kappa    # the percent of cases to be controlled when robust method is used
#
ep=0.00000001  # convergence criteria
Z=x
p=ncol(x)
HT=HuberTun(kappa,p)
r=HT$r
tau=HT$tau
H=MARONNA.sub(Z,r,tau,ep,p)
list(center=t(H$center),cov=H$cov)
}
MARONNA.sub<-function(Z,r,tau,ep,p){
 # Starting values
      mu0=MeanCov(Z)$zbar
      Sigma0=MeanCov(Z)$S
      Sigin=solve(Sigma0)

      diverg=0 # convergence flag

      for (k in 1:200) {
                sumu1=0
                mu=matrix(0,p,1)
                Sigma=matrix(0,p,p)
                d=rep(NA,n)
                u1=rep(NA,n)
                u2=rep(NA,n)

                for (i in 1:n) {                        zi=Z[i,]
                        zi0=zi-mu0
                        di2=t(zi0)%*%Sigin%*%zi0
                        di=as.numeric(sqrt(di2))
                        d[i]=di

                #get u1i,u2i
                        if (di<=r) {
                           u1i=1.0
                           u2i=1.0/tau
                        }else {
                           u1i=r/di
                           u2i=u1i^2/tau
                   }
                        u1[i]=u1i
                        u2[i]=u2i

                        sumu1=sumu1+u1i
                        mu=mu+u1i*zi
                        Sigma=Sigma+u2i*zi0%*%t(zi0)

                } # end of loop i

                mu1=mu/sumu1
                Sigma1=Sigma/n
                Sigdif=Sigma1-Sigma0
                dt=sum(Sigdif^2)

                mu0=mu1
                Sigma0=Sigma1
                Sigin=solve(Sigma0)
  if (dt<ep) {break}

          } # end of loop k


       if (k==200) {
                        diverg=1
                        mu0=rep(0,p)
                        sigma0=matrix(NA,p,p)

          }
list(center=mu0,cov=Sigma0)
}
marpca<-function(x,p=ncol(x)-1,N1=3,N2=2,tol=.001,N2p=10,Nran=50,
Nkeep=10,SEED=TRUE,LSCALE=TRUE,SCORES=FALSE){
#
# Marrona (2005, Technometrics, 47, 264-273) robust PCA
#
# x is an n by m matrix, p<m
#
# Given p, find a p-dim manifold that minimizes orthogonal distances
#  Eq for manifold is Bx=a; this function returns B and a plus
#
# var.op = optimal proportion of unexplained variance,  given p
#
x<-elimna(x)
Cmat<-NULL
if(Nkeep>Nran)stop("Must have Nkeep<=Nran")
if(SEED)set.seed(2)
n<-nrow(x)
m<-ncol(x)
q<-m-p
if(q<0)stop("p should have value between 0 and ncol(x)")
if(q>0){
bkeep<-array(dim=c(q,m,Nran))
akeep<-matrix(nrow=Nran,ncol=q)
sig.val<-NA
for(it in 1:Nran){
temp<-marpca.sub(x,p,N1=N1,N2=N2,tol=tol,LSCALE=LSCALE)
bkeep[,,it]<-temp$B
akeep[it,]<-temp$a
sig.val[it]<-temp$var.op
}
ord<-order(sig.val)
bkeep2<-array(dim=c(q,m,Nkeep))
cmatkeep<-array(dim=c(m,m,Nkeep))
akeep2<-matrix(nrow=Nkeep,ncol=q)
sig.val2<-NA
for(it in 1:Nkeep){
temp<-marpca.sub(x,p,N1=0,N2=N2p,tol=tol,B=bkeep[,,ord[it]],a=akeep[ord[it],],
LSCALE=LSCALE)
bkeep2[,,it]<-temp$B
akeep2[it,]<-temp$a
sig.val2[it]<-temp$var.op
cmatkeep[,,it]<-temp$wt.cov
}
ord<-order(sig.val2)
B<-bkeep2[,,ord[1]]
a<-akeep2[ord[1],]
var.op<-sig.val2[ord[1]]
Cmat<-cmatkeep[,,ord[1]]
}
wt.mu<-NULL
if(q==0){
output<-marpca.sub(x,0,LSCALE=LSCALE)
B<-output$B
a<-output$a
var.op<-output$var.op
wt.mu<-output$mu
Cmat<-output$wt.cov
}
scores<-NULL
if(SCORES){
ev<-eigen(Cmat)
ord.val<-order(ev$values)
mn1<-m-p+1
wt.mu<-marpca.sub(x,p=p)$mu
Bp<-ev$vectors[,ord.val[mn1:m]] #m by m
xmmu<-x
for(j in 1:m)xmmu[,j]<-x[,j]-wt.mu[j]
scores<-matrix(ncol=p,nrow=n)
for(i in 1:n)scores[i,]<-t(Bp)%*%as.matrix(xmmu[i,])
}
list(B=B,a=a,var.op=var.op,wt.cov=Cmat,wt.mu=wt.mu,scores=scores)
}



marpca.sub<-function(x,p=ncol(x)-1,N1=3,N2=2,tol=.001,B=NULL,a=NULL,
LSCALE=TRUE){
#
# Marrona (2005, Technometrics, 47, 264-273) robust PCA
#
# Note: setting
# p=0 causes B to be the identity matrix, which is used in the case
# p=ncol(x) to estimate proportion of unexplained variance.
#
wt.cov<-NULL
if(!is.null(B)){
B<-as.matrix(B)
if(ncol(B)==1)B<-t(B)
}
n<-nrow(x)
m<-ncol(x)
q<-m-p
if(q<0)stop("p and q should have values between 1 and ncol(x)")
hval<-floor((n + m - q + 2)/2)
DEL<-Inf
sig0<-Inf
if(is.null(B)){
if(p>0 && p<m){
B<-matrix(runif(q*m),nrow=q,ncol=m)
B<-t(ortho(t(B)))
}
if(p==0)B<-diag(rep(1,m))
}
it<-1
Bx<-matrix(NA,ncol=q,nrow=n) #q by n
for(i in 1:n)Bx[i,]<-B%*%as.matrix(x[i,]) # q by 1
#Bx<-as.matrix(elimna(Bx))
if(is.null(a))a<-apply(Bx,2,FUN="median") # Initial a (coordinatewise medians)
while(it<N1+N2 && DEL>tol){
r<-NA
for(i in 1:n)r[i]<-sum(Bx[i,]-a)^2
if(LSCALE)sig<-lscale(r,m,q)
if(!LSCALE){
delta<-delta<-(n-m+q-1)/(2*n)
sig<-mscale(r,delta)
}
DEL<-1-sig/sig0
sig0<-sig
ord.r<-order(r)
w<-rep(0,n)
w[ord.r[1:hval]]<-1
xx<-x
for(i in 1:n)xx[i,]<-x[i,]*w[i]
mu<-apply(xx,2,FUN="sum")/sum(w) #m by 1 locations
Cmat<-matrix(0,nrow=m,ncol=m)
for(i in 1:n){
temp<-w[i]*as.matrix(x[i,]-mu)%*%t(as.matrix(x[i,]-mu))
Cmat<-Cmat+temp
}
wt.cov<-Cmat/sum(w)
if(it>N1){
temp<-eigen(wt.cov)
ord.eig<-order(temp$values)
for(iq in 1:q)B[iq,]<-temp$vectors[,ord.eig[iq]]
}
a<-B%*%mu
it<-it+1
}
list(B=B,a=a,var.op=sig,mu=mu,wt.cov=wt.cov)
}




# ----------------------------------------------------------------------------

# mat2grp

# ----------------------------------------------------------------------------

mat2grp<-function(m,coln){
#
#  For data in a matrix m, divide the data into groups based
#  on the values in column indicated
#  by the argument coln
#  and store the data in list mode.
#
#  All columns of m are retained including  column coln.
#
if(!is.null(dim(m)))m=as.matrix(m)
if(!is.matrix(m))stop("Data must be stored in a matrix or data frame")
if(length(coln)!=1)stop("The argument coln must have length 1")
x<-list()
flagna=!is.na(m[,coln])
m=m[flagna,]
grpn<-sort(unique(m[,coln]))
for (ig in 1:length(grpn)){
flag<-(m[,coln]==grpn[ig])
x[[ig]]<-m[flag,]
}
print("Group Levels:")
print(grpn)
x
}




# ----------------------------------------------------------------------------

# mat2list

# ----------------------------------------------------------------------------

mat2list<-function(m,grp.dat){
#
#  For data in a matrix m, divide the data into groups based
#  on the values in column indicated
#  by the argument grp.dat
#  and store the data in list mode.
#
# This function is like fac2list, only it handles matrices
#
# Example: z=mat2list(m[,2:5],m[,9])
# will divide the rows of data in columns 2-5 into groups based
# on the group id data in column 9
# This is done via the function mat2grp
#
# z[[1]] will contain the data in m[,2:5] that is associated with first group
# z[[2]] will contain the data in m[,2:5] that is associated with second group, etc.
#
# If any entry in grp.dat is NA, this row is eliminated from m
#
if(!is.null(dim(m)))m=as.matrix(m)
if(!is.matrix(m))stop("Data must be stored in a matrix or data frame")
p=ncol(m)
p1=p+1
M=cbind(m,grp.dat)
#print(dim(M))
x<-mat2grp(M[,1:p1],p1)
for(i in 1:length(x))x[[i]]=x[[i]][,1:p]
x
}

MAT2list<-function(x,J=NULL,p=NULL){
#
# Store the data in a matrix or data frame in a new
# R variable having list mode.
# The results are stored in y, having list mode
# Col 1 to p of x will be stored as a matrix in  y[[1]],
# Col p+1 to 2p are stored  in y[[2]], and so on.
#
# The function assumes ncol(x)=J*P
# either J, the number of groups, or p, the number of variables,
# must be specified.
#
#  This function is used by the R function linconMpb when testing
#  hypotheses about linear contrasts based on multivariate data.
#
if(is.null(dim(x)))stop("The argument x must be a matrix or data frame")
y<-list()
if(is.null(J) && is.null(p))stop("Specify J or P")
if(is.null(J))J=ncol(x)/p
if(is.null(p))p=ncol(x)/J
Jp=floor(J)*floor(p)
if(Jp != ncol(x))stop("Jp is not equal to the number of columns")
lp=1-p
up=0
for(j in 1:J){
lp=lp+p
up=up+p
y[[j]]<-as.matrix(x[,lp:up])
}
y
}
mat2table<-function(x,SP1,SP2){
#
#   Split the data in into groups based on the values indicated by SP1 and SP2 and
#   the counts
#
flag=(x[,1]<=SP1 & x[,2]<=SP2)
n=sum(flag,na.rm=TRUE)
flag=(x[,1]<=SP1 & x[,2]>SP2)
n[2]=sum(flag,na.rm=TRUE)
flag=(x[,1]>SP1 & x[,2]<=SP2)
n[3]=sum(flag,na.rm=TRUE)
flag=(x[,1]>SP1 & x[,2]>SP2)
n[4]=sum(flag,na.rm=TRUE)
m=matrix(n,2,2,byrow=TRUE)
dimnames(m)=list(c('V1.less','V1.greater'),c('V2.less','V2.greater'))
m
}




# ----------------------------------------------------------------------------

# matsplit

# ----------------------------------------------------------------------------

matsplit<-function(m,coln=NULL){
#
# Column coln of matrix m is assumed to have a binary variable
# This functions removes rows with missing values
# and then splits m into two matrices based on the values
# in column coln
#
if(is.null(coln))stop("specify coln")
x<-m[,coln]
val<-unique(x)
if(length(val)>2)stop("More than two values detected in specified column")
flag<-(x==val[1])
m1<-m[flag,]
m2<-m[!flag,]
list(m1=m1,m2=m2)
}
mcdcen<-function(x){
#
# Compute MCD measure of location only.
#
res=covmcd(x)$center
res
}




# ----------------------------------------------------------------------------

# mch2num

# ----------------------------------------------------------------------------

mch2num<-function(x){
# convert character, stored in matrix, to numeric data.
m=matrix(NA,nrow=nrow(x),ncol=ncol(x))
for(j in 1:ncol(x))m[,j]=as.numeric(x[,j])
m
}




# ----------------------------------------------------------------------------

# mcnemar.AC

# ----------------------------------------------------------------------------

mcnemar.AC<-function(x, y = NULL,method='AC',alpha=.05){
if (is.matrix(x)) {
        r <- nrow(x)
        if ((r!= 2) || (ncol(x) != 2))
            stop('x must be square with  two rows and columns')
        if (any(x < 0) || anyNA(x))
            stop("all entries of 'x' must be nonnegative and finite")
        DNAME <- deparse(substitute(x))
    }
    else {
        if (is.null(y))
            stop("if 'x' is not a matrix, 'y' must be given")
        if (length(x) != length(y))
            stop('x and y must have the same lengt')
        DNAME <- paste(deparse(substitute(x)), "and", deparse(substitute(y)))
        OK <- complete.cases(x, y)
        x <- as.factor(x[OK])
        y <- as.factor(y[OK])
        r <- nlevels(x)
        if ((r !=2) || (nlevels(y) != 2))
            stop('x and y must have the same number of levels, 2)')
        x <- table(x, y)
    }
x
B=x[1,2]
C=x[2,1]
n=B+C
if(n<5)res='B+C is less than 4'
if(n>=5)res=binom.conf(B,n,method=method,alpha=alpha,pr=FALSE)
res
}




# ----------------------------------------------------------------------------

# mcskew

# ----------------------------------------------------------------------------

mcskew <- function(z)
{
	n=length(z)
	y1=0
	y2=0
	left=0
	right=0
	q=0
	p=0
	eps=0.0000000000001
	z=-z
	xmed=pull(z,n,floor(n/2)+1)
	if (n%%2 == 0)
	{
		xmed=(xmed+pull(z,n,floor(n/2)))/2
	}
	z=z-xmed
	y=-sort(z)
	y1=y[y>-eps]
	y2=y[y<=eps]
	h1=length(y1)
	h2=length(y2)
	left[1:h2]=1
	right[1:h2]=h1
	nl=0
	nr=h1*h2
	knew=floor(nr/2)+1
	IsFound=0
	while ((nr-nl>n) & (IsFound==0))
	{
		weight=0
		work=0
		j=1
		for (i in 1:h2)
		{
			if (left[i]<=right[i])
			{
				weight[j]=right[i]-left[i]+1
				k=left[i]+floor(weight[j]/2)
				work[j]=calwork(y1[k],y2[i],k,i,h1+1,eps)
				j=j+1
			}
		}
		trial=whimed(work,weight,j-1)
		j=1
		for (i in h2:1)
		{
			while ((j<=h1)&(calwork(y1[min(j,h1)],y2[i],j,i,h1+1,eps)>trial))
			{
				j=j+1
			}
			p[i]=j-1
		}
		j=h1
		for (i in 1:h2)
		{
			while ((j>=1)&(calwork(y1[max(j,1)],y2[i],j,i,h1+1,eps)<trial))
			{
				j=j-1
			}
			q[i]=j+1
		}
		sump=sum(p[1:h2])
		sumq=sum(q[1:h2])-h2
		if (knew<=sump)
		{
			right[1:h2]=p[1:h2]
			nr=sump
		}
		else
		{
			if (knew>sumq)
			{
				left[1:h2]=q[1:h2]
				nl=sumq
			}
			else
			{
				medc=trial
				IsFound=1
			}
		}
	}
	if (IsFound==0)
	{work=0
		j=1
		for (i in 1:h2)
		{
			if (left[i]<=right[i])
			{
				for (jj in left[i]:right[i])
				{
					work[j]=0-calwork(y1[jj],y2[i],jj,i,h1+1,eps)
					j=j+1
				}
			}
		}
		medc=0-pull(work,j-1,knew-nl)
	}
	medc
}




# ----------------------------------------------------------------------------

# mcslope

# ----------------------------------------------------------------------------

mcslope<-function(X, Y, con, k){
J=length(X)
slopes<-numeric(J)
covar<-numeric(J)
for(j in 1:J){
tempx<-as.matrix(X[[j]])
tempy<-as.matrix(Y[[j]])
slopes[j]<-lsfit(tempx, tempy)$coef[k] #Slopes for original data
covar[j]<-lsfitNci4(tempx, tempy)$cov[k,k] #original HC4 for coefficient(slope)
}
dif.slopes<-t(con)%*%slopes
o.se<-t(con^2)%*%covar
o.stat<-dif.slopes/sqrt(o.se) #original test statistics
om<-max(abs(o.stat))
om
}




# ----------------------------------------------------------------------------

# mdiflcr

# ----------------------------------------------------------------------------

mdiflcr<-function(m1,m2,tr=.5,nullv=rep(0,ncol(m1)),plotit=TRUE,
SEED=TRUE,pop=1,fr=.8,nboot=600){
#
# For two independent groups, let D=X-Y.
# Let theta_D be median of marginal distributions
# Goal: Test theta_D=0
#
# This is a multivariate analog of Wilcoxon-Mann-Whitney method
# Only alpha=.05 can be used.
#
# When plotting:
# pop=1 Use scatterplot
# pop=2 Use expected frequency curve.
# pop=3 Use adaptive kernel density
#
if(!is.matrix(m1))stop("m1 is not a matrix")
if(!is.matrix(m2))stop("m2 is not a matrix")
if(ncol(m1)!=ncol(m2))stop("number of columns for m1 and m2 are not equal")
n1<-nrow(m1)
n2<-nrow(m2)
if(SEED)set.seed(2)
data1 <- matrix(sample(n1, size = n1 * nboot, replace = T), nrow = nboot)
data2 <- matrix(sample(n2, size = n2 * nboot, replace = T), nrow = nboot)
bcon <- matrix(NA, ncol = ncol(m1), nrow = nboot)
for(j in 1:nboot)bcon[j,]<-mdifloc(m1[data1[j,],],m2[data2[j,],],est=lloc,tr=tr)
tvec<-mdifloc(m1,m2,est=lloc,tr=tr)
tempcen <- apply(bcon, 1, mean)
smat <- var(bcon - tempcen + tvec)
temp <- bcon - tempcen + tvec
bcon <- rbind(bcon, nullv)
dv <- mahalanobis(bcon, tvec, smat)
bplus <- nboot + 1
sig.level <- 1 - sum(dv[bplus] >= dv[1:nboot])/nboot
if(plotit && ncol(m1)==2){
if(pop==2)rdplot(mdif,fr=fr)
if(pop==1){
plot(mdif[,1],mdif[,2],xlab="VAR 1",ylab="VAR 2",type="n")
points(mdif[,1],mdif[,2],pch=".")
points(center[1],center[2],pch="o")
points(0,0,pch="+")
}
if(pop==3)akerdmul(mdif,fr=fr)
}
list(p.value=sig.level,center=tvec)
}

qindbt.sub<-function(isub,x,y,qval){
#
#  Perform regression using x[isub] to predict y[isub]
#  isub is a vector of length n,
#  a bootstrap sample from the sequence of integers
#  1, 2, 3, ..., n
#
#  This function is used by other functions when computing
#  bootstrap estimates.
#
#  regfun is some regression method already stored in R
#  It is assumed that regfun$coef contains the  intercept and slope
#  estimates produced by regfun.  The regression methods written for
#  this  book, plus regression functions in R, have this property.
#
#  x is assumed to be a matrix containing values of the predictors.
#
xmat<-matrix(x[isub,],nrow(x),ncol(x))
regboot<-NA
for(i in 1:length(qval)){
regboot[i]<-qreg(xmat,y[isub],qval[i])$coef[2]
}
regboot
}




# ----------------------------------------------------------------------------

# mean.pred.ci

# ----------------------------------------------------------------------------

mean.pred.ci<-function(M,sd,orig.n,new.n,tr=0,alpha=.05){
#
#
# Generalization of prediction method in Spence & Stanley
# Advances in Methods and Practices in Psychological Science January-March 2024, Vol. 7, No. 1,
#pp. 1-13
#
# M = observed mean or can be a trimmed mean
# tr= amount of trimming
# sd = Winsorized standard deviation, which is the usual standard deviation when t=0
orig.n=orig.n/(1-2*tr)^2
new.n=new.n/(1-2*tr)^2
se=sqrt(sd^2/orig.n+sd^2/new.n)
g=floor(tr*orig.n)
df=orig.n-2*g-1
crit=qt(1-alpha/2,df)
ci=M-crit*se
ci=c(ci,M+crit*se)
ci
}


mee<-function(x,y,alpha=.05){
#
#  For two independent groups, compute a 1-\alpha confidence interval
#  for p=P(X<Y) using Mee's method, which assumes there are no ties.
#  If ties are detected among the pooled observations, a warning message is
#  printed. The type I error probability might exceed the nominal level by
#  an unacceptable amount. Also, mee(x,y) might give  different  results than
#  mee(y,x). (One might reject and the other might not.)
#
x<-x[!is.na(x)]  # Remove missing values from x
y<-y[!is.na(y)]  # Remove missing values from y
xy<-c(x,y)
tiexy<-sum(abs(c(1:length(xy))-sort(rank(xy))))
if(tiexy > 0){print("Warning: Tied values detected")
print("so even if distributions are identical,")
print("P(X<Y) is not necessarily equal to .5")
}
u<-outer(x,y,FUN="<")
p1<-0
p2<-0
for (j in 1:length(y)){
temp<-outer(u[,j],u[,j])
p1<-p1+sum(temp)-sum(u[,j]*u[,j])
}
for (i in 1: length(x)){
temp<-outer(u[i,],u[i,])
p2<-p2+sum(temp)-sum(u[i,]*u[i,])
}
p<-sum(u)/(length(x)*length(y))
p1<-p1/(length(x)*length(y)*(length(x)-1))
p2<-p2/(length(x)*length(y)*(length(y)-1))
b1<-(p1-p^2)/(p-p^2)
b2<-(p2-p^2)/(p-p^2)
A<-((length(x)-1)*b1+1)/(1-1/length(y))+((length(y)-1)*b2+1)/(1-1/length(x))
nhat<-length(x)*length(y)/A
crit<-(qnorm(1-alpha/2))^2/nhat
D<-sqrt(crit*(p*(1-p)+.25*crit))
low<-(p+.5*crit-D)/(1+crit)
hi<-(p+.5*crit+D)/(1+crit)
list(phat=p,ci=c(low,hi))
}




# ----------------------------------------------------------------------------

# meemul

# ----------------------------------------------------------------------------

meemul<-function(x,alpha=.05){
#
#  Perform Mee's method for all pairs of J independent groups.
#  The familywise type I error probability is controlled by using
#  a critical value from the Studentized maximum modulus distribution.
#
#  The data are assumed to be stored in $x$ in list mode.
#  Length(x) is assumed to correspond to the total number of groups, J
#  It is assumed all groups are independent.
#
#  Missing values are automatically removed.
#
#  The default value for alpha is .05. Any other value results in using
#  alpha=.01.
#
if(!is.list(x))stop("Data must be stored in list mode.")
J<-length(x)
CC<-(J^2-J)/2
test<-matrix(NA,CC,5)
for(j in 1:J){
xx<-!is.na(x[[j]])
val<-x[[j]]
x[[j]]<-val[xx]  # Remove missing values
}
dimnames(test)<-list(NULL,c("Group","Group","phat","ci.lower","ci.upper"))
jcom<-0
crit<-smmcrit(200,CC)
if(alpha!=.05)crit<-smmcrit01(200,CC)
alpha<-1-pnorm(crit)
for (j in 1:J){
for (k in 1:J){
if (j < k){
temp<-mee(x[[j]],x[[k]],alpha)
jcom<-jcom+1
test[jcom,1]<-j
test[jcom,2]<-k
test[jcom,3]<-temp$phat
test[jcom,4]<-temp$ci[1]
test[jcom,5]<-temp$ci[2]
}}}
list(test=test)
}

mgvar<-function(m,se=FALSE,op=0,cov.fun=covmve,SEED=TRUE){
#
# Find the center of a scatterplot, add point that
# increases the generalized variance by smallest amount
# continue for all points
# return the generalized variance
#  values corresponding to each point.
# The central values and point(s) closest to it get NA
#
# op=0 find central points using pairwise differences
# op!=0 find central points using measure of location
# used by cov.fun
#
# choices for cov.fun include
# covmve
# covmcd
# tbs (Rocke's measures of location
# rmba (Olive's median ball algorithm)
#
if(op==0)temp<-apgdis(m,se=se)$distance
if(op!=0)temp<-out(m,cov.fun=cov.fun,plotit=FALSE,SEED=SEED)$dis
flag<-(temp!=min(temp))
temp2<-temp
temp2[!flag]<-max(temp)
flag2<-(temp2!=min(temp2))
flag[!flag2]<-F
varvec<-NA
while(sum(flag)>0){
ic<-0
chk<-NA
remi<-NA
for(i in 1:nrow(m)){
if(flag[i]){
ic<-ic+1
chk[ic]<-gvar(rbind(m[!flag,],m[i,]))
remi[ic]<-i
}}
sor<-order(chk)
k<-remi[sor[1]]
varvec[k]<-chk[sor[1]]
flag[k]<-F
}
varvec
}

mgvdep<-function(m,se=FALSE){
#
# Find the center of a scatterplot, add point that
# increases the generalized variance by smallest amount
# continue for all points
# return the MGV depths.
#
# Essentially the same as mgvar which
# determine MGV distances, only here,
# follow convention that deepest points
# have the largest numerical value. Here
# depth of the deepest values equal one.
#
temp<-apgdis(m,se=se)$distance
icen<-ncol(m)
temp3<-order(temp)
chkit<-sum(duplicated(temp[temp3[1:icen]]))
icen<-icen+chkit
flag<-rep(TRUE,length(temp))
flag[temp3[1:icen]]<-FALSE
# set duplicated central values to F
varvec<-0
varvec[!flag]<-NA
while(sum(flag)>0){
ic<-0
chk<-NA
remi<-NA
for(i in 1:nrow(m)){
if(flag[i]){
ic<-ic+1
chk[ic]<-gvar(rbind(m[!flag,],m[i,]))
remi[ic]<-i
}}
sor<-order(chk)
k<-remi[sor[1]]
varvec[k]<-chk[sor[1]]
flag[k]<-F
}
varvec[is.na(varvec)]<-0
varvec<-1/(1+varvec)
varvec
}


miss2na<-function(m,na.val=NULL){
#
# Convert any missing value, indicatd by na.val,
# to NA.
#
#  Example, if 999 is missing value, use miss2na(m,999)
#
if(is.null(na.val))stop("Specify a missing value")
if(is.vector(m)){
if(!is.list(m)){
flag=(m==na.val)
m[flag]=NA
}}
if(is.matrix(m)){
for(j in 1:ncol(m)){
x=m[,j]
flag=(x==na.val)
x[flag]=NA
m[,j]=x
}}
if(is.list(m)){
for(j in 1:length(m)){
x=m[[j]]
flag=(x==na.val)
x[flag]=NA
m[[j]]=x
}}
m
}




# ----------------------------------------------------------------------------

# mjse

# ----------------------------------------------------------------------------

mjse<-function(x,q=.5,na.rm=FALSE){
#
#    Compute the Maritz-Jarrett estimate of the standard error of
#    X sub m, m=[qn+.5]
#    The default value for q is .5
#
if(na.rm)x=elimna(x)
n<-length(x)
m<-floor(q*n+.5)
vec<-seq(along=x)
w<-pbeta(vec/n,m-1,n-m)-pbeta((vec-1)/n,m-1,n-m)  # W sub i values
y<-sort(x)
c1<-sum(w*y)
c2<-sum(w*y*y)
mjse<-sqrt(c2-c1^2)
mjse
}

mlrGtest<-function(x,y,regfun=mlrreg,nboot=100,SEED=TRUE){
#
#  Test hypothesis that all slopes=0  based
#  on some multivariate regression estimator.
#  By default the Rousseeuw et al.
#  multivariate regression estimator (RADA) is used.
#
#  Strategy: Use bootstrap estimate of standard errors followed by
#  Hotelling type test.
#
if(SEED)set.seed(2)
estall=regfun(x,y)$coef
est=as.vector(estall[-1,])
n=nrow(x)
JK=ncol(x)*ncol(y)
vals=matrix(0,nrow=nboot,ncol=JK)
for(i in 1:nboot){
bsam=sample(n,replace=TRUE)
vals[i,]=as.vector(regfun(x[bsam,],y[bsam,])$coef[-1,])
}
Sv=cov(vals)
est=as.matrix(est)
k=1/JK
test <- k * crossprod(est, solve(Sv, est))[1, ]
v1=JK-1
v2=n-JK
pval=1-pf(test,v1,v2)
list(test.stat=test,p.value=pval,est=estall)
}
mlrplot2 <- function(x, Y)
{
# Forward response plot and residual plot for two mbareg estimators.
# Workstation need to activate a graphics
# device with command "X11()" or "motif()."
# R needs command "library(lqs)" if a robust estimator replaces lsfit.
# Advance the view with the right mouse button.
	x <- as.matrix(x)
	out <- mbareg(x, Y)
	bhat <- out$coef
	FIT <- bhat[1] + x %*% bhat[-1]
	par(mfrow = c(2, 2))
	plot(FIT, Y)
	abline(0, 1)
	identify(FIT, Y)
	title("MBA Forward Response Plot")
	RES <- Y - FIT
	plot(FIT, RES)
	identify(FIT, RES)
	title("MBA Residual Plot")
#
        out <- mbalata(x, Y)
	bhat <- out$coef
	FIT <- bhat[1] + x %*% bhat[-1]
	plot(FIT, Y)
	abline(0, 1)
	identify(FIT, Y)
	title("MBALATA Forward Response Plot")
	RES <- Y - FIT
	plot(FIT, RES)
	identify(FIT, RES)
	title("MBALATA Residual Plot")
}


mplot<-
function(x)
{
# Makes a DD plot only using the MDi, the RDi are not used.
	p <- dim(x)[2]
	center <- apply(x, 2, mean)
	cov <- var(x)
	md2 <- mahalanobis(x, center, cov)
	md <- sqrt(md2)
	rd <- md
	const <- sqrt(qchisq(0.5, p))/median(rd)
	rd <- const * rd
	plot(md, rd)
	abline(0, 1)
	identify(md, rd)
}

nav<-
function(alpha = 0.01, k = 5)
{
#gets n(asy var) for the alpha trimmed mean
#and T_(A,n)(k) if errors are N(0,1)
	z <-  - qnorm(alpha)
	den <- 1 - (2 * z * dnorm(z))/(2 * pnorm(z) - 1
		)
	val <- den/(1 - 2 * alpha)
	ntmav <- val + (2 * alpha * z^2)/(1 - 2 * alpha
		)^2
	zj <- k * qnorm(0.75)
	alphaj <- pnorm( - zj)
	alphaj <- ceiling(100 * alphaj)/100
	zj <-  - qnorm(alphaj)
	den <- 1 - (2 * zj * dnorm(zj))/(2 * pnorm(zj) -
		1)
	val <- den/(1 - 2 * alphaj)
	natmav <- val + (2 * alphaj * zj^2)/(1 - 2 *
		alphaj)^2
	return(ntmav, natmav)
}

nltv<-
function(gam = 0.5)
{
# Gets asy var for lts(h) and lta(h) at standard normal
# where h/n -> gam.
	k <- qnorm(0.5 + gam/2)
	den <- gam - 2 * k * dnorm(k)
	ltsv <- 1/den
	tem <- (1 - exp( - (k^2)/2))^2
	ltav <- (2 * pi * gam)/(4 * tem)
	return(ltsv, ltav)
}

oddata<-
function(n = 100, q = 5, theta = 1)
{
# Generates overdispersion (negative binomial) data for loglinear regression.
#
	y <- 1:n
	pr <- 1/(1 + theta)
	beta <- 0 * 1:q
	beta[1:3] <- 1
	alpha <- -2.5
	x <- matrix(rnorm(n * q), nrow = n, ncol = q)
	x <- 0.5 * x + 1
	SP <- alpha + x %*% beta
	y <- rnbinom(n, size = ceiling(exp(SP)), pr)
	list(x = x, y = y)
}

pifclean<-
function(k, gam)
{
	p <- floor(log(3/k)/log(1 - gam))
	list(p = p)
}

mlts<-function(x,y,gamma,ns=500,nc=10,delta=0.01)
{
  d <- dim(x); n <- d[1]; p <- d[2]
  q <- ncol(y)
  h <- floor(n*(1-gamma))+1
  obj0 <- 1e10
  for (i in 1:ns)
  { sorted <- sort(runif(n),na.last = NA,index.return=TRUE)
    istart <- sorted$ix[1:(p+q)]
    xstart <- x[istart,]
    ystart <- y[istart,]
    bstart <- solve(t(xstart)%*%xstart,t(xstart)%*%ystart)
    sigmastart <- (t(ystart-xstart%*%bstart))%*%(ystart-xstart%*%bstart)/q
    for (j in 1:nc)
    { res  <-  y - x %*% bstart
      tres <- t(res)
      dist2 <- colMeans(solve(sigmastart,tres)*tres)
      sdist2 <- sort(dist2,na.last = NA,index.return = TRUE)
      idist2 <- sdist2$ix[1:h]
      xstart <- x[idist2,]
      ystart <- y[idist2,]
      bstart <- solve(t(xstart)%*%xstart,t(xstart)%*%ystart)
      sigmastart <- (t(ystart-xstart%*%bstart))%*%(ystart-xstart%*%bstart)/(h-p)
    }
    obj <- det(sigmastart)
    if (obj < obj0)
    { result.beta <- bstart
      result.sigma <- sigmastart
      obj0 <- obj
    }
  }
  cgamma <- (1-gamma)/pchisq(qchisq(1-gamma,q),q+2)
  result.sigma <- cgamma * result.sigma
  res <- y - x %*% result.beta
  tres<-t(res)
  result.dres <- colSums(solve(result.sigma,tres)*tres)
  result.dres <- sqrt(result.dres)

  qdelta <- sqrt(qchisq(1-delta,q))
  good  <- (result.dres <= qdelta)
  xgood <- x[good,]
  ygood <- y[good,]
  result.betaR <- solve(t(xgood)%*%xgood,t(xgood)%*%ygood)
  result.sigmaR <- (t(ygood-xgood%*%result.betaR)) %*%
    (ygood-xgood%*%result.betaR)/(sum(good)-p)
  cdelta <- (1-delta)/pchisq(qdelta^2,q+2)
  result.sigmaR<-cdelta*result.sigmaR
  resR<-y-x%*%result.betaR
  tresR<-t(resR)
  result.dresR <- colSums(solve(result.sigmaR,tresR)*tresR)
  result.dresR <- sqrt(result.dresR)
  list(beta=result.beta,sigma=result.sigma,dres=result.dres,
    betaR=result.betaR,sigmaR=result.sigmaR,dresR=result.dresR)
}

modgen<-function(p,adz=FALSE){
#
# Used by regpre to generate all models
# p=number of predictors
# adz=T, will add the model where only a measure
# of location is used.
#
#
model<-list()
if(p>5)stop("Current version is limited to 5 predictors")
if(p==1)model[[1]]<-1
if(p==2){
model[[1]]<-1
model[[2]]<-2
model[[3]]<-c(1,2)
}
if(p==3){
for(i in 1:3)model[[i]]<-i
model[[4]]<-c(1,2)
model[[5]]<-c(1,3)
model[[6]]<-c(2,3)
model[[7]]<-c(1,2,3)
}
if(p==4){
for(i in 1:4)model[[i]]<-i
model[[5]]<-c(1,2)
model[[6]]<-c(1,3)
model[[7]]<-c(1,4)
model[[8]]<-c(2,3)
model[[9]]<-c(2,4)
model[[10]]<-c(3,4)
model[[11]]<-c(1,2,3)
model[[12]]<-c(1,2,4)
model[[13]]<-c(1,3,4)
model[[14]]<-c(2,3,4)
model[[15]]<-c(1,2,3,4)
}
if(p==5){
for(i in 1:5)model[[i]]<-i
model[[6]]<-c(1,2)
model[[7]]<-c(1,3)
model[[8]]<-c(1,4)
model[[9]]<-c(1,5)
model[[10]]<-c(2,3)
model[[11]]<-c(2,4)
model[[12]]<-c(2,5)
model[[13]]<-c(3,4)
model[[14]]<-c(3,5)
model[[15]]<-c(4,5)
model[[16]]<-c(1,2,3)
model[[17]]<-c(1,2,4)
model[[18]]<-c(1,2,5)
model[[19]]<-c(1,3,4)
model[[20]]<-c(1,3,5)
model[[21]]<-c(1,4,5)
model[[22]]<-c(2,3,4)
model[[23]]<-c(2,3,5)
model[[24]]<-c(2,4,5)
model[[25]]<-c(3,4,5)
model[[26]]<-c(1,2,3,4)
model[[27]]<-c(1,2,3,5)
model[[28]]<-c(1,2,4,5)
model[[29]]<-c(1,3,4,5)
model[[30]]<-c(2,3,4,5)
model[[31]]<-c(1,2,3,4,5)
}
if(adz){
ic<-length(model)+1
model[[ic]]<-0
}
model
}



Mpca<-function(x,N1=3,N2=2,tol=.001,N2p=10,Nran=50,
Nkeep=10,SEED=TRUE,op.pro=.1,SCORES=FALSE,pval=NULL){
#
# Robust PCA using Marrona's method (2005, Technometrics)
#
# x is an N by m matrix containing data
# N1, N2, N2p, Nran and Nkeep indicate how many
# iterations are used in the various portions of the
# Marrona robust PCA; see Marrona's paper.
#
# op.pro is the maximum  proportion of unexplained
#   variance that is desired. If pval is not specified, will
#  add variables until this proportion is less than op.pro.
#
# pval, if specified, will use p=pval of the m variables only and report
#  the proportion of unexplained variance.
#  The weighted covariance matrix is returned as well.
#
# SCORES=T, scores are reported and return based on the number of
# variables indicated by pval. pval must be specified.
#
# pval not specified, computes proportion of unexplained  variance
# using p=1, 2 ... variables; results returned in
#
scores<-NULL
wt.cov<-NULL
x<-elimna(x)
if(SEED)set.seed(2)
m<-ncol(x)
n<-nrow(x)
bot<-marpca(x,p=0,N1=N1,N2=N2,tol=tol,N2p=N2p,Nran=Nran,Nkeep=Nkeep,SEED=SEED)
bot<-bot$var.op
mn1<-m-1
rat<-1
it<-0
ratval<-NULL
if(is.null(pval)){
ratval<-matrix(nrow=mn1,ncol=2)
dimnames(ratval)<-list(NULL,c("p","pro.unex.var"))
ratval[,1]<-c(1:mn1)
for(it in 1:mn1){
if(rat>op.pro){
temp<-marpca(x,p=it,N1=N1,N2=N2,tol=tol,N2p=N2p,Nran=Nran,Nkeep=Nkeep,
SEED=SEED)
rat<-temp$var.op/bot
ratval[it,2]<-rat
}}}
if(!is.null(pval)){
if(pval>=m)stop("This method assumes pval<ncol(x)")
temp<-marpca(x,p=pval,N1=N1,N2=N2,tol=tol,N2p=N2p,Nran=Nran,Nkeep=Nkeep,
SEED=SEED)
wt.cov<-temp$wt.cov
ratval<-temp$var.op/bot
}
if(SCORES){
if(is.null(pval))stop("When computing scores, need to specify pval")
temp2<-marpca(x,ncol(x))
ev<-eigen(temp2$wt.cov)
ord.val<-order(ev$values)
mn1<-m-pval+1
Bp<-ev$vectors[,ord.val[mn1:m]] #m by m
xmmu<-x
for(j in 1:m)xmmu[,j]<-x[,j]-temp2$wt.mu[j]
scores<-matrix(ncol=pval,nrow=n)
for(i in 1:n)scores[i,]<-t(Bp)%*%as.matrix(xmmu[i,])
}
list(B=temp$B,a=temp$a,var.op=temp$var.op,unexplained.pro.var=ratval,
scores=scores,wt.cov=wt.cov)
}
mrm1way<-function(x,q=.5,grp=NA,bop=FALSE,SEED=TRUE,mop=FALSE){
#  Perform a within groups one-way ANOVA using medians
#
#  If grp specified, do analysis on only the groups in grp.
#  Example: grp=(c(1,4)), compare groups 1 and 4 only.
#
#  bop=F, use non-bootstrap estimate of covariance matrix
#  bop=T, use bootstrap
#
#  mop=T, use usual median, otherwise use single order statistic
#
if(is.data.frame(x))x=as.matrix(x)
if(SEED)set.seed(2)
if(is.matrix(x))x<-listm(x)
K<-length(x) # Number of groups
p<-K
if(is.na(grp[1]))grp<-c(1:p)
x<-x[grp]
if(!is.list(x))stop("Data are not stored in list mode or a matrix")
tmeans<-0
n<-length(x[[1]])
v<-matrix(0,p,p)
if(!mop){
for (i in 1:p)tmeans[i]<-qest(x[[i]],q=q)
if(!bop)v<-covmmed(x,q=q)
if(bop)v<-bootcov(x,pr=FALSE,est=qest,q=q)
}
if(mop){
tmeans[i]<-median(x[[i]])
v<-bootcov(x,pr=FALSE)
}
km1<-K-1
ck<-diag(1,km1,K)
for (i in 1:km1)ck[i,i+1]<-0-1
Qb<-johansp(ck,tmeans,v,n,1,K)
p.value<-Qb$p.value
if(n>=20)p.value<-1-pf(Qb$teststat,K-1,999)
list(test.stat=Qb$teststat,p.value=p.value)
}

mscale<-function(x,del){
#
# Compute the M-scale as used by Marrona
# Technometrics, 2005, 47, 264-273
#
# so it is assumed that values in x have been centered
# (a measure of location has been subtracted from each value)
# and the results squared.
#
#  del is asdefined in Marrona. For principal components, want to reduce
#  to p dimensional data, q=ncol(x)-p, and del is a function of n and q
#
START<-mad(x)
val<-nelderv2(x,1,mscale.sub,START=START,del=del)
val
}

mscale.sub<-function(x,theta,del){
chival<-x/theta[1]
ones<-rep(1,length(x))
vals<-1-(1-chival)^3
chival<-apply(cbind(ones,vals),1,FUN="min")
val<-abs(mean(chival)-del)
val
}

msplit<-function(J,K,data,grp=c(1:p),p=J*K,q=.5){
#  Perform a J by K anova using medians with
#  repeated measures on the second factor. That is, a split-plot design
#  is assumed, with the first factor consisting of independent groups.
#
#  The R variable data is assumed to contain the raw
#  data stored in list mode. data[[1]] contains the data
#  for the first level of both factors: level 1,1.
#  data[[2]] is assumed to contain the data for level 1 of the
#  first factor and level 2 of the second: level 1,2
#  data[[K]] is the data for level 1,K
#  data[[K+1]] is the data for level 2,1, data[2K] is level 2,K, etc.
#
#  It is assumed that data has length JK, the total number of
#  groups being tested, but a subset of the data can be analyzed
#  using grp
#
x<-data
       if(is.matrix(x)) {
                y <- list()
                for(j in 1:ncol(x))
                        y[[j]] <- x[, j]
                data <- y
        }
if(!is.list(data))stop("Data are not stored in list mode or a matrix")
if(p!=length(data)){
print("The total number of groups, based on the specified levels, is")
print(p)
print("The number of groups in data is")
print(length(data))
print("Warning: These two values are not equal")
}
if(p!=length(grp))stop("Apparently a subset of the groups was specified that does not match the total number of groups indicated by the values for J and K.")
tmeans<-0
h<-0
v<-matrix(0,p,p)
klow<-1-K
kup<-0
#for (i in 1:p)tmeans[i]<-median(data[[grp[i]]],na.rm=TRUE)
for (i in 1:p)tmeans[i]<-qest(data[[grp[i]]],q=q)
for (j in 1:J){
h[j]<-length(data[[grp[j]]])
#    h is the  sample size for the jth level of factor A
#   Use covmtrim to determine blocks of squared standard errors and
#   covariances.
klow<-klow+K
kup<-kup+K
sel<-c(klow:kup)
v[sel,sel]<-covmmed(data[grp[klow:kup]],q=q)
}
ij<-matrix(c(rep(1,J)),1,J)
ik<-matrix(c(rep(1,K)),1,K)
jm1<-J-1
cj<-diag(1,jm1,J)
for (i in 1:jm1)cj[i,i+1]<-0-1
km1<-K-1
ck<-diag(1,km1,K)
for (i in 1:km1)ck[i,i+1]<-0-1
#  Do test for factor A
cmat<-kron(cj,ik)  # Contrast matrix for factor A
Qa<-johansp(cmat,tmeans,v,h,J,K)
Qa.siglevel<-1-pf(Qa$teststat,J-1,999)
# Do test for factor B
cmat<-kron(ij,ck)  # Contrast matrix for factor B
Qb<-johansp(cmat,tmeans,v,h,J,K)
Qb.siglevel<-1-pf(Qb$teststat,K-1,999)
# Do test for factor A by B interaction
cmat<-kron(cj,ck)  # Contrast matrix for factor A by B
Qab<-johansp(cmat,tmeans,v,h,J,K)
Qab.siglevel<-1-pf(Qab$teststat,(J-1)*(K-1),999)
list(Qa=Qa$teststat,Qa.siglevel=Qa.siglevel,
Qb=Qb$teststat,Qb.siglevel=Qb.siglevel,
Qab=Qab$teststat,Qab.siglevel=Qab.siglevel)
}




# ----------------------------------------------------------------------------

# MULAOVp

# ----------------------------------------------------------------------------

MULAOVp<-function(x,J=NULL,p=NULL,tr=.2){
#
#  Do Multivariate ANOVA with trimmed means using
#  Johansen's method
#
#  x is assumed to have list mode with J=number of groups
#  x[[j]] is an n_j by p  matrix
#
alval<-c(1:999)/1000
for(i in 1:999){
irem<-i
Qa<-MULtr.anova(x,J=J,p=p,tr=tr,alpha=alval[i])
if(Qa$test.stat>Qa$crit.value)break
}
list(test.stat=Qa$test.stat,p.value=alval[i])
}




# ----------------------------------------------------------------------------

# mulcen.region

# ----------------------------------------------------------------------------

mulcen.region<-function(m,region=.05,plotit=TRUE,est=median,
xlab="VAR 1",ylab="VAR 2",...){
#
#
# m is an n-by-2 matrix
#
#  region=.05 means that the function
#  determine the 1-.05=.95  deepest points and then plots the convex hull
#  containing these points.
#
#  Returns the points that form the convex hull
#
#
m<-as.matrix(m)
est=apply(m,2,est)
if(ncol(m)!=2)stop('Argument m should be a matrix with two columns')
temp<-fdepth(m,plotit=FALSE,center=est)  #Defaults to using the marginal medians
flag=(temp>=qest(temp,region))
xx<-m[flag,]
xord<-order(xx[,1])
xx<-xx[xord,]
temp<-chull(xx)
if(plotit){
plot(m[,1],m[,2],xlab=xlab,ylab=ylab)
lines(xx[temp,])
lines(xx[c(temp[1],temp[length(temp)]),])
}
list(center=est,convex.hull.pts=xx[temp,])
}




# ----------------------------------------------------------------------------

# mulcen.region.MF

# ----------------------------------------------------------------------------

mulcen.region.MF<-function(m,region=.05,est=median){
#
# region=.05 means that the function
#  determine the 1-.05=.95
#  This is done for each formula
#  Assume m is a matrix or data frame having
#  J columns. First two columns first formula, next two columns next formula..
#
#  So J should be an even integer
#
J=ncol(m)
N=J/2
if(N != floor(N))stop('Should have an even number of columns')
region=list()
centers=list()
id=c(-1,0)
for(j in 1:N){
id=id+2
a=mulcen.region(m[,id],plotit=FALSE,est=est)
centers[[j]]=a$center
region[[j]]=a$convex.hull.pts
n=nrow(elimna(m[id]))
n=as.integer(n)
centers[[j]]=c(a$center,n)
names(centers[[j]])=c('V1','V2','N')
}
list(centers=centers,convex.hull.pts=region)
}

#' Compute Confidence Regions for Bivariate Medians in Astigmatism Data
#'
#' @description
#' Computes confidence regions for the median of bivariate astigmatism prediction
#' error data. Creates convex hull confidence regions for each formula pair based
#' on depth-based methods.
#'
#' @param m Matrix or data frame with even number of columns (J). First two columns
#'   represent first formula, next two the second formula, etc.
#' @param Region Confidence level (default: 0.05 for 95% confidence region)
#' @param plotit Logical; if TRUE, plot the confidence regions
#' @param xlab Label for x-axis (default: 'V1')
#' @param ylab Label for y-axis (default: 'V2')
#'
#' @details
#' For each formula (pair of columns), computes a 1-Region confidence region for the
#' bivariate median using depth-based methods. The confidence region is represented
#' as a convex hull. Uses marginal medians as the center.
#'
#' @return
#' List with components:
#' \itemize{
#'   \item \code{centers} - List of center points (medians) for each formula with sample sizes
#'   \item \code{convex.hull.pts} - List of convex hull boundary points for each formula
#' }
#'
#' @seealso \code{\link{oph.astig.meanconvexpoly}}, \code{\link{mulcen.region}}
#'
#' @keywords ophthalmology
#' @export
oph.astig.datasetconvexpoly.median<-function(m,Region=.05,plotit=FALSE,xlab='V1',ylab='V2'){
#
# region=.05 means that the function
#  determine the 1-.05=.95
#  This is done for each formula
#  Assume m is a matrix or data frame having
#  J columns. First two columns first formula, next two columns next formula..
#
#  So J should be an even integer
#
J=ncol(m)
N=J/2
if(N != floor(N))stop('Should have an even number of columns')
region=list()
centers=list()
id=c(-1,0)
for(j in 1:N){
id=id+2
a=mulcen.region(elimna(m[,id]),region=Region,plotit=FALSE,xlab=xlab,ylab=ylab)
centers[[j]]=a$center
region[[j]]=a$convex.hull.pts
n=nrow(elimna(m[id,]))
n=as.integer(n)
centers[[j]]=c(a$center,n)
names(centers[[j]])=c('V1','V2','N')
}
if(plotit){
M=m
if(N>1)par(mfrow=c(2,2))
id=c(-1,0)
for(j in 1:N){
id=id+2
m=M[,id]
m=elimna(m)
m=as.matrix(m)
temp<-fdepth(m,plotit=FALSE)  #Defaults to using the marginal medians
flag=(temp>=qest(temp,Region))
xx<-m[flag,]
xord<-order(xx[,1])
xx<-xx[xord,]
temp<-chull(xx)
plot(m[,1],m[,2],xlab=xlab,ylab=ylab,pch=pch,xlim=c(-1.5,1.5),ylim=c(-1.5,1.5),asp=2/3)
lines(xx[temp,])
lines(xx[c(temp[1],temp[length(temp)]),])
}
par(mfrow=c(1,1))
}
list(centers=centers,convex.hull.pts=region)
}




# ----------------------------------------------------------------------------

# MULNC

# ----------------------------------------------------------------------------

MULNC<-function(x1,x2,alpha=.05,SEED=TRUE,nboot=500,nullval=.5,PV=FALSE,pr=TRUE){
#
#  bivariate analog of Cliff's method, which is an analog of the
#  Wilcoxon--Mann--Whitney test
#
#
#   Let PG=  event that x1 'dominates' x2.
#   That is,  for the ith and jth randomly sampled points
#   x1[i,1]>x2[j,1] and x1[i,2]>x2[j,2]
#    PL=  event that x2 is 'dominates' x1.
#
#  Function returns:
#
#  phat.GT: the estimated probability of event PG
#  phat.LT: the estimated probability of event PL
#  d.ci: confidence interval for Pr(PL)-Pr(PG), the difference between the probabilities of these two events.
#  phat, the estimated probability that the event PL is more likely  than PG
#  phat.ci: confidence interval for the estimand corresponding to phat
#  p.value: testing Pr(PL)=Pr(PG)
#
x1=elimna(x1)
x2=elimna(x2)
n1=nrow(x1)
n2=nrow(x2)
m=matrix(0,nrow=n1,ncol=n2)
if(ncol(x1)!=ncol(x2))stop('x1 and x2 should be matrices with two columns')
if(ncol(x1)!=2)stop('x1 and x2 should be matrices with two columns')
for(i in 1:n1){
m[i,]=(x1[i,1]>x2[,1])*(x1[i,2]>x2[,2])
id=x1[i,1]<x2[,1] & x1[i,2]<x2[,2]
m[i,id]=-1
}
d<-mean(m)
phat<-(1-d)/2
sigdih<-sum((m-d)^2)/(n1*n2-1)
di<-apply(m,2,mean)
dh<-apply(m,1,mean)
s1sq=sum((di-d)^2)/(n1-1)
s2sq=sum((dh-d)^2)/(n2-1)
stil=sum((m-d)^2)/(n1*n2)
sh<-((n1-1)*s1sq+(n2-1)*s2sq+stil)/(n1*n2)
zv<-qnorm(alpha/2)
cu<-(d-d^3-zv*sqrt(sh)*sqrt((1-d^2)^2+zv^2*sh))/(1-d^2+zv^2*sh)
cl<-(d-d^3+zv*sqrt(sh)*sqrt((1-d^2)^2+zv^2*sh))/(1-d^2+zv^2*sh)
d.ci=c(cl,cu)
pci=c((1-cu)/2,(1-cl)/2)

if(n1==n2){   # compute a p-value

alph<-c(1:99)/100
for(i in 1:99){
irem<-i
chkit<-MULNC.sub(x1,x2,alpha=alph[i])$ci
if(chkit[1]>nullval || chkit[2]<nullval)break
}
p.value<-irem/100
if(p.value<=.01){
iup<-(irem+1)/100
alph<-seq(.001,iup,.001)
for(i in 1:length(alph)){
p.value<-alph[i]
chkit<-MULNC.sub(x1,x2,alpha=alph[i])$ci
if(chkit[1]>nullval || chkit[2]<nullval)break
}}
if(p.value<=.001){
alph<-seq(.0001,.001,.0001)
for(i in 1:length(alph)){
p.value<-alph[i]
chkit<-MULNC.sub(x1,x2,alpha=alph[i])$ci
if(chkit[1]>nullval || chkit[2]<nullval)break
}}}

if(n1!=n2){
if(pr){
if(!PV)print('To get a p-value, set PV=TRUE')
}
res=MULNC.Ppb(x1,x2,alpha=alpha,SEED=SEED,nboot=nboot)
pci=res$ci.p
d.ci=res$d.ci
p.value=NULL
if(PV){
alph<-c(1:99)/100
for(i in 1:99){
irem<-i
chkit<-MULNC.Ppb(x1,x2,alpha=alph[i])$ci.p
if(chkit[1]>nullval || chkit[2]<nullval)break
}
p.value<-irem/100
if(p.value<=.01){
iup<-(irem+1)/100
alph<-seq(.001,iup,.001)
for(i in 1:length(alph)){
p.value<-alph[i]
chkit<-MULNC.Ppb(x1,x2,alpha=alph[i])$ci.p
if(chkit[1]>nullval || chkit[2]<nullval)break
}}
}
}
list(n1=n1,n2=n2,d=d,d.ci=d.ci,sq.se=sh,phat.GT=mean(m==1),phat.LT=mean(m==-1),phat=phat,ci=pci,p.value=p.value)
}




# ----------------------------------------------------------------------------

# MULNC.int

# ----------------------------------------------------------------------------

MULNC.int<-function(J,K,x,x1=NULL,x2=NULL,x3=NULL,x4=NULL,alpha=.05,plotit=TRUE,SEED=TRUE,pr=TRUE){
#
#  Rank-based multiple comparisons for all interactions when dealing with bivariate data.
#  in J by K design. The method is based on an
#  extension of the Patel-Hoel definition of no interaction.
#
#  The familywise type I error probability is controlled by using
#  a critical value from the Studentized maximum modulus distribution.
#
#  It is assumed all groups are independent.
#
#  Missing values are automatically removed.
#
#  x is assumed to have list mode, x[[1]]... x[[JK]] contain bivariate data stored in an  n-by-2 matrix
#
#    Consider a 2-by-2 design.
#   Let PG=  event that x1 is 'dominates' x2.
#   That is,  for the ith and jth randomly sampled points
#   x1[i,1]>x2[j,1] and x1[i,2]>x2[j,2]
#    PL=  event that x2 is 'dominates' x1.
#    Let P1= Pr(PL)-Pr(PG), the difference between the probabilities of these two events.
#    Define P2 in an analogous fashion for x3 and x4
#   no interaction is taken to mean P1=P2.

#
if(!is.null(x1)){
x=list(x1,x2,x3,x4)
J=2
K=2
}
if(!is.list(x))stop('Data for each group must be stored  in list mode.')
p=J*K
grp=c(1:p)
if(p>4){
if(pr)print('Confidence intervals are adjusted so that the simultaneous probability coverage is approximately 1-alpha')
}
CCJ<-(J^2-J)/2
CCK<-(K^2-K)/2
CC<-CCJ*CCK
test<-matrix(NA,CC,8)
test.p<-matrix(NA,CC,7)
nv=NA
for(j in 1:p){
if(ncol(x[[j]])!=2)stop('One or more groups do not contain bivariate data')
x[[j]]=elimna(x[[j]])
nv[j]=length(x[[j]])
}
if(var(nv)!=0)stop('Unequal sample sizes detected, use MULNCpb.int instead')
mat<-matrix(grp,ncol=K,byrow=TRUE)
dimnames(test)<-list(NULL,c('Factor A','Factor A','Factor B','Factor B','delta','ci.lower','ci.upper','p.value'))
jcom<-0
crit<-smmcrit(200,CC)
if(alpha!=.05)crit<-smmcrit01(200,CC)
alpha<-1-pnorm(crit)
for (j in 1:J){
for (jj in 1:J){
if (j < jj){
for (k in 1:K){
for (kk in 1:K){
if (k < kk){
jcom<-jcom+1
test[jcom,1]<-j
test[jcom,2]<-jj
test[jcom,3]<-k
test[jcom,4]<-kk
temp1<-MULNC.sub(x[[mat[j,k]]],x[[mat[j,kk]]])
temp2<-MULNC.sub(x[[mat[jj,k]]],x[[mat[jj,kk]]])
delta=temp2$d-temp1$d
sqse<-temp1$sq.se+temp2$sq.se
test[jcom,5]<-delta/2
test[jcom,6]<-delta/2-crit*sqrt(sqse/4)
test[jcom,7]<-delta/2+crit*sqrt(sqse/4)
test[jcom,8]=2*(1-pnorm(abs((delta/2)/sqrt(sqse/4))))
}}}}}}
list(test=test)
}
MULNC.sub<-function(x1,x2,alpha=.05){
#
#  bivariate analog of Cliff's method, which is an analog of the
#  Wilcoxon--Mann--Whitney test
#
#
#   Let PG=  event that x1 is 'greater than' x2. That is,  for the ith and jth randomly sampled points
#   x1[i,1]>x2[j,1] and x1[i,2]>x2[j,2]
#    PL=  event that x1 is 'less than' x2.
#
#  Function returns:
#
#  phat.GT: the estimated probability of event PG
#  phat.LT: the estimated probability of event PL

#  phat, the estimated probability that the event PL is more likely  than PG
#
x1=elimna(x1)
x2=elimna(x2)
n1=nrow(x1)
n2=nrow(x2)
m=matrix(0,nrow=n1,ncol=n2)
if(ncol(x1)!=ncol(x2))stop('x1 and x2 should be matrices with two columns')
if(ncol(x1)!=2)stop('x1 and x2 should be matrices with two columns')
for(i in 1:n1){
m[i,]=(x1[i,1]>x2[,1])*(x1[i,2]>x2[,2])
id=x1[i,1]<x2[,1] & x1[i,2]<x2[,2]
m[i,id]=-1
}
d<-mean(m)
phat<-(1-d)/2
sigdih<-sum((m-d)^2)/(n1*n2-1)
di<-apply(m,2,mean)
dh<-apply(m,1,mean)
s1sq=sum((di-d)^2)/(n1-1)
s2sq=sum((dh-d)^2)/(n2-1)
stil=sum((m-d)^2)/(n1*n2)
sh<-((n1-1)*s1sq+(n2-1)*s2sq+stil)/(n1*n2)
zv<-qnorm(alpha/2)
cu<-(d-d^3-zv*sqrt(sh)*sqrt((1-d^2)^2+zv^2*sh))/(1-d^2+zv^2*sh)
cl<-(d-d^3+zv*sqrt(sh)*sqrt((1-d^2)^2+zv^2*sh))/(1-d^2+zv^2*sh)
pci=c((1-cu)/2,(1-cl)/2)
list(n1=n1,n2=n2,d=d,d.ci=c(cl,cu),sq.se=sh,phat.GT=mean(m==1),phat.LT=mean(m==-1),phat=phat,ci.p=pci)
}




# ----------------------------------------------------------------------------

# MULNC.Ppb

# ----------------------------------------------------------------------------

MULNC.Ppb<-function(x1,x2,alpha=.05,SEED=TRUE,nboot=500){
#
#  (X_1, X_2) is said to  partially dominate
#  (Y_1, Y_2) if simultaneously X_1 > Y_1 and X_2 < Y_2 or if
#  X_1 < Y_1 and X_2 > Y_2.
#
#   Let PG=  event that simulatensouly X_1 > Y_1 and X_2 < Y_2
#       PL= event thatsimulatensouly X_1 < Y_1 and X_2 > Y_2
#
#  Function returns:
#
#  phat1: the estimated probability of event PG
#  phat2: the estimated probability of event PL

#  phat, the estimated probability that the event PL is more likely  than PG
#  ci.p = A confidence interval for the estimand corresponding to phat
#
if(SEED)set.seed(2)
x1=elimna(x1)
x2=elimna(x2)
n1=nrow(x1)
n2=nrow(x2)
d=NA
ci.mat=matrix(NA,nrow=nboot,ncol=2)
d.mat=matrix(NA,nrow=nboot,ncol=2)
n=min(c(n1,n2))
for(i in 1:nboot){
id1=sample(n,n,replace=TRUE)
id2=sample(n,n,replace=TRUE)
v=MULNC.sub(x1[id1,],x2[id2,],alpha=alpha)
ci.mat[i,]=v$ci
d.mat[i,]=v$d.ci
}
v=MULNC.sub(x1,x2)
ci=mean(ci.mat[,1])
ci[2]=mean(ci.mat[,2])
dci=mean(d.mat[,1])
dci[2]=mean(d.mat[,2])
list(n1=n1,n2=n2,phat.GT=v$phat.GT,phat.LT=v$phat.LT,d.ci=dci,phat=v$phat,ci.p=ci)
}




# ----------------------------------------------------------------------------

# MULNCpb

# ----------------------------------------------------------------------------

MULNCpb<-function(x1,x2,alpha=.05,SEED=TRUE,nboot=500){
#
#  bivariate analog of Cliff's method, which is an analog of the
#  Wilcoxon--Mann--Whitney test
#
#
#   Let PG=  event that x1 is 'greater than' x2. That is,  for the ith and jth randomly sampled points
#   x1[i,1]>x2[j,1] and x1[i,2]>x2[j,2]
#    PL=  event that x1 is 'less than' x2.
#
#  Function returns:
#
#  phat.GT: the estimated probability of event PG
#  phat.LT: the estimated probability of event PL

#  phat, the estimated probability that the event PL is more likely  than PG
#
if(SEED)set.seed(2)
x1=elimna(x1)
x2=elimna(x2)
n1=nrow(x1)
n2=nrow(x2)
d=NA
for(i in 1:nboot){
id1=sample(n1,n1,replace=TRUE)
id2=sample(n2,n2,replace=TRUE)
d[i]=MULNC.sub(x1[id1,],x2[id2,])$phat
}
low=round(alpha*nboot/2)
up=nboot-low
low=low+1
ds=sort(d)
ci=ds[low]
ci[2]=ds[up]
pv=mean(ds<.5)+.5*mean(ds==.5)
pv=2*min(c(pv,1-pv))
phat=MULNC.sub(x1,x2)$phat
list(n1=n1,n2=n2,phat=phat,ci.p=ci,p.value=pv)
}




# ----------------------------------------------------------------------------

# MULNCpb.int

# ----------------------------------------------------------------------------

MULNCpb.int<-function(J,K,x,x1=NULL,x2=NULL,x3=NULL,x4=NULL,alpha=.05,plotit=TRUE,nboot=500,SEED=TRUE,method='hoch'){
#
#  Rank-based multiple comparisons for all interactions when dealing with bivariate data
#  in a J-by-K design. The method is based on an
#  extension of  the Patel-Hoel definition of no interaction.
#
#  It is assumed all groups are independent.
#
#  Missing values are automatically removed.
#
#  x is assumed to have list mode, x[[1]]... x[[JK]] contain bivariate data stored in an  n-by-2 matrix
#
#  For a 2-by-2 design, data can be stored in the arguments
#  x1, x2, x3, x4
#  where each of these arguments is an n-by-2 matrix.
#
#    Consider a 2-by-2 design.
#   Let PG=  event that x1 is 'dominates' x2.
#   That is,  for the ith and jth randomly sampled points
#   x1[i,1]>x2[j,1] and x1[i,2]>x2[j,2]
#    PL=  event that x2 is 'dominates' x1.
#    Let P1= Pr(PL)-Pr(PG), the difference between the probabilities of these two events.
#    Define P2 in an analogous fashion for x3 and x4
#   no interaction is taken to mean P1=P2.

#
if(!is.null(x1)){
x=list(x1,x2,x3,x4)
J=2
K=2
}
if(!is.list(x))stop('Data for each group must be stored  in list mode.')
if(SEED)set.seed(2)
p=J*K
grp=c(1:p)
CCJ<-(J^2-J)/2
CCK<-(K^2-K)/2
CC<-CCJ*CCK
test<-matrix(NA,CC,9)
test.p<-matrix(NA,CC,7)
for(j in 1:p){
if(ncol(x[[j]])!=2)stop('One or more groups do not contain bivariate data')
x[[j]]=elimna(x[[j]])
}
mat<-matrix(grp,ncol=K,byrow=TRUE)
dimnames(test)<-list(NULL,c('Factor A','Factor A','Factor B','Factor B','phat','ci.lower','ci.upper','p.value','adjusted p-value'))
jcom<-0
low=round(alpha*nboot/2)+1
up=nboot-low
for (j in 1:J){
for (jj in 1:J){
if (j < jj){
for (k in 1:K){
for (kk in 1:K){
if (k < kk){
jcom<-jcom+1
test[jcom,1]<-j
test[jcom,2]<-jj
test[jcom,3]<-k
test[jcom,4]<-kk
d=NA
for(b in 1:nboot){
n1=nrow(x[[mat[j,k]]])
n2=nrow(x[[mat[j,kk]]])
n3=nrow(x[[mat[jj,k]]])
n4=nrow(x[[mat[jj,kk]]])
X1=x[[mat[j,k]]]
X2=x[[mat[j,kk]]]
X3=x[[mat[jj,k]]]
X4=x[[mat[jj,kk]]]
id1=sample(n1,n1,replace=TRUE)
id2=sample(n2,n2,replace=TRUE)
id3=sample(n3,n3,replace=TRUE)
id4=sample(n4,n4,replace=TRUE)
d[b]=MULNC.sub(X1[id1,],X2[id2,])$phat-MULNC.sub(X3[id3,],X4[id4,])$phat
}
d=sort(d)
ci=d[low+1]
ci[2]=d[up]
pv=mean(d<0)
pv=2*min(c(pv,1-pv))
test[jcom,5]=MULNC.sub(X1,X2)$phat-MULNC.sub(X3,X4)$phat
test[jcom,6]<-ci[1]
test[jcom,7]<-ci[2]
test[jcom,8]<-pv
}}}}}}
test[,9]=p.adjust(test[,8],method=method)
list(test=test)
}

mulquant<-function(x,q=c(1:9)/10,HD=TRUE,type=8){
#
#  Estimate multiple quantiles for the data in vector x
# By default estimate the deciles
#  HD=TRUE: use the Harrell-Davis estimate of the qth quantile
#   HD=FALSE:use R function quantile
#
x=elimna(x)
nq=length(q)
if(HD){
xs<-sort(x)
n<-length(x)
vecx<-seq(along=x)
xq<-0
for (i in 1:nq){
m1<-(n+1)*q[i]
m2<-(n+1)*(1-q[i])
wx<-pbeta(vecx/n,m1,m2)-pbeta((vecx-1)/n,m1,m2)  # W sub i values
xq[i]<-sum(wx*xs)
}}
if(!HD){
xq=quantile(x,probs=q,type=type)
}
xq
}

matbin2v<-binmat2v<-function(m,col=c(1,2),int1=c(.5,.5),int2=c(.5,.5),INC=TRUE){
#
# pull out the rows of the matrix m based on the values in the column
# indicated by the argument
#  int1 indicates intervals for first variable
#  int2 indicates intervals for second variable
#  By default,  split at the median for both variables.

# col  indicates the columns of m by which the splits are made.

#
#  Example: binmat(m,c(1,3),c(10,15),c(2:6)) will return all rows such that the
#  values in column 1 are between 10 and 15, inclusive.
#  values in col 15 are between 2 and 5
#
if(is.null(m))stop('First argument should be a matrix with two or more columns')
if(ncol(m)==1)stop('First argument should be a matrix with two or more columns')
if(INC){
flag1=m[,col[1]]>=int1[1]
flag2=m[,col[1]]<=int1[2]
flag3=m[,col[2]]>=int2[1]
flag4=m[,col[2]]<=int2[2]
}
if(!INC){
flag1=m[,col[1]]>int1[1]
flag2=m[,col[1]]<int1[2]
flag3=m[,col[2]]>int2[1]
flag4=m[,col[2]]<int2[2]
}
flag=as.logical(flag1*flag2*flag3*flag4)
m[flag,]
}



MULR.yhat<-function(x,y,pts=x,regfun=MULMreg,
xout=FALSE,outfun=outpro,...){
#
# Compute predicted Y values based on some multivariate regression
# estimator.
#
#  Use MULMreg by default
#
#
x<-as.matrix(x)
y<-as.matrix(y)
n.keep=nrow(x)
if(xout){
x<-as.matrix(x)
flag<-outfun(x,plotit=FALSE)$keep
x<-x[flag,]
y<-y[flag,]
x<-as.matrix(x)
n.keep=nrow(x)
}
if(is.null(pts))pts=x
q=ncol(y)
p1=ncol(x)+1
yhat=matrix(NA,nrow=nrow(pts),ncol=q)
coef=regfun(x,y)$coef
slope=as.matrix(coef[2:p1,])
for(j in 1:q){
for(i in 1:nrow(pts)){
yhat[i,j]=coef[1,j]+sum(slope[,j]*x[i,])
}}
list(yhat=yhat)
}
corCOMmcp_sub<-function(data,x,y,corfun=wincor,...){
#
#
rv=NA
for(j in 1:ncol(x))rv[j]=corfun(x[data,j],y[data])$cor
rv
}
mulrank<-function(J,K,x,grp=c(1:p),p=J*K){
#
# Perform the Munzel and Brunner
# multivariate one-way rank-based ANOVA
# (Munzel and Brunner, Biometrical J., 2000, 42, 837--854
#
# x can be a matrix with columns corresponding to groups
#
# Have a J by K design with J independent levels and K dependent
# measures
#
# or it can have list mode.
#
newx=list()
GV=matrix(c(1:p),ncol=K,byrow=TRUE)
if(is.list(x)){
temp=NA
jk=0
for(j in 1:J){
temp=elimna(matl(x[GV[j,]]))
for(k in 1:K){
jk=jk+1
newx[[jk]]=temp[,k]
}}
x=NA
x=newx
}
if(is.matrix(x)){
x=elimna(x)
x<-listm(x)
}
xx<-list()
nvec<-NA
for(j in 1:p){
xx[[j]]<-x[[grp[j]]]
nvec[j]<-length(xx[[j]])
}
Nrow=nvec[GV[,1]]
v<-matrix(0,p,p)
Ja<-matrix(1,J,J)
Ia<-diag(1,J)
Pa<-Ia-Ja/J
Jb<-matrix(1,K,K)
Ib<-diag(1,K)
Pb<-Ib-Jb/K
cona<-kron(Pa,Ib)
xr<-list()
N<-0
jj=0
for(k in 1:K){
temp<-x[[k]]
jk<-k
for (j in 2:J){
jj=jj+1
jk<-jk+K
temp<-c(temp,x[[jk]])
}
N<-length(temp)
pr<-rank(temp)
xr[[k]]<-pr[1:nvec[k]] #Put ranks of pooled data for first
#                       variable in xr
top<-nvec[k]
jk<-k
bot<-1
for (j in 2:J){
jk<-jk+K
bot<-bot+nvec[jk]
top<-top+nvec[jk]
xr[[jk]]<-pr[bot:top] # Put midranks in xr
}}
phat<-NA
botk<-0
for(j in 1:J){
for(k in 1:K){
botk<-botk+1
phat[botk]<-(mean(xr[[botk]])-.5)/N
}}
klow<-1-K
kup<-0
for(j in 1:J){
klow<-klow+K
kup<-kup+K
sel<-c(klow:kup)
v[sel,sel]<-covmtrim(xr[klow:kup],tr=0)/N
}
qhat<-matrix(phat,J,K,byrow=TRUE)
test<-N*t(phat)%*%cona%*%phat/sum(diag(cona%*%v))
nu1<-sum(diag(cona%*%v))^2/sum(diag(cona%*%v%*%cona%*%v))
sig.level<-1-pf(test,nu1,1000000)
list(test.stat=test[1,1],nu1=nu1,p.value=sig.level,N=N,q.hat=qhat)
}


multsm<-function(x,y,pts=x,fr=.5,xout=FALSE,outfun=outpro,plotit=TRUE,pr=TRUE,
xlab='X',ylab='Prob',ylab2='Y',zlab='Prob',ticktype='det',vplot=NULL,scale=TRUE,
L=TRUE,...){
#
#
# A smoother for multinomial regression based on logSM
#
# Example: Assuming x is a vector, and possible values
# for y are 0,1 and 2.
#  multsm(x,y,c(-1,0,1))
#  This would  estimate
# P(Y=0|x=-1), P(Y=1|x=-1), P(Y=2|x=-1), P(Y=0|x=0), etc.
#
# Returns estimates of the probabilities associated with
# each possible value of y given a value for independent variable that is stored in pts
#
#  vplot indicates the value of the dependent variable for which  probabilities will be plotted.
#  vplot=1 means that the first largest value will be used.
#  By default, vplot=NULL meaning that all values of y will when there is  a single independent variable.
#
#  vplot=c(1,3)   means that the first and third values will be used. If the first value is 5, say and the third is 8,
#  plot P(y=5|pts) and P(Y=8|pts)
#  For more than one independent variable, the first value in vplot is used only. If no value is specified, the smallest y value is used.
#
# scale =TRUE is the default and is relevant when plotting and  there are two dependent variables. See the function lplot.
#
#
#  L=TRUE: for p=2, use LOESS (lplot) to plot the regression surface; otherwise use a running interval smoother (rplot).
#
# VALUE:
# For each value in pts, returns the probabilities for each of the y values.
#
if(pr){
if(!xout)print('Suggest also looking at  result using xout=TRUE')
}
xy=cbind(x,y)
xy=elimna(xy)
p1=ncol(xy)
p=p1-1
if(p==1)pts=sort(pts)
x=xy[,1:p]
y=xy[,p1]
x=as.matrix(x)
if(xout){
flag=outfun(x,plotit=FALSE,...)$keep
x=x[flag,]
y=y[flag]
}
x=standm(x,est=median,scar=madsq)
rempts=pts
pts=standm(pts,est=median,scar=madsq)
n=length(y)
temp<-sort(unique(y))
nv=length(temp)
nv1=nv+1
if(p==1){
x=as.matrix(x)
pts=sort(pts)
}
pts=as.matrix(pts)
res=matrix(NA,nrow=nrow(pts),ncol=nv)
lab=NULL
for(k in 1:nv){
est=logSMpred(x,y==temp[k],pts=pts)
res[,k]=est
lab=c(lab,paste('Y value',temp[k]))
}
dimnames(res)=list(NULL,lab)
if(plotit){
if(is.null(vplot))vplot=c(1:nv)
#vplot=vplot+1  # adjustment to match col of res
if(p==1){
nlines=min(ncol(res),6)
nlines=nlines-1
plot(c(rep(rempts,length(vplot))),c(as.vector(res[,vplot])),type='n',xlab=xlab,ylab=ylab,ylim=c(0,1))
for(k in 1:length(vplot))lines(rempts,res[,vplot[k]],lty=k)
}
if(p>1){
if(p==2){
if(L)lplot(rempts,res[,vplot[1]],xlab=xlab,ylab=ylab2,zlab=zlab,ticktype=ticktype,scale=scale,pr=FALSE)
if(!L)rplot(rempts,res[,vplot[1]],xlab=xlab,ylab=ylab2,zlab=zlab,ticktype=ticktype,scale=scale,pr=FALSE)
}
}}
list(estimates=res,pts=pts)
}



MULtsreg<-function(x,y,tr=.2,RMLTS=TRUE){
# Multivariate Least Trimmed Squares Estimator
# Input:
#   x: data-matrix (n,p)
#   y: data-matrix (n,q)
#   tr: proportion of trimming
#   This function calls an R function written by Kristel Joossens
#
# Output:
#     If MLTS=T coef: matrix (p,q) of MLTS-regression coefficients
#     IF MLTS=F betaR : matrix (p,q) of RMLTS-regression coefficients
#
# Ref: Agullo,J., Croux, C., and Van Aelst, S. (2008)
#      The Multivariate Least Trimmed Squares Estimator,
#      Journal of multivariate analysis, 99, 311-338.
#
x=as.matrix(x)
xy=elimna(cbind(x,y))
xx=as.matrix(cbind(rep(1,nrow(xy)),xy[,1:ncol(x)]))
p1=ncol(x)+1
y=as.matrix(xy[,p1:ncol(xy)])
outp=mlts(xx,y,tr)
if(!RMLTS)coef=outp$beta
if(RMLTS)coef=outp$betaR
list(coef=coef)
}




# ----------------------------------------------------------------------------

# mulwmw.dist.new

# ----------------------------------------------------------------------------

mulwmwv2<-function(m1,m2,plotit=TRUE,cop=3,alpha=.05,nboot=1000,pop=4,fr=.8,pr=FALSE){
#
# same as function mulwmw, only report explanatory effect size.
#
# Determine center correpsonding to two
# independent groups, project all  points onto line
# connecting the centers,
# then based on the projected distances,
# estimate p=probability that a randomly sampled
# point from group 1 is less than a point from group 2
# based on the projected distances.
#
#
# plotit=TRUE creates a plot of the projected data
# pop=1 plot two dotplots based on projected distances
# pop=2 boxplots
# pop=3 expected frequency curve.
# pop=4 adaptive kernel density
#
#  There are three options for computing the center of the
#  cloud of points when computing projections:
#  cop=1 uses Donoho-Gasko median
#  cop=2 uses MCD center
#  cop=3 uses median of the marginal distributions.
#
#  When using cop=2 or 3, default critical value for outliers
#  is square root of the .975 quantile of a
#  chi-squared distribution with p degrees
#  of freedom.
#
#  Donoho-Gasko (Tukey) median is marked with a cross, +.
#
#if(is.matrix(m1)){print("Data are assumed to be stored in")
if(is.null(dim(m1))){print("Data are assumed to be stored in")
print(" a matrix or data frame having two or more columns.")
stop(" For univariate data, use the function outbox or out")
}
m1<-elimna(m1) # Remove missing values
m2<-elimna(m2)
n1v=nrow(m1)
n2v=nrow(m2)
if(cop==1){
if(ncol(m1)>2){
center1<-dmean(m1,tr=.5)
center2<-dmean(m2,tr=.5)
}
if(ncol(m1)==2){
tempd<-NA
for(i in 1:nrow(m1))
tempd[i]<-depth(m1[i,1],m1[i,2],m1)
mdep<-max(tempd)
flag<-(tempd==mdep)
if(sum(flag)==1)center1<-m1[flag,]
if(sum(flag)>1)center1<-apply(m1[flag,],2,mean)
for(i in 1:nrow(m2))
tempd[i]<-depth(m2[i,1],m2[i,2],m2)
mdep<-max(tempd)
flag<-(tempd==mdep)
if(sum(flag)==1)center2<-m2[flag,]
if(sum(flag)>1)center2<-apply(m2[flag,],2,mean)
}}
if(cop==2){
center1<-cov.mcd(m1)$center
center2<-cov.mcd(m2)$center
}
if(cop==3){
center1<-apply(m1,2,median)
center2<-apply(m2,2,median)
}
if(cop==4){
center1<-smean(m1)
center2<-smean(m2)
}
center<-(center1+center2)/2
B<-center1-center2
if(sum(center1^2)<sum(center2^2))B<-(0-1)*B
BB<-B^2
bot<-sum(BB)
disx<-NA
disy<-NA
if(bot!=0){
for (j in 1:nrow(m1)){
AX<-m1[j,]-center
tempx<-sum(AX*B)*B/bot
disx[j]<-sign(sum(AX*B))*sqrt(sum(tempx^2))
}
for (j in 1:nrow(m2)){
AY<-m2[j,]-center
tempy<-sum(AY*B)*B/bot
disy[j]<-sign(sum(AY*B))*sqrt(sum(tempy^2))
}
}
es=yuenv2(disx,disy)$Effect.Size
if(plotit){
if(pop==1){
par(yaxt="n")
xv<-rep(2,length(disx))
yv<-rep(1,length(disy))
plot(c(disx,disy),c(xv,yv),type="n",xlab="",ylab="")
xv<-rep(1.6,length(disx))
yv<-rep(1.4,length(disy))
points(disx,xv)
points(disy,yv)
#par(yaxt="c")
}
if(pop==2)boxplot(disx,disy)
if(pop==3)rd2plot(disx,disy,fr=fr)
if(pop==4)g2plot(disx,disy,fr=fr)
}
m<-outer(disx,disy,FUN="-")
m<-sign(m)
phat<-(1-mean(m))/2
if(bot==0)phat<-.5
print("Computing critical values")
m1<-t(t(m1)-center1)
m2<-t(t(m2)-center2)
v1<-mulwmwcrit(m1,m2,cop=cop,alpha=alpha,iter=nboot,pr=pr)
list(phat=phat,lower.crit=v1[1],upper.crit=v1[2],Effect.Size=abs(es),n1=n1v,n2=n2v)
}
mve.cor<-function(x,y){
xy=cbind(x,y)
a=MVECOR(x=xy)[1,2]
list(cor=a)
}

mvecen<-function(x){
#
# Compute MCD measure of location only.
#
res=covmve(x)$center
res
}

near3dl1<-function(x,pt,fr=1,m){
dis<-mahalanobis(x,pt,m$cov)
sdis<-sqrt(sort(dis))
dflag<-(dis < fr & dis > sdis[3])
dflag
}

nearl<-function(x,pt,fr=1){
# determine which values in x are near and less than pt
# based on fr * mad
m<-mad(x)
if(m==0){
temp<-idealf(x)
m<-(temp$qu-temp$ql)/(qnorm(.75)-qnorm(.25))
}
if(m==0)m<-sqrt(winvar(x)/.4129)
if(m==0)stop("All measures of dispersion are equal to 0")
dis<-abs(x-pt)
dflag<-dis <= fr*m
flag2<-(x<pt)
dflag<-dflag*flag2
dflag
}




# ----------------------------------------------------------------------------

# nearNN

# ----------------------------------------------------------------------------

nearNN<-function(x,pt=x,K=10,mcov,...){
#
# identify the K rows in x that are closest to  vector in pt
# mcov: some type of covariance	matrix associated with x
#
if(!is.matrix(x)& !is.data.frame(x))stop('Data are not stored in a matrix or data frame.')
pt=as.vector(pt)
x=elimna(x)
n=nrow(pts)
if(K>nrow(x))stop(' Cannot have K>n')
dis=sqrt(mahalanobis(x,t(pt),mcov))
chk.dup=sum(duplicated(dis))
if(chk.dup>0)dis=jitter(dis)
ord=sort(dis)
id=which(dis<=ord[K])
id
}

nearr<-function(x,pt,fr=1){
# determine which values in x are near and less than pt
# based on fr * mad
m<-mad(x)
if(m==0){
temp<-idealf(x)
m<-(temp$qu-temp$ql)/(qnorm(.75)-qnorm(.25))
}
if(m==0)m<-sqrt(winvar(x)/.4129)
if(m==0)stop("All measures of dispersion are equal to 0")
dis<-abs(x-pt)
dflag<-dis <= fr*m
flag2<-(x>pt)
dflag<-dflag*flag2
dflag
}
neg.colM<-function(x,id=NULL){
#
#  Columns of the matrix are mutliplied	by -1
#
x[,id]=x[,id]*-1
x
}




# ----------------------------------------------------------------------------

# nelder

# ----------------------------------------------------------------------------

nelder<-function(x,N,FN,START=c(rep(1,N)),STEP=c(rep(1,N)),
XMIN=c(rep(0,N)),XSEC=c(rep(0,N))){
#     NELDER-MEAD method for minimzing a function
#
#     TAKEN FROM OLSSON, J QUALITY TECHNOLOGY, 1974, 6, 56.
#
#     x= n by p matrix containing data; it is used by
#        function to be minimized.
#     N= number of parameters
#
#     FN=the function to be minimized
#     FORM: FN(x,theta), theta is vector containing
#     values for N parameters.
#
#     START = starting values.
#     STEP=initial step.
#     This function returns the N values for theta that minimize FN
#
      ICOUNT<-500
      REQMIN<-.0000001
      NN<-N+1
      P<-matrix(NA,nrow=N,ncol=NN)
      P[,NN]<-START
      PBAR<-NA
      RCOEFF<-1
      ECOEFF<-2
      CCOEFF<-.5
      KCOUNT<-ICOUNT
      ICOUNT<-0
      DABIT<-2.04067e-35
      BIGNUM<-1.e38
      KONVGE<-5
      XN<-N
      DN<-N
      Y<-rep(0,NN)
      Y[NN]<-FN(x,START)
      ICOUNT<-ICOUNT+1
      for(J in 1:N){
      DCHK<-START[J]
      START[J]<-DCHK+STEP[J]
      for(I in 1:N){
      P[I,J]<-START[I]
}
      Y[J]<-FN(x,START)
      ICOUNT<-ICOUNT+1
      START[J]<-DCHK
}
      I1000<-T
       while(I1000){
      YLO<-Y[1]
      YNEWLO<-YLO
      ILO<-1
      IHI<-1
      for(I in 2:NN){
      if(Y[I] <  YLO){
      YLO<-Y[I]
      ILO<-I}
      if(Y[I] > YNEWLO){
      YNEWLO<-Y[I]
      IHI<-I}
}
      DCHK<-(YNEWLO+DABIT)/(YLO+DABIT)-1
      if(abs(DCHK) < REQMIN){
      I1000<-F
      next
}
      KONVGE<-KONVGE-1
      if(KONVGE == 0){
      KONVGE<-5
      for(I in 1:N){
      COORD1<-P[I,1]
      COORD2<-COORD1
      for(J in 2:NN){
      if(P[I,J] < COORD1)COORD1<-P[I,J]
      if(P[I,J] > COORD2)COORD2<-P[I,J]
}     # 2010 CONTINUE
      DCHK<-(COORD2+DABIT)/(COORD1+DABIT)-1
      if(abs(DCHK) > REQMIN)break
}
}
     if(ICOUNT >= KCOUNT){
      I1000<-F
      next
}
      for(I in 1:N){
      Z<-0.0
      Z<-sum(P[I,1:NN]) # 6
      Z<-Z-P[I,IHI]
  PBAR[I]<-Z/DN
}
    PSTAR<-(1.+RCOEFF)*PBAR-RCOEFF*P[,IHI]
      YSTAR<-FN(x,PSTAR)
      ICOUNT<-ICOUNT+1
      if(YSTAR < YLO && ICOUNT >= KCOUNT){
       P[,IHI]<-PSTAR
       Y[IHI]<-YSTAR
       next
}
  IFLAG<-T
      if(YSTAR < YLO){
    P2STAR<-ECOEFF*PSTAR+(1-ECOEFF)*PBAR
      Y2STAR<-FN(x,P2STAR)
      ICOUNT<-ICOUNT+1
      if(Y2STAR >= YSTAR){
       P[,IHI]<-PSTAR
       Y[IHI]<-YSTAR
       next #In essence, go to 19 which goes to 1000
}
      IFLAG<-T
      while(YSTAR < Y[IHI]){
      P[,IHI]<-P2STAR
      Y[IHI]<-Y2STAR
      IFLAG<-F
     break
     L<-sum(Y[1:NN] > YSTAR)
      if(L > 1){
       P[,IHI]<-PSTAR
       Y[IHI]<-YSTAR
       IFLAG<-T
       break
}
       if(L > 1)break # go to 19
      if(L != 0){
      P[1:N,IHI]<-PSTAR[1:N]
      Y[IHI]<-YSTAR
}
I1000<-F
break
  if(ICOUNT >= KCOUNT){
      I1000<-F
      next
}
   P2STAR[1:N]<-CCOEFF*P[1:N,IHI]+(1-CCOEFF)*PBAR[1:N]
      Y2STAR<-FN(x,P2STAR)
      ICOUNT<-ICOUNT+1
}   # END WHILE
}
if(IFLAG){
for(J in 1:NN){
P[,J]<-(P[,J]+P[,ILO])*.5
   XMIN<-P[,J]
      Y[J]<-FN(x,XMIN)
}
      ICOUNT<-ICOUNT+NN
      if(ICOUNT < KCOUNT)next
      I1000<-F
next
}
      P[1:N,IHI]<-PSTAR[1:N]
      Y[IHI]<-YSTAR
}
    for(J in 1:NN){
      XMIN[1:N]<-P[1:N,J]
}
      Y[J]<-FN(x,XMIN)
      YNEWLO<-BIGNUM
  for(J in 1:NN){
      if (Y[J] < YNEWLO){
      YNEWLO<-Y[J]
      IBEST<-J
}}
      Y[IBEST]<-BIGNUM
      YSEC<-BIGNUM
for(J in 1:NN){
if(Y[J] < YSEC){
      YSEC<-Y[J]
      ISEC<-J
}}
      XMIN[1:N]<-P[1:N,IBEST]
      XSEC[1:N]<-P[1:N,ISEC]
XMIN
}




# ----------------------------------------------------------------------------

# nelderv2

# ----------------------------------------------------------------------------

nelderv2<-function(x,N,FN,START=c(rep(1,N)),STEP=c(rep(1,N)),REQMIN=.0001,
XMIN=c(rep(0,N)),XSEC=c(rep(0,N)),...){
#     NELDER-MEAD method for minimzing a function
#
#     TAKEN FROM OLSSON, J QUALITY TECHNOLOGY, 1974, 6, 56.
#
#     x= n by p matrix containing data; it is used by
#        function to be minimized.
#     N= number of parameters
#
#     FN=the function to be minimized
#     FORM: FN(x,theta), theta is vector containing
#     values for N parameters.
#
#     START = starting values.
#     STEP=initial step.
#     This function returns the N values for theta that minimize FN
#
      ICOUNT<-500
      NN<-N+1
      P<-matrix(NA,nrow=N,ncol=NN)
      P[,NN]<-START
      PBAR<-NA
      RCOEFF<-1
      ECOEFF<-2
      CCOEFF<-.5
      KCOUNT<-ICOUNT
      ICOUNT<-0
      DABIT<-2.04067e-35
      BIGNUM<-1.e38
      KONVGE<-5
      XN<-N
      DN<-N
      Y<-rep(0,NN)
      Y[NN]<-FN(x,START,...)
      ICOUNT<-ICOUNT+1
      for(J in 1:N){
      DCHK<-START[J]
      START[J]<-DCHK+STEP[J]
      for(I in 1:N){
      P[I,J]<-START[I]
}
      Y[J]<-FN(x,START,...)
      ICOUNT<-ICOUNT+1
      START[J]<-DCHK
}
      I1000<-TRUE
       while(I1000){
      YLO<-Y[1]
      YNEWLO<-YLO
      ILO<-1
      IHI<-1
      for(I in 2:NN){
      if(Y[I] <  YLO){
      YLO<-Y[I]
      ILO<-I}
      if(Y[I] > YNEWLO){
      YNEWLO<-Y[I]
      IHI<-I}
}
      DCHK<-(YNEWLO+DABIT)/(YLO+DABIT)-1
      if(abs(DCHK) < REQMIN){
      I1000<-FALSE
      next
}
      KONVGE<-KONVGE-1
      if(KONVGE == 0){
      KONVGE<-5
      for(I in 1:N){
      COORD1<-P[I,1]
      COORD2<-COORD1
      for(J in 2:NN){
      if(P[I,J] < COORD1)COORD1<-P[I,J]
      if(P[I,J] > COORD2)COORD2<-P[I,J]
}     # 2010 CONTINUE
      DCHK<-(COORD2+DABIT)/(COORD1+DABIT)-1
      if(abs(DCHK) > REQMIN)break
}
}
     if(ICOUNT >= KCOUNT){
      I1000<-F
      next
}
      for(I in 1:N){
      Z<-0.0
      Z<-sum(P[I,1:NN]) # 6
      Z<-Z-P[I,IHI]
  PBAR[I]<-Z/DN
}
    PSTAR<-(1.+RCOEFF)*PBAR-RCOEFF*P[,IHI]
      YSTAR<-FN(x,PSTAR,...)
      ICOUNT<-ICOUNT+1
      if(YSTAR < YLO && ICOUNT >= KCOUNT){
       P[,IHI]<-PSTAR
       Y[IHI]<-YSTAR
       next
}
  IFLAG<-TRUE
      if(YSTAR < YLO){
    P2STAR<-ECOEFF*PSTAR+(1-ECOEFF)*PBAR
      Y2STAR<-FN(x,P2STAR,...)
      ICOUNT<-ICOUNT+1
      if(Y2STAR >= YSTAR){
       P[,IHI]<-PSTAR
       Y[IHI]<-YSTAR
       next #In essence, go to 19 which goes to 1000
}
      IFLAG<-TRUE
      while(YSTAR < Y[IHI]){
      P[,IHI]<-P2STAR
      Y[IHI]<-Y2STAR
      IFLAG<-FALSE
     break
     L<-sum(Y[1:NN] > YSTAR)
      if(L > 1){
       P[,IHI]<-PSTAR
       Y[IHI]<-YSTAR
       IFLAG<-TRUE
       break
}
       if(L > 1)break # go to 19
      if(L != 0){
      P[1:N,IHI]<-PSTAR[1:N]
      Y[IHI]<-YSTAR
}
I1000<-FALSE
break
  if(ICOUNT >= KCOUNT){
      I1000<-FALSE
      next
}
   P2STAR[1:N]<-CCOEFF*P[1:N,IHI]+(1-CCOEFF)*PBAR[1:N]
      Y2STAR<-FN(x,P2STAR,...)
      ICOUNT<-ICOUNT+1
}   # END WHILE
}
if(IFLAG){
for(J in 1:NN){
P[,J]=(P[,J]+P[,ILO])*.5
   XMIN<-P[,J]
      Y[J]<-FN(x,XMIN,...)
}
      ICOUNT<-ICOUNT+NN
      if(ICOUNT < KCOUNT)next
      I1000<-F
next
}
      P[1:N,IHI]<-PSTAR[1:N]
      Y[IHI]<-YSTAR
}
    for(J in 1:NN){
      XMIN[1:N]<-P[1:N,J]
}
      Y[J]<-FN(x,XMIN,...)
      YNEWLO<-BIGNUM
  for(J in 1:NN){
      if (Y[J] < YNEWLO){
      YNEWLO<-Y[J]
      IBEST<-J
}}
      Y[IBEST]<-BIGNUM
      YSEC<-BIGNUM
for(J in 1:NN){
if(Y[J] < YSEC){
      YSEC<-Y[J]
      ISEC<-J
}}
      XMIN[1:N]<-P[1:N,IBEST]
      XSEC[1:N]<-P[1:N,ISEC]
XMIN
}




# ----------------------------------------------------------------------------

# NMpca

# ----------------------------------------------------------------------------

NMpca<-function(x,B,...){
#
# Robust PCA using orthogonal matrices and
# robust generalized variance method
# This function is used by Ppca
#
n<-x[1]
m<-x[2]
p=x[3]
x=matrix(x[4:length(x)],ncol=m)
B=matrix(B,ncol=m)
vals<-NA
z<-matrix(nrow=n,ncol=p)
B <- t(ortho(t(B))) # so rows are orthogonal
for(i in 1:n)z[i,]<-B%*%as.matrix(x[i,])
vals<-0-gvarg(z)
vals
}

normTmm<-function(x,SEED=TRUE,nboot=2000){
#
# Test that the tails of the distribution of x
# have more outliers than expected under normality
#
if(SEED)set.seed(45)
no=out(x,SEED=FALSE)$n.out
val=NA
x=elimna(x)
n=length(x)
for(i in 1:nboot)val[i]=out(rnorm(n),SEED=FALSE)$n.out
list(n.out=no,p.value=mean(val>=no))
}

ODDSR.CI<-function(x,y=NULL,alpha=.05){
#
#  Compute confidence interval of the odds ratio.
#
#  x is either a two-by-two contingency table or a
#  vector of 0's and 1's, in which case
#  y is also a  vector of 0's and 1's
#
# if x is a 2-by-2 matrix, assume col 1 is X=1, col 2 is X=0
# row 1 is Y=1 and row 2 is Y=0.
#
if(is.matrix(x)){
if(ncol(x)!=2)stop("If x is a matrix, should have 2 columns")
if(nrow(x)!=2)stop("If x is a matrix, should have 2 rows")
n=sum(x)
x1=rep(1,x[1,1])
y1=rep(1,x[1,1])
x2=rep(0,x[1,2])
y2=rep(1,x[1,2])
x3=rep(1,x[2,1])
y3=rep(0,x[2,1])
x4=rep(0,x[2,2])
y4=rep(0,x[2,2])
x=c(x1,x2,x3,x4)
y=c(y1,y2,y3,y4)
}
temp=logreg(x,y)
z=qnorm(1-alpha/2)
ci=c(exp(temp[2,1]-z*temp[2,2]),exp(temp[2,1]+z*temp[2,2]))
list(odds.ratio=exp(temp[2,1]),ci=ci)
}

ogk<-function(x,sigmamu=taulc,v=gkcov,n.iter=1,beta=.9,...){
#
# Compute robust (weighted) covariance matrix in Maronna and Zamar
# (2002, Technometrics, eq. 7).
#
# x is an n by p matrix
# n.iter number of iterations. 1 seems to be best
# sigmamu is any user supplied function having the form
#   sigmamu(x,mu.too=F) and which computes a robust measure of
#   of dispersion if mu.too=F. If mu.too=T, it returns
#   a robust measure of location as well.
# v is any robust covariance
#
if(!is.matrix(x))stop("x should be a matrix")
x<-elimna(x)  # remove any rows with missing data
temp<-ogk.pairwise(x,sigmamu=sigmamu,v=v,n.iter=n.iter,beta=beta,...)
list(center=temp$wcenter,cov=temp$wcovmat)
}




# ----------------------------------------------------------------------------

# OGK

# ----------------------------------------------------------------------------

OGK<-function(x,niter=2,beta=.9,control){
#
#  OGK estimator via the R package rrcov
#
library(rrcov)
v=CovOgk(x,niter=niter,beta=beta,control)
list(center=v@center,cov=v@cov)
}




# ----------------------------------------------------------------------------

# ogk.pairwise

# ----------------------------------------------------------------------------

ogk.pairwise <- function(X,n.iter=1,sigmamu=taulc,v=gkcov,beta=.9,...)
#weight.fn=hard.rejection,beta=.9,...)
{
# Downloaded (and modified slightly) from www.stats.ox.ac.uk/~konis/pairwise.q
# Corrections noted by V. Todorov have been incorporated
#
  data.name <- deparse(substitute(X))
  X <- as.matrix(X)
  n <- dim(X)[1]
  p <- dim(X)[2]
  Z <- X
  U <- diag(p)
  A <- list()
  # Iteration loop.
  for(iter in 1:n.iter) {
    # Compute the vector of standard deviations d and
    # the correlation matrix U.
    d <- apply(Z, 2, sigmamu, ...)
    Z <- sweep(Z, 2, d, '/')

    for(i in 1:(p - 1)) {
      for(j in (i + 1):p) {
        U[j, i] <- U[i, j] <- v(Z[ , i], Z[ , j], ...)
      }
    }

    # Compute the eigenvectors of U and store them in
    # the columns of E.

    E <- eigen(U, symmetric = TRUE)$vectors

    # Compute A, there is one A for each iteration.

    A[[iter]] <- d * E

    # Project the data onto the eigenvectors.

    Z <- Z %*% E
  }

  # End of orthogonalization iterations.

  # Compute the robust location and scale estimates for
  # the transformed data.

#  sqrt.gamma <- apply(Z, 2, sigmamu, mu.too = TRUE, ...)
  sqrt.gamma <- apply(Z, 2, sigmamu, mu.too = TRUE)
  center <- sqrt.gamma[1, ]
  sqrt.gamma <- sqrt.gamma[2, ]

  # Compute the mahalanobis distances.

  Z <- sweep(Z, 2, center)
  Z <- sweep(Z, 2, sqrt.gamma, '/')
  distances <- rowSums(Z^2)

  # From the inside out compute the robust location and
  # covariance matrix estimates.  See equation (5).

  covmat <- diag(sqrt.gamma^2)

  for(iter in seq(n.iter, 1, -1)) {
    covmat <- A[[iter]] %*% covmat %*% t(A[[iter]])
    center <- A[[iter]] %*% center
  }

  center <- as.vector(center)

  # Compute the reweighted estimate.  First, compute the
  # weights using the user specified weight function.

  #weights <- weight.fn(distances, p, ...)
weights <- hard.rejection(distances, p, beta=beta,...)
  sweights <- sum(weights)

  # Then compute the weighted location and covariance
  # matrix estimates.

  wcenter <- colSums(sweep(X, 1, weights, '*')) / sweights

  Z <- sweep(X, 2, wcenter)
  Z <- sweep(Z, 1, sqrt(weights), '*')
  wcovmat <- (t(Z) %*% Z) / sweights;

  list(center = center,
       covmat = covmat,
       wcenter = wcenter,
       wcovmat = wcovmat,
       distances = distances,
       sigmamu = deparse(substitute(sigmamu)),
       v = deparse(substitute(v)),
       data.name = data.name,
       data = X)
}




# ----------------------------------------------------------------------------

# ols.pred.ci

# ----------------------------------------------------------------------------

ols.pred.ci<-function(x,y,xlab="X",ylab="Y",alpha=.05,xout=FALSE,RETURN=FALSE,newx=NULL){
#
# plot the ols regression line and a 1-alpha
# confidence interval for the predicted values
#
#  RETURN=T means the function will return predicted values and
#  and confidence interval for the x values indicated by the argument
#  newx
#  newx=NULL, means predicted Y will be for seq(min(x), max(x), 0.1)
#
# xout=T removes leverage points.
#
if(ncol(as.matrix(x))!=1)stop("One predictor is allowed")
xy=elimna(cbind(x,y))
x=xy[,1]
y=xy[,2]
if(xout){
flag=out(x)$keep
x=x[flag]
y=y[flag]
}
tmp.lm=lm(y~x)
if(is.null(newx))newx=seq(min(x), max(x), 0.1)
a=predict(tmp.lm,interval="confidence",level=1-alpha,newdata=data.frame(x=newx))
plot(x,y,xlab=xlab,ylab=ylab)
abline(ols(x,y,plotit=FALSE)$coef)
lines(newx,a[,2],lty=2)
lines(newx,a[,3],lty=2)
res=NULL
if(RETURN)res=a
res
}

ols.ridge<-function(x,y,k=NULL,xout=FALSE,outfun=outpro,MSF=TRUE){
x=as.matrix(x)
if(ncol(x)==1)stop('Should have two or more independent variables.')
xy=elimna(cbind(x,y))
x=as.matrix(x)
p=ncol(x)
p1=p+1
x=xy[,1:p]
y=xy[,p1]
x=as.matrix(x)
if(xout){
flag<-outfun(x)$keep
x<-x[flag,]
x<-as.matrix(x)
y<-y[flag]
}
if(is.null(k)){
if(!MSF)k=ridge.est.k(x,y)
else{
ires=ols(x,y)$residuals
sigh=sqrt(sum(ires^2)/(n-p-1))
k=p^(1+1/p)*sigh
}
}
a=lm.ridge(y~x,lambda=k)
a=coef(a)
list(coef=a)
}




# ----------------------------------------------------------------------------

# ols1way

# ----------------------------------------------------------------------------

ols1way<-function(x,y,xout=FALSE,outfun=outpro,STAND=TRUE,
alpha=.05,pr=TRUE,BLO=FALSE,HC3=FALSE,...){
#
#  Test hypothesis that for two or more independent groups, all regression parameters
#  (the intercepts and slopes) are equal
#  using OLS  estimator.
#
#  (To compare slopes only, use ols1way2g)
#
#  Strategy: Use HC4 or HC3 estimate of standard errors followed by
#  Johansen MANOVA type test statistic.
#
#  x and y are assumed to have list mode having length J equal to the number of groups
#  For example, x[[1]] and y[[1]] contain the data for group 1.
#
#   xout=T will eliminate leverage points using the function outfun,
#   which defaults to the MVE method.
#
#   BLO=TRUE, only bad leverage points are removed.
#
#  OUTPUT:
#   n is sample size after missing values are removed
#   nv.keep is sample size after leverage points are removed.
#
if(!is.list(x))stop('Argument x should have list mode')
J=length(x) # number of groups
x=lapply(x,as.matrix)
pchk=lapply(x,ncol)
temp=matl(pchk)
if(var(as.vector(temp))!=0)stop('Something is wrong. Number of covariates differs among the groups being compared')
nv=NULL
p=ncol(x[[1]])
p1=p+1
for(j in 1:J){
xy=elimna(cbind(x[[j]],y[[j]]))
x[[j]]=xy[,1:p]
y[[j]]=xy[,p1]
x[[j]]=as.matrix(x[[j]])
nv=c(nv,nrow(x[[j]]))
}
nv.keep=nv
critrad=NULL
if(BLO)xout=FALSE
if(xout){
temp1=lapply(x,outfun,plotit=FALSE,STAND=STAND,...)
for(j in 1:J){
x[[j]]=x[[j]][temp1[[j]]$keep,]
y[[j]]=y[[j]][temp1[[j]]$keep]
}}
if(BLO){
for(j in 1:J){
temp1=reglev(x[[j]],y[[j]],plotit=FALSE)
ad1=c(temp1$levpoints,temp1$regout)
flag1=duplicated(ad1)
if(sum(flag1)>0){
flag1=ad1[flag1]
x[[j]]=as.matrix(x[[j]])
x[[j]]=x[[j]][-flag1,]
y[[j]]=y[[j]][-flag1]
}}}
x=lapply(x,as.matrix)
K=p1
est=matrix(NA,nrow=J,ncol=p1)
grpnum=NULL
for(j in 1:J)grpnum[j]=paste("Group",j)
vlabs="Intercept"
for(j in 2:p1)vlabs[j]=paste("Slope",j-1)
dimnames(est)=list(grpnum,vlabs)
ecov=list()
ecovinv=list()
W=rep(0,p1)
gmean=rep(0,p1)
for(j in 1:J){
est[j,]=ols(x[[j]],y[[j]],xout=FALSE,plotit=FALSE,...)$coef
nv.keep[j]=nrow(x[[j]])
ecov[[j]]=olshc4(x[[j]],y[[j]],HC3=HC3)$cov
ecovinv[[j]]=solve(ecov[[j]])  #W_j
gmean=gmean+ecovinv[[j]]%*%est[j,]
W=W+ecovinv[[j]]
}
estall=solve(W)%*%gmean
F=0
for(k in 1:K){
for(m in 1:K){
for(j in 1:J){
F=F+ecovinv[[j]][k,m]*(est[j,k]-estall[k])*(est[j,m]-estall[m])
}}}
pvalad=NULL
df=K*(J-1)
iden=diag(p1)
Aw=0
for(j in 1:J){
temp=iden-solve(W)%*%ecovinv[[j]]
tempsq=temp%*%temp
Aw=Aw+(sum(diag(tempsq))+(sum(diag(temp)))^2)/(nv[j]-1)
}
Aw=Aw/2
crit=qchisq(alpha,df)
crit=crit+(crit/(2*df))*(Aw+3*Aw*crit/(df+2))
alval<-c(1:999)/1000
for(i in 1:999){
irem<-i
crit=qchisq(alval[i],df)
critad=crit+(crit/(2*df))*(Aw+3*Aw*crit/(df+2))
if(F<critad)break
pvalad=1-irem/1000
}
#
pval=1-pchisq(F,df)
crit=qchisq(1-alpha,df)
critad=NULL
critad=crit+(crit/(2*df))*(Aw+3*Aw*crit/(df+2))
est=data.frame(est)
list(n=nv,n.keep=nv.keep,test.stat=F,crit.value=crit,adjusted.crit=critad,p.value=pval,adjusted.p.value=pvalad,est=est)
}




# ----------------------------------------------------------------------------

# ols1way2g

# ----------------------------------------------------------------------------

ols1way2g<-function(x,y,grp=c(1,2),iv=1,xout=FALSE,outfun=outpro,STAND=TRUE,alpha=.05,pr=TRUE,BLO=FALSE,...){
#
#  Test hypothesis that for two or more independent groups, all slope parameters
#  are equal using OLS  estimator.
#
#  (ols1way tests the hypothesis that all parameters are equal,	not just slopes.)
#
#  Use Johansen MANOVA type test statistic in conjunction with HC4 estimate of covariances.
#
#  x and y are assumed to have list mode having length J equal to the number of groups
#  For example, x[[1]] and y[[1]] contain the data for group 1.
#
#   xout=T will eliminate leverage points using the function outfun,
#   which defaults to the MVE method.
#
#   BLO=TRUE, only bad leverage points are removed.
#
#  OUTPUT:
#   n is sample size after missing values are removed
#   nv.keep is sample size after leverage points are removed.
#
if(!is.list(x))stop('Argument x should have list mode')
iv1=iv+1
J=length(x) # number of groups
x=lapply(x,as.matrix)
pchk=lapply(x,ncol)
temp=matl(pchk)
if(var(as.vector(temp))!=0)stop('Something is wrong. Number of covariates differs among the groups being compared')
nv=NULL
p=ncol(x[[1]])
p1=p+1
for(j in 1:J){
xy=elimna(cbind(x[[j]],y[[j]]))
x[[j]]=xy[,1:p]
y[[j]]=xy[,p1]
x[[j]]=as.matrix(x[[j]])
nv=c(nv,nrow(x[[j]]))
}
nv.keep=nv
critrad=NULL
if(BLO)xout=FALSE
if(xout){
temp=lapply(x,outfun,plotit=FALSE,STAND=STAND,...)
for(j in 1:J){
x[[j]]=x[[j]][temp[[j]]$keep,]
y[[j]]=y[[j]][temp[[j]]$keep]
}}
if(BLO){
for(j in 1:J){
temp1=reglev(x[[j]],y[[j]],plotit=FALSE)
ad1=c(temp1$levpoints,temp1$regout)
flag1=duplicated(ad1)
if(sum(flag1)>0){
flag1=ad1[flag1]
x[[j]]=as.matrix(x[[j]])
x[[j]]=x[[j]][-flag1,]
y[[j]]=y[[j]][-flag1]
}}}
x=lapply(x,as.matrix)
K=p1
est=matrix(NA,nrow=J,ncol=p1)
grpnum=NULL
for(j in 1:J)grpnum[j]=paste("Group",j)
vlabs="Intercept"
for(j in 2:p1)vlabs[j]=paste("Slope",j-1)
dimnames(est)=list(grpnum,vlabs)
ecov=list()
ecovinv=list()
W=rep(0,p1)
gmean=rep(0,K)
for(j in 1:J){
est[j,]=ols(x[[j]],y[[j]],xout=FALSE,plotit=FALSE,...)$coef
nv.keep[j]=nrow(x[[j]])
ecov[[j]]=olshc4(x[[j]],y[[j]])$cov
ecovinv[[j]]=solve(ecov[[j]])  #W_j
gmean=gmean+ecovinv[[j]]%*%est[j,]
W=W+ecovinv[[j]]
}
estall=solve(W)%*%gmean
F=0
for(j in 1:2){
F=F+ecovinv[[grp[j]]][iv1,iv1]*(est[grp[j],iv1]-estall[iv1])*(est[grp[j],iv1]-estall[iv1])
}
pvalad=NULL
df=1
# Adjust critical value:
iden=1
Aw=0
for(j in 1:J){
temp=iden-solve(W[iv1,iv1])%*%ecovinv[[grp[j]]][iv1,iv1]
tempsq=temp%*%temp
Aw=Aw+(sum(diag(tempsq))+(sum(diag(temp)))^2)/(nv[grp[j]]-1)
}
Aw=Aw/2
crit=qchisq(alpha,df)
crit=crit+(crit/(2*df))*(Aw+3*Aw*crit/(df+2))
alval<-c(1:999)/1000
for(i in 1:999){
irem<-i
crit=qchisq(alval[i],df)
critad=crit+(crit/(2*df))*(Aw+3*Aw*crit/(df+2))
if(F<critad)break
pvalad=1-irem/1000
}
#
pval=1-pchisq(F,df)
if(is.null(pvalad))pvalad=pval
crit=qchisq(1-alpha,df)  #UNADJUSTED CRITICAL VALUE
critad=NULL
critad=crit+(crit/(2*df))*(Aw+3*Aw*crit/(df+2))
est=data.frame(est)
list(n=nv,n.keep=nv.keep,test.stat=F,crit.value=crit,
adjusted.crit=critad,p.value=pval,adjusted.p.value=pvalad,est=est)
}




# ----------------------------------------------------------------------------

# ols1wayISO

# ----------------------------------------------------------------------------

ols1wayISO<-function(x,y,xout=FALSE,outfun=outpro,STAND=TRUE,alpha=.05,pr=TRUE,BLO=FALSE,...){
#
#  Test hypothesis that for two or more independent groups, all slope parameters
#  are equal using OLS  estimator.
#
#  Use Johansen MANOVA type test statistic in conjunction with HC4 estimate of covariances.
#
#  x and y are assumed to have list mode having length J equal to the number of groups
#  For example, x[[1]] and y[[1]] contain the data for group 1.
#
#   xout=T will eliminate leverage points using the function outfun,
#   which defaults to the MVE method.
#
#   BLO=TRUE, only bad leverage points are removed.
#
#  OUTPUT:
#   n is sample size after missing values are removed
#   nv.keep is sample size after leverage points are removed.
#
if(!is.list(x))stop('Argument x should have list mode')
J=length(x) # number of groups
x=lapply(x,as.matrix)
pchk=lapply(x,ncol)
temp=matl(pchk)
if(var(as.vector(temp))!=0)stop('Something is wrong. Number of covariates differs among the groups being compared')
nv=NULL
p=ncol(x[[1]])
p1=p+1
for(j in 1:J){
xy=elimna(cbind(x[[j]],y[[j]]))
x[[j]]=xy[,1:p]
y[[j]]=xy[,p1]
x[[j]]=as.matrix(x[[j]])
nv=c(nv,nrow(x[[j]]))
}
nv.keep=nv
critrad=NULL
if(BLO)xout=FALSE
if(xout){
temp=lapply(x,outfun,plotit=FALSE,STAND=STAND,...)
for(j in 1:J){
x[[j]]=x[[j]][temp[[j]]$keep,]
y[[j]]=y[[j]][temp[[j]]$keep]
}}
if(BLO){
for(j in 1:J){
temp1=reglev(x[[j]],y[[j]],plotit=FALSE)
ad1=c(temp1$levpoints,temp1$regout)
flag1=duplicated(ad1)
if(sum(flag1)>0){
flag1=ad1[flag1]
x[[j]]=as.matrix(x[[j]])
x[[j]]=x[[j]][-flag1,]
y[[j]]=y[[j]][-flag1]
}}}
x=lapply(x,as.matrix)
K=p1
est=matrix(NA,nrow=J,ncol=p1)
grpnum=NULL
for(j in 1:J)grpnum[j]=paste("Group",j)
vlabs="Intercept"
for(j in 2:p1)vlabs[j]=paste("Slope",j-1)
dimnames(est)=list(grpnum,vlabs)
ecov=list()
ecovinv=list()
W=rep(0,p1)
gmean=rep(0,p)
for(j in 1:J){
est[j,]=ols(x[[j]],y[[j]],xout=FALSE,plotit=FALSE,...)$coef
nv.keep[j]=nrow(x[[j]])
ecov[[j]]=olshc4(x[[j]],y[[j]])$cov
ecovinv[[j]]=solve(ecov[[j]])  #W_j
gmean=gmean+ecovinv[[j]][2:K,2:K]%*%est[j,2:K]
W=W+ecovinv[[j]]
}
estall=solve(W[2:K,2:K])%*%gmean
estall=c(0,estall)
F=0
for(k in 2:K){
for(m in 2:K){
for(j in 1:J){
F=F+ecovinv[[j]][k,m]*(est[j,k]-estall[k])*(est[j,m]-estall[m])
}}}
pvalad=NULL
df=p*(J-1)
# Adjust critical value:
iden=diag(p)
Aw=0
for(j in 1:J){
temp=iden-solve(W[2:K,2:K])%*%ecovinv[[j]][2:K,2:K]
tempsq=temp%*%temp
Aw=Aw+(sum(diag(tempsq))+(sum(diag(temp)))^2)/(nv[j]-1)
}
Aw=Aw/2
crit=qchisq(alpha,df)
crit=crit+(crit/(2*df))*(Aw+3*Aw*crit/(df+2))
alval<-c(1:999)/1000
for(i in 1:999){
irem<-i
crit=qchisq(alval[i],df)
critad=crit+(crit/(2*df))*(Aw+3*Aw*crit/(df+2))
if(F<critad)break
pvalad=1-irem/1000
}
#
pval=1-pchisq(F,df)
crit=qchisq(1-alpha,df)  #UNADJUSTED CRITICAL VALUE
critad=NULL
critad=crit+(crit/(2*df))*(Aw+3*Aw*crit/(df+2))
est=data.frame(est)
list(n=nv,n.keep=nv.keep,test.stat=F,crit.value=crit,adjusted.crit=critad,p.value=pval,adjusted.p.value=pvalad,est=est)
}
corbsubMC<-function(isub,x,y,corfun,...){
isub=as.vector(isub)
corbsub<-corfun(x[isub],y[isub],...)$cor
corbsub
}

ols2ci<-function(x1,y1,x2,y2,xout=FALSE,outfun=outpro,STAND=TRUE,alpha=05,
method='hoch',SO=TRUE,HC3=FALSE,plotit=TRUE,xlab='X',ylab='Y',...){
#
# Compare the OLS regression parameters for two independent groups
# SO=TRUE means p-values adjusted only for the slopes. SO=FALSE
#. include the intercept when adjusting
#
p=ncol(as.matrix(x1))
p1=p+1
if(p==1 && plotit)reg2plot(x1,y1,x2,y2,xlab='X',ylab='Y',xout=xout,outfun=outpro,regfun=ols,...)
m1=elimna(cbind(x1,y1))
m2=elimna(cbind(x2,y2))
x1=m1[,1:p]
y1=m1[,p1]
x2=m2[,1:p]
y2=m2[,p1]
x=list()
y=list()
x[[1]]=x1
x[[2]]=x2
y[[1]]=y1
y[[2]]=y2
ivl=c(1:ncol(as.matrix(x1)))
iv=ncol(as.matrix(x1))
iv1=iv+1
rlab=paste('slope',ivl)
rlab=c('intercept',rlab)
res=olsWmcp(x,y,xout=xout,outfun=outfun,STAND=STAND,alpha=alpha,HC3=HC3)
outp=matrix(NA,nrow=nrow(res$output),ncol=7)
dimnames(outp)=list(rlab,c('Est.1','Est.2','Dif','ci.low','ci.up','p.value','adj.p.value'))
print(res)
outp[,1]=ols(x1,y1,xout=xout,outfun=outfun)$coef
outp[,2]=ols(x2,y2,xout=xout,outfun=outfun)$coef
outp[,3]=outp[,1]-outp[,2]
outp[,4]=res$output[,3]
outp[,5]=res$output[,4]
outp[,6]=res$output[,5]
if(!SO)outp[,7]=p.adjust(outp[,6],method=method)
else outp[2:p1,7]=p.adjust(outp[2:p1,6],method=method)
list(n=res$n,output=outp)
}


olshc4.band<-function(x,y,alpha=.05,xout=FALSE,outfun=outpro,plotit=TRUE,pr=TRUE,
xlab='X',ylab='Y',nreps=5000,pch='.',CI=FALSE,ADJ=TRUE,SEED=TRUE){
#
#  Heterocedastic confidence band with simultaneous
#  probability coverate 1-alpha
#
#  CI=TRUE, confidence intervals are returned.
#  CI=FALSE, only a plot is created.
#
#
if(!CI){
if(pr)print('This function returns the confidence intervals when CI=TRUE')
}
xy=elimna(cbind(x,y))
if(ncol(xy)!=2)stop('Only one independent variable allowed')
x=xy[,1]
y=xy[,2]
if(xout){
m<-cbind(x,y)
flag<-outfun(x,plotit=FALSE)$keep
m<-m[flag,]
x<-m[,1]
y<-m[,2]
}
flagdup=duplicated(x)
xu=x[!flagdup]
yu=y[!flagdup]
xs=order(xu)
xu=xu[xs]
yu=yu[xs]
n=length(yu)
df=n-2
nv=c(20,30,40,50,75,100,200,300,500,1000,2000)
cr=c(0.009649481,0.01115032,0.0125336,0.01196315,0.01350826,
0.01346237,0.01326992,0.01291531,0.01465537,0.01488745, 0.01595954)
ysqse=trimse(y,tr=0)^2
temp=olshc4(x,y)
b1sqse=temp$cov[2,2]
yhat=temp$ci[1,2]+temp$ci[2,2]*xu
se=sqrt(ysqse+b1sqse*(xu-mean(xu))^2)
ci=matrix(NA,ncol=4,nrow=length(xu))
ci[,1]=xu
ci[,2]=yhat
if(!ADJ)crit=qt(1-alpha/2,df)
if(ADJ){
adj=NA
if(alpha==.05){
if(n>=20 && n<=2000)adj=lplot.pred(1/nv,cr,1/n)$yhat
if(is.na(adj))adj=olshc4.bandCV(n=n,nreps=nreps,alpha=alpha,SEED=SEED)
}
crit=qnorm(1-adj/2) # Don't need Student's T, adjustment deals with n.
}
ci[,3]=yhat-crit*se
ci[,4]=yhat+crit*se
dimnames(ci)=list(NULL,c('X','Yhat','ci.low','ci.up'))
if(plotit){
plot(x,y,pch=pch,xlab=xlab,ylab=ylab)
abline(temp$ci[1,2],temp$ci[2,2])
lines(xu,ci[,3],lty=2)
lines(xu,ci[,4],lty=2)
}
if(!CI)ci=NA
list(conf.intervals=ci)
}




# ----------------------------------------------------------------------------

# olshc4.bandCV

# ----------------------------------------------------------------------------

olshc4.bandCV<-function(n,nreps=5000,alpha=.05,SEED=TRUE){
#
#  Heterocedastic confidence band with simultaneous
#  probability coverage 1-alpha
#
if(SEED)set.seed(2)
pv=NA
for(i in 1:nreps){
x=rnorm(n)
y=rnorm(n)
ysqse=trimse(y,tr=0)^2
temp=olshc4(x,y)
ysqse=trimse(y,tr=0)^2
b1sqse=temp$cov[2,2]
yhat=temp$ci[1,2]+temp$ci[2,2]*x
se=sqrt(ysqse+b1sqse*(x-mean(x))^2)
pv[i]=min(2*(1-pnorm(abs(yhat)/se)))
}
hd(pv,q=alpha)
}

olshc4band=olshc4.band

olshomci<-function(x,y,alpha=.05){
#
#  Computes confidence interval for the slope of the
#  least squares regression line  assuming homoscedasticity and that there is
#  only one independent variable.
#
# Not recommended in practice; use a method that allows heteroscedasticity
#
if(length(x)!=length(y))stop('x and y have different lengths')
temp=ols(x,y)
df=temp$n-2  #degrees of freedom
df
temp$summary
ci=temp$summar[2,1]-qt(1-alpha/2,df)*temp$summary[2,2]
ci[2]=temp$summar[2,1]+qt(1-alpha/2,df)*temp$summary[2,2]
list(ci=ci)
}
olsJ2<-function(x1,y1,x2,y2,xout=FALSE,outfun=outpro,
STAND=TRUE,plotit=TRUE,xlab="X",ylab="Y",ISO=FALSE,...){
#
#  Test hypothesis that for two independent groups, all regression parameters are equal
#  Least squares regression is used.
#
#  Strategy: Use HC4 estimate of standard errors followed by
#  Johansen type test statistic.
#
#  ISO=TRUE, test slopes, ignoring intercept.
#
x1=as.matrix(x1)
p=ncol(x1)
p1=p+1
xy=elimna(cbind(x1,y1))
x1=xy[,1:p]
y1=xy[,p1]
x2=as.matrix(x2)
p=ncol(x2)
p1=p+1
xy=elimna(cbind(x2,y2))
x2=xy[,1:p]
y2=xy[,p1]
if(plotit){
xx1=x1
yy1=y1
xx2=x2
yy2=y2
if(ncol(as.matrix(x1))==1){
if(xout){
flag=outfun(xx1,plotit=FALSE,...)$keep
xx1=x1[flag]
yy1=y1[flag]
flag=outfun(xx2,plotit=FALSE,...)$keep
xx2=x2[flag]
yy2=y2[flag]
}
plot(c(xx1,xx2),c(yy1,yy2),type="n",xlab=xlab,ylab=ylab)
points(xx1,yy1)
points(xx2,yy2,pch="+")
abline(lsfit(xx1,yy1,...)$coef)
abline(lsfit(xx2,yy2,...)$coef,lty=2)
}}
x=list()
y=list()
x[[1]]=x1
x[[2]]=x2
y[[1]]=y1
y[[2]]=y2
if(!ISO)output=ols1way(x,y,xout=xout,outfun=outfun,STAND=STAND,...)
if(ISO)output=ols1wayISO(x,y,xout=xout,outfun=outfun,STAND=STAND,...)
output
}
olsLmcp<-function(x,y,xout=TRUE,outfun=outpro,ISO=FALSE,STAND=TRUE,alpha=.05,pr=TRUE,BLO=FALSE,HC3=FALSE,...){
#
#  All pairwise comparison of regression models among J independent groups
#  That is, for groups j and k, all j<k, test H_0: all corresponding
#  parameters are equal
#
#  Strategy: Use HC4  estimate of standard errors followed by
#  Johansen type test statistic.
#
#  Hochberg to control FWE
#
#  x and y are assumed to have list mode having length J equal to the number of groups
#  For example, x[[1]] and y[[1]] contain the data for group 1.
#
#   xout=T will eliminate leverage points using the function outfun,
#   which defaults to the projection method.
#
#  OUTPUT:
#   n is sample size after missing values are removed
#   nv.keep is sample size after leverage points are removed.
#   output contains all pairwise comparisons.
#   For each parameter, FWE is controlled using Hochberg's method
#
if(!is.list(x))stop('Argument x should have list mode')
if(!is.list(y))stop('Argument y should have list mode')
J=length(x) # number of groups
x=lapply(x,as.matrix)
pchk=lapply(x,ncol)
temp=matl(pchk)
if(var(as.vector(temp))!=0)stop('Something is wrong.
Number of covariates differs among the groups being compared')
nv=NULL
p=ncol(x[[1]])
p1=p+1
for(j in 1:J){
xy=elimna(cbind(x[[j]],y[[j]]))
x[[j]]=xy[,1:p]
y[[j]]=xy[,p1]
x[[j]]=as.matrix(x[[j]])
nv=c(nv,nrow(x[[j]]))
}
nv.keep=nv
critrad=NULL
if(xout){
temp=lapply(x,outfun,plotit=FALSE,STAND=STAND,...)
for(j in 1:J){
x[[j]]=x[[j]][temp[[j]]$keep,]
y[[j]]=y[[j]][temp[[j]]$keep]
nv.keep[j]=length(y[[j]])
}}
tot=(J^2-J)/2
dvec<-alpha/c(1:tot)
outl=list()
nr=tot*p1
outp=matrix(NA,ncol=5,nrow=tot)
x=lapply(x,as.matrix)
xx=list()
yy=list()
iall=0
ivp=c(1,tot)-tot
i=0
for(j in 1:J){
for(k in 1:J){
if(j < k){
i=i+1
xx[[1]]=x[[j]]
xx[[2]]=x[[k]]
yy[[1]]=y[[j]]
yy[[2]]=y[[k]]
if(!ISO)all=ols1way(xx,yy,HC3=HC3)
if(ISO)all=ols1wayISO(xx,yy,HC3=HC3)
temp=all$adjusted.p.value
if(is.null(temp))temp=all$p.value
outp[i,1]=j
outp[i,2]=k
outp[i,3]=temp
}}
temp2<-order(0-outp[,3])
icc=c(1:tot)
icc[temp2]=dvec
outp[,4]=icc
}
flag=(outp[,3]<=outp[,4])
outp[,5]=rep(0,tot)
outp[flag,5]=1
dimnames(outp)=list(NULL,c('Group','Group','p.value','p.crit','sig'))
list(n=nv,n.keep=nv.keep,output=outp)
}
olsJmcp=olsLmcp




# ----------------------------------------------------------------------------

# olsMUL

# ----------------------------------------------------------------------------

olsMUL<-function(x,y,xout=FALSE,outfun=out){
#
# y is assumed to be a matrix. For each
#  outcome variable (for each column of  y), this function returns
# confidence intervals for each slope parameter and intercept.
#
# Performs OLS regression calling built-in R or S+ funtions.
#
# xout=T will eliminate any leverage points (outliers among x values)
#
m<-elimna(cbind(x,y))
x<-as.matrix(x)
p<-ncol(x)
pp<-ncol(m)
p1=p+1
x<-m[,1:p]
y<-m[,p:pp]
if(xout){
m<-cbind(x,y)
flag<-outfun(x,plotit=FALSE)$keep
m<-m[flag,]
x<-m[,1:p]
y<-m[,pp]
}
x<-as.matrix(x)
y<-as.matrix(y)
py=ncol(y)
for(j in 1:py){
print(paste("Response", j))
temp<-summary(lm(y[,j]~x))
coef<-temp[4]$coefficients
Ftest<-temp[10]$fstatistic
Ftest.p.value<-1-pf(Ftest[1],Ftest[2],Ftest[3])
Rval=Rsq.ols(x,y[,j])
print(list(coef=coef,Ftest.p.value=Ftest.p.value,R.squared=Rval))
}
}

olstest<-function(x,y,nboot=500,SEED=TRUE,RAD=TRUE,xout=FALSE,outfun=outpro,...){
#
# Test the hypothesis that all OLS slopes are zero.
# Heteroscedasticity is allowed.
#
# RAD=T: use Rademacher function to generate wild bootstrap values.
# RAD=F, use standardized uniform distribution.
#
if(SEED)set.seed(2)
m<-elimna(cbind(x,y))
x<-as.matrix(x)
p<-ncol(x)
pp<-p+1
x<-m[,1:p]
y<-m[,pp]
if(xout){
m<-cbind(x,y)
if(identical(outfun,outblp))flag=outblp(x,y,plotit=FALSE)$keep
else
flag<-outfun(x,plotit=FALSE,...)$keep
m<-m[flag,]
x<-m[,1:p]
y<-m[,pp]
}
x<-as.matrix(x)
temp<-lsfit(x,y)
yhat<-mean(y)
res<-y-yhat
test<-sum(temp$coef[2:pp]^2)
if(RAD)data<-matrix(ifelse(rbinom(length(y)*nboot,1,0.5)==1,-1,1),nrow=nboot)
if(!RAD){
data<-matrix(runif(length(y)*nboot),nrow=nboot)#
data<-(data-.5)*sqrt(12) # standardize the random numbers.
}
rvalb<-apply(data,1,olstests1,yhat,res,x)
p.val<-sum(rvalb>=test)/nboot
list(p.value=p.val)
}




# ----------------------------------------------------------------------------

# olstests1

# ----------------------------------------------------------------------------

olstests1<-function(vstar,yhat,res,x){
ystar <- yhat + res * vstar
p<-ncol(x)
pp<-p+1
vals<-lsfit(x,ystar)$coef[2:pp]
test<-sum(vals^2)
test
}
olsW2g<-function(x,y,grp=c(1,2),iv=1,xout=FALSE,outfun=outpro,STAND=TRUE,alpha=.05,
pr=TRUE,BLO=FALSE,HC3=FALSE,...){
#
#  Test hypothesis that for two or more independent groups, all slope parameters
#  are equal using OLS  estimator.
#
#  Use Welch type test statistic in conjunction with HC4 estimate of covariances.
#
#  x and y are assumed to have list mode having length J equal to the number of groups
#  For example, x[[1]] and y[[1]] contain the data for group 1.
#
#   xout=T will eliminate leverage points using the function outfun,
#   which defaults to the MVE method.
#
#   BLO=TRUE, only bad leverage points are removed.
#
#  OUTPUT:
#   n is sample size after missing values are removed
#   nv.keep is sample size after leverage points are removed.
#
if(!is.list(x))stop('Argument x should have list mode')
iv1=iv+1
J=length(x) # number of groups
x=lapply(x,as.matrix)
pchk=lapply(x,ncol)
temp=matl(pchk)
if(var(as.vector(temp))!=0)stop('Something is wrong. Number of covariates differs among the groups being compared')
nv=NULL
p=ncol(x[[1]])
p1=p+1
for(j in 1:J){
xy=elimna(cbind(x[[j]],y[[j]]))
x[[j]]=xy[,1:p]
y[[j]]=xy[,p1]
x[[j]]=as.matrix(x[[j]])
nv=c(nv,nrow(x[[j]]))
}
nv.keep=nv
critrad=NULL
if(BLO)xout=FALSE
if(xout){
temp=lapply(x,outfun,plotit=FALSE,STAND=STAND,...)
for(j in 1:J){
x[[j]]=x[[j]][temp[[j]]$keep,]
y[[j]]=y[[j]][temp[[j]]$keep]
}}
if(BLO){
for(j in 1:J){
temp1=reglev(x[[j]],y[[j]],plotit=FALSE)
ad1=c(temp1$levpoints,temp1$regout)
flag1=duplicated(ad1)
if(sum(flag1)>0){
flag1=ad1[flag1]
x[[j]]=as.matrix(x[[j]])
x[[j]]=x[[j]][-flag1,]
y[[j]]=y[[j]][-flag1]
}}}
x=lapply(x,as.matrix)
K=p1
est=matrix(NA,nrow=J,ncol=p1)
grpnum=NULL
for(j in 1:J)grpnum[j]=paste("Group",j)
vlabs="Intercept"
for(j in 2:p1)vlabs[j]=paste("Slope",j-1)
dimnames(est)=list(grpnum,vlabs)
ecov=list()
ecovinv=list()
W=rep(0,p1)
for(j in 1:J){
est[j,]=ols(x[[j]],y[[j]],xout=FALSE,plotit=FALSE,...)$coef
nv.keep[j]=nrow(x[[j]])
ecov[[j]]=olshc4(x[[j]],y[[j]],HC3=HC3)$cov
}
q1=ecov[[grp[1]]][iv1,iv1]
q2=ecov[[grp[2]]][iv1,iv1]
top=est[grp[1]]-est[grp[2]]
F=(est[grp[1],iv1]-est[grp[2],iv1])/sqrt(q1+q2)
df=(q1+q2)^2/(q1^2/(nv[grp[1]]-1)+q2^2/(nv[grp[2]]-1))
pv=2*(1-pt(abs(F),df))
crit=qt(1-alpha/2,df)
ci=est[grp[1],iv1]-est[grp[2],iv1]-crit*sqrt(q1+q2)
ci[2]=est[grp[1],iv1]-est[grp[2],iv1]+crit*sqrt(q1+q2)
list(n=nv,n.keep=nv.keep,test.stat=F,conf.interval=ci,
est=c(est[grp[1],iv1],est[grp[2],iv1]),est.dif=est[grp[1],iv1]-est[grp[2],iv1],p.value=pv)
}

olswbtest<-function(x,y,nboot=500,SEED=TRUE,RAD=TRUE,alpha=.05){
#
# Compute confidence intervals for all OLS slopes
# using HC4 wild bootstrap and Wald test.
#
# This function calls the functions
# olshc4 and
# lstest4
#
if(SEED)set.seed(2)
x<-as.matrix(x)
# First, eliminate any rows of data with missing values.
temp <- cbind(x, y)
        temp <- elimna(temp)
        pval<-ncol(temp)-1
        x <- temp[,1:pval]
        y <- temp[, pval+1]
x<-as.matrix(x)
p<-ncol(x)
pp<-p+1
temp<-lsfit(x,y)
yhat<-mean(y)
res<-y-yhat
s<-olshc4(x, y)$cov[-1, -1]
si<-solve(s)
b<-temp$coef[2:pp]
test=abs(b)*sqrt(diag(si))
if(RAD)data<-matrix(ifelse(rbinom(length(y)*nboot,1,0.5)==1,-1,1),nrow=nboot)
if(!RAD){
data<-matrix(runif(length(y)*nboot),nrow=nboot)
data<-(data-.5)*sqrt(12) # standardize the random numbers.
}
rvalb<-apply(data,1,olswbtest.sub,yhat,res,x) #a p by nboot matrix
rvalb=abs(rvalb)
ic=round((1-alpha)*nboot)
if(p==1)rvalb=t(as.matrix(rvalb))
temp=apply(rvalb,1,sort) # nboot by p matrix
pvals=NA
for(j in 1:p)pvals[j]=mean((rvalb[j,]>=test[j]))
cr=temp[ic,]
ci=b-cr/diag(sqrt(si)) #dividing because si is reciprocal of sq se
ci=cbind(ci,b+cr/diag(sqrt(si)))
ci=cbind(b,ci)
ci=cbind(c(1:nrow(ci)),ci,test,pvals)
dimnames(ci)<-
list(NULL,c("Slope_No.","Slope_est","Lower.ci","Upper.ci","Test.Stat","p.value"))
ci
}
olswbtest.sub<-function(vstar,yhat,res,x){
ystar <- yhat + res * vstar
p<-ncol(x)
pp<-p+1
vals<-t(as.matrix(lsfit(x,ystar)$coef[2:pp]))
sa<-olshc4(x, ystar)$cov[-1, -1]
sai<-solve(sa)
test<-vals*sqrt(diag(sai))
test
}



olsWmcp<-function(x,y,xout=TRUE,outfun=outpro,STAND=TRUE,
alpha=.05,pr=TRUE,BLO=FALSE,HC3=FALSE,...){
#
#  All pairwise comparison of intercepts, followed by all pairwise
#  comparison of first slope, etc.
#  using OLS  estimator.
#
#  Strategy: Use HC4 estimate of standard errors followed by
#  Welch-type test statistic.
#
#  x and y are assumed to have list mode having length J equal to the number of groups
#  For example, x[[1]] and y[[1]] contain the data for group 1.
#
#   xout=T will eliminate leverage points using the function outfun,
#   which defaults to the projection method.
#
#  OUTPUT:
#   n is sample size after missing values are removed
#   nv.keep is sample size after leverage points are removed.
#   output contains all pairwise comparisons.
#   For each parameter, FWE is controlled usingHochberg's method
#   So by default, for the intercepts,
#   all pairwise comparisons are performed with FWE=.05
#   For the first slope, all pairwise comparisons
#   are performed with FWE=.05, etc.
#
if(!is.list(x))stop('Argument x should have list mode')
if(!is.list(y))stop('Argument y should have list mode')
J=length(x) # number of groups
x=lapply(x,as.matrix)
pchk=lapply(x,ncol)
temp=matl(pchk)
if(var(as.vector(temp))!=0)stop('Something is wrong.
Number of covariates differs among the groups being compared')
nv=NULL
p=ncol(x[[1]])
p1=p+1
for(j in 1:J){
xy=elimna(cbind(x[[j]],y[[j]]))
x[[j]]=xy[,1:p]
y[[j]]=xy[,p1]
x[[j]]=as.matrix(x[[j]])
nv=c(nv,nrow(x[[j]]))
}
nv.keep=nv
critrad=NULL
if(xout){
temp=lapply(x,outfun,plotit=FALSE,STAND=STAND,...)
for(j in 1:J){
x[[j]]=x[[j]][temp[[j]]$keep,]
y[[j]]=y[[j]][temp[[j]]$keep]
}}

tot=(J^2-J)/2
dvec<-alpha/c(1:tot)
outl=list()
nr=tot*p1
outp=matrix(NA,ncol=7,nrow=nr)
x=lapply(x,as.matrix)
rlab=rep('Intercept',tot)
xx=list()
yy=list()
iall=0
ivp=c(1,tot)-tot
for(ip in 1:p1){
iv=ip-1
i=0
if(iv>0)rlab=c(rlab,rep(paste('slope',iv),tot))
for(j in 1:J){
for(k in 1:J){
if(j < k){
i=i+1
xx[[1]]=x[[j]][,1:p]
xx[[2]]=x[[k]][,1:p]
yy[[1]]=y[[j]]
yy[[2]]=y[[k]]
all=olsW2g(xx,yy,iv=iv,BLO=BLO,HC3=HC3)
temp=all$p.value
iall=iall+1
outp[iall,1]=j
outp[iall,2]=k
outp[iall,3]=all$conf.interval[1]
outp[iall,4]=all$conf.interval[2]
outp[iall,5]=temp
}}}
ivp=ivp+tot
temp2<-order(0-outp[ivp[1]:ivp[2],5])
icc=c(ivp[1]:ivp[2])
icc[temp2]=dvec
outp[ivp[1]:ivp[2],6]=icc
D=rep('NO',tot)
flag=(outp[ivp[1]:ivp[2],5]<=outp[ivp[1]:ivp[2],4])
}
flag=(outp[,5]<=outp[,6])
outp[,7]=rep(0,nr)
outp[flag,7]=1
dimnames(outp)=list(rlab,c('Group','Group','ci.low','ci.up','p.value','p.crit','sig'))
list(n=nv,n.keep=nv.keep,output=outp)
}



omega<-function(x,beta=.1){
#   Compute the estimate of the measure omega as described in
#   chapter 3.
#   The default value is beta=.1 because this function is used to
#   compute the percentage bend midvariance.
#
y<-abs(x-median(x))
y<-sort(y)
m<-floor((1-beta)*length(x)+.5)
omega<-y[m]/qnorm(1-beta/2) # omega is rescaled to equal sigma
#                             under normality
omega
}




# ----------------------------------------------------------------------------

# onesampb

# ----------------------------------------------------------------------------

onesampb<-function(x,est=onestep,alpha=.05,nboot=2000,SEED=TRUE,nv=0,null.value=NULL,...){
#
#   Compute a bootstrap, .95 confidence interval for the
#   measure of location corresponding to the argument est.
#   By default, a one-step
#   M-estimator of location based on Huber's Psi is used.
#   The default number of bootstrap samples is nboot=500
#
#    nv=null value when  computing a p-value
#
if(!is.null(null.value))nv=null.value
if(SEED)set.seed(2) # set seed of random number generator so that
#             results can be duplicated.
#print("Taking bootstrap samples. Please wait.")
x=elimna(x)
data<-matrix(sample(x,size=length(x)*nboot,replace=TRUE),nrow=nboot)
bvec<-apply(data,1,est,...)
bvec<-sort(bvec)
low<-round((alpha/2)*nboot)
up<-nboot-low
low<-low+1
pv=mean(bvec>nv)+.5*mean(bvec==nv)
pv=2*min(c(pv,1-pv))
estimate=est(x,...)
list(ci=c(bvec[low],bvec[up]),n=length(x),estimate=estimate,p.value=pv)
}




# ----------------------------------------------------------------------------

# ord.loc.ex

# ----------------------------------------------------------------------------

ord.loc.ex<-function(x,alpha=.05,tr=.2){
#
#
#  Determine the order of the population trimmed means.
#  Can a decision be made that the correct order has been identified?
#
#  p.crit:  If NULL, critical p-values are determined so that that FWE is alpha
#  This is done using a simulation to determine the null distribution based on
#  iter=5000 replications.
#
#
#  Returns:
#   ORD='No Decision' if a complete ordering cannot be made.
#   DEC= the pairs of  group for which a decision can be made
#
#   Confidence intervals having simultaneous probability coverage 1-alpha
#  using the adjusted level.
#
x=elimna(x)
if(is.matrix(x))x=listm(x)
J=length(x)
if(J<3)stop('Should have 3 or more groups')
Jm1=J-1
est=lapply(x,tmean,tr=tr)
n=lapply(x,length)
est=matl(est)
n=as.vector(matl(n))
R=order(est)
ic=0
pvec=NA
for(i in 2:J){
im1=i-1
a=yuen(x[[R[im1]]],x[[R[[i]]]])
ic=ic+1
pvec[ic]=a$p.value
}
pvec
}

ord.loc.PV<-function(x,p.crit=NULL,alpha=.05,tr=.2,iter=5000,SEED=TRUE){
#
#
#  Determine the order of the population trimmed means.
#  Can a decision be made that the correct order has been identified?
#
#  p.crit:  If NULL, critical p-values are determined so that that FWE is alpha
#  This is done using a simulation to determine the null distribution based on
#  iter=5000 replications.
#
#  Strategy: For J groups, sort the  groups so that estimates of the trimmed mean are in ascending order.
# Next,  compare group j  to j+1, j=1, ... J-1
#  FWE is controlled using a method designed specifically for the problem at hand.
#   If all J-1 hypotheses are rejected, make a decision
#
#  Returns:
#   ORD='No Decision' if a decision about complete ordering cannot be made.
#   output: contains the results for each of the J-1 tests.
#
#   Confidence intervals having simultaneous probability coverage 1-alpha
#  using the adjusted level.
#
if(is.vector(x))stop('x should be a matrix with ncol>2, or data.frame or list mode with length >2')
x=elimna(x)
if(is.matrix(x))x=listm(x)
J=length(x)
if(J<3)stop('Should have 3 or more groups')
Jm1=J-1
est=lapply(x,tmean,tr=tr)
n=lapply(x,length)
est=matl(est)
n=as.vector(matl(n))
R=order(est)
pvec=NA
aval=c(seq(.001,.1,.001),seq(.11,.99,.01))
if(length(id)==0)stop('alpha must be one of values .001(.001).1 or 11(.01).99')
v=ord.loc.crit.det(J=J,n=n,iter=iter,alpha=alpha,SEED=SEED)
p.crit=v[id,]

pvdist=NULL
if(is.null(p.crit)){
v=ord.loc.crit(J=J,n=n,iter=iter,alpha=alpha,SEED=SEED)
p.crit=v$fin.crit
pvdist=v$pvdist
}
output<-matrix(NA,Jm1,9)
dimnames(output)=list(NULL,c('Grp.L','Grp.R','Est.L','Est.R','Dif','ci.low','ci.up','p.value','p.crit'))
for(i in 2:J){
im1=i-1
a=yuen(x[[R[im1]]],x[[R[[i]]]],alpha=p.crit[im1])
pvec[im1]=a$p.value
output[im1,]=c(R[im1],R[i],a$est.1,a$est.2,a$dif,a$ci[1],a$ci[2],a$p.value,p.crit[im1])
}
dpv=NA
if(!is.null(pvdist)){
chk=0
for(i in 1:iter){
flag=0
for(j in 1:Jm1)if(pvdist[i,j]<=output[j,7])flag=flag+1
if(flag>0)chk=chk+1
}
dpv=chk/iter
}
# Determine p-value for overall decision
na=length(aval)
for(i in 1:na){
chk=sum(output[,7]<=v[i,])
pv=aval[i]
if(chk==Jm1)break
}
ORD.ID='NO'
id=output[,8]<=output[,9]
if(sum(id)==Jm1)ORD.ID='YES'
setClass('BIN',slots=c('Make.a.Decison','Decision.p.value','Estimates','n','output'))
put=new('BIN',Make.a.Decison=ORD.ID,Decision.p.value=pv,Estimates=est,n=n,output=output)
put
}

ord.loc.crit<-function(J,n=30,alpha=.05,tr=.2,iter=5000,SEED=TRUE){
#
#  Determine critical p-values for ord.loc
#
if(SEED)set.seed(2)
Jm1=J-1
rem=matrix(NA,iter,Jm1)
for(k in 1:iter){
if(length(n)==1){
x=rmul(n,p=J)
x=listm(x)
}
else{
x=list()
if(length(n)!=J)stop('J is not equal to the length  of n')
for(j in 1:J)x[[j]]=rnorm(n[j])
}
rem[k,]=ord.loc.ex(x,tr=tr)
}
init=apply(rem,2,qest,alpha)
#print(apply(rem,2,mean))
#print(init)
z=optim(0,ord.loc.fun,init=init,iter=iter,rem=rem,Jm1=Jm1,alpha=alpha,method='Brent',lower=0,upper=1)
fin.crit=z$par*init
list(fin.crit=fin.crit,pvdist=rem)
}




# ----------------------------------------------------------------------------

# ord.loc.fun

# ----------------------------------------------------------------------------

ord.loc.fun<-function(a,init,iter,rem,Jm1,alpha){
#
chk=0
init=a*init
for(i in 1:iter){
flag=0
for(j in 1:Jm1)if(rem[i,j]<=init[j])flag=flag+1
if(flag>0)chk=chk+1
}
chk=chk/iter
dif=abs(chk-alpha)
dif
}

ord.loc.crit.det<-function(J,n,alpha=.05,tr=.2,iter=5000,SEED=TRUE){
#
#  Determine critical p-values for anc.best
#
if(SEED)set.seed(2)
Jm1=J-1
rem=matrix(NA,iter,Jm1)
for(k in 1:iter){
if(length(n)==1){
x=rmul(n,p=J)
x=listm(x)
}
else{
x=list()
if(length(n)!=J)stop('J is not equal to the length  of n')
for(j in 1:J)x[[j]]=rnorm(n[j])
}
rem[k,]=ord.loc.ex(x,tr=tr)
}
aval=c(seq(.001,.1,.001),seq(.011,.99,.01))
na=length(aval)
fin.crit=matrix(NA,na,Jm1)
for(i in 1:na){
init=apply(rem,2,qest,aval[i])
z=optim(0,ord.loc.fun,init=init,iter=iter,rem=rem,Jm1=Jm1,alpha=aval[i],method='Brent',lower=0,upper=1)
fin.crit[i,]=z$par*init
}
fin.crit
}

ortho<-function(x){
# Orthnormalize x
#
y<-qr(x)
y<-qr.Q(y)
y
}




# ----------------------------------------------------------------------------

# OS.lasso

# ----------------------------------------------------------------------------

OS.lasso<- function(x,y,lambda.lasso.try=NULL,lambda.gamma.try=NULL,xout=FALSE,outfun=outpro,details=FALSE,...){
#
# Outlier Shifting lasso
# Jung, Y., Lee, S., and Hu, J. (2016). Robust regression for highly corrupted response
# by shifting outliers. Statistical Modelling, 16(1):1--23.
#
#
X=x
Y=y
if(is.null(lambda.lasso.try))lambda.lasso.try=seq(0.01,0.6,length.out=100)
if(is.null(lambda.gamma.try))lambda.gamma.try = seq(1,4,length.out=50)
library(glmnet)
X<-as.matrix(X)
p1<-ncol(X)+1
p<-ncol(X)
xy<-cbind(X,Y)
xy<-elimna(xy)
X<-xy[,1:p]
Y<-xy[,p1]
if(xout){
X<-as.matrix(X)
flag<-outfun(X,plotit=FALSE,...)$keep
X<-X[flag,]
Y<-Y[flag]
X<-as.matrix(X)
n.keep=nrow(X)
}
x=X
y=Y
n<-length(Y)
Y.orgn<- Y
model.for.cv<- cv.glmnet(X, Y, family="gaussian",lambda=lambda.lasso.try)
lambda.lasso.opt<- model.for.cv$lambda.min
model.est<- glmnet(X,Y,family="gaussian",lambda=lambda.lasso.opt)
fit.lasso<- predict(model.est,X,s=lambda.lasso.opt)
res.lasso<- Y - fit.lasso
sigma.est<- mad(Y-fit.lasso)
beta.est<- as.numeric(model.est$beta)
gamma.est<-rep(0,n)
n.fold<- 5
n.cv <- n/n.fold
CV.error2<-CV.error<-rep(NA,length(lambda.gamma.try))
Y.pred.cv<-matrix(NA,nrow=length(Y),ncol=length(lambda.gamma.try))
Y.new<- Y
for (tt in 1:length(lambda.gamma.try))
{
gamma.est.cv<-rep(0,n-n.cv)
for (jj in 1:n.fold)
{
sample.out.index<- (1+n.cv*(jj-1)):(n.cv*(jj))
X.train<- X[-sample.out.index,]
Y.train<- Y[-sample.out.index]
X.test<- X[sample.out.index,]
model.train.temp<- glmnet(X.train,Y.train,family="gaussian",
lambda=lambda.lasso.opt)
beta.pre<-beta.post<- c(model.train.temp$a0,
as.numeric(model.train.temp$beta))
tol<-100; n.iter <- 0
while(tol>1e-6 & n.iter<100)
{resid.temp<- Y.train-cbind(rep(1,n-n.cv),X.train)%*%beta.pre
nonzero<-which(abs(resid.temp)>=sigma.est*lambda.gamma.try[tt])
gamma.est.cv[nonzero]<- resid.temp[nonzero]
Y.train.new <- Y.train - gamma.est.cv
model.train.temp<-glmnet(X.train,Y.train.new,
family="gaussian",lambda=lambda.lasso.opt)
beta.post <- c(model.train.temp$a0,
as.numeric(model.train.temp$beta))
tol<- sum((beta.pre-beta.post)^2)
n.iter<- n.iter+1
beta.pre<-beta.post
}
Y.pred.cv[sample.out.index,tt] <-cbind(rep(1,n.cv),
X.test)%*%beta.post
}
CV.error[tt]<- mean((Y.pred.cv[,tt]-Y.orgn)^2)
CV.error2[tt]<- mean(abs(Y.pred.cv[,tt]-Y.orgn))
}
lambda.gamma.opt<- lambda.gamma.try[which.min(CV.error)]
model.opt<- glmnet(X,Y.orgn,family="gaussian",lambda=lambda.lasso.opt)
beta.pre<- beta.post<- c(model.opt$a0,as.numeric(model.opt$beta))
tol<-100; n.iter <- 0
while(tol>1e-6 & n.iter<100)
{
resid.opt<- Y.orgn-cbind(rep(1,n),X)%*%beta.pre
nonzero<-which(abs(resid.opt)>=sigma.est*lambda.gamma.opt)
gamma.est[nonzero]<- resid.opt[nonzero]
Y.new <- Y.orgn - gamma.est
model.opt<- glmnet(X,Y.new,family="gaussian",lambda=lambda.lasso.opt)
beta.post <- c(model.opt$a0,as.numeric(model.opt$beta))
tol<- mean((beta.pre-beta.post)^2)
n.iter<- n.iter+1
beta.pre<-beta.post}
Y.fit<- cbind(rep(1,n),X)%*%beta.post
res=y-Y.fit
if(details)
list(coef=beta.post,fit=fit.lasso,iter = n.iter,
sigma.est = sigma.est,CV.error=CV.error,
n.outlier=length(which(gamma.est!=0)),
gamma.est=gamma.est,lambda.opt=lambda.gamma.opt)
else list(coef=beta.post,residuals=res)
}

p.crit.n100<-function(alpha=.05,p.obs=NULL){
p.table=c(
0.516 ,0.516 ,0.14 ,0.42 ,0.124
,0.872 ,0.396 ,0.692 ,0.92 ,0.452
,0.048 ,0.34 ,0.18 ,0.952 ,0.348
,0.004 ,0.74 ,0.04 ,0.612 ,0.052
,0.888 ,0.416 ,0.096 ,0.16 ,0.62
,0.132 ,0.536 ,0.96 ,0.24 ,0.512
,0.672 ,0.54 ,0.204 ,0.128 ,0.296
,0.828 ,0.4 ,0.324 ,0.388 ,0.988
,0.568 ,0.764 ,0.82 ,0.392 ,0.992
,0.996 ,0.804 ,0.416 ,0.308 ,0.924
,0.1 ,0.016 ,0.368 ,0.264 ,0.94
,0.56 ,0.156 ,0.68 ,0.616 ,0.252
,0.724 ,0.38 ,0.544 ,0.02 ,0.092
,0.692 ,0.912 ,0.04 ,0.58 ,0.072
,0.416 ,0.912, 1 ,0.252 ,0.428
,0.068 ,0.504 ,0.692 ,0.452 ,0.932
,0.668 ,0.912 ,0.056 ,0.86 ,0.908
,0.276 ,0.54 ,0.424 ,0.556 ,0.248
,0.42 ,0.94 ,0.632 ,0.544 ,0.168
,0.824 ,0.44 ,0.136 ,0.9 ,0.984
,0.34 ,0.984 ,0.428 ,0.208 ,0.216
,0.024 ,0.744 ,0.34 ,0.644 ,0.488
,0.716 ,0.984 ,0.292 ,0.512 ,0.908
,0.336 ,0.908 ,0.528 ,0.364 ,0.924
,0.884 ,0.928 ,0.608 ,0.06 ,0.624
,0.588 ,0.536 ,0.88 ,0.224 ,0.124
,0.184 ,0.416 ,0.004 ,0.84 ,0.784
,0.016 ,0.692 ,0.892 ,0.04 ,0.084
,0.844 ,0.38 ,0.512 ,0.392 ,0.28
,0.312 ,0.96 ,0.084 ,0.216 ,0.34
,0.544 ,0.952 ,0.88 ,0.024 ,0.292
,0.456 ,0.076 ,0.932 ,0.78 ,0.304
,0.06 ,0.692 ,0.996 ,0.132 ,0.82
,0.592 ,0.872 ,0.812 ,0.132 ,0.392
,0.84 ,0.736 ,0.188 ,0.132 ,0.736
,0.236 ,0.44 ,0.684 ,0.62 ,0.484
,0.828 ,0.2 ,0.688 ,0.404 ,0.78
,0.948 ,0.968 ,0.284 ,0.776 ,0.672
,0.016 ,0.088 ,0.712 ,0.776 ,0.324
,0.788 ,0.74 ,0.584 ,0.456 ,0.572
,0.896 ,0.976 ,0.164 ,0.576 ,0.112
,0.724 ,0.268 ,0.18 ,0.232 ,0.224
,0.996 ,0.692 ,0.552 ,0.524 ,0.756
,0.1 ,0.684 ,0.416 ,0.98 ,0.02
,0.452 ,0.6 ,0.94 ,0.148 ,0.808
,0.272 ,0.484 ,0.404 ,0.064 ,0.64
,0.564 ,0.108 ,0.832 ,0.232 ,0.784
,0.504 ,0.364 ,0.784 ,0.584 ,0.616
,0.708 ,0.576 ,0.668 ,0.984 ,0.428
,0.984 ,0.72 ,0.736 ,0.66 ,0.876
,0.508 ,0.2 ,0.444 ,0.94 ,0.808
,0.404 ,0.436 ,0.104 ,0.656 ,0.616
,0.524 ,0.884 ,0.24 ,0.34 ,0.424
,0.052 ,0.46 ,0.872 ,0.452 ,0.132
,0.704 ,0.508 ,0.22 ,0.972 ,0.632
,0.5 ,0.544 ,0.02 ,0.112, 1
,0.204 ,0.728 ,0.46 ,0.556 ,0.732
,0.228 ,0.472 ,0.068 ,0.344 ,0.972
,0.404 ,0.816 ,0.368 ,0.38 ,0.912
,0.948 ,0.108 ,0.308 ,0.584 ,0.816
,0.692 ,0.024 ,0.604 ,0.74 ,0.696
,0.48 ,0.888 ,0.356 ,0.464 ,0.548
,0.232 ,0.76 ,0.892 ,0.096 ,0.18
,0.596 ,0.556 ,0.56 ,0.776 ,0.368
,0.66 ,0.856 ,0.4 ,0.968 ,0.356
,0.752 ,0.648 ,0.62 ,0.888 ,0.932
,0.86 ,0.448 ,0.248 ,0.936 ,0.22
,0.364 ,0.92 ,0.424 ,0.316 ,0.684
,0.812 ,0.652 ,0.372 ,0.308 ,0.616
,0.816 ,0.04 ,0.304 ,0.864 ,0.356
,0.632 ,0.82 ,0.976 ,0.612 ,0.112
,0.088 ,0.7 ,0.568 ,0.948 ,0.084
,0.848 ,0.452 ,0.004 ,0.392 ,0.796
,0.528 ,0.372 ,0.472 ,0.404 ,0.48
,0.244 ,0.144 ,0.944 ,0.852 ,0.52
,0.436 ,0.396 ,0.388 ,0.596 ,0.496
,0.972 ,0.424 ,0.424 ,0.144 ,0.772
,0.1 ,0.748 ,0.192 ,0.508 ,0.78
,0.176 ,0.944 ,0.136 ,0.564 ,0.724
,0.76 ,0.6 ,0.032 ,0.504 ,0.78
,0.936 ,0.76 ,0.124 ,0.62 ,0.092
,0.768 ,0.852 ,0.556 ,0.456 ,0.468
,0.984 ,0.7 ,0.568 ,0.288 ,0.048
,0.928 ,0.892 ,0.728 ,0.32 ,0.836
,0.788 ,0.352 ,0.748 ,0.128 ,0.92
,0.352 ,0.236 ,0.56 ,0.192 ,0.72
,0.516 ,0.668 ,0.652 ,0.056 ,0.876
,0.916 ,0.124 ,0.176 ,0.652 ,0.088
,0.508 ,0.8 ,0.112 ,0.1 ,0.264
,0.836 ,0.912 ,0.376 ,0.792 ,0.876
,0.892 ,0.284 ,0.892 ,0.088 ,0.376
,0.24 ,0.98 ,0.416 ,0.96 ,0.848
,0.632 ,0.304 ,0.5 ,0.716 ,0.388
,0.976 ,0.24 ,0.32 ,0.808 ,0.168
,0.352 ,0.064 ,0.7 ,0.42 ,0.224
,0.176 ,0.988 ,0.224 ,0.904 ,0.592
,0.696 ,0.42 ,0.376 ,0.696 ,0.44
,0.328 ,0.816 ,0.312 ,0.968 ,0.432
,0.784 ,0.92 ,0.6 ,0.928 ,0.784
,0.96 ,0.54 ,0.872 ,0.816 ,0.728
,0.688 ,0.436 ,0.796 ,0.42 ,0.464
,0.112 ,0.328 ,0.168 ,0.244 ,0.052
,0.864 ,0.312 ,0.992 ,0.832 ,0.584
,0.396 ,0.596 ,0.72 ,0.684 ,0.448
,0.88 ,0.9 ,0.748 ,0.424 ,0.66
,0.644 ,0.188 ,0.528 ,0.98 ,0.58
,0.716 ,0.352 ,0.512 ,0.716 ,0.548
,0.988, 0 ,0.36 ,0.4 ,0.784
,0.212 ,0.756 ,0.956 ,0.5 ,0.512
,0.852 ,0.216 ,0.004 ,0.596 ,0.504
,0.56 ,0.672 ,0.988 ,0.76 ,0.924
,0.108 ,0.62 ,0.412 ,0.44 ,0.816
,0.664 ,0.924 ,0.62 ,0.816 ,0.184
,0.244 ,0.644 ,0.172 ,0.3 ,0.22
,0.02 ,0.572 ,0.156 ,0.768 ,0.484
,0.576 ,0.992 ,0.192 ,0.428 ,0.736
,0.708 ,0.888 ,0.136 ,0.192 ,0.948
,0.244 ,0.528 ,0.188 ,0.956 ,0.036
,0.016 ,0.636 ,0.048 ,0.42 ,0.184
,0.16 ,0.496 ,0.512 ,0.18 ,0.052
,0.972 ,0.488 ,0.988 ,0.284 ,0.252
,0.876 ,0.244 ,0.072 ,0.712 ,0.684
,0.78 ,0.468 ,0.504 ,0.732 ,0.84
,0.496 ,0.168 ,0.256 ,0.092 ,0.916
,0.76 ,0.152 ,0.72 ,0.14 ,0.356
,0.824 ,0.172 ,0.248 ,0.624 ,0.932
,0.856 ,0.612 ,0.52 ,0.584 ,0.904
,0.576 ,0.528 ,0.092 ,0.132 ,0.288
,0.096 ,0.516 ,0.316 ,0.448 ,0.792
,0.756 ,0.748 ,0.016 ,0.52 ,0.64
,0.924 ,0.688 ,0.824 ,0.392 ,0.908
,0.852 ,0.652 ,0.184 ,0.244 ,0.288
,0.324 ,0.324 ,0.736 ,0.628 ,0.816
,0.868 ,0.576 ,0.584 ,0.92 ,0.812
,0.104 ,0.48 ,0.884 ,0.816 ,0.24
,0.024 ,0.204 ,0.32 ,0.452 ,0.44
,0.036 ,0.96 ,0.716 ,0.648 ,0.04
,0.256 ,0.684 ,0.296 ,0.484 ,0.772
,0.52 ,0.068 ,0.26 ,0.352 ,0.096
,0.348 ,0.18 ,0.556 ,0.276 ,0.956
,0.944 ,0.052 ,0.916 ,0.52 ,0.624
,0.668 ,0.98 ,0.112 ,0.144 ,0.644
,0.348 ,0.488 ,0.88 ,0.16 ,0.056
,0.66 ,0.964 ,0.316 ,0.14 ,0.36
,0.62 ,0.168 ,0.124 ,0.852 ,0.58
,0.512 ,0.424 ,0.024 ,0.676 ,0.192
,0.636 ,0.544 ,0.78 ,0.712 ,0.368
,0.156 ,0.068 ,0.056 ,0.112 ,0.26
,0.448 ,0.68 ,0.316 ,0.9 ,0.78
,0.62 ,0.264 ,0.856 ,0.04 ,0.936
,0.6 ,0.128 ,0.328 ,0.924 ,0.94
,0.708 ,0.476 ,0.044 ,0.172 ,0.648
,0.356 ,0.688 ,0.888 ,0.136 ,0.976
,0.212 ,0.652 ,0.624 ,0.9 ,0.76
,0.396 ,0.736 ,0.812 ,0.1 ,0.868
,0.844 ,0.952 ,0.076 ,0.616 ,0.964
,0.064 ,0.28 ,0.576 ,0.944 ,0.532
,0.596 ,0.704 ,0.952 ,0.832 ,0.18
,0.928 ,0.504 ,0.072 ,0.4 ,0.432
,0.644 ,0.168 ,0.784 ,0.516 ,0.408
,0.224 ,0.476 ,0.992 ,0.588 ,0.668
,0.324 ,0.064 ,0.592 ,0.96 ,0.652
,0.24 ,0.964 ,0.988 ,0.06 ,0.068
,0.828 ,0.736 ,0.432 ,0.508 ,0.92
,0.34 ,0.832 ,0.504 ,0.192 ,0.88
,0.532 ,0.168 ,0.128 ,0.46 ,0.456
,0.992 ,0.328 ,0.728 ,0.72 ,0.488
,0.728 ,0.848 ,0.372 ,0.688 ,0.324
,0.748 ,0.628 ,0.936 ,0.556 ,0.8
,0.088 ,0.26 ,0.66 ,0.412 ,0.264
,0.432 ,0.148 ,0.24 ,0.672 ,0.216
,0.268 ,0.512 ,0.296 ,0.412 ,0.564
,0.224 ,0.196 ,0.42 ,0.424 ,0.168
,0.852 ,0.144 ,0.96 ,0.008 ,0.568
,0.196 ,0.704 ,0.624 ,0.6 ,0.452
,0.628 ,0.272 ,0.056 ,0.536 ,0.364
,0.364 ,0.696 ,0.272 ,0.472 ,0.008
,0.112 ,0.924 ,0.82 ,0.428 ,0.96
,0.748 ,0.308 ,0.912 ,0.472 ,0.544
,0.744 ,0.072 ,0.36 ,0.604 ,0.98
,0.308 ,0.304 ,0.352 ,0.024 ,0.956
,0.82 ,0.692 ,0.572 ,0.036 ,0.86
,0.896 ,0.976 ,0.284 ,0.764 ,0.24
,0.68 ,0.788 ,0.316 ,0.068 ,0.06
,0.688 ,0.26 ,0.496 ,0.416 ,0.388
,0.58 ,0.2 ,0.508 ,0.412 ,0.344
,0.212 ,0.932 ,0.588 ,0.212 ,0.936
,0.232 ,0.26 ,0.82 ,0.764 ,0.648
,0.924 ,0.508 ,0.84 ,0.66 ,0.46
,0.788 ,0.596 ,0.092 ,0.656 ,0.244
,0.68 ,0.628 ,0.732 ,0.56 ,0.596
,0.908 ,0.376 ,0.64 ,0.628 ,0.824
,0.36 ,0.764 ,0.484 ,0.892 ,0.576
,0.128 ,0.972 ,0.4 ,0.444 ,0.856
,0.744 ,0.32 ,0.004 ,0.58 ,0.116
,0.636 ,0.368 ,0.696 ,0.904 ,0.536
,0.04 ,0.132 ,0.812 ,0.916 ,0.468
,0.736 ,0.108 ,0.684 ,0.96 ,0.284
,0.068 ,0.832 ,0.736 ,0.248 ,0.624
,0.26 ,0.964 ,0.316 ,0.504 ,0.2
,0.36 ,0.46 ,0.8 ,0.164 ,0.284
,0.372 ,0.792 ,0.808 ,0.716 ,0.148
,0.232 ,0.724 ,0.86 ,0.692 ,0.204
,0.484 ,0.444 ,0.432 ,0.384 ,0.256
,0.732 ,0.084 ,0.316 ,0.264 ,0.18
,0.236 ,0.592 ,0.42 ,0.76 ,0.556
,0.116 ,0.72 ,0.252 ,0.632 ,0.72
,0.476 ,0.896 ,0.784 ,0.328 ,0.852
,0.548 ,0.132 ,0.692 ,0.92 ,0.596
,0.268 ,0.204 ,0.852 ,0.948 ,0.88
,0.82 ,0.328 ,0.7 ,0.684 ,0.16
,0.868 ,0.44 ,0.912 ,0.192 ,0.168
,0.844 ,0.4 ,0.32 ,0.768 ,0.52
,0.8 ,0.464 ,0.884 ,0.448 ,0.908
,0.116 ,0.136 ,0.036 ,0.368 ,0.076
,0.424 ,0.224 ,0.72 ,0.008 ,0.932
,0.732 ,0.764 ,0.088 ,0.788 ,0.852
,0.392 ,0.66 ,0.904 ,0.524 ,0.612
,0.256 ,0.892 ,0.912 ,0.512 ,0.984
,0.912 ,0.94 ,0.204 ,0.408 ,0.156
,0.868 ,0.1 ,0.988)
cp=hd(p.table,alpha)
pv=NULL
if(!is.null(p.obs))w=optimize(hdpv,interval=c(.001,.999),dat=p.table,obs=p.obs)$minimum
list(crit.p.value=cp,adj.p.value=w)
}


p.crit.n30<-function(alpha=.05,p.obs=NULL){
p.table=c(
0.696 ,0.828 ,0.208 ,0.328 ,0.152
,0.632 ,0.184 ,0.452 ,0.86 ,0.988
,0.772 ,0.832 ,0.288 ,0.288 ,0.944
,0.672 ,0.868 ,0.476 ,0.148 ,0.292
,0.792 ,0.852 ,0.236 ,0.9, 1
,0.484 ,0.932 ,0.704 ,0.4 ,0.904
,0.656 ,0.32 ,0.104 ,0.676 ,0.572
,0.936 ,0.14 ,0.148 ,0.86 ,0.508
,0.748 ,0.328 ,0.816 ,0.268 ,0.364
,0.152 ,0.816 ,0.5 ,0.972 ,0.684
,0.156 ,0.676 ,0.244 ,0.948 ,0.612
,0.28 ,0.092 ,0.712 ,0.152 ,0.704
,0.192 ,0.904 ,0.372 ,0.908 ,0.992
,0.692 ,0.956 ,0.704 ,0.964 ,0.484
,0.496 ,0.768 ,0.172 ,0.336 ,0.108
,0.052 ,0.62 ,0.192 ,0.664 ,0.716
,0.148 ,0.6 ,0.384 ,0.52 ,0.536
,0.992 ,0.272 ,0.68 ,0.232 ,0.368
,0.788 ,0.572 ,0.516 ,0.476 ,0.832
,0.988 ,0.72 ,0.432 ,0.756 ,0.564
,0.792 ,0.44 ,0.996 ,0.388 ,0.456
,0.848 ,0.364, 1 ,0.584 ,0.104
,0.68 ,0.848 ,0.268 ,0.424 ,0.836
,0.316 ,0.828 ,0.12 ,0.392 ,0.588
,0.38 ,0.544 ,0.108 ,0.248 ,0.972
,0.94 ,0.184 ,0.156 ,0.444 ,0.28
,0.612 ,0.22 ,0.544 ,0.888 ,0.808
,0.436 ,0.736 ,0.424 ,0.792 ,0.324
,0.672 ,0.012 ,0.36 ,0.656 ,0.54
,0.872 ,0.596 ,0.788 ,0.896 ,0.532
,0.34 ,0.664 ,0.28 ,0.484 ,0.888
,0.66 ,0.824 ,0.032 ,0.524 ,0.496
,0.84 ,0.564 ,0.432 ,0.668 ,0.664
,0.332 ,0.576 ,0.568 ,0.388 ,0.876
,0.06 ,0.948 ,0.664 ,0.764 ,0.12
,0.416 ,0.956 ,0.936 ,0.848 ,0.904
,0.596 ,0.792 ,0.232 ,0.68 ,0.404
,0.556 ,0.356 ,0.44 ,0.936 ,0.748
,0.968 ,0.528 ,0.8 ,0.152 ,0.68
,0.792 ,0.664 ,0.872 ,0.856 ,0.176
,0.908 ,0.124 ,0.744 ,0.708 ,0.632
,0.68 ,0.972 ,0.244 ,0.984 ,0.76
,0.828 ,0.256 ,0.888 ,0.688 ,0.312
,0.828 ,0.124 ,0.296 ,0.396 ,0.8
,0.756 ,0.104 ,0.228 ,0.884 ,0.948
,0.96 ,0.52 ,0.724 ,0.824 ,0.436
,0.672 ,0.868 ,0.772 ,0.612 ,0.48
,0.036 ,0.868 ,0.52 ,0.268 ,0.232
,0.608 ,0.676 ,0.476 ,0.588 ,0.904
,0.508 ,0.236 ,0.952 ,0.848 ,0.628
,0.924 ,0.132 ,0.812 ,0.696 ,0.3
,0.948 ,0.904 ,0.868 ,0.392 ,0.072
,0.38 ,0.624 ,0.608 ,0.756 ,0.332
,0.088 ,0.42 ,0.764 ,0.648 ,0.084
,0.428 ,0.04 ,0.408 ,0.548 ,0.216
,0.636 ,0.784 ,0.24 ,0.9 ,0.512
,0.476 ,0.504 ,0.288 ,0.812 ,0.6
,0.696 ,0.492 ,0.42 ,0.068 ,0.236
,0.604 ,0.564 ,0.888 ,0.816 ,0.52
,0.092 ,0.096 ,0.372 ,0.54 ,0.328
,0.96 ,0.276 ,0.38 ,0.1 ,0.412
,0.732 ,0.184 ,0.044 ,0.772 ,0.892
,0.244 ,0.344 ,0.976 ,0.04 ,0.088
,0.032 ,0.796 ,0.24 ,0.524 ,0.808
,0.472 ,0.472 ,0.152 ,0.696 ,0.728
,0.756 ,0.784 ,0.452 ,0.764 ,0.764
,0.144 ,0.988 ,0.552 ,0.788 ,0.5
,0.46 ,0.42 ,0.468 ,0.516 ,0.832
,0.528 ,0.724 ,0.148 ,0.648 ,0.456
,0.28 ,0.804 ,0.496 ,0.464 ,0.52
,0.864 ,0.228 ,0.544 ,0.708 ,0.912
,0.528 ,0.18 ,0.188 ,0.092 ,0.44
,0.452 ,0.596 ,0.424 ,0.32 ,0.808
,0.036 ,0.508 ,0.836 ,0.064 ,0.924
,0.4 ,0.324 ,0.464 ,0.888 ,0.948
,0.688 ,0.856 ,0.76 ,0.16 ,0.44
,0.372 ,0.328 ,0.088 ,0.984 ,0.496
,0.428 ,0.892 ,0.636 ,0.236 ,0.704
,0.704 ,0.416 ,0.9 ,0.716 ,0.976
,0.908 ,0.524 ,0.604 ,0.436 ,0.332
,0.996 ,0.428, 1 ,0.244 ,0.712
,0.456 ,0.808 ,0.984 ,0.804 ,0.62
,0.552 ,0.732 ,0.264 ,0.488 ,0.604
,0.424 ,0.936 ,0.808 ,0.356 ,0.164
,0.152 ,0.34 ,0.34 ,0.644 ,0.4
,0.784 ,0.308 ,0.296 ,0.672 ,0.664
,0.64 ,0.76 ,0.24 ,0.464 ,0.656
,0.84 ,0.76 ,0.176 ,0.148 ,0.184
,0.296 ,0.516 ,0.62 ,0.396 ,0.384
,0.84 ,0.984 ,0.964 ,0.46 ,0.224
,0.968 ,0.292 ,0.78 ,0.696 ,0.128
,0.384 ,0.98 ,0.852 ,0.408 ,0.644
,0.744 ,0.876 ,0.688 ,0.924 ,0.06
,0.36 ,0.4 ,0.528 ,0.084 ,0.216
,0.4 ,0.984 ,0.488 ,0.152 ,0.608
,0.332 ,0.5 ,0.884 ,0.78 ,0.912
,0.236 ,0.368 ,0.276 ,0.74 ,0.96
,0.912 ,0.36 ,0.608 ,0.804 ,0.9
,0.688 ,0.348 ,0.748 ,0.544 ,0.956
,0.384 ,0.892 ,0.728 ,0.164 ,0.392
,0.876 ,0.836 ,0.54 ,0.604 ,0.456
,0.144 ,0.3 ,0.848 ,0.272 ,0.668
,0.908 ,0.004 ,0.812 ,0.408 ,0.676
,0.928 ,0.224 ,0.052 ,0.756 ,0.928
,0.428 ,0.096 ,0.996 ,0.996 ,0.828
,0.504 ,0.616 ,0.788 ,0.644 ,0.26
,0.764 ,0.616 ,0.248 ,0.556 ,0.972
,0.912 ,0.66 ,0.72 ,0.792 ,0.204
,0.904 ,0.32 ,0.228 ,0.628 ,0.912
,0.804 ,0.072 ,0.656 ,0.456 ,0.992
,0.3 ,0.808 ,0.692 ,0.84 ,0.544
,0.072 ,0.652 ,0.524 ,0.884 ,0.168
,0.208 ,0.216 ,0.948 ,0.896 ,0.92
,0.964 ,0.784 ,0.812 ,0.708 ,0.936
,0.508 ,0.488 ,0.156 ,0.94 ,0.088
,0.508 ,0.72 ,0.636 ,0.552 ,0.016
,0.464 ,0.348 ,0.576 ,0.904 ,0.248
,0.324 ,0.516 ,0.988 ,0.616 ,0.716
,0.664 ,0.576 ,0.336 ,0.792 ,0.824
,0.896 ,0.804 ,0.524 ,0.332 ,0.804
,0.94 ,0.424 ,0.964 ,0.644 ,0.604
,0.4 ,0.984 ,0.38 ,0.696 ,0.248
,0.244 ,0.772 ,0.836 ,0.048 ,0.696
,0.724 ,0.576 ,0.6 ,0.348 ,0.88
,0.776 ,0.376 ,0.644 ,0.648 ,0.08
,0.424 ,0.912 ,0.964 ,0.224 ,0.984
,0.476 ,0.928 ,0.64 ,0.944 ,0.512
,0.644 ,0.596 ,0.388 ,0.28 ,0.124
,0.212 ,0.388 ,0.416 ,0.884 ,0.964
,0.996 ,0.428 ,0.832 ,0.464 ,0.88
,0.984 ,0.256 ,0.664 ,0.344 ,0.496
,0.192 ,0.124 ,0.392 ,0.268 ,0.4
,0.944 ,0.816 ,0.648 ,0.252 ,0.16
,0.24 ,0.716 ,0.272 ,0.136 ,0.832
,0.212 ,0.548 ,0.776 ,0.328 ,0.492
,0.952 ,0.62 ,0.688 ,0.26 ,0.084
,0.264 ,0.856 ,0.912 ,0.796 ,0.78
,0.276 ,0.692 ,0.628 ,0.26 ,0.592
,0.66 ,0.66 ,0.912 ,0.84 ,0.244
,0.66 ,0.892 ,0.332 ,0.092 ,0.584
,0.804 ,0.408 ,0.036 ,0.22 ,0.02
,0.648 ,0.52 ,0.212 ,0.34 ,0.4
,0.38 ,0.156 ,0.464 ,0.32 ,0.944
,0.84 ,0.98 ,0.676 ,0.396 ,0.86
,0.884 ,0.272 ,0.712 ,0.444 ,0.24
,0.296 ,0.956 ,0.436 ,0.096 ,0.448
,0.796 ,0.084 ,0.872 ,0.368 ,0.828
,0.656 ,0.192 ,0.984 ,0.668 ,0.452
,0.992 ,0.904 ,0.572 ,0.768 ,0.42
,0.444 ,0.42, 0 ,0.456 ,0.464
,0.908 ,0.884 ,0.704 ,0.164 ,0.604
,0.924 ,0.748 ,0.688 ,0.648 ,0.968
,0.332 ,0.636 ,0.472 ,0.956 ,0.924
,0.6 ,0.788 ,0.488 ,0.156 ,0.904
,0.892 ,0.372 ,0.948 ,0.868 ,0.06
,0.58 ,0.604 ,0.9 ,0.212 ,0.824
,0.632 ,0.416 ,0.5 ,0.576 ,0.932
,0.472 ,0.932 ,0.936 ,0.96 ,0.26
,0.556 ,0.372 ,0.748 ,0.368 ,0.256
,0.076 ,0.676 ,0.292 ,0.504 ,0.6
,0.216 ,0.796 ,0.488 ,0.132 ,0.076
,0.02 ,0.48 ,0.848 ,0.772 ,0.524
,0.22 ,0.908 ,0.432 ,0.952 ,0.556
,0.12 ,0.868 ,0.756 ,0.732 ,0.56
,0.084 ,0.7 ,0.34 ,0.2 ,0.704
,0.336 ,0.092 ,0.22 ,0.944 ,0.044
,0.844 ,0.356 ,0.72 ,0.276 ,0.664
,0.828 ,0.492 ,0.392 ,0.368 ,0.32
,0.304 ,0.804 ,0.856 ,0.528 ,0.6
,0.056 ,0.908 ,0.124 ,0.448 ,0.632
,0.232 ,0.008 ,0.2 ,0.552 ,0.884
,0.82 ,0.92 ,0.744 ,0.26 ,0.492
,0.94 ,0.96 ,0.572 ,0.536 ,0.196
,0.992 ,0.524 ,0.356 ,0.116 ,0.072
,0.084 ,0.46 ,0.604 ,0.884 ,0.752
,0.812 ,0.36 ,0.492 ,0.508 ,0.42
,0.54 ,0.132 ,0.084 ,0.328 ,0.984
,0.104 ,0.592 ,0.172 ,0.992 ,0.688
,0.572 ,0.312 ,0.304 ,0.596 ,0.796
,0.488 ,0.388 ,0.188 ,0.456 ,0.716
,0.168 ,0.292 ,0.36 ,0.848 ,0.02
,0.756 ,0.6 ,0.956 ,0.676 ,0.864
,0.96 ,0.304 ,0.276 ,0.576 ,0.32
,0.324 ,0.776 ,0.66 ,0.652 ,0.832
,0.052 ,0.24 ,0.08 ,0.844 ,0.668
,0.44 ,0.844 ,0.476 ,0.224 ,0.604
,0.876 ,0.436 ,0.8 ,0.228 ,0.364
,0.792 ,0.052 ,0.94 ,0.444 ,0.796
,0.436 ,0.276 ,0.908 ,0.092 ,0.74
,0.128 ,0.76 ,0.256 ,0.56 ,0.376
,0.604 ,0.82 ,0.864 ,0.328 ,0.24
,0.244 ,0.28 ,0.648 ,0.452 ,0.56
,0.712 ,0.14 ,0.908 ,0.256 ,0.544
,0.176 ,0.36 ,0.924 ,0.584 ,0.216
,0.68 ,0.82 ,0.628 ,0.828 ,0.316
,0.52 ,0.34 ,0.172 ,0.916 ,0.54
,0.88 ,0.636 ,0.796 ,0.696 ,0.976
,0.68 ,0.368 ,0.456 ,0.764 ,0.736
,0.356 ,0.188 ,0.992 ,0.94 ,0.572
,0.112 ,0.736 ,0.476 ,0.58 ,0.772
,0.944 ,0.348 ,0.248 ,0.292 ,0.992
,0.916 ,0.128 ,0.904 ,0.804 ,0.66
,0.972 ,0.044 ,0.228 ,0.82 ,0.296
,0.92 ,0.368 ,0.924 ,0.96 ,0.928
,0.38 ,0.184 ,0.86 ,0.8 ,0.136
,0.304 ,0.512 ,0.684 ,0.612 ,0.624
,0.868 ,0.908 ,0.548 ,0.396 ,0.436
,0.668 ,0.92 ,0.196 ,0.156 ,0.176
,0.088 ,0.888 ,0.524 ,0.196 ,0.736
,0.736 ,0.884 ,0.072 ,0.824 ,0.456
,0.404 ,0.212 ,0.664 ,0.404 ,0.608
,0.532 ,0.62 ,0.816 ,0.496 ,0.836
,0.328 ,0.868 ,0.48 ,0.636 ,0.836
,0.668 ,0.424 ,0.364 ,0.276 ,0.376
,0.744 ,0.228 ,0.604 ,0.656 ,0.936
,0.344 ,0.54 ,0.868 ,0.876 ,0.184
,0.204 ,0.976 ,0.752 ,0.796 ,0.324
,0.88 ,0.108 ,0.552 ,0.92 ,0.132
,0.44 ,0.312 ,0.184 ,0.936 ,0.44
,0.62 ,0.492 ,0.976 ,0.764 ,0.94
,0.48 ,0.908 ,0.888 ,0.332 ,0.74
,0.532 ,0.64 ,0.976 ,0.668 ,0.992
,0.988 ,0.892 ,0.516 ,0.496 ,0.56
,0.016 ,0.616 ,0.224 ,0.3 ,0.684
,0.616 ,0.452 ,0.976 ,0.248 ,0.132
,0.256 ,0.136 ,0.956 ,0.144 ,0.96
,0.664 ,0.26 ,0.772 ,0.108 ,0.868
,0.516 ,0.268 ,0.376 ,0.532 ,0.68
,0.56 ,0.428 ,0.64 ,0.272 ,0.808
,0.22 ,0.156 ,0.184 ,0.436 ,0.452
,0.128 ,0.924 ,0.488 ,0.268 ,0.584
,0.596 ,0.892 ,0.284 ,0.916 ,0.424
,0.576 ,0.844 ,0.212 ,0.696 ,0.2
,0.88 ,0.548 ,0.728 ,0.88 ,0.72
,0.468 ,0.208 ,0.524 ,0.896 ,0.06
,0.516 ,0.736 ,0.508 ,0.524 ,0.9
,0.408 ,0.82 ,0.68 ,0.16 ,0.776
,0.84 ,0.756 ,0.236 ,0.8 ,0.84
,0.548 ,0.628 ,0.54 ,0.768 ,0.328
,0.476 ,0.604 ,0.22 ,0.844 ,0.396
,0.704 ,0.556 ,0.128 ,0.068 ,0.08
,0.424 ,0.544 ,0.556 ,0.464 ,0.74
,0.716 ,0.752 ,0.068 ,0.804 ,0.024
,0.632 ,0.68 ,0.868 ,0.328 ,0.448
,0.14 ,0.364 ,0.596 ,0.916 ,0.148
,0.504 ,0.62 ,0.3 ,0.536 ,0.024
,0.892 ,0.932 ,0.056 ,0.532 ,0.084
,0.248 ,0.268 ,0.944 ,0.212 ,0.92
,0.5 ,0.1 ,0.736 ,0.648 ,0.648
,0.236 ,0.604 ,0.588 ,0.416 ,0.88
,0.92 ,0.956 ,0.6 ,0.988 ,0.848
,0.54 ,0.384 ,0.868 ,0.748 ,0.256
,0.18 ,0.196 ,0.988 ,0.588 ,0.94
,0.856 ,0.856 ,0.512 ,0.008 ,0.748
,0.46 ,0.672 ,0.848 ,0.932 ,0.14
,0.708 ,0.812 ,0.608 ,0.692 ,0.756
,0.424 ,0.84 ,0.16 ,0.744 ,0.92
,0.892 ,0.676 ,0.68 ,0.164 ,0.796
,0.656 ,0.496 ,0.576 ,0.44 ,0.088
,0.232 ,0.592 ,0.04 ,0.808 ,0.632
,0.86 ,0.112 ,0.392 ,0.196 ,0.696
,0.912 ,0.872 ,0.72 ,0.484 ,0.348
,0.424 ,0.556 ,0.408 ,0.612 ,0.592
,0.636 ,0.584 ,0.088 ,0.28 ,0.444
,0.332 ,0.444 ,0.952 ,0.172 ,0.664
,0.008 ,0.468 ,0.624 ,0.7 ,0.3
,0.356 ,0.516 ,0.308 ,0.964 ,0.38
,0.984 ,0.956 ,0.96 ,0.604 ,0.044
,0.436 ,0.956 ,0.192 ,0.24 ,0.164
,0.888 ,0.904 ,0.98 ,0.924 ,0.584
,0.22 ,0.988 ,0.644 ,0.644 ,0.652
,0.712 ,0.676 ,0.136 ,0.144 ,0.656
,0.548 ,0.836 ,0.804 ,0.856 ,0.492
,0.86 ,0.744 ,0.808 ,0.404 ,0.62
,0.772 ,0.852 ,0.712 ,0.44 ,0.5
,0.768 ,0.728 ,0.276 ,0.776 ,0.316
,0.396 ,0.656 ,0.676 ,0.764 ,0.4
,0.988 ,0.276 ,0.952 ,0.32 ,0.552
,0.976 ,0.244 ,0.676 ,0.916 ,0.204
,0.152 ,0.548 ,0.708 ,0.764 ,0.524
,0.564 ,0.244 ,0.656 ,0.928 ,0.068
,0.984 ,0.524 ,0.9 ,0.792 ,0.636
,0.488 ,0.56 ,0.352 ,0.452 ,0.328
,0.504 ,0.348 ,0.804 ,0.272 ,0.348
,0.6 ,0.972 ,0.816 ,0.208 ,0.28
,0.652 ,0.944 ,0.468 ,0.1 ,0.676
,0.7 ,0.664 ,0.948 ,0.688 ,0.112
,0.816 ,0.088 ,0.572 ,0.236 ,0.912
,0.408 ,0.752 ,0.532 ,0.84 ,0.464
,0.292 ,0.052 ,0.088 ,0.784 ,0.396
,0.592 ,0.652 ,0.3 ,0.24 ,0.588
,0.936 ,0.084 ,0.696 ,0.74 ,0.516
,0.952 ,0.684 ,0.564 ,0.636 ,0.968
,0.184 ,0.3 ,0.256 ,0.804 ,0.64
,0.256 ,0.844 ,0.8 ,0.992 ,0.66
,0.492 ,0.428 ,0.94 ,0.064 ,0.748
,0.424 ,0.212 ,0.092 ,0.076 ,0.144
,0.776 ,0.228 ,0.48 ,0.596 ,0.324
,0.348 ,0.804 ,0.812 ,0.944 ,0.976
,0.864 ,0.956 ,0.996 ,0.54 ,0.736
,0.408 ,0.172 ,0.732 ,0.876 ,0.564
,0.028 ,0.2 ,0.444 ,0.612 ,0.252
,0.584 ,0.208 ,0.992 ,0.32 ,0.684
,0.144 ,0.38 ,0.852 ,0.084 ,0.292
,0.576 ,0.504 ,0.532 ,0.788 ,0.768
,0.664 ,0.86 ,0.728 ,0.556 ,0.664
,0.28 ,0.588 ,0.48 ,0.616 ,0.576
,0.796 ,0.412 ,0.596 ,0.216 ,0.972
,0.952 ,0.572 ,0.836 ,0.772 ,0.672
,0.176 ,0.96 ,0.892 ,0.04 ,0.416
,0.808 ,0.78 ,0.68 ,0.896 ,0.424
,0.404 ,0.556 ,0.824 ,0.004 ,0.816
,0.632 ,0.06 ,0.708 ,0.352 ,0.136
,0.416 ,0.78 ,0.94 ,0.872 ,0.128
,0.072 ,0.74 ,0.96 ,0.308 ,0.472
,0.252 ,0.112 ,0.376 ,0.816 ,0.408
,0.332 ,0.964 ,0.364 ,0.624 ,0.728
,0.764 ,0.088 ,0.024 ,0.052 ,0.032
,0.348 ,0.388 ,0.672 ,0.816 ,0.188
,0.064 ,0.62 ,0.744 ,0.408 ,0.572
,0.672 ,0.06 ,0.38 ,0.92 ,0.676
,0.848 ,0.756 ,0.504 ,0.92 ,0.092
,0.532 ,0.928 ,0.076 ,0.552 ,0.572
,0.28 ,0.916 ,0.788 ,0.312 ,0.868
,0.536 ,0.448 ,0.724 ,0.032 ,0.38
,0.828 ,0.836 ,0.328 ,0.18 ,0.544
,0.228 ,0.352 ,0.572 ,0.4 ,0.872
,0.5 ,0.36 ,0.48 ,0.324 ,0.656
,0.96 ,0.484 ,0.152 ,0.744 ,0.804
,0.908 ,0.844 ,0.216 ,0.968 ,0.784
,0.476 ,0.84 ,0.384 ,0.26 ,0.576
,0.444 ,0.472 ,0.636 ,0.272 ,0.8
,0.476 ,0.512 ,0.54 ,0.512 ,0.96
,0.272 ,0.764 ,0.324 ,0.952 ,0.604
,0.568 ,0.764 ,0.912 ,0.652 ,0.988
,0.192 ,0.408 ,0.684 ,0.208 ,0.164
,0.16 ,0.36 ,0.22 ,0.512 ,0.636
,0.372 ,0.376 ,0.548 ,0.636 ,0.824
,0.132 ,0.308 ,0.72 ,0.916 ,0.688
,0.556 ,0.556 ,0.608 ,0.708 ,0.22
,0.308 ,0.272 ,0.612 ,0.936 ,0.5
,0.608 ,0.956 ,0.76 ,0.832 ,0.668
,0.776 ,0.852 ,0.728 ,0.812 ,0.892
,0.408 ,0.96 ,0.708 ,0.744 ,0.408
,0.832 ,0.556 ,0.764 ,0.116 ,0.896
,0.052 ,0.452 ,0.9 ,0.232 ,0.484
,0.776 ,0.672 ,0.536 ,0.252 ,0.504
,0.044 ,0.584 ,0.908 ,0.96 ,0.932
,0.24 ,0.824 ,0.84 ,0.672 ,0.856
,0.116 ,0.104 ,0.912 ,0.648 ,0.852
,0.644 ,0.612 ,0.82 ,0.408 ,0.86
,0.724 ,0.684 ,0.68 ,0.516 ,0.78
,0.632 ,0.432 ,0.98 ,0.956 ,0.216
,0.228 ,0.576 ,0.304 ,0.4 ,0.448
,0.616 ,0.292 ,0.412 ,0.572 ,0.136
,0.568 ,0.488 ,0.228 ,0.46 ,0.5
,0.452 ,0.804 ,0.596 ,0.076 ,0.22
,0.92 ,0.868 ,0.492 ,0.428 ,0.524
,0.424 ,0.212 ,0.512 ,0.592 ,0.604
,0.784 ,0.688 ,0.48 ,0.588 ,0.564
,0.052 ,0.484 ,0.176 ,0.932 ,0.196
,0.968 ,0.744 ,0.9 ,0.648 ,0.832
,0.836 ,0.22 ,0.632 ,0.804 ,0.436
,0.184 ,0.588 ,0.864 ,0.884 ,0.82
,0.696 ,0.58 ,0.768 ,0.584 ,0.148
,0.66 ,0.696 ,0.268 ,0.04 ,0.716
,0.116 ,0.536 ,0.988 ,0.704 ,0.612
,0.872 ,0.052 ,0.352 ,0.624 ,0.624
,0.332 ,0.044 ,0.8 ,0.476 ,0.992
,0.788 ,0.872 ,0.276 ,0.208 ,0.632
,0.804 ,0.644 ,0.4 ,0.868 ,0.708
,0.636 ,0.088 ,0.036 ,0.936 ,0.04
,0.804 ,0.532 ,0.396 ,0.284 ,0.652
,0.928 ,0.768 ,0.82 ,0.668 ,0.408
,0.944 ,0.972 ,0.84 ,0.608 ,0.652
,0.304 ,0.316 ,0.728 ,0.968 ,0.804
,0.284 ,0.644 ,0.244 ,0.532 ,0.076
,0.748 ,0.048 ,0.384 ,0.424 ,0.752
,0.748 ,0.984 ,0.148 ,0.58 ,0.58
,0.28 ,0.248 ,0.7 ,0.572 ,0.648
,0.632 ,0.276 ,0.888 ,0.932 ,0.572
,0.308 ,0.072 ,0.5 ,0.392 ,0.068
,0.468 ,0.856 ,0.232 ,0.268,1
,0.164 ,0.484 ,0.032 ,0.852 ,0.86
,0.912 ,0.272 ,0.724 ,0.536 ,0.304
,0.456 ,0.752 ,0.344 ,0.18 ,0.968
,0.676 ,0.204 ,0.664 ,0.948 ,0.504
,0.696 ,0.468 ,0.508 ,0.324 ,0.292
,0.364 ,0.476 ,0.152 ,0.272 ,0.948
,0.684 ,0.352 ,0.26 ,0.836 ,0.084
,0.912 ,0.876 ,0.944 ,0.164 ,0.504
,0.704 ,0.604 ,0.216 ,0.248 ,0.536
,0.112 ,0.48 ,0.976 ,0.94 ,0.648
,0.764 ,0.664 ,0.54 ,0.012 ,0.424
,0.02 ,0.48 ,0.512 ,0.02 ,0.436
,0.42 ,0.072 ,0.54 ,0.76 ,0.576
,0.192 ,0.096 ,0.436 ,0.58 ,0.86
,0.196 ,0.368 ,0.984 ,0.552 ,0.632
,0.72 ,0.644 ,0.952 ,0.228 ,0.056
,0.376 ,0.368 ,0.04 ,0.316 ,0.464)
cp=hd(p.table,alpha)
pv=NULL
if(!is.null(p.obs))w=optimize(hdpv,interval=c(.001,.999),dat=p.table,obs=p.obs)$minimum
list(crit.p.value=cp,adj.p.value=w)
}




# ----------------------------------------------------------------------------

# p.crit.n60

# ----------------------------------------------------------------------------

p.crit.n60<-function(alpha, p.obs = NULL){
p.table=c(0.24 ,0.656 ,0.648 ,0.364 ,0.856
,0.052 ,0.476 ,0.304 ,0.216 ,0.72
,0.476 ,0.908 ,0.912 ,0.216 ,0.212
,0.476 ,0.488 ,0.868 ,0.072 ,0.088
,0.504 ,0.26 ,0.192 ,0.244 ,0.512
,0.432 ,0.32 ,0.236, 1 ,0.396
,0.504 ,0.4 ,0.756 ,0.996 ,0.08
,0.14 ,0.224 ,0.964 ,0.492 ,0.94
,0.528 ,0.652 ,0.884 ,0.84 ,0.648
,0.592 ,0.844 ,0.572 ,0.104 ,0.712
,0.948 ,0.612 ,0.88 ,0.684 ,0.552
,0.716 ,0.156 ,0.996 ,0.296 ,0.62
,0.02 ,0.164 ,0.532 ,0.372 ,0.104
,0.78 ,0.996 ,0.84 ,0.552 ,0.588
,0.668 ,0.088 ,0.78 ,0.76 ,0.708
,0.208 ,0.976 ,0.336 ,0.052 ,0.904
,0.648 ,0.588 ,0.668 ,0.108 ,0.996
,0.808 ,0.824 ,0.312 ,0.808 ,0.936
,0.616 ,0.212 ,0.496 ,0.628 ,0.736
,0.152 ,0.24 ,0.504 ,0.964 ,0.808
,0.528 ,0.232 ,0.2 ,0.356 ,0.26
,0.984 ,0.832 ,0.424 ,0.584 ,0.2
,0.356 ,0.432 ,0.568 ,0.348 ,0.784
,0.364 ,0.368 ,0.92 ,0.124 ,0.556
,0.096 ,0.828 ,0.676 ,0.752 ,0.724
,0.724 ,0.168 ,0.524 ,0.064 ,0.876
,0.112 ,0.408 ,0.544 ,0.56 ,0.104
,0.288 ,0.808 ,0.116 ,0.54 ,0.008
,0.988 ,0.46 ,0.616 ,0.644 ,0.64
,0.996 ,0.36 ,0.472 ,0.544 ,0.316
,0.848 ,0.868 ,0.872 ,0.46 ,0.816
,0.16 ,0.444 ,0.688 ,0.008 ,0.96
,0.296 ,0.132 ,0.868 ,0.452 ,0.352
,0.996 ,0.696 ,0.816 ,0.668 ,0.748
,0.024 ,0.968 ,0.692 ,0.8 ,0.2
,0.548 ,0.632 ,0.824 ,0.668 ,0.216
,0.228 ,0.336 ,0.388 ,0.824 ,0.824
,0.376 ,0.728 ,0.72 ,0.932 ,0.1
,0.136 ,0.1 ,0.96 ,0.988 ,0.516
,0.86 ,0.576 ,0.952 ,0.78 ,0.84
,0.948 ,0.94 ,0.536 ,0.704 ,0.816
,0.352 ,0.164 ,0.716 ,0.264 ,0.94
,0.228 ,0.404 ,0.704 ,0.744 ,0.308
,0.156 ,0.468 ,0.124 ,0.708 ,0.676
,0.432 ,0.472 ,0.244 ,0.124 ,0.124
,0.908 ,0.36 ,0.668 ,0.34 ,0.8
,0.48 ,0.112 ,0.792 ,0.428 ,0.724
,0.28 ,0.724 ,0.768 ,0.972 ,0.524
,0.436 ,0.008 ,0.664 ,0.648 ,0.704
,0.94 ,0.12 ,0.308 ,0.884 ,0.824
,0.248 ,0.112 ,0.572 ,0.492 ,0.052
,0.664 ,0.788 ,0.604 ,0.344 ,0.288
,0.996 ,0.696 ,0.996 ,0.852 ,0.28
,0.004 ,0.276 ,0.732 ,0.964 ,0.248
,0.456 ,0.044 ,0.232 ,0.776 ,0.196
,0.344 ,0.248 ,0.84 ,0.716 ,0.764
,0.628 ,0.312 ,0.616 ,0.352 ,0.944
,0.156 ,0.032 ,0.948 ,0.532 ,0.3
,0.792 ,0.844 ,0.148 ,0.224 ,0.512
,0.328 ,0.104 ,0.344 ,0.652 ,0.932
,0.972 ,0.356 ,0.168 ,0.284 ,0.364
,0.276 ,0.68 ,0.376 ,0.436 ,0.016
,0.936 ,0.416 ,0.212 ,0.664 ,0.824,
1 ,0.9 ,0.652 ,0.836 ,0.2
,0.036 ,0.072 ,0.88 ,0.748 ,0.668
,0.964 ,0.6 ,0.772 ,0.288 ,0.968
,0.484 ,0.928 ,0.436 ,0.588 ,0.976
,0.364 ,0.508 ,0.064 ,0.784 ,0.884
,0.54 ,0.08 ,0.252 ,0.768 ,0.156
,0.872, 0 ,0.672 ,0.572 ,0.94
,0.272 ,0.936 ,0.792 ,0.824 ,0.092
,0.884 ,0.492 ,0.336 ,0.724 ,0.64
,0.124 ,0.896 ,0.308 ,0.224 ,0.64
,0.932 ,0.712 ,0.1 ,0.884 ,0.76
,0.808 ,0.1 ,0.384 ,0.416 ,0.828
,0.992 ,0.6 ,0.288 ,0.02 ,0.392
,0.34 ,0.9 ,0.444 ,0.892 ,0.76
,0.964 ,0.88 ,0.428 ,0.612 ,0.728
,0.104 ,0.268 ,0.488 ,0.348 ,0.488
,0.208 ,0.52 ,0.96 ,0.572 ,0.18
,0.976 ,0.812 ,0.668 ,0.064 ,0.768
,0.98 ,0.484 ,0.84 ,0.876 ,0.132
,0.56 ,0.392 ,0.536 ,0.48 ,0.096
,0.608 ,0.556 ,0.196 ,0.884 ,0.744
,0.944 ,0.432 ,0.104 ,0.444 ,0.324
,0.896 ,0.472 ,0.716 ,0.792 ,0.984
,0.528 ,0.472 ,0.716 ,0.368 ,0.644
,0.432 ,0.288 ,0.004 ,0.716 ,0.688
,0.684 ,0.94 ,0.772 ,0.856 ,0.16
,0.784 ,0.244 ,0.412 ,0.384 ,0.264
,0.028 ,0.128 ,0.14 ,0.328 ,0.896
,0.224 ,0.676 ,0.32 ,0.668 ,0.156
,0.852 ,0.648 ,0.232 ,0.6 ,0.804
,0.692 ,0.528 ,0.728 ,0.588 ,0.64
,0.252 ,0.304 ,0.8 ,0.052 ,0.492
,0.76 ,0.4 ,0.708 ,0.628 ,0.888
,0.992 ,0.22 ,0.276 ,0.64 ,0.644
,0.232 ,0.476 ,0.584 ,0.72 ,0.5
,0.688 ,0.08 ,0.208 ,0.488 ,0.836
,0.708, 1 ,0.948 ,0.364 ,0.424
,0.636 ,0.94 ,0.308 ,0.172 ,0.352
,0.536 ,0.112 ,0.968 ,0.832 ,0.192
,0.36 ,0.888 ,0.552 ,0.784 ,0.376
,0.956 ,0.2 ,0.504 ,0.676 ,0.224
,0.66 ,0.172 ,0.256 ,0.664 ,0.324
,0.648 ,0.936 ,0.676 ,0.184 ,0.552
,0.648 ,0.812 ,0.304 ,0.464 ,0.564
,0.264 ,0.14 ,0.088 ,0.52 ,0.516
,0.404 ,0.464 ,0.852 ,0.2 ,0.66
,0.668 ,0.228 ,0.024 ,0.5 ,0.436
,0.712 ,0.776 ,0.264 ,0.576 ,0.956
,0.912 ,0.892 ,0.864 ,0.268 ,0.56
,0.528 ,0.368 ,0.036 ,0.536 ,0.412
,0.784 ,0.776 ,0.484 ,0.7 ,0.016
,0.732 ,0.672 ,0.544 ,0.312 ,0.128
,0.292 ,0.832 ,0.34 ,0.74 ,0.916
,0.764 ,0.34 ,0.256 ,0.164 ,0.704
,0.552 ,0.3 ,0.032 ,0.736 ,0.62
,0.192 ,0.532 ,0.38 ,0.552 ,0.264
,0.504 ,0.148 ,0.856 ,0.16 ,0.252
,0.236 ,0.048 ,0.144 ,0.08 ,0.912
,0.08 ,0.828 ,0.072 ,0.708 ,0.5
,0.324 ,0.196 ,0.48 ,0.304 ,0.28
,0.764 ,0.516 ,0.42 ,0.948 ,0.352
,0.408 ,0.248 ,0.212 ,0.596 ,0.252
,0.54 ,0.008 ,0.984 ,0.512 ,0.46
,0.716 ,0.396 ,0.288 ,0.416 ,0.544
,0.164 ,0.676 ,0.636 ,0.68 ,0.52
,0.108 ,0.484 ,0.936 ,0.892 ,0.292
,0.988 ,0.808 ,0.432 ,0.712 ,0.96
,0.296 ,0.956 ,0.4 ,0.576 ,0.52
,0.328 ,0.708 ,0.272 ,0.58 ,0.832
,0.476 ,0.544 ,0.676 ,0.636 ,0.096
,0.712 ,0.076 ,0.98 ,0.584 ,0.816
,0.524 ,0.08 ,0.244 ,0.392 ,0.784
,0.588 ,0.256 ,0.372 ,0.212 ,0.512
,0.936 ,0.176 ,0.636 ,0.088 ,0.62
,0.928 ,0.608 ,0.564 ,0.54 ,0.152
,0.736 ,0.732 ,0.1 ,0.412 ,0.596
,0.58 ,0.044 ,0.792 ,0.876 ,0.464
,0.88 ,0.732 ,0.112 ,0.304 ,0.88
,0.748 ,0.944 ,0.64 ,0.74 ,0.26
,0.184 ,0.532 ,0.256 ,0.172 ,0.808
,0.828 ,0.44 ,0.976 ,0.356 ,0.316,
1 ,0.736, 1 ,0.116 ,0.292
,0.908 ,0.86 ,0.344 ,0.236 ,0.476
,0.444 ,0.64 ,0.668 ,0.98 ,0.416
,0.344 ,0.916 ,0.84 ,0.5 ,0.548
,0.92 ,0.68 ,0.82 ,0.956 ,0.368
,0.764 ,0.536 ,0.192 ,0.272 ,0.892
,0.148 ,0.2 ,0.644 ,0.252 ,0.468
,0.304 ,0.248 ,0.96 ,0.768 ,0.632
,0.268 ,0.588 ,0.54 ,0.444 ,0.548
,0.856 ,0.732 ,0.884 ,0.672 ,0.324
,0.62 ,0.524 ,0.568 ,0.08 ,0.992
,0.744 ,0.484 ,0.628 ,0.644 ,0.612,
0 ,0.592 ,0.948 ,0.128 ,0.892
,0.972 ,0.108 ,0.68 ,0.72 ,0.876
,0.328 ,0.236 ,0.612 ,0.716 ,0.912
,0.168 ,0.612 ,0.12 ,0.676 ,0.58
,0.504 ,0.34 ,0.964 ,0.256 ,0.668
,0.584 ,0.388 ,0.016 ,0.796 ,0.68
,0.628 ,0.476 ,0.776 ,0.696 ,0.348
,0.656 ,0.036 ,0.036 ,0.596 ,0.824
,0.464 ,0.732 ,0.58 ,0.364 ,0.38
,0.632 ,0.488 ,0.108 ,0.832 ,0.856
,0.448 ,0.272 ,0.932 ,0.388 ,0.788
,0.476 ,0.576 ,0.776 ,0.672 ,0.312
,0.7 ,0.86 ,0.784 ,0.988 ,0.328
,0.792 ,0.196 ,0.236 ,0.344 ,0.396
,0.596 ,0.98 ,0.972 ,0.492 ,0.624
,0.68 ,0.744 ,0.996 ,0.548 ,0.976
,0.828 ,0.764 ,0.784 ,0.408 ,0.768
,0.452 ,0.232 ,0.572 ,0.112 ,0.468
,0.96 ,0.772 ,0.668 ,0.928 ,0.788
,0.832 ,0.128 ,0.104 ,0.652 ,0.972
,0.888 ,0.852 ,0.572 ,0.484 ,0.272
,0.44 ,0.948 ,0.352 ,0.684 ,0.932
,0.46 ,0.576 ,0.044 ,0.456 ,0.972
,0.904 ,0.784 ,0.048 ,0.312 ,0.352
,0.844 ,0.8 ,0.616 ,0.676 ,0.604
,0.836 ,0.936 ,0.732 ,0.728 ,0.372
,0.34 ,0.344 ,0.988 ,0.312 ,0.688
,0.04 ,0.272 ,0.632 ,0.576 ,0.244
,0.188 ,0.46 ,0.78 ,0.648 ,0.248
,0.308 ,0.02 ,0.712 ,0.572 ,0.392
,0.884 ,0.64 ,0.048 ,0.34 ,0.724
,0.832 ,0.932 ,0.664 ,0.336 ,0.8
,0.52 ,0.352 ,0.844 ,0.084 ,0.424
,0.548 ,0.692 ,0.188 ,0.364 ,0.872
,0.136 ,0.084 ,0.952 ,0.652 ,0.592
,0.028 ,0.836 ,0.804 ,0.536 ,0.984
,0.932 ,0.984 ,0.628 ,0.84 ,0.8
,0.496 ,0.196 ,0.664 ,0.608 ,0.416
,0.032 ,0.448 ,0.348 ,0.984 ,0.912
,0.448 ,0.856 ,0.588 ,0.896 ,0.776
,0.092 ,0.728 ,0.928 ,0.496 ,0.432
,0.056 ,0.468 ,0.364 ,0.488 ,0.224
,0.788 ,0.084 ,0.096 ,0.788 ,0.62
,0.22 ,0.804 ,0.18 ,0.532 ,0.368
,0.496 ,0.476 ,0.904 ,0.156 ,0.744
,0.344 ,0.396 ,0.152 ,0.644 ,0.5
,0.888 ,0.276 ,0.756 ,0.604 ,0.76
,0.92 ,0.412 ,0.872 ,0.536 ,0.612
,0.54 ,0.216 ,0.668 ,0.6 ,0.148
,0.78 ,0.672 ,0.472 ,0.816 ,0.844
,0.964 ,0.42 ,0.824 ,0.78 ,0.296
,0.956 ,0.104 ,0.704 ,0.288 ,0.92
,0.984 ,0.228 ,0.528 ,0.804 ,0.5
,0.668 ,0.356 ,0.86 ,0.536 ,0.412
,0.028 ,0.04 ,0.284 ,0.536 ,0.348
,0.524 ,0.796 ,0.872 ,0.912 ,0.216
,0.496 ,0.912 ,0.684 ,0.18 ,0.14
,0.132 ,0.48 ,0.64 ,0.524 ,0.992
,0.416 ,0.764 ,0.484 ,0.848 ,0.788
,0.764 ,0.588 ,0.284 ,0.7 ,0.056
,0.76 ,0.408 ,0.664 ,0.744 ,0.104
,0.86 ,0.96 ,0.66 ,0.816 ,0.256
,0.892 ,0.62 ,0.692 ,0.832 ,0.592
,0.052 ,0.84 ,0.96 ,0.888 ,0.828
,0.244 ,0.388 ,0.036 ,0.188 ,0.34
,0.94 ,0.304 ,0.472 ,0.436 ,0.728
,0.636 ,0.796 ,0.836 ,0.748 ,0.328
,0.452 ,0.248 ,0.352 ,0.212 ,0.892
,0.8 ,0.892 ,0.816 ,0.708 ,0.356
,0.344 ,0.5 ,0.232 ,0.548 ,0.744
,0.56 ,0.28 ,0.964 ,0.284 ,0.352
,0.268 ,0.908 ,0.924 ,0.664 ,0.788
,0.56 ,0.616 ,0.748 ,0.208 ,0.476
,0.632 ,0.88 ,0.364 ,0.192 ,0.824
,0.368 ,0.188 ,0.86 ,0.872 ,0.196
,0.86 ,0.204 ,0.616 ,0.572 ,0.56
,0.44 ,0.972 ,0.952 ,0.812 ,0.268
,0.14 ,0.276 ,0.736 ,0.6 ,0.312
,0.404 ,0.636 ,0.376 ,0.064 ,0.416
,0.4 ,0.248 ,0.904 ,0.412 ,0.748
,0.316 ,0.064 ,0.524 ,0.632 ,0.588
,0.536 ,0.656 ,0.768 ,0.68 ,0.14
,0.448 ,0.568 ,0.708 ,0.156 ,0.628
,0.596 ,0.84 ,0.788 ,0.052 ,0.14
,0.724 ,0.856 ,0.544 ,0.616 ,0.544
,0.496 ,0.268 ,0.776 ,0.316 ,0.152
,0.72 ,0.684 ,0.1 ,0.84 ,0.04
,0.788 ,0.948 ,0.868 ,0.196 ,0.508
,0.212 ,0.412 ,0.872 ,0.652 ,0.072
,0.412 ,0.988 ,0.86 ,0.912 ,0.156
,0.432 ,0.54 ,0.492 ,0.624 ,0.508
,0.508 ,0.992 ,0.852 ,0.788 ,0.244
,0.632 ,0.824 ,0.112 ,0.036 ,0.184
,0.668 ,0.756 ,0.884 ,0.788 ,0.52
,0.44 ,0.084 ,0.932 ,0.752 ,0.6
,0.656 ,0.84 ,0.832 ,0.6 ,0.664
,0.96 ,0.664 ,0.984 ,0.652 ,0.832
,0.928 ,0.592 ,0.44 ,0.304 ,0.86
,0.04 ,0.724 ,0.636 ,0.188 ,0.96
,0.9 ,0.564 ,0.28 ,0.34 ,0.492
,0.456 ,0.988 ,0.328 ,0.612 ,0.76
,0.964 ,0.736 ,0.744 ,0.64 ,0.272
,0.436 ,0.064 ,0.688 ,0.416 ,0.956
,0.824 ,0.14 ,0.648 ,0.492 ,0.072
,0.244 ,0.436 ,0.728 ,0.48 ,0.188
,0.168 ,0.432 ,0.46 ,0.044 ,0.824
,0.924 ,0.336 ,0.328 ,0.224 ,0.96
,0.552 ,0.948 ,0.712 ,0.96 ,0.396
,0.716 ,0.16 ,0.176 ,0.988 ,0.62
,0.892 ,0.18 ,0.576 ,0.264 ,0.924
,0.16 ,0.356 ,0.724 ,0.772 ,0.532
,0.944 ,0.744 ,0.268 ,0.184 ,0.972
,0.968 ,0.044 ,0.732 ,0.732 ,0.616
,0.716 ,0.804 ,0.86 ,0.344 ,0.704
,0.932 ,0.568 ,0.356 ,0.944 ,0.232
,0.452 ,0.212 ,0.692 ,0.88 ,0.86
,0.4 ,0.904 ,0.008 ,0.528 ,0.576
,0.86 ,0.696 ,0.116 ,0.444 ,0.472
,0.224 ,0.372 ,0.248 ,0.68 ,0.104
,0.476 ,0.228 ,0.532 ,0.24 ,0.648
,0.828 ,0.416 ,0.08 ,0.48 ,0.76
,0.3 ,0.556 ,0.628 ,0.396 ,0.864
,0.272 ,0.564 ,0.984 ,0.312 ,0.968
,0.448 ,0.044 ,0.664 ,0.408 ,0.732
,0.464 ,0.532 ,0.76 ,0.712 ,0.132
,0.324 ,0.936 ,0.872 ,0.768 ,0.432
,0.848 ,0.464 ,0.72 ,0.496 ,0.464
,0.752 ,0.808 ,0.372 ,0.204 ,0.604
,0.432 ,0.128 ,0.268 ,0.336 ,0.728
,0.824 ,0.212 ,0.704 ,0.172 ,0.408
,0.9 ,0.924 ,0.448 ,0.912 ,0.688
,0.748 ,0.672 ,0.044 ,0.704 ,0.568
,0.356 ,0.116 ,0.94 ,0.688 ,0.948
,0.776 ,0.664 ,0.732 ,0.108 ,0.72
,0.24 ,0.964 ,0.42 ,0.412 ,0.764
,0.104 ,0.868 ,0.308 ,0.62 ,0.608
,0.404 ,0.6 ,0.664 ,0.152 ,0.432
,0.544 ,0.26 ,0.604 ,0.584 ,0.64
,0.404 ,0.08 ,0.244 ,0.452 ,0.78
,0.7 ,0.896 ,0.66, 0 ,0.5
,0.368 ,0.468 ,0.432 ,0.76 ,0.68
,0.528 ,0.548 ,0.076 ,0.448 ,0.288
,0.592 ,0.64 ,0.668 ,0.548 ,0.32
,0.852 ,0.916 ,0.9 ,0.696 ,0.6
,0.968 ,0.22 ,0.992 ,0.456 ,0.788
,0.168 ,0.988 ,0.896 ,0.268 ,0.552
,0.596 ,0.54 ,0.384 ,0.596 ,0.896
,0.896 ,0.14 ,0.588 ,0.52 ,0.508
,0.784 ,0.892 ,0.548 ,0.652 ,0.34
,0.94 ,0.78 ,0.76 ,0.8 ,0.22
,0.78 ,0.428 ,0.504 ,0.592 ,0.084
,0.928 ,0.324 ,0.664 ,0.732 ,0.784
,0.98 ,0.38 ,0.812 ,0.236 ,0.092
,0.156 ,0.712 ,0.424 ,0.776 ,0.612
,0.156 ,0.544 ,0.332 ,0.292 ,0.644
,0.804 ,0.42 ,0.368 ,0.004 ,0.74
,0.52 ,0.472 ,0.06 ,0.664 ,0.572
,0.684 ,0.592 ,0.476 ,0.116 ,0.296
,0.564 ,0.24 ,0.556 ,0.488 ,0.588
,0.168 ,0.324 ,0.408 ,0.284 ,0.472
,0.56 ,0.752 ,0.76 ,0.992 ,0.16
,0.42 ,0.564 ,0.984 ,0.82 ,0.72
,0.356 ,0.328 ,0.96 ,0.356 ,0.644
,0.268 ,0.544 ,0.104 ,0.84 ,0.972
,0.556 ,0.248 ,0.04 ,0.372 ,0.592
,0.588 ,0.468 ,0.968 ,0.44 ,0.64
,0.216 ,0.792 ,0.476 ,0.724 ,0.068
,0.472 ,0.992 ,0.484 ,0.888 ,0.908
,0.376 ,0.84 ,0.468 ,0.748 ,0.356
,0.712 ,0.628 ,0.912 ,0.496 ,0.648
,0.124 ,0.396 ,0.508 ,0.312 ,0.128
,0.788 ,0.92 ,0.468 ,0.372 ,0.216
,0.54 ,0.84 ,0.608 ,0.464 ,0.744
,0.336 ,0.184 ,0.496 ,0.44 ,0.444
,0.136 ,0.504 ,0.568 ,0.852 ,0.804
,0.376 ,0.708 ,0.676 ,0.476 ,0.708
,0.06 ,0.98 ,0.436 ,0.796 ,0.448
,0.46 ,0.452 ,0.144 ,0.504 ,0.592
,0.848 ,0.628 ,0.5 ,0.784 ,0.492
,0.444 ,0.196 ,0.876 ,0.832 ,0.636
,0.24 ,0.908 ,0.484 ,0.544 ,0.808
,0.256 ,0.664 ,0.272 ,0.716 ,0.196
,0.272 ,0.484 ,0.94 ,0.168 ,0.956
,0.856 ,0.82, 0 ,0.868 ,0.796
,0.44 ,0.656 ,0.82 ,0.208 ,0.924
,0.352 ,0.832 ,0.844 ,0.324 ,0.216
,0.832 ,0.348 ,0.904 ,0.244 ,0.324
,0.816 ,0.64 ,0.892 ,0.116 ,0.392
,0.472 ,0.772 ,0.464 ,0.556 ,0.892
,0.232 ,0.224 ,0.788 ,0.5 ,0.04
,0.532 ,0.284 ,0.62 ,0.464 ,0.924
,0.804 ,0.252 ,0.224 ,0.38 ,0.244
,0.076 ,0.424 ,0.988 ,0.78 ,0.324
,0.076 ,0.496 ,0.844 ,0.496 ,0.288
,0.556 ,0.696 ,0.76 ,0.352 ,0.952
,0.26 ,0.752 ,0.084 ,0.08 ,0.324
,0.776 ,0.632 ,0.712 ,0.868 ,0.12
,0.808 ,0.76 ,0.444 ,0.664 ,0.16
,0.308 ,0.912 ,0.16 ,0.04 ,0.692
,0.336 ,0.672 ,0.664 ,0.556 ,0.876
,0.172 ,0.52 ,0.188 ,0.904 ,0.552
,0.32 ,0.292 ,0.216 ,0.58 ,0.988
,0.724 ,0.704 ,0.212 ,0.74 ,0.348
,0.912 ,0.592 ,0.108 ,0.332 ,0.536
,0.22 ,0.452 ,0.124 ,0.98 ,0.364
,0.908 ,0.272 ,0.564 ,0.556 ,0.668
,0.432 ,0.928 ,0.596 ,0.992 ,0.456
,0.832 ,0.624 ,0.832 ,0.064 ,0.52
,0.096 ,0.492 ,0.62 ,0.416 ,0.448
,0.928 ,0.68 ,0.976 ,0.192 ,0.728
,0.496 ,0.756 ,0.372 ,0.18 ,0.196
,0.808 ,0.816 ,0.996 ,0.352 ,0.648
,0.48 ,0.172 ,0.94 ,0.864 ,0.844
,0.228 ,0.4 ,0.12 ,0.12 ,0.344
,0.952 ,0.632 ,0.376 ,0.264 ,0.88
,0.252 ,0.332 ,0.56 ,0.756 ,0.468
,0.144 ,0.26 ,0.316 ,0.528 ,0.224
,0.512 ,0.568 ,0.724 ,0.912 ,0.384
,0.616 ,0.304 ,0.652 ,0.2 ,0.996
,0.292 ,0.36 ,0.788 ,0.612 ,0.768
,0.748 ,0.624 ,0.664 ,0.696 ,0.792
,0.64 ,0.176 ,0.78 ,0.8 ,0.6
,0.108 ,0.568 ,0.168 ,0.92 ,0.044
,0.872 ,0.848 ,0.296 ,0.9 ,0.648
,0.544 ,0.124 ,0.66 ,0.664 ,0.28
,0.164 ,0.564 ,0.768 ,0.552 ,0.852
,0.508 ,0.652 ,0.8 ,0.532 ,0.596
,0.204 ,0.036 ,0.22 ,0.076 ,0.972
,0.684 ,0.148 ,0.248 ,0.24 ,0.948
,0.356 ,0.06 ,0.684 ,0.244 ,0.516
,0.192 ,0.912 ,0.388 ,0.656 ,0.852
,0.644 ,0.704 ,0.976 ,0.9 ,0.664
,0.98 ,0.744 ,0.156 ,0.676 ,0.78
,0.936 ,0.78 ,0.284 ,0.3 ,0.904
,0.38 ,0.324 ,0.524 ,0.228 ,0.7
,0.264 ,0.868 ,0.62 ,0.416 ,0.356
,0.772 ,0.464 ,0.92 ,0.9 ,0.148
,0.204 ,0.364 ,0.956 ,0.888 ,0.536
,0.196 ,0.048 ,0.232 ,0.872 ,0.496
,0.524 ,0.576 ,0.7 ,0.368 ,0.248
,0.532 ,0.408 ,0.372 ,0.492 ,0.432
,0.508 ,0.468 ,0.576 ,0.704 ,0.84
,0.472 ,0.08 ,0.728 ,0.548 ,0.336
,0.572 ,0.564 ,0.032 ,0.352 ,0.84)
cp=hd(p.table,alpha)
pv=NULL
if(!is.null(p.obs))w=optimize(hdpv,interval=c(.001,.999),dat=p.table,obs=p.obs)$minimum
list(crit.p.value=cp,adj.p.value=w)
}




# ----------------------------------------------------------------------------

# p.crit.n80

# ----------------------------------------------------------------------------

p.crit.n80<-function(alpha=.05,p.obs=NULL){
p.table=c(
0.46 ,0.36 ,0.66 ,0.56 ,0.704
,0.848 ,0.008 ,0.232 ,0.072 ,0.784
,0.944 ,0.096, 0 ,0.252 ,0.464
,0.132, 1 ,0.116 ,0.288 ,0.236
,0.512 ,0.056 ,0.68 ,0.356 ,0.164
,0.36 ,0.444 ,0.448 ,0.656 ,0.464
,0.616 ,0.296 ,0.7 ,0.34 ,0.152
,0.248 ,0.776 ,0.516 ,0.084 ,0.908
,0.084 ,0.268 ,0.048 ,0.612 ,0.876
,0.752 ,0.108 ,0.916 ,0.756 ,0.424
,0.772 ,0.044 ,0.788 ,0.936 ,0.48
,0.824 ,0.784 ,0.944 ,0.5 ,0.236
,0.564 ,0.956 ,0.1 ,0.536 ,0.772
,0.82 ,0.956 ,0.556 ,0.76 ,0.78
,0.144 ,0.512 ,0.964 ,0.928 ,0.04
,0.86 ,0.364 ,0.98 ,0.252 ,0.548
,0.252 ,0.264 ,0.96 ,0.46 ,0.744
,0.932 ,0.58 ,0.448 ,0.708 ,0.928
,0.976 ,0.288 ,0.224 ,0.436 ,0.84
,0.056 ,0.68 ,0.04 ,0.848 ,0.46
,0.104 ,0.5 ,0.736 ,0.808 ,0.436
,0.692 ,0.944 ,0.552 ,0.024 ,0.36
,0.668 ,0.764 ,0.952 ,0.62 ,0.072
,0.972 ,0.816 ,0.392 ,0.964 ,0.356
,0.62 ,0.272 ,0.416 ,0.68 ,0.116
,0.632 ,0.904 ,0.36 ,0.212 ,0.632
,0.664 ,0.576 ,0.56 ,0.516 ,0.876
,0.82 ,0.736 ,0.044 ,0.648 ,0.36
,0.328 ,0.42 ,0.708 ,0.868 ,0.356
,0.076 ,0.856 ,0.828 ,0.124 ,0.72
,0.4 ,0.5 ,0.028 ,0.64 ,0.936
,0.492 ,0.96 ,0.28 ,0.688 ,0.488
,0.684 ,0.24 ,0.828 ,0.624 ,0.928
,0.492 ,0.696 ,0.2 ,0.424 ,0.868
,0.22 ,0.532 ,0.204 ,0.376 ,0.256
,0.612 ,0.64 ,0.72 ,0.76 ,0.216
,0.468 ,0.756 ,0.576 ,0.856 ,0.132
,0.916 ,0.248 ,0.364 ,0.828 ,0.896
,0.536 ,0.792 ,0.6 ,0.72 ,0.86
,0.592 ,0.056 ,0.628 ,0.016 ,0.688
,0.656 ,0.484 ,0.26 ,0.66 ,0.304
,0.476 ,0.648 ,0.848 ,0.68 ,0.832
,0.92 ,0.568, 1 ,0.056 ,0.236
,0.648 ,0.42 ,0.284 ,0.708 ,0.296
,0.944 ,0.952 ,0.336 ,0.48 ,0.356
,0.86 ,0.496 ,0.976 ,0.692 ,0.624
,0.1 ,0.568 ,0.236 ,0.88 ,0.36
,0.864 ,0.124 ,0.94 ,0.884 ,0.512
,0.22 ,0.896 ,0.3 ,0.684 ,0.7
,0.316 ,0.696 ,0.66 ,0.864 ,0.548
,0.884 ,0.656 ,0.204 ,0.88 ,0.936
,0.264 ,0.604 ,0.34 ,0.832 ,0.728
,0.644 ,0.924 ,0.524 ,0.808 ,0.612
,0.36 ,0.936 ,0.884 ,0.904 ,0.748
,0.6 ,0.648 ,0.16 ,0.8 ,0.312
,0.42 ,0.544 ,0.744 ,0.292 ,0.5
,0.028 ,0.804 ,0.9 ,0.648 ,0.984
,0.432 ,0.844 ,0.936 ,0.796 ,0.948
,0.608 ,0.976 ,0.552 ,0.94 ,0.424
,0.848 ,0.916 ,0.728 ,0.764 ,0.604
,0.508 ,0.74 ,0.468 ,0.268 ,0.748
,0.072 ,0.468 ,0.82 ,0.24 ,0.596
,0.18 ,0.188 ,0.612 ,0.152 ,0.996
,0.96 ,0.332 ,0.72 ,0.44 ,0.364
,0.704 ,0.612 ,0.248 ,0.72 ,0.568
,0.956 ,0.524 ,0.352 ,0.708 ,0.368
,0.924 ,0.384 ,0.476 ,0.912 ,0.736
,0.368 ,0.412 ,0.232 ,0.348 ,0.016
,0.568 ,0.34 ,0.608 ,0.356 ,0.772
,0.944 ,0.336 ,0.504 ,0.908 ,0.812
,0.292 ,0.904 ,0.16 ,0.076 ,0.928
,0.912 ,0.12 ,0.28 ,0.156 ,0.248
,0.988 ,0.44 ,0.764 ,0.088 ,0.256
,0.208 ,0.08 ,0.288 ,0.172 ,0.428
,0.428 ,0.276 ,0.084 ,0.344 ,0.132
,0.492 ,0.728 ,0.26 ,0.956 ,0.56
,0.344 ,0.176 ,0.864 ,0.54 ,0.24
,0.724 ,0.384 ,0.916 ,0.956 ,0.692
,0.88 ,0.66 ,0.372 ,0.128 ,0.568
,0.636 ,0.28 ,0.288 ,0.888 ,0.872
,0.42 ,0.356 ,0.604 ,0.72 ,0.852
,0.408 ,0.976 ,0.52 ,0.556 ,0.9
,0.364 ,0.716 ,0.588 ,0.72 ,0.312
,0.224 ,0.26 ,0.116 ,0.952 ,0.404
,0.952 ,0.948 ,0.22 ,0.676 ,0.58
,0.724 ,0.144 ,0.084 ,0.396 ,0.664
,0.16 ,0.412 ,0.796 ,0.476 ,0.284
,0.8 ,0.348 ,0.736 ,0.26 ,0.672
,0.372 ,0.904 ,0.768 ,0.82 ,0.736
,0.548 ,0.788 ,0.068 ,0.008 ,0.548
,0.304, 1 ,0.2 ,0.12 ,0.168
,0.4 ,0.504 ,0.68 ,0.96 ,0.924
,0.884 ,0.348 ,0.044 ,0.236 ,0.416
,0.32 ,0.612 ,0.512 ,0.34 ,0.604
,0.868 ,0.412 ,0.376 ,0.376 ,0.88
,0.864 ,0.928 ,0.364 ,0.42 ,0.048
,0.116 ,0.66 ,0.916 ,0.344 ,0.596
,0.768 ,0.84 ,0.964 ,0.92 ,0.948
,0.54 ,0.828 ,0.44 ,0.932 ,0.972
,0.244 ,0.948 ,0.1 ,0.228 ,0.88
,0.808 ,0.404 ,0.016 ,0.996 ,0.236
,0.88 ,0.076 ,0.156 ,0.172 ,0.692
,0.312 ,0.248 ,0.968 ,0.264 ,0.088
,0.296 ,0.824 ,0.444 ,0.24 ,0.996
,0.42 ,0.744 ,0.5 ,0.872 ,0.556
,0.68 ,0.172 ,0.216 ,0.688 ,0.94
,0.136 ,0.78 ,0.408 ,0.768 ,0.348
,0.568 ,0.324 ,0.116 ,0.968 ,0.132
,0.528 ,0.92 ,0.98 ,0.308 ,0.528
,0.112 ,0.056 ,0.1 ,0.616 ,0.636
,0.628 ,0.288 ,0.576 ,0.296 ,0.992
,0.048 ,0.088 ,0.664, 1 ,0.044
,0.796 ,0.284 ,0.02 ,0.692 ,0.488
,0.524 ,0.344 ,0.472 ,0.796 ,0.244
,0.112 ,0.9 ,0.012 ,0.328 ,0.508
,0.664 ,0.892 ,0.404 ,0.792 ,0.744
,0.752 ,0.864 ,0.448 ,0.756 ,0.252
,0.1 ,0.788 ,0.948 ,0.448 ,0.964
,0.416 ,0.5 ,0.236 ,0.828 ,0.344
,0.964 ,0.552 ,0.392 ,0.948 ,0.864
,0.908 ,0.12, 0 ,0.14 ,0.516
,0.856 ,0.476 ,0.828 ,0.232 ,0.636
,0.612 ,0.668 ,0.892 ,0.792 ,0.76
,0.968 ,0.072 ,0.896 ,0.636 ,0.62
,0.32 ,0.072 ,0.684 ,0.6 ,0.9
,0.452 ,0.196 ,0.892 ,0.788 ,0.532
,0.46 ,0.576 ,0.6 ,0.948 ,0.98
,0.992 ,0.156 ,0.292 ,0.956 ,0.7
,0.472 ,0.428 ,0.6 ,0.772 ,0.864
,0.388 ,0.636 ,0.308 ,0.492 ,0.188
,0.144 ,0.916 ,0.808 ,0.76 ,0.212
,0.516 ,0.556 ,0.056 ,0.2 ,0.676
,0.076 ,0.62 ,0.984 ,0.824 ,0.204
,0.024 ,0.656 ,0.176 ,0.804 ,0.936
,0.576 ,0.316 ,0.544 ,0.94 ,0.128
,0.62 ,0.464 ,0.116 ,0.188 ,0.372
,0.732 ,0.956 ,0.256 ,0.832 ,0.816
,0.152 ,0.22 ,0.632 ,0.712 ,0.364
,0.988 ,0.504 ,0.728 ,0.984 ,0.776
,0.8 ,0.876 ,0.612 ,0.896 ,0.152
,0.532 ,0.88 ,0.968 ,0.256 ,0.456
,0.552 ,0.056 ,0.352 ,0.808 ,0.64
,0.172 ,0.176 ,0.092 ,0.62 ,0.9
,0.768 ,0.024 ,0.1 ,0.896 ,0.36
,0.212 ,0.42 ,0.52 ,0.884 ,0.684
,0.896 ,0.596 ,0.664 ,0.848 ,0.432
,0.04 ,0.688 ,0.884 ,0.032 ,0.628
,0.2 ,0.832 ,0.508 ,0.784 ,0.476
,0.956 ,0.628 ,0.232 ,0.844 ,0.94
,0.068 ,0.952 ,0.212 ,0.352 ,0.6
,0.196 ,0.808 ,0.628 ,0.112 ,0.628
,0.94 ,0.216 ,0.816 ,0.212 ,0.788
,0.524 ,0.9 ,0.72 ,0.364 ,0.436
,0.152 ,0.176 ,0.544 ,0.56 ,0.152
,0.772 ,0.732 ,0.324 ,0.224 ,0.456
,0.732 ,0.304 ,0.124 ,0.524 ,0.256
,0.376 ,0.464 ,0.836 ,0.66 ,0.964
,0.784 ,0.772 ,0.624 ,0.56 ,0.04
,0.584 ,0.168 ,0.132 ,0.62 ,0.276
,0.096 ,0.928 ,0.588 ,0.66 ,0.952
,0.908 ,0.224 ,0.348 ,0.64 ,0.456
,0.22 ,0.052 ,0.232 ,0.908 ,0.92
,0.648 ,0.492 ,0.416 ,0.444 ,0.98
,0.92 ,0.624 ,0.612 ,0.684 ,0.744
,0.624 ,0.068 ,0.58 ,0.048 ,0.656
,0.352 ,0.56 ,0.708 ,0.124 ,0.7
,0.9 ,0.268 ,0.836 ,0.208 ,0.752
,0.136 ,0.54 ,0.136 ,0.336 ,0.988
,0.78 ,0.612 ,0.772 ,0.36 ,0.452
,0.616 ,0.424 ,0.736 ,0.856 ,0.5
,0.472 ,0.62 ,0.736 ,0.896 ,0.08
,0.8 ,0.276 ,0.124 ,0.116 ,0.692
,0.404 ,0.78 ,0.484 ,0.268 ,0.624
,0.776 ,0.28 ,0.908 ,0.576 ,0.208
,0.028 ,0.752 ,0.58 ,0.904 ,0.672
,0.716 ,0.364 ,0.732 ,0.3 ,0.444
,0.568 ,0.388 ,0.476 ,0.356 ,0.124
,0.432 ,0.996 ,0.492 ,0.964 ,0.356
,0.58 ,0.792 ,0.948 ,0.204 ,0.392
,0.808 ,0.296 ,0.252 ,0.404, 0
,0.836 ,0.096 ,0.336 ,0.892 ,0.112
,0.476 ,0.54 ,0.364 ,0.916 ,0.9
,0.548 ,0.808 ,0.272 ,0.212 ,0.38
,0.384 ,0.656 ,0.38 ,0.436 ,0.58
,0.728 ,0.464 ,0.88 ,0.988 ,0.888
,0.208 ,0.476 ,0.28 ,0.984 ,0.536
,0.692 ,0.28 ,0.396 ,0.632 ,0.66
,0.812 ,0.636 ,0.728 ,0.12 ,0.896
,0.548 ,0.536 ,0.032 ,0.74 ,0.336
,0.572 ,0.932 ,0.188 ,0.196 ,0.82
,0.456 ,0.892 ,0.424 ,0.276 ,0.848
,0.948 ,0.952 ,0.656 ,0.332 ,0.92
,0.552 ,0.664 ,0.536 ,0.708 ,0.972
,0.44 ,0.864 ,0.076, 1 ,0.104
,0.416 ,0.104 ,0.324 ,0.24 ,0.708
,0.992 ,0.996 ,0.184 ,0.156 ,0.18
,0.852 ,0.836 ,0.092 ,0.896 ,0.08
,0.816 ,0.384 ,0.268 ,0.324 ,0.1
,0.708 ,0.188 ,0.732 ,0.056 ,0.776
,0.768 ,0.744 ,0.22 ,0.648 ,0.236
,0.648 ,0.752 ,0.936 ,0.672 ,0.796
,0.4 ,0.732 ,0.64 ,0.9 ,0.208
,0.536 ,0.828 ,0.54 ,0.4 ,0.04
,0.084 ,0.5 ,0.232 ,0.724 ,0.752
,0.076 ,0.564 ,0.836 ,0.352 ,0.808
,0.916 ,0.928 ,0.752 ,0.584 ,0.344
,0.584 ,0.62 ,0.316 ,0.072 ,0.604
,0.656 ,0.676 ,0.524 ,0.392 ,0.168
,0.744 ,0.56 ,0.568 ,0.936 ,0.792
,0.592 ,0.804 ,0.2 ,0.852 ,0.972
,0.736 ,0.616 ,0.744 ,0.284 ,0.764
,0.824 ,0.484 ,0.76 ,0.804 ,0.472
,0.968 ,0.868 ,0.72 ,0.496 ,0.168
,0.868 ,0.676 ,0.388 ,0.948 ,0.06
,0.104 ,0.552 ,0.432 ,0.368 ,0.472
,0.376 ,0.244 ,0.556 ,0.86 ,0.976
,0.116 ,0.784 ,0.748 ,0.736 ,0.68
,0.828 ,0.156 ,0.916 ,0.22 ,0.16
,0.576 ,0.408 ,0.752 ,0.92 ,0.216
,0.484 ,0.832 ,0.96 ,0.584 ,0.616
,0.968 ,0.728 ,0.776 ,0.664 ,0.796
,0.36 ,0.656 ,0.336 ,0.816 ,0.892
,0.808 ,0.728 ,0.432 ,0.376 ,0.456
,0.496 ,0.524 ,0.684 ,0.84 ,0.7
,0.448 ,0.264 ,0.28 ,0.476 ,0.68
,0.328 ,0.844 ,0.24 ,0.528 ,0.456
,0.872 ,0.924 ,0.58 ,0.78 ,0.772
,0.636 ,0.144 ,0.684 ,0.212 ,0.284
,0.22 ,0.324 ,0.28 ,0.468 ,0.692
,0.604 ,0.008 ,0.888 ,0.236 ,0.88
,0.468 ,0.984 ,0.656 ,0.736 ,0.532
,0.084 ,0.444 ,0.824 ,0.552 ,0.288
,0.588 ,0.148 ,0.704 ,0.632 ,0.556
,0.024 ,0.508 ,0.852 ,0.18 ,0.112
,0.164 ,0.248 ,0.364 ,0.588 ,0.268
,0.192 ,0.564 ,0.172 ,0.052 ,0.98
,0.22 ,0.648 ,0.032 ,0.84 ,0.512
,0.528 ,0.324 ,0.28 ,0.296 ,0.108
,0.02 ,0.368 ,0.704 ,0.048 ,0.36
,0.536 ,0.36 ,0.252 ,0.732 ,0.816
,0.544 ,0.216 ,0.416 ,0.924 ,0.64
,0.14 ,0.8 ,0.528 ,0.196 ,0.884
,0.08 ,0.572 ,0.748 ,0.372 ,0.6
,0.928 ,0.144 ,0.668 ,0.896 ,0.524
,0.636 ,0.792 ,0.46 ,0.748 ,0.996
,0.02 ,0.94 ,0.18 ,0.12 ,0.236
,0.072 ,0.256 ,0.992 ,0.98 ,0.204
,0.796 ,0.556 ,0.844 ,0.952 ,0.924
,0.72 ,0.004 ,0.712 ,0.62 ,0.864
,0.4 ,0.724 ,0.86 ,0.876, 0
,0.192 ,0.532 ,0.28 ,0.128 ,0.576
,0.14 ,0.124 ,0.648 ,0.924 ,0.284
,0.724 ,0.46 ,0.208 ,0.94 ,0.256
,0.828 ,0.048 ,0.608 ,0.36 ,0.94
,0.924 ,0.664 ,0.988 ,0.476 ,0.88
,0.204 ,0.188 ,0.656 ,0.192 ,0.416
,0.528 ,0.408 ,0.032 ,0.448 ,0.572
,0.352 ,0.18 ,0.032 ,0.46 ,0.468
,0.276 ,0.944 ,0.304 ,0.072 ,0.316
,0.504 ,0.76 ,0.904 ,0.076 ,0.46
,0.592 ,0.768 ,0.42 ,0.484 ,0.728
,0.528 ,0.06 ,0.924 ,0.616 ,0.34
,0.208, 1 ,0.428 ,0.564 ,0.26
,0.136 ,0.608 ,0.544 ,0.508 ,0.82
,0.264 ,0.296 ,0.156 ,0.192 ,0.628
,0.472 ,0.996 ,0.136 ,0.104 ,0.112
,0.412 ,0.896 ,0.776 ,0.016 ,0.232
,0.116 ,0.408 ,0.26 ,0.372 ,0.46
,0.944 ,0.28 ,0.932 ,0.636 ,0.632
,0.168 ,0.124 ,0.412 ,0.228 ,0.292
,0.676 ,0.68 ,0.644 ,0.996 ,0.948
,0.312 ,0.444 ,0.832 ,0.356 ,0.408
,0.952 ,0.848 ,0.184 ,0.404 ,0.892
,0.92 ,0.896 ,0.604 ,0.064 ,0.416
,0.436 ,0.312 ,0.668 ,0.948 ,0.172
,0.996 ,0.508 ,0.536 ,0.444 ,0.832
,0.772 ,0.26 ,0.916 ,0.12 ,0.436
,0.652 ,0.732 ,0.872 ,0.104 ,0.02
,0.328 ,0.692 ,0.464 ,0.096 ,0.148
,0.348 ,0.772 ,0.84 ,0.472 ,0.416
,0.132 ,0.388 ,0.168 ,0.92 ,0.012
,0.764 ,0.484 ,0.148 ,0.3 ,0.392
,0.36 ,0.68 ,0.5 ,0.18, 1
,0.676 ,0.596 ,0.856 ,0.952 ,0.992
,0.812 ,0.612 ,0.66 ,0.66 ,0.132
,0.344 ,0.784 ,0.052 ,0.356 ,0.464
,0.476 ,0.24 ,0.296 ,0.768 ,0.584
,0.588 ,0.628 ,0.484 ,0.228 ,0.556
,0.34 ,0.94 ,0.964 ,0.42 ,0.008
,0.268 ,0.976 ,0.288 ,0.276 ,0.344
,0.496 ,0.384 ,0.796 ,0.692 ,0.244
,0.368 ,0.764 ,0.672 ,0.24 ,0.204
,0.224 ,0.276 ,0.764 ,0.34 ,0.9
,0.548 ,0.532 ,0.54 ,0.388 ,0.788
,0.168 ,0.532 ,0.172 ,0.976 ,0.788
,0.724 ,0.972 ,0.628 ,0.616 ,0.408
,0.832 ,0.968 ,0.788 ,0.384 ,0.34
,0.636 ,0.592 ,0.404 ,0.168 ,0.792
,0.572 ,0.636 ,0.656 ,0.892 ,0.06
,0.22 ,0.832 ,0.404 ,0.496 ,0.256
,0.136 ,0.82 ,0.212 ,0.948 ,0.696
,0.508 ,0.892 ,0.64 ,0.3 ,0.564
,0.996 ,0.42 ,0.232 ,0.08 ,0.952
,0.544 ,0.464 ,0.304 ,0.876 ,0.608
,0.372 ,0.1 ,0.74 ,0.968 ,0.996
,0.692 ,0.876 ,0.62 ,0.672 ,0.648
,0.316 ,0.816 ,0.352 ,0.376 ,0.972
,0.844 ,0.528 ,0.752 ,0.916 ,0.448
,0.244 ,0.668 ,0.232 ,0.744 ,0.132
,0.524 ,0.492 ,0.912 ,0.936 ,0.94
,0.188 ,0.516 ,0.18 ,0.312 ,0.16
,0.932 ,0.084 ,0.384 ,0.604 ,0.688
,0.48 ,0.116 ,0.936 ,0.576 ,0.8
,0.952 ,0.316 ,0.488 ,0.084 ,0.736
,0.392 ,0.232 ,0.576 ,0.396 ,0.896
,0.692 ,0.292 ,0.788 ,0.864 ,0.192
,0.28 ,0.044 ,0.14 ,0.808 ,0.992
,0.876 ,0.288 ,0.86 ,0.908 ,0.064
,0.228 ,0.964 ,0.796 ,0.272 ,0.344
,0.524 ,0.232 ,0.648 ,0.68 ,0.064
,0.104 ,0.392 ,0.772 ,0.196 ,0.868
,0.08 ,0.428 ,0.856 ,0.98 ,0.076
,0.456 ,0.556 ,0.22 ,0.816 ,0.512
,0.128 ,0.276 ,0.64 ,0.316 ,0.516
,0.768 ,0.368 ,0.828 ,0.044 ,0.676
,0.8 ,0.28 ,0.176 ,0.936 ,0.056
,0.016 ,0.34 ,0.72 ,0.764 ,0.272
,0.604 ,0.152 ,0.86 ,0.096 ,0.292
,0.984 ,0.832 ,0.356 ,0.152 ,0.732
,0.812 ,0.304 ,0.276 ,0.76 ,0.88
,0.676 ,0.212 ,0.204 ,0.352 ,0.476
,0.244 ,0.84 ,0.812 ,0.176 ,0.028
,0.352 ,0.54 ,0.832 ,0.48 ,0.416
,0.732 ,0.376 ,0.528 ,0.416 ,0.012
,0.196 ,0.46 ,0.7 ,0.5 ,0.096
,0.072 ,0.612 ,0.304 ,0.472 ,0.376
,0.476 ,0.408 ,0.5 ,0.828 ,0.32
,0.22 ,0.036 ,0.172 ,0.712 ,0.756
,0.52 ,0.632 ,0.26 ,0.316 ,0.16
,0.688 ,0.484 ,0.692 ,0.336 ,0.736
,0.444 ,0.936 ,0.624 ,0.276 ,0.504
,0.58 ,0.376 ,0.648 ,0.296 ,0.12
,0.864, 1 ,0.832 ,0.668 ,0.924
,0.916 ,0.668 ,0.48 ,0.828 ,0.724
,0.448 ,0.624 ,0.82 ,0.624 ,0.944
,0.1 ,0.044 ,0.804 ,0.436 ,0.22
,0.808 ,0.548 ,0.32 ,0.264 ,0.284
,0.872 ,0.876 ,0.74 ,0.568 ,0.824
,0.608 ,0.544 ,0.176 ,0.288 ,0.084
,0.092 ,0.916 ,0.764 ,0.168 ,0.872
,0.376 ,0.52 ,0.288 ,0.88 ,0.888
,0.904 ,0.892 ,0.372 ,0.42 ,0.984
,0.52 ,0.372 ,0.476 ,0.348 ,0.756
,0.044 ,0.736 ,0.252 ,0.732 ,0.776
,0.632 ,0.976 ,0.08 ,0.36 ,0.596
,0.72 ,0.228 ,0.36 ,0.2 ,0.924
,0.676 ,0.744 ,0.58 ,0.644 ,0.3
,0.82 ,0.296 ,0.44 ,0.516 ,0.716
,0.46 ,0.428 ,0.46 ,0.372 ,0.604
,0.16 ,0.484 ,0.164 ,0.38 ,0.708
,0.964 ,0.988 ,0.844 ,0.216 ,0.912
,0.228 ,0.368 ,0.22 ,0.064 ,0.384
,0.72 ,0.636 ,0.852 ,0.776 ,0.444
,0.944 ,0.992 ,0.74 ,0.384 ,0.528
,0.536 ,0.296 ,0.056 ,0.34 ,0.152
,0.152 ,0.316 ,0.148 ,0.816 ,0.576
,0.936 ,0.896 ,0.212 ,0.792 ,0.392
,0.728 ,0.692 ,0.324 ,0.496 ,0.68
,0.536 ,0.372 ,0.316 ,0.276 ,0.9
,0.664 ,0.008 ,0.464 ,0.564 ,0.088
,0.52 ,0.032 ,0.584 ,0.396 ,0.864
,0.18 ,0.708 ,0.62 ,0.82 ,0.248
,0.736 ,0.496 ,0.284 ,0.496 ,0.152
,0.66 ,0.524 ,0.536 ,0.56 ,0.232
,0.084 ,0.396 ,0.38 ,0.028 ,0.708
,0.184 ,0.824 ,0.264 ,0.892 ,0.62
,0.204 ,0.62 ,0.728 ,0.984 ,0.948
,0.656 ,0.876 ,0.92 ,0.84 ,0.932
,0.572 ,0.516 ,0.024 ,0.248 ,0.756
,0.428 ,0.28 ,0.572 ,0.936 ,0.228
,0.416 ,0.04 ,0.44 ,0.252 ,0.872
,0.408 ,0.484 ,0.468 ,0.036 ,0.388
,0.1 ,0.956 ,0.64 ,0.904 ,0.436
,0.152 ,0.144 ,0.428 ,0.628 ,0.748
,0.632 ,0.132 ,0.204 ,0.6 ,0.236
,0.732 ,0.508 ,0.128 ,0.42 ,0.724
,0.04 ,0.876 ,0.528 ,0.852 ,0.844
,0.652 ,0.968 ,0.26 ,0.924 ,0.124
,0.312 ,0.884 ,0.96 ,0.132 ,0.464
,0.304 ,0.1 ,0.684 ,0.22 ,0.42
,0.404 ,0.94 ,0.244 ,0.884 ,0.484
,0.788 ,0.42 ,0.88 ,0.836 ,0.112
,0.468 ,0.928 ,0.52 ,0.592 ,0.452
,0.192 ,0.408 ,0.94 ,0.148 ,0.216
,0.456 ,0.06 ,0.968 ,0.444 ,0.236
,0.348 ,0.652 ,0.716 ,0.628 ,0.16
,0.084 ,0.392 ,0.284 ,0.34 ,0.988
,0.404 ,0.476 ,0.724 ,0.108 ,0.988
,0.632 ,0.84 ,0.588 ,0.744 ,0.008
,0.232 ,0.336 ,0.804 ,0.368 ,0.604)
cp=hd(p.table,alpha)
pv=NULL
if(!is.null(p.obs))w=optimize(hdpv,interval=c(.001,.999),dat=p.table,obs=p.obs)$minimum
list(crit.p.value=cp,adj.p.value=w)
}




# ----------------------------------------------------------------------------

# pb3trmcp

# ----------------------------------------------------------------------------

pb3trmcp<-function(J,K,L,data,tr=.2,grp=c(1:p),alpha=.05,p=J*K*L,nboot=NA,
SEED=TRUE,bhop=FALSE){
#
#  Multiple comparisons for a  three-way anova, independent groups,
#  based on trimmed means
#
#  That is, there are three factors with a total of JKL independent groups.
#
#  A percentile bootstrap method is used to perform multiple comparisons
#  The variable data is assumed to contain the raw
#  data stored in list mode. data[[1]] contains the data
#  for the first level of all three factors: level 1,1,1.
#  data][2]] is assumed to contain the data for level 1 of the
#  first two factors and level 2 of the third factor: level 1,1,2
#  data[[L]] is the data for level 1,1,L
#  data[[L+1]] is the data for level 1,2,1. data[[2L]] is level 1,2,L.
#  data[[KL+1]] is level 2,1,1, etc.
#
#  The default amount of trimming is tr=.2
#
#  It is assumed that data has length JKL, the total number of
#  groups being tested.
#
if(SEED)set.seed(2)
if(is.list(data))data=listm(elimna(matl(data)))
if(is.matrix(data))data=listm(elimna(data))
if(!is.list(data))stop("Data are not stored in list mode or a matrix")
if(p!=length(data)){
print("The total number of groups, based on the specified levels, is")
print(p)
print("The number of groups in data is")
print(length(data))
print("Warning: These two values are not equal")
}
temp=con3way(J,K,L)
conA<-temp$conA
conB<-temp$conB
conC<-temp$conC
conAB<-temp$conAB
conAC<-temp$conAC
conBC<-temp$conBC
conABC=temp$conABC
Factor.A<-pbtrmcp(x,con=conA,tr=tr,alpha=alpha,nboot=nboot,bhop=bhop)
Factor.B<-pbtrmcp(x,con=conB,tr=tr,alpha=alpha,nboot=nboot,bhop=bhop)
Factor.C<-pbtrmcp(x,con=conC,tr=tr,alpha=alpha,nboot=nboot,bhop=bhop)
Factor.AB<-pbtrmcp(x,con=conAB,tr=tr,alpha=alpha,nboot=nboot,bhop=bhop)
Factor.AC<-pbtrmcp(x,con=conAC,tr=tr,alpha=alpha,nboot=nboot,bhop=bhop)
Factor.BC<-pbtrmcp(x,con=conBC,tr=tr,alpha=alpha,nboot=nboot,bhop=bhop)
Factor.ABC<-pbtrmcp(x,con=conABC,tr=tr,alpha=alpha,nboot=nboot,bhop=bhop)
list(Factor.A=Factor.A,Factor.B=Factor.B,Factor.C=Factor.C,
Factor.AB=Factor.AB,Factor.AC=Factor.AC,Factor.BC=Factor.BC,
Factor.ABC=Factor.ABC)
}


pbad2way<-function(J,K,x,est=tmean,conall=TRUE,alpha=.05,nboot=2000,grp=NA,
op=FALSE,pro.dis=TRUE,MM=FALSE,pr=TRUE,...){
#
# This function is like the function pbadepth,
# only it is assumed that main effects and interactions for a
# two-way design are to be tested.
#
        #   The data are assumed to be stored in x in list mode or in a matrix.
        #  If grp is unspecified, it is assumed x[[1]] contains the data
        #  for the first level of both factors: level 1,1.
        #  x[[2]] is assumed to contain the data for level 1 of the
        #  first factor and level 2 of the second factor: level 1,2
        #  x[[j+1]] is the data for level 2,1, etc.
        #  If the data are in wrong order, grp can be used to rearrange the
        #  groups. For example, for a two by two design, grp<-c(2,4,3,1)
        #  indicates that the second group corresponds to level 1,1;
        #  group 4 corresponds to level 1,2; group 3 is level 2,1;
        #  and group 1 is level 2,2.
        #
        #   Missing values are automatically removed.
        #
        if(pr){
        print('As of June, 2022, the default measure of location is tmean, a 20% trimmed mean')
        print('The default for measuring depth is a projection method rather than Mahalanobis distance')
        }
        JK <- J * K
        if(is.matrix(x))
                x <- listm(x)
        if(!is.na(grp[1])) {
                yy <- x
                for(j in 1:length(grp))
                        x[[j]] <- yy[[grp[j]]]
        }
        if(!is.list(x))
                stop("Data must be stored in list mode or a matrix.")
        for(j in 1:JK) {
                xx <- x[[j]]
                x[[j]] <- xx[!is.na(xx)]
        }
        #
        # Create the three contrast matrices
        #
        if(!conall){
        ij <- matrix(c(rep(1, J)), 1, J)
        ik <- matrix(c(rep(1, K)), 1, K)
        jm1 <- J - 1
        cj <- diag(1, jm1, J)
        for(i in 1:jm1)
                cj[i, i + 1] <- 0 - 1
        km1 <- K - 1
        ck <- diag(1, km1, K)
        for(i in 1:km1)
                ck[i, i + 1] <- 0 - 1
        conA <- t(kron(cj, ik))
        conB <- t(kron(ij, ck))
        conAB <- t(kron(cj, ck))
        conAB <- t(kron(abs(cj), ck))
}
if(conall){
temp<-con2way(J,K)
conA<-temp$conA
conB<-temp$conB
conAB<-temp$conAB
}
        ncon <- max(nrow(conA), nrow(conB), nrow(conAB))
        if(JK != length(x))
                warning("The number of groups does not match the number of contrast coefficients.")
if(!is.na(grp[1])){  # Only analyze specified groups.
xx<-list()
for(i in 1:length(grp))xx[[i]]<-x[[grp[i]]]
x<-xx
}
mvec<-NA
for(j in 1:JK){
temp<-x[[j]]
temp<-temp[!is.na(temp)] # Remove missing values.
x[[j]]<-temp
mvec[j]<-est(temp,...)
}
bvec<-matrix(NA,nrow=JK,ncol=nboot)
set.seed(2) # set seed of random number generator so that
#             results can be duplicated.
#print("Taking bootstrap samples. Please wait.")
for(j in 1:JK){
data<-matrix(sample(x[[j]],size=length(x[[j]])*nboot,replace=TRUE),nrow=nboot)
bvec[j,]<-apply(data,1,est,...) # J by nboot matrix, jth row contains
#                          bootstrapped  estimates for jth group
}
bconA<-t(conA)%*%bvec #C by nboot matrix
tvecA<-t(conA)%*%mvec
tvecA<-tvecA[,1]
tempcenA<-apply(bconA,1,mean)
veczA<-rep(0,ncol(conA))
bconA<-t(bconA)
smatA<-var(bconA-tempcenA+tvecA)
bconA<-rbind(bconA,veczA)
if(!pro.dis){
if(!op)dv<-mahalanobis(bconA,tvecA,smatA)
if(op){
dv<-out(bconA)$dis
}}
if(pro.dis)dv=pdis(bconA,MM=MM)
bplus<-nboot+1
sig.levelA<-1-sum(dv[bplus]>=dv[1:nboot])/nboot
bconB<-t(conB)%*%bvec #C by nboot matrix
tvecB<-t(conB)%*%mvec
tvecB<-tvecB[,1]
tempcenB<-apply(bconB,1,mean)
veczB<-rep(0,ncol(conB))
bconB<-t(bconB)
smatB<-var(bconB-tempcenB+tvecB)
bconB<-rbind(bconB,veczB)
if(!pro.dis){
if(!op)dv<-mahalanobis(bconB,tvecB,smatB)
if(op){
dv<-out(bconA)$dis
}}
if(pro.dis)dv=pdis(bconB,MM=MM)
sig.levelB<-1-sum(dv[bplus]>=dv[1:nboot])/nboot
bconAB<-t(conAB)%*%bvec #C by nboot matrix
tvecAB<-t(conAB)%*%mvec
tvecAB<-tvecAB[,1]
tempcenAB<-apply(bconAB,1,mean)
veczAB<-rep(0,ncol(conAB))
bconAB<-t(bconAB)
smatAB<-var(bconAB-tempcenAB+tvecAB)
bconAB<-rbind(bconAB,veczAB)
if(!pro.dis){
if(!op)dv<-mahalanobis(bconAB,tvecAB,smatAB)
if(op){
dv<-out(bconAB)$dis
}}
if(pro.dis)dv=pdis(bconAB,MM=MM)
sig.levelAB<-1-sum(dv[bplus]>=dv[1:nboot])/nboot
list(sig.levelA=sig.levelA,sig.levelB=sig.levelB,sig.levelAB=sig.levelAB,conA=conA,conB=conB,conAB=conAB)

}




pbad3way<-function(J,K,L,x,est=tmean,alpha=.05,nboot=2000,MC=FALSE){
#
#  Three-way ANOVA for robust measures of locaton
# To compare medians, use est=hd,  in case which tied values are allowed.
#
if(is.matrix(x)|| is.data.frame(x))x=listm(x)
chkcar=NA
for(j in 1:length(x))chkcar[j]=length(unique(x[[j]]))
if(min(chkcar)<14){
print('Warning: Sample size is less than')
print('14 for one more groups. Type I error might not be controlled')
}
con=con3way(J,K,L)
A=pbadepth(x,est=est,con=con$conA,alpha=alpha,nboot=nboot,MC=MC)
B=pbadepth(x,est=est,con=con$conB,alpha=alpha,nboot=nboot,MC=MC)
C=pbadepth(x,est=est,con=con$conC,alpha=alpha,nboot=nboot,MC=MC)
AB=pbadepth(x,est=est,con=con$conAB,alpha=alpha,nboot=nboot,MC=MC)
AC=pbadepth(x,est=est,con=con$conAC,alpha=alpha,nboot=nboot,MC=MC)
BC=pbadepth(x,est=est,con=con$conBC,alpha=alpha,nboot=nboot,MC=MC)
ABC=pbadepth(x,est=est,con=con$conABC,alpha=alpha,nboot=nboot,MC=MC)
list(Fac.A=A,Fac.B=B,Fac.C=C,Fac.AB=AB,Fac.AC=AC,Fac.BC=BC,Fac.ABC=ABC)
}

 ph.inter<-function(x,alpha=.05,p=J*K,grp=c(1:p),plotit=TRUE,op=4){
#
# Patel--Hoel  interaction for a
#  in 2 by 2 design. The method is based on an
#  extension of Cliff's heteroscedastic technique for
#  handling tied values and the Patel-Hoel definition of no interaction.
#
#  It is assumed all groups are independent.
#
#  Missing values are automatically removed.
#
#  The default value for alpha is .05. Any other value results in using
#  alpha=.01.
#
#  Argument grp can be used to rearrange the order of the data.
#
 if(is.matrix(x))x<-listm(x)
if(!is.list(x))stop('Data must be stored in a matrix or in list mode.')
a=rimul(2,2,x,alpha=alpha,plotit=FALSE)
 e1=cid(x[[1]],x[[2]],,plotit=FALSE)$phat
 e2=cid(x[[3]],x[[4]],,plotit=FALSE)$phat
if(plotit){
m1<-outer(x[[1]],x[[2]],FUN='-')
m2<-outer(x[[3]],x[[4]],FUN='-')
m1<-as.vector(m1)
m2<-as.vector(m2)
g2plot(m1,m2,op=op)
}
list(Est.1=e1,Est.2=e2,dif=e1-e2,ci.lower=a$test[1,6],ci.upper=a$test[1,7],p.value=a$test[8])
}

 ph.inter<-function(x,alpha=.05,p=J*K,grp=c(1:p),plotit=TRUE,op=4,SW=FALSE){
#
# Patel--Hoel  interaction for a
#  in 2 by 2 design. The method is based on an
#  extension of Cliff's heteroscedastic technique for
#  handling tied values and the Patel-Hoel definition of no interaction.
#
# The function rimul deals with the J by K design
#
#  It is assumed all groups are independent.
#
#  Missing values are automatically removed.
#
#  The default value for alpha is .05. Any other value results in using
#  alpha=.01.
#
#  Argument grp can be used to rearrange the order of the data.
#
 if(is.matrix(x))x<-listm(x)
if(!is.list(x))stop('Data must be stored in a matrix or in list mode.')
if(SW)x=x[c(1,3,2,4)]
a=rimul(2,2,x,alpha=alpha,plotit=FALSE)
 e1=cid(x[[1]],x[[2]],,plotit=FALSE)$phat
 e2=cid(x[[3]],x[[4]],,plotit=FALSE)$phat
if(plotit){
m1<-outer(x[[1]],x[[2]],FUN='-')
m2<-outer(x[[3]],x[[4]],FUN='-')
m1<-as.vector(m1)
m2<-as.vector(m2)
g2plot(m1,m2,op=op)
}
list(Est.1=e1,Est.2=e2,dif=e1-e2,ci.lower=a$test[1,6],ci.upper=a$test[1,7],p.value=a$test[8])
}




# ----------------------------------------------------------------------------

# pball

# ----------------------------------------------------------------------------

pball<-function(m,beta=.2){
#
#    Compute the percentage bend correlation matrix for the
#    data in the n by p matrix m.
#
#    This function also returns the two-sided significance level
#    for all pairs of variables, plus a test of zero correlations
#    among all pairs. (See chapter 6 for details.)
#
if(!is.matrix(m))stop("Data must be stored in an n by p matrix")
pbcorm<-matrix(0,ncol(m),ncol(m))
temp<-matrix(1,ncol(m),ncol(m))
siglevel<-matrix(NA,ncol(m),ncol(m))
cmat<-matrix(0,ncol(m),ncol(m))
for (i in 1:ncol(m)){
ip1<-i
for (j in ip1:ncol(m)){
if(i<j){
pbc<-pbcor(m[,i],m[,j],beta)
pbcorm[i,j]<-pbc$cor
temp[i,j]<-pbcorm[i,j]
temp[j,i]<-pbcorm[i,j]
siglevel[i,j]<-pbc$p.value
siglevel[j,i]<-siglevel[i,j]
}
}
}
tstat<-pbcorm*sqrt((nrow(m)-2)/(1-pbcorm^2))
cmat<-sqrt((nrow(m)-2.5)*log(1+tstat^2/(nrow(m)-2)))
bv<-48*(nrow(m)-2.5)^2
cmat<-cmat+(cmat^3+3*cmat)/bv-(4*cmat^7+33*cmat^5+240^cmat^3+855*cmat)/(10*bv^2+8*bv*cmat^4+1000*bv)
H<-sum(cmat^2)
df<-ncol(m)*(ncol(m)-1)/2
h.siglevel<-1-pchisq(H,df)
list(pbcorm=temp,p.value=siglevel,H=H,H.p.value=h.siglevel)
}






pbprotm<-function(x,est=tmean,con=0,alpha=.05,nboot=2000,grp=NA,op=3,allp=TRUE,
MM=FALSE,MC=FALSE,cop=3,SEED=TRUE,na.rm=FALSE,...){
#
#   Test the hypothesis that C linear contrasts all have a value of zero.
#   By default, a 20% trimmed meanis used
#
#   Independent groups are assumed.
#
#   The data are assumed to be stored in x in list mode or in a matrix.
#   If stored in list mode,
#   x[[1]] contains the data for the first group, x[[2]] the data
#   for the second group, etc. Length(x)=the number of groups = J, say.
#   If stored in a matrix, columns correspond to groups.
#
#   By default, all pairwise differences are used, but contrasts
#   can be specified with the argument con.
#   The columns of con indicate the contrast coefficients.
#   Con should have J rows, J=number of groups.
#   For example, con[,1]=c(1,1,-1,-1,0,0) and con[,2]=c(,1,-1,0,0,1,-1)
#   will test two contrasts: (1) the sum of the first
#   two measures of location is
#   equal to the sum of the second two, and (2) the difference between
#   the first two is equal to the difference between the
#   measures of location for groups 5 and 6.
#
#   The default number of bootstrap samples is nboot=2000
#
#   op controls how depth is measured
#   op=1, Mahalanobis
#   op=2, Mahalanobis based on MCD covariance matrix
#   op=3, Projection distance
#
#   MC=TRUE, use a multicore processor when op=3
#
#   for arguments MM and cop, see pdis.
#
con<-as.matrix(con)
if(is.matrix(x) || is.data.frame(x))x=listm(x)
if(!is.list(x))stop("Data must be stored in list mode or in matrix mode.")
if(!is.na(grp)){  # Only analyze specified groups.
xx<-list()
for(i in 1:length(grp))xx[[i]]<-x[[grp[i]]]
x<-xx
}
J<-length(x)
mvec<-NA
nvec=NA
for(j in 1:J){
temp<-x[[j]]
if(na.rm)temp<-temp[!is.na(temp)] # Remove missing values.
x[[j]]<-temp
mvec[j]<-est(temp,...)
nvec[j]=length(temp)
}
Jm<-J-1
d<-ifelse(con==0,(J^2-J)/2,ncol(con))
if(sum(con^2)==0){
if(allp){
con<-matrix(0,J,d)
id<-0
for (j in 1:Jm){
jp<-j+1
for (k in jp:J){
id<-id+1
con[j,id]<-1
con[k,id]<-0-1
}}}
if(!allp){
con<-matrix(0,J,Jm)
for (j in 1:Jm){
jp<-j+1
con[j,j]<-1
con[jp,j]<-0-1
}}}
bvec<-matrix(NA,nrow=J,ncol=nboot)
if(SEED)set.seed(2) # set seed of random number generator so that
#             results can be duplicated.
for(j in 1:J){
data<-matrix(sample(x[[j]],size=length(x[[j]])*nboot,replace=TRUE),nrow=nboot)
bvec[j,]<-apply(data,1,est,na.rm=na.rm,...) # J by nboot matrix, jth row contains
#                          bootstrapped  estimates for jth group
}
chkna=sum(is.na(bvec))
if(chkna>0){
print("Bootstrap estimates of location could not be computed")
print("This can occur when using an M-estimator")
print("Might try est=tmean")
}
bcon<-t(con)%*%bvec #C by nboot matrix
tvec<-t(con)%*%mvec
tvec<-tvec[,1]
tempcen<-apply(bcon,1,mean)
vecz<-rep(0,ncol(con))
bcon<-t(bcon)
smat<-var(bcon-tempcen+tvec)
temp<-bcon-tempcen+tvec
bcon<-rbind(bcon,vecz)
if(op==1)dv<-mahalanobis(bcon,tvec,smat)
if(op==2){
smat<-cov.mcd(temp)$cov
dv<-mahalanobis(bcon,tvec,smat)
}
if(op==3){
if(!MC)dv<-pdis(bcon,MM=MM,cop=cop)
if(MC)dv<-pdisMC(bcon,MM=MM,cop=cop)
}
bplus<-nboot+1
sig.level<-1-sum(dv[bplus]>=dv[1:nboot])/nboot
list(p.value=sig.level,psihat=tvec,con=con,n=nvec)
}




# ----------------------------------------------------------------------------

# pbtest

# ----------------------------------------------------------------------------

pbtest<-function(m,beta=.1){
#
#    Test H0: R(pb)=I, the hypothesis that the percentage
#    bend correlation matrix equal the identity matrix
#    for the data stored in the n by p matrix m
#
#
n<-nrow(m)
nu<-n-2
a<-nu-.5
b<-48*a^2
nmm<-ncol(m)-1
c<-matrix(0,ncol(m),ncol(m))
for (i in 1:nmm){
ip1<-i+1
for (j in ip1:ncol(m)){
tjk<-tjk*sqrt(nu/(1.-tjk^2))
c[i,j]<-sqrt(a*log(1+tjk^2/nu))
c[i,j]<-c[i,j]+(c[i,j]^3+3*c[i,j])/b
c[i,j]<-c[i,j]-(4*c[i,j]^7+33*c[i,j]^5+240*c[i,j]^3+855*c[i,j])/(10*b^2+8*b*c[i,j]^4+1000*b)
}
}
h<-sum(c)
sig<-1-pchisq(h,ncol(m)*(ncol(m)-1)/2)
list(teststat=h,p.value=sig)
}

pcorbsub<-function(isub, x, y)
{
        #
        #  Compute Pearson's correlation using x[isub] and y[isub]
        #  isub is a vector of length n,
        #  a bootstrap sample from the sequence of integers
        #  1, 2, 3, ..., n
        #
        pcorbsub<-cor(x[isub],y[isub])
        pcorbsub
}

pdclose<-function(x,pts=x,fr=1,MM=FALSE,MC=FALSE,STAND=TRUE){
#
# For each point in pts, determine which points
# (values in x)
# are close to it based on projected distances.
#
x<-as.matrix(x)
pts<-as.matrix(pts)
if(ncol(x)>1){
if(STAND){
x=standm(x)
m1=apply(x,1,mean)
v=apply(x,1,sd)
for(j in 1:ncol(x))pts[,j]=(pts[,j]-m1[j])/v[j]
}}
outmat<-matrix(NA,ncol=nrow(x),nrow=nrow(pts))
for(i in 1:nrow(pts)){
center<-pts[i,]
if(!MC)blob<-pdis(x,center=center,MM=MM)
if(MC)blob<-pdisMC(x,center=center,MM=MM)
#
# Note: distances already divided by
# interquartile range
#
# Determine which points in m are close to pts
flag2<-(blob < fr)
outmat[i,]<-flag2
}
# Return matrix, ith row indicates which points
# in x are close to pts[i,]
#
outmat
}




# ----------------------------------------------------------------------------

# pdep

# ----------------------------------------------------------------------------

pdep<-function(x,y,alpha=.05){
#
# For two dependent variables, x and y,
# estimate p=P(X<Y)
#
dif<-(x<y)
temp<-binomci(y=dif)
phat<-temp$phat
ci<-temp$ci
list(phat=phat,ci=ci)
}




# ----------------------------------------------------------------------------

# pghdist

# ----------------------------------------------------------------------------

pghdist<-function (q, g = 0, h = 0.) {
#
# P(X<=q) for a g-and-h distribution
#
e=NA
pr=c(1:999)/1000
for(i in 1:999)e[i]=qghdist(i/1000,g=g,h=h)
dif=abs(e-q)
id=which(dif==min(dif))
pr[id]
}




# ----------------------------------------------------------------------------

# phiBY3

# ----------------------------------------------------------------------------

phiBY3 <- function(s,y,c3)
{
  s=as.double(s)
  dev=log(1+exp(-abs(s)))+abs(s)*((y-0.5)*s<0)
  return(rhoBY3(dev,c3)+GBY3Fs(s,c3)+GBY3Fsm(s,c3))
}




# ----------------------------------------------------------------------------

# PHinter.mcp

# ----------------------------------------------------------------------------

PHinter.mcp<-function(J,K,x,alpha=.05,SW=FALSE){
#
#  Interactions based on  Patel--Hoel measure of effect size
#
if(is.matrix(x)  || is.data.frame(x))x=listm(x)
con=con2way(J,K)$conAB
if(SW){
JK=J*K
M=matrix(c(1:JK),nrow=J,byrow=TRUE)
M=as.vector(M)
x=x[M]
con=con2way(K,J)$conAB
}
num=ncol(con)
CON=matrix(NA,nrow=num,ncol=8)
dimnames(CON)=list(NULL,c('Con.num','Est.1','Est.2','Dif','ci.low','ci.up','p.value','p.adjusted'))
for(j in 1:ncol(con)){
id=which(con[,j]!=0)
dat=x[id]
temp=ph.inter(dat,alpha,plotit=FALSE)
temp=pool.a.list(temp)
CON[j,1]=j
CON[j,2:7]=temp
}
CON[,8]=p.adjust(CON[,7],method='hoch')
list(CON=CON,con=con)
}




# ----------------------------------------------------------------------------

# pisim

# ----------------------------------------------------------------------------

pisim<-function(n = 100, q = 7, nruns = 100, alpha = 0.05, eps = 0.1, shift = 9, type
	 = 1)
{
# compares new and classical PIs for multiple linear regression
# if type = 1 for N(0,1) errors, 2 for t3 errors, 3 for exp(1) - 1 errors
# 4 for uniform(-1,1) errors, 5 for (1-eps) N(0,1) + eps N(0,(1+shift)^2) errors
# constant = 1 so there are p = q+1 coefficients
	b <- 0 * 1:q + 1
	cpicov <- 0
	npicov <- 0
	acpicov <- 0
	opicov <- 0
	val3 <- 1:nruns
	val4 <- val3
	val5 <- val3
	pilen <- matrix(0, nrow = nruns, ncol = 4)
	coef <- matrix(0, nrow = nruns, ncol = q + 1)
	corfac <- (1 + 15/n) * sqrt(n/(n - q - 1))
	corfac2 <- sqrt(n/(n - q - 1))
	for(i in 1:nruns) {
		x <- matrix(rnorm(n * q), nrow = n, ncol = q)
		if(type == 1) {
			y <- 1 + x %*% b + rnorm(n)
			xf <- rnorm(q)
			yf <- 1 + xf %*% b + rnorm(1)
		}
		if(type == 2) {
			y <- 1 + x %*% b + rt(n, df = 3)
			xf <- rnorm(q)
			yf <- 1 + xf %*% b + rt(1, df = 3)
		}
		if(type == 3) {
			y <- 1 + x %*% b + rexp(n) - 1
			xf <- rnorm(q)
			yf <- 1 + xf %*% b + rexp(1) - 1
		}
		if(type == 4) {
			y <- 1 + x %*% b + runif(n, min = -1, max = 1)
			xf <- rnorm(q)
			yf <- 1 + xf %*% b + runif(1, min = -1, max = 1)
		}
		if(type == 5) {
			err <- rnorm(n, sd = 1 + rbinom(n, 1, eps) * shift)
			y <- 1 + x %*% b + err
			xf <- rnorm(q)
			yf <- 1 + xf %*% b + rnorm(1, sd = 1 + rbinom(1, 1, eps
				) * shift)
		}
		out <- lsfit(x, y)
		fres <- out$resid
		coef[i,  ] <- out$coef
		yfhat <- out$coef[1] + xf %*% out$coef[-1]
		w <- cbind(1, x)
		xtxinv <- solve(t(w) %*% w)
		xf <- c(1, xf)
		hf <- xf %*% xtxinv
		hf <- hf %*% xf
		val <- sqrt(1 + hf)	#get classical PI
		mse <- sum(fres^2)/(n - q - 1)
		val2 <- qt(1 - alpha/2, n - q - 1) * sqrt(mse) * val
		up <- yfhat + val2
		low <- yfhat - val2
		pilen[i, 1] <- up - low
		if(low < yf && up > yf) cpicov <- cpicov + 1
	#get semiparametric PI
		val2 <- quantile(fres, c(alpha/2, 1 - alpha/2))
		val3[i] <- as.single(corfac * val2[1] * val)
		val4[i] <- as.single(corfac * val2[2] * val)
		up <- yfhat + val4[i]
		low <- yfhat + val3[i]
		pilen[i, 2] <- up - low
		if(low < yf && up > yf) npicov <- npicov + 1
	# asymptotically conservative PI
		val6 <- corfac2 * max(abs(val2))
		val5[i] <- val6 * val
		up <- yfhat + val5[i]
		low <- yfhat - val5[i]
		pilen[i, 3] <- up - low
		if(low < yf && up > yf) acpicov <- acpicov + 1
	# asymptotically optimal PI
		sres <- sort(fres)
		cc <- ceiling(n * (1 - alpha))
		rup <- sres[cc]
		rlow <- sres[1]
		olen <- rup - rlow
		if(cc < n) {
			for(j in (cc + 1):n) {
				zlen <- sres[j] - sres[j - cc + 1]
				if(zlen < olen) {
				  olen <- zlen
				  rup <- sres[j]
				  rlow <- sres[j - cc + 1]
				}
			}
		}
		up <- yfhat + corfac * val * rup
		low <- yfhat + corfac * val * rlow
		pilen[i, 4] <- up - low
		if(low < yf && up > yf)
			opicov <- opicov + 1
	}
	pimnlen <- apply(pilen, 2, mean)
	mnbhat <- apply(coef, 2, mean)
	lcut <- mean(val3)
	hcut <- mean(val4)
	accut <- mean(val5)
	cpicov <- cpicov/nruns
	npicov <- npicov/nruns
	acpicov <- acpicov/nruns
	opicov <- opicov/nruns
	list(mnbhat = mnbhat, pimenlen = pimnlen, cpicov = cpicov, npicov =
		npicov, acpicov = acpicov, opicov = opicov, lcut = lcut, hcut
		 = hcut, accut = accut)
}

ratmn<-
function(x, k1 = 6, k2 = 6)
{
#robust 2 stage asymmetically trimmed  mean
	madd <- mad(x, constant = 1)
	med <- median(x)
	LM <- sum(x < (med - k1 * madd))
	nmUM <- sum(x > (med + k2 * madd))
	n <- length(x)
	# ll (hh) is the percentage to be trimmed to the left (right)
	ll <- ceiling((100 * LM)/n)
	hh <- ceiling((100 * (nmUM))/n)
	tem <- sort(x)
	ln <- floor((ll * n)/100)
	un <- floor((n * (100 - hh))/100)
	low <- ln + 1
	val1 <- tem[low]
	val2 <- tem[un]
	rtmn <- mean(x[(x >= val1) & (x <= val2)])
  trmn
}

rmaha<-
function(x)
{
# Produces robust Mahalanobis distances (scaled for normal data).
	p <- dim(x)[2]
	out <- cov.mcd(x)
	center <- out$center
	cov <- out$cov
	rd <- mahalanobis(x, center, cov)
	const <- sqrt(qchisq(0.5, p))/median(rd)
	return(const * sqrt(rd))
}




# ----------------------------------------------------------------------------

# plotCI

# ----------------------------------------------------------------------------

plotCI <- function (x, y = NULL, uiw=NULL, liw = uiw, aui=NULL, ali=aui,
                    err="y", ylim=NULL, sfrac = 0.01, gap=0, add=FALSE,
                    col=par("col"), lwd=par("lwd"), slty=par("lty"), xlab=NULL,
                    ylab=NULL, ...) {
## originally from Bill Venables, R-list
  if (is.list(x)) {
    y <- x$y
    x <- x$x
  }
  if (is.null(y)) {
    if (is.null(x))
      stop("both x and y NULL")
    y <- as.numeric(x)
    x <- seq(along = x)
  }
  if (missing(xlab)) xlab <- deparse(substitute(x))
  if (missing(ylab)) ylab <- deparse(substitute(y))
  if (missing(uiw)) { ## absolute limits
    ui <- aui
    li <- ali
  }
  else { ## relative limits
    if (err=="y") z <- y else z <- x
    if(is.null(uiw))stop("Argument uiw, the width of the interval, must be specified")
    ui <- z + uiw
    li <- z - liw
  }
  if (is.null(ylim)) ylim <- range(c(y, ui, li), na.rm=TRUE)
  if (add) {
    points(x, y, col=col, lwd=lwd, ...)
  } else {
    plot(x, y, ylim = ylim, col=col, lwd=lwd, xlab=xlab, ylab=ylab, ...)
  }
  if (gap==TRUE) gap <- 0.01 ## default gap size
  ul <- c(li, ui)
  if (err=="y") {
    gap <- rep(gap,length(x))*diff(par("usr")[3:4]) # smidge <- diff(par("usr")[1:2]) * sfrac
    smidge <- par("fin")[1] * sfrac
# segments(x , li, x, pmax(y-gap,li), col=col, lwd=lwd, lty=slty)
# segments(x , ui, x, pmin(y+gap,ui), col=col, lwd=lwd, lty=slty)
    arrows(x , li, x, pmax(y-gap,li), col=col, lwd=lwd, lty=slty, angle=90, length=smidge, code=1)
    arrows(x , ui, x, pmin(y+gap,ui), col=col, lwd=lwd, lty=slty, angle=90, length=smidge, code=1)
    ## horizontal segments
# x2 <- c(x, x)
# segments(x2 - smidge, ul, x2 + smidge, ul, col=col, lwd=lwd)
  }
  else if (err=="x") {
    gap <- rep(gap,length(x))*diff(par("usr")[1:2])
    smidge <- par("fin")[2] * sfrac
# smidge <- diff(par("usr")[3:4]) * sfrac
    arrows(li, y, pmax(x-gap,li), y, col=col, lwd=lwd, lty=slty, angle=90, length=smidge, code=1)
    arrows(ui, y, pmin(x+gap,ui), y, col=col, lwd=lwd, lty=slty, angle=90, length=smidge, code=1)
    ## vertical segments
# y2 <- c(y, y)
# segments(ul, y2 - smidge, ul, y2 + smidge, col=col, lwd=lwd)
  }
  invisible(list(x = x, y = y))
}
PMD.PB.PCD<-function(n=NULL,delta=.5,x=NULL,est=tmean, SIG=NULL,alpha=.05,iter=1000,SEED=TRUE,...){
#
#  Which group has the largest measures of location?
#
#  Use an indifference zone. Given
#  n a vector of sample sizes, determine the
#  probability of making a decision and the probability of
#  of correct decision given that a decision is made.
#
#  Number of groups is length(n)
#
if(SEED)set.seed(2)
if(is.null(n) & is.null(x))stop('Either n or x must be specified')
if(SEED)set.seed(2)
if(!is.null(x)){
x=elimna(x)
if(is.matrix(x) || is.data.frame(x))x=listm(x)
J=length(x)
e=lapply(x,est,...)
e=as.vector(matl(e))
ID=which(e==max(e))
n=as.vector(matl(lapply(x,length)))
}
J=length(n)
if(J<=1)stop('n should have 2 or more values')
z=list()
PMD=0
PCD=0
sig=rep(1,J)
if(is.null(SIG)){
if(!is.null(x)){
for(j in 1:J)sig[j]=sd(x[[j]])
}}
if(!is.null(SIG))sig=SIG
for(i in 1:iter){
for(j in 1:J)z[[j]]=sig[j]*rnorm(n[j])
z[[1]]=z[[1]]+delta*sig[ID]
pv=anc.best.PB(z,est=est,nboot=nboot,SEED=FALSE,...)
if(pv$p.value<=alpha){
PMD=PMD+1
id=which(pv$Est==max(pv$Est))
if(id==1)PCD=PCD+1
}}
PCD.ci=NA
PMD.ci=binom.conf(PMD,iter,pr=FALSE)$ci
if(PMD>0)PCD.ci=binom.conf(PCD,PMD,pr=FALSE)$ci
PCD=PCD/max(PMD,1)
PMD=PMD/iter
list(PMD=PMD,PMD.ci=PMD.ci,PCD=PCD,PCD.ci=PCD.ci)
}




# ----------------------------------------------------------------------------

# PMD.PCD

# ----------------------------------------------------------------------------

PMD.PCD<-function(n=NULL,delta=.5,x=NULL, tr=.2, SIG=NULL,alpha=.05,p.crit=NULL,iter=5000,SEED=TRUE){
#
#  Which group has the largest measures of location?
#
#  Use an indifference zone. Given
#  n a vector of sample sizes, determine the
#  probability of making a decision and the probability of
#  of correct decision given that a decision is made.
#
#  Number of groups is length(n)
#
if(is.null(n) & is.null(x))stop('Either n or x must be specified')
if(SEED)set.seed(2)
if(!is.null(x)){
x=elimna(x)
if(is.matrix(x) || is.data.frame(x))x=listm(x)
J=length(x)
est=lapply(x,tmean,tr=tr)
est=as.vector(matl(est))
ID=which(est==max(est))
n=as.vector(matl(lapply(x,length)))
}
J=length(n)
if(J<=1)stop('n should have 2 or more values')
if(is.null(p.crit))p.crit=anc.best.crit(J,n,alpha=alpha,tr=tr,iter=iter,SEED=SEED)$fin.crit
z=list()
PMD=0
PCD=0
sig=rep(1,J)
if(is.null(SIG)){
if(!is.null(x)){
for(j in 1:J)sig[j]=sd(x[[j]])
}}
if(!is.null(SIG))sig=SIG
for(i in 1:iter){
for(j in 1:J)z[[j]]=sig[j]*rnorm(n[j])
z[[1]]=z[[1]]+delta*sig[ID]
pv=anc.best(z,p.crit=p.crit, tr=tr,SEED=F)
if(pv@Larger.than[1]=='All'){
PMD=PMD+1
if(pv@Group.with.largest.estimate==1)PCD=PCD+1
}}
PCD.ci=NA
PMD.ci=binom.conf(PMD,iter,pr=FALSE)$ci
if(PMD>0)PCD.ci=binom.conf(PCD,PMD,pr=FALSE)$ci
PCD=PCD/max(PMD,1)
PMD=PMD/iter
list(PMD=PMD,PMD.ci=PMD.ci,PCD=PCD,PCD.ci=PCD.ci)
}




# ----------------------------------------------------------------------------

# pmodchk

# ----------------------------------------------------------------------------

pmodchk<-function(x,y,regfun=tsreg,gfun=lplot.pred,op=1,xout=FALSE,outfun=outpro,fr=1,est=median,...){
#
# Compare  regression fit to smooth
#  gfun can be  either lplot.pred or rplot.pred
#
stop('Use instead the function reg.vs.rplot')
xy=elimna(cbind(x,y))
p1=ncol(xy)
p=p1-1
x=xy[,1:p]
y=xy[,p1]
if(xout){
x<-as.matrix(x)
flag<-outfun(x,plotit=FALSE,...)$keep
x<-x[flag,]
y<-y[flag]
x<-as.matrix(x)
n.keep=nrow(x)
}
fit2=NULL
fit1=regYhat(x,y,regfun=regfun)
if(identical(gfun,lplot.pred))fit2<-lplot.pred(x,y)$yhat
if(identical(gfun,rplot.pred))fit2<-rplot.pred(x,y,est=est,fr=fr)$Y.hat
if(is.null(fit2))stop("gfun should be either lplot.pred or rplot.pred")
if(op==0)plot(fit1,fit2,xlab="Reg. Fit",ylab="Gen. Fit")
if(op==1)lplot(fit1,fit2)
if(op==2)runmean(fit1,fit2,fr=fr)
abline(0,1)
}




# ----------------------------------------------------------------------------

# pool.fun

# ----------------------------------------------------------------------------

pool.fun<-function(J,K,x){
#
#  x is assumed to have list mode.
#
# For a between-by-within design
# For data in list mode, pool the data
# over Factor A	(between)
# and store in a new variable have list	model with length K
#
#  That is, ignore levels of A
#
JK=J*K
imat=matrix(c(1:JK),ncol=K,byrow=TRUE)
B=list()
for(k in 1:K){
id=imat[,k]
B[[k]]=as.vector(matl(x[id]))
}
B
}

best.cell.sub<-function(x,alpha=.05,LARGEST=TRUE,method='AC',AUTO=FALSE){
#
#  For a multinomial distribution, can a decision be made
#  about which cell has the highest probability
#
# x  Assumed to contain the  cell frequencies
#
x=elimna(x)
n=sum(x)
NCELL=length(x)
NCm1=NCELL-1
xor=order(x,decreasing = LARGEST)
output=NA
ic=0
for(j in 2:NCELL){
ic=ic+1
output[ic]=cell.com.pv(x,xor[1],xor[j])
}
output
}


cell.com.pv<-function(x,i=1,j=2,method='AC'){
#
# For a multinomial distribution, compute a confidence interval
#  for p_i-p_j, the difference between the probabilities asscoiated with cells i and j
#
# x= cell frequencies
#
n=sum(x)
p1=x[i]/n
p2=x[j]/n
COR=0-sqrt(p1*p1/((1-p1)*(1-p2)))
a=seq(.001,.1,.001)
a=c(a,seq(.1,.99,.01))
a=rev(a)

if(x[i]==x[j])pv=1
if(x[i]!=x[j]){
for(k in 1:length(a)){
c2=acbinomci(x[j],n,alpha=a[k])$ci
c1=acbinomci(x[i],n,alpha=a[k])$ci
T1=(p1-c1[1])^2+(p2-c2[2])^2-2*COR*(p1-c1[1])*(c2[2]-p2)
T2=(p1-c1[2])^2+(p2-c2[1])^2-2*COR*(c1[2]-p1)*(p2-c2[1])
T2=max(c(0,T2))
T1=max(c(0,T1))
L=p1-p2-sqrt(T1)
U=p1-p2+sqrt(T2)
pv=a[k]
if(sign(L*U)<0)break
}}
if(n<=35){
if(x[i]==x[j])pvnew=1
else{
pv.up=pv+.1
anew=seq(pv,pv.up,.01)
for(k in 1:length(anew)){
c1=binom.conf(x[i],n,AUTO=TRUE,method=method,alpha=anew[k],pr=FALSE)$ci
c2=binom.conf(x[j],n,AUTO=TRUE,method=method,alpha=anew[k],pr=FALSE)$ci
T1=(p1-c1[1])^2+(p2-c2[2])^2-2*COR*(p1-c1[1])*(c2[2]-p2)
T2=(p1-c1[2])^2+(p2-c2[1])^2-2*COR*(c1[2]-p1)*(p2-c2[1])
T2=max(c(0,T2))
T1=max(c(0,T1))
L=p1-p2-sqrt(T1)
U=p1-p2+sqrt(T2)
pvnew=anew[k]
if(sign(L*U)>0)break
}}
pv=pvnew
}
pv
}




# ----------------------------------------------------------------------------

# Ppca

# ----------------------------------------------------------------------------

Ppca<-function(x,p=ncol(x)-1,locfun=L1medcen,loc.val=NULL,SCORES=FALSE,
gvar.fun=cov.mba,pr=TRUE,SEED=TRUE,gcov=rmba,SCALE=TRUE,...){
#
# Robust PCA aimed at finding scores that maximize a
# robust generalized variance given the goal of reducing data from
# m dimensions to
# p, which defaults to m-1
#
#  locfun, location used to center design space.
#  by default, use the spatial median
#  alternatives are mcd, tauloc, ...
#
#  # data are centered based on measure of location indicated by
#  locfun: default is spatial median.
#
#  SCALE=T means the marginal distributions are rescaled using the
#  measure and scatter indicated by
#  gcov, which defaults to median ball measure of location and variance
#
#  Output: the projection matrix. If
#  SCORES=T, the projected scores are returned.
#
x=as.matrix(x)
x<-elimna(x)
n<-nrow(x)
m<-ncol(x)
xdat=c(n,m,p,as.vector(x))
if(!SCALE){
if(is.null(loc.val))info<-locfun(x,...)$center
if(!is.null(loc.val))info<-loc.val
for(i in 1:n)x[i,]<-x[i,]-info
}
if(SCALE){
ms=gcov(x)
for(i in 1:n)x[i,]<-x[i,]-ms$center
for(j in 1:m)x[,j]<-x[,j]/sqrt(ms$cov[j,j])
}
vals<-NA
z<-matrix(nrow=n,ncol=p)
np=p*m
B=robpca(x,pval=p,plotit=FALSE,pr=pr,SEED=SEED,scree=FALSE)$P
B=t(B)
Bs=nelderv2(xdat,np,NMpca,START=B)
Bop=matrix(Bs,nrow=p,ncol=m)
Bop=t(ortho(t(Bop)))
z<-matrix(nrow=n,ncol=p)
zval<-NULL
for(i in 1:n)z[i,]<-Bop%*%as.matrix(x[i,])
if(SCORES)zval<-z
val=gvarg(z)
list(B=Bop,gen.sd=sqrt(val),scores=zval)
}
Ppca.sum.sub<-function(j,x,SCALE=TRUE){
#
res=Ppca(x,p=j,pr=FALSE,SCALE=SCALE)$gen.sd
res
}




# ----------------------------------------------------------------------------

# Ppca.summary

# ----------------------------------------------------------------------------

Ppca.summary<-function(x,MC=FALSE,SCALE=TRUE,p=NULL){
#
#   x is assumed to be a matrix with p columns
#   Using robust principal components (Ppca)
#   compute generalized variance for each dimension reduction
#   from 1 to p.
#
#   report values plus proportion relative to largest value found
#
x=as.matrix(x)
if(!is.matrix(x))stop("x should be a matrix")
x=elimna(x)
gv=NA
if(is.null(p))p=ncol(x)
if(!MC)for(j in 1:p)gv[j]=Ppca(x,p=j,pr=FALSE,SCALE=SCALE)$gen.sd
if(MC){
y=list()
for(j in 1:p)y[[j]]=j
gv=mclapply(y,Ppca.sum.sub,x,SCALE=SCALE,mc.preschedule=TRUE)
gv=as.vector(matl(gv))
}
res=matrix(NA,nrow=3,ncol=p)
res[1,]=c(1:p)
res[2,]=gv
res[3,]=gv/max(gv)
dimnames(res)=list(c("Num. of Comp.","Gen.Stand.Dev","Relative Size"),NULL)
list(summary=res)
}

predict.robust.Forest<-function(object,newdata=NULL, what=tmean,...  ){
#
#  Goal: estimate a measure of location for newdata based on the regions stemming from the Random Forest method
#
#  what: a function indicating the measure of location to be estimated. Default is a 20% trimmed mean
#
#  Example:
#  a=quantregForest(x,y)
#  predict.robust.Forest(a,newdata = new,what=hd)
#
#   For convenience, these steps are combined in the function regR.Forest, which calls this function and eliminates the need for the
#   library command.
#
#  For each region generated by the random Forest method,this would estimate the median based on the Harrell-Davis estimator.
#
# predict.robust.Forest(a,newdata = new,what=mean,tr=.1)  10% trimmed mean
#
# To estimate one or more quantiles, can use predict.quantregForest or could use this function
#  with what containing the quantiles to be estimated. For example what=c(.25,.75) would estimate the
#  lower and upper quartiles.
#
# This code is based on modifications of code written by L. Michel
#
    class(object) <- 'randomForest'
    if(is.null(newdata)){
        if(is.null(object[['valuesOOB']])) stop('need to fit with option keep.inbag=TRUE  if trying to get out-of-bag observations')
        valuesPredict <- object[['valuesOOB']]
    }else{
        predictNodes <- attr(predict(object,newdata=newdata,nodes=TRUE),'nodes')
        rownames(predictNodes) <- NULL
        valuesPredict <- 0*predictNodes
        ntree <- ncol(object[['valuesNodes']])
        for (tree in 1:ntree){
            valuesPredict[,tree] <- object[['valuesNodes']][ predictNodes[,tree],tree]
        }
    }
    if(is.function(what)){
        if(is.function(what(1:4))){
            result <- apply(valuesPredict,1,what)
        }else{
            if(length(what(1:4))==1){
                result <- apply(valuesPredict,1,what,...)
            }else{
                result <- t(apply(valuesPredict,1,what))
            }
        }
    }else{
        if( !is.numeric(what)) stop('  argument what needs to be either a function or a vector with quantiles')
        if( min(what)<0) stop(' if what specifies quantiles, the minimal values needs to be non-negative')
        if( max(what)>1) stop(' if what specifies quantiles, the maximal values cannot exceed 1')
        if(length(what)==1){
            result <- apply( valuesPredict,1,quantile, what,na.rm=TRUE)
        }else{
            result <- t(apply( valuesPredict,1,quantile, what,na.rm=TRUE))
            colnames(result) <- paste('quantile=',what)
        }
    }
    return(result)
}

pro.class<-function(train=NULL,test=NULL,g=NULL,x1=NULL,x2=NULL,nonpar=TRUE,rule=.5,fr=2,SEED=TRUE){
#
#  Project the data onto a line, then estimate the probability that
#  a value in test data is in first group.  Impacted by unequal sample sizes. To avoid this use
#  pro.class.bag.  Or use this function but with equal samples sizes for the test data.
#
#  nonpar=TRUE: use a smoother to estimate probabilities
#                           in which case
#                            fr is the span.
#                FALSE: use logistic regression
#
if(is.null(test))stop('Argument test is NULL')
if(!is.null(train)){
if(is.null(g))stop('Argument g, group labels, must be specified')
if(is.matrix(g)){
if(dim(g)>1)stop('Argument g should be a vector')
}}
CHK=FALSE
if(!is.null(x1)){
if(!is.null(x2)){
if(ncol(x1)!=ncol(x2))stop('x1 and x2 have different number of columns')
x1=elimna(x1)
x2=elimna(x2)
n1=nrow(x1)
n2=nrow(x2)
g=c(rep(0,n1),rep(1,n2))
CHK=TRUE
d=mulwmw.dist.new(x1,x2,test)
}}
if(!CHK){
if(is.null(g))stop('The argument g should contain the group id values')
xg=elimna(cbind(train,g))
p=ncol(train)
p1=p+1
train=xg[,1:p]
g=xg[,p1]
if(length(unique(g))!=2)stop('Should have only two unique values in g')
flag=g==min(g)
x1=train[flag,]
x2=train[!flag,]
 d=mulwmw.dist.new(x1,x2,test)
}
flag=g==min(g)
gnum=g
gnum[flag]=0
gnum[!flag]=1
x=c(d$dist1,d$dist2)
if(nonpar){
g1=rep(0,n1)
v=logSMpred(x,gnum,d[[3]],fr=fr,SEED=SEED)
}
if(!nonpar){
v=logreg.pred(x,gnum,d[[3]])
}
dec=rep(2,nrow(test))
dec[v<rule]=1
dec
}




# ----------------------------------------------------------------------------

# pro.class.probs

# ----------------------------------------------------------------------------

pro.class.probs<-function(train=NULL,test=NULL,g=NULL,x1=NULL,x2=NULL,nonpar=TRUE,rule=.5,fr=2){
#
# Same as pro.class, but also reports probabilities fo being in second class for each vector in test.
#
#  project the data onto a line, then estimate the probability that
#  a value in test data is in first group.
#
if(rule<=0 || rule>=1)stop('rule should be greater than 0 and less than 1')
if(!is.null(train)){
if(is.null(g))stop('Argument g, group ids, must be specified')
if(is.matrix(g)){
if(dim(g)>1)stop('Argument g should be a vector')
}}
CHK=FALSE
if(!is.null(x1)){
if(!is.null(x2)){
if(ncol(x1)!=ncol(x2))stop('x1 and x2 have different number of columns')
x1=elimna(x1)
x2=elimna(x2)
n1=nrow(x1)
n2=nrow(x2)
g=c(rep(0,n1),rep(1,n2))
CHK=TRUE
d=mulwmw.dist.new(x1,x2,test)
}}
if(!CHK){
if(is.null(g))stop('The argument g should contain the group id values')
xg=elimna(cbind(train,g))
p=ncol(train)
p1=p+1
train=xg[,1:p]
g=xg[,p1]
if(length(unique(g))!=2)stop('Should have only two unique values in g')
flag=g==min(g)
x1=train[flag,]
x2=train[!flag,]
 d=mulwmw.dist.new(x1,x2,test)
}
flag=g==min(g)
gnum=g
gnum[flag]=0
gnum[!flag]=1
x=c(d$dist1,d$dist2)
if(nonpar){
v=logSMpred(x,gnum,d[[3]],fr=fr)
}
if(!nonpar){
v=logreg.pred(x,gnum,d[[3]])
}
dec=rep(2,nrow(test))
dec[v<rule]=1
list(Decision=dec,prob.in.second.class=v)
}


pro.classPD<-function(train=NULL,test=NULL,g=NULL,x1=NULL,x2=NULL,nonpar=TRUE,rule=1,SEED=NULL){
#
#  project the data onto a line, between the center of the two data clouds
#  then classify based on estimate of the likelihood that
#  f(x), the pdf, is larger for the first group versus the second.
#  Can change the rule via the argument rule. Example
# ruile=.5
# Classify in group 1 if f_1/f_2>.5
#
#  SEED=NULL, done for convenience when this function is called by other functions.
#
if(is.null(test))stop('Argument test is null, contains  no data')
if(is.null(test))stop('Argument test is null, contains  no data')
if(!is.null(train)){
if(is.null(g))stop('Argument g, group ids, must be specified')
if(is.matrix(g)){
if(dim(g)>1)stop('Argument g should be a vector')
}}
CHK=FALSE
if(!is.null(x1)){
if(!is.null(x2)){
if(ncol(x1)!=ncol(x2))stop('x1 and x2 have different number of columns')
x1=elimna(x1)
x2=elimna(x2)
n1=nrow(x1)
n2=nrow(x2)
g=c(rep(0,n1),rep(1,n2))
CHK=TRUE
d=mulwmw.dist.new(x1,x2,test)
}}
if(!CHK){
if(is.null(g))stop('The argument g should contain the group id values')
xg=elimna(cbind(train,g))
p=ncol(train)
p1=p+1
train=xg[,1:p]
g=xg[,p1]
if(length(unique(g))!=2)stop('Should have only two unique values in g')
flag=g==min(g)
x1=train[flag,]
x2=train[!flag,]
 d=mulwmw.dist.new(x1,x2,test)
}
pdf1=NA
pdf2=NA
for(i in 1:length(d$dis.new)){    #Avoid sorting issue done by akerd
pdf1[i]=akerd(d$dist1,pts=d$dis.new[i],pyhat=TRUE,plotit=FALSE)
pdf2[i]=akerd(d$dist2,pts=d$dis.new[i],pyhat=TRUE,plotit=FALSE)
}
dec=rep(2,nrow(test))
dec[pdf1/pdf2>rule]=1
dec
}

psiBY3 <- function(t,c3)
{(exp(-sqrt(c3))*as.numeric(t <= c3))+(exp(-sqrt(t))*as.numeric(t >c3))}




# ----------------------------------------------------------------------------

# psihat

# ----------------------------------------------------------------------------

psihat<-function(x,est=hd,con,...){
#
#  x has list mode or is a matrix or a data frame
#  compute estimates of linear contrasts
#  indicated by con using estimator est
#  using data in x
#
if(is.matrix(x))x=listm(x)
res=NULL
xbar=lapply(x,est,...)
xbar=as.vector(matl(xbar))
for(d in 1:ncol(con))res[d]=sum(con[,d]*xbar)
res
}




# ----------------------------------------------------------------------------

# psip.bt

# ----------------------------------------------------------------------------

psip.bt <- function(x,c1,M)
{
    x1 <- (x-M)/c1
    ivec1 <- (x1 < 0)
    ivec2 <- (x1 >  1)
    return(ivec1+(1-ivec1-ivec2)*((1-x1^2)^2+4*x*x1*(1-x1^2)/c1))
}




# ----------------------------------------------------------------------------

# ptests

# ----------------------------------------------------------------------------

ptests<-function(pv,Fisher=TRUE){
#
# pv: p-values based on N independent tests
# Test hypothesis that all N null hypotheses are true.
# Fisher=TRUE, use Fisher's method
# Fisher=FALSE, use Chen-Nadarajah method
#
ntests=length(pv)
if(Fisher){
res=(0-2)*sum(log(pv))  # Fisher test statistic
pvF=1-pchisq(res,2*ntests)   #Fisher p-value based on all tests.
}
if(!Fisher){
res=sum(qnorm(pv/2)^2)  # C-N test
pvF=1-pchisq(res,ntests)
}
list(test.stat=res,p.value=pvF)
}
pts.to.equivalence <- function(x, y, start.pts=3, step=1, max.pts=30, eqbound=0.05, q=0.25, nboot=1000, alpha=0.05, SEED=TRUE, first.stop='YES') {
  if (SEED) set.seed(2)
  
  results <- data.frame(pts=integer(), Est.dif=numeric(),
                        CI95_low=numeric(), CI95_up=numeric(),
                        dif.pval=numeric(),
                        CI90_low=numeric(), CI90_up=numeric(),
                        low.test.p=numeric(), up.test.p=numeric(),
                        low.test.reject=logical(), up.test.reject=logical(),
                        TOST.p.value=numeric(), equivalence=logical())
  
  pts.seq <- seq(from=start.pts, to=max.pts, by=step)
  print(pts.seq)
  
  for (pt in unique(sort(pts.seq))) {
    pt_scalar <- as.numeric(pt)[1]  # Ensure pts is scalar numeric
    cat("Testing pts =", pt_scalar, "n")
    
    res <- wmw.ref.dif.TOST(x, y, pts=pt_scalar, eqbound=eqbound, nboot=nboot, alpha=alpha, SEED=FALSE, plotit=TRUE)
    tost <- res$TOST
    
    CI95_low <- res$ci95[1]
    CI95_up <- res$ci95[2]

    results <- rbind(results, data.frame(
      pts=pt_scalar,
      Est.dif=res$Est.dif,
      CI95_low=CI95_low,
      CI95_up=CI95_up,
      dif.pval=res$p.value,
      CI90_low=tost$ci90[1],
      CI90_up=tost$ci90[2],
      low.test.p=tost$lower_tail$p.value,
      up.test.p=tost$upper_tail$p.value,
      low.test.reject=tost$lower_tail$rejected,
      up.test.reject=tost$upper_tail$rejected,
      TOST.p.value=max(c(tost$lower_tail$p.value,tost$upper_tail$p.value)),
      equivalence=tost$equivalence
    ))
    
       if(first.stop=="YES"){
      if(tost$equivalence=="YES") break
    }
  }
 
 svalues.calc<-function(x){
   x2=ifelse(x<0.001,0.001, x)
   round(-log2(x2))
 }
  p2.df=data.frame(pts=rep(results$pts,2),
                   svals= svalues.calc(c(results$dif.pval,results$TOST.p.value)),
                   test=rep(c("Difference S-value","Equivalence S-value"),each=max(results$pts)))
  
  p.formater<-function(x){
    x2=ifelse(x<0.001,"p<0.001",paste("p=", round(x,3)))
    return(x2)
  }
  results$dif.pval=p.formater (results$dif.pval)
  results$low.test.p=p.formater (results$low.test.p)
  results$up.test.p=p.formater (results$up.test.p)
 
  ylimit=round(max(p2.df$svals))+1
  p <- ggplot(results, aes(x=pts)) +
    geom_rect(aes(ymin=-eqbound, ymax=eqbound, xmin=-Inf, xmax=Inf), fill="gray", alpha=0.3, color=NA)+
    geom_linerange(aes(ymin=CI90_low, ymax=CI90_up), color='black', linewidth=0.5)+
    geom_line(aes(y=Est.dif), color='black', linewidth=0.25, linetype="dashed") +
    geom_hline(yintercept=0, color="red", linetype="dashed") +
    geom_label(aes(y=Est.dif, fill=TOST.p.value, label=paste("TOST p=",p.formater(TOST.p.value),sep="n")),size=7/.pt) +
    scale_fill_gradientn(colors=c("green","orange"), limits=c(0,1)) +
    scale_x_continuous(breaks=results$pts, sec.axis = sec_axis(~ . * 1,breaks=results$pts,labels=results$dif.pval, name="P-value difference probabilities")) +
    scale_y_continuous( sec.axis = sec_axis(~ . * 1, name = "Equivalence bounds",breaks=c(0-eqbound,eqbound)))+
    labs(title='Estimates and Confidence Intervals vs MESOI',
         x='MESOI (Number of pts)', y='Difference between estimated probabilities (and CI)') +
    theme_minimal()+
    theme(axis.title.y.right = element_text(hjust = 1),
          plot.title=element_text(hjust=0.55))
  
 
  p2<-ggplot(data= p2.df)+
    geom_point(aes(x=pts,y=svals, color=test),size=3)+
    geom_line(aes(x=pts,y=svals, color=test,group=test),linetype="dashed")+
    scale_x_continuous(breaks=unique(p2.df$pts))+
    scale_y_continuous(limits=c(0,ylimit), breaks=seq(0,ylimit,2))+
    scale_color_manual(name="", values=c("purple","magenta"))+
    labs(x="SESOI (number of pts)", y="Surprise values",title="Surprise values vs. SESOI")+
    theme(axis.title.y.right = element_text(hjust = 1),
          plot.title=element_text(hjust=0.55))+
    theme_minimal()+coord_cartesian(clip="off")+
    theme(plot.title=element_text(hjust=0.6),
          legend.position = "bottom",
          legend.direction = "horizontal",
          plot.background = element_rect(color="lightgray"))
  
  list(results=results, main.plot=p, splot=p2)
}

#BOTTOMS




# ----------------------------------------------------------------------------

# pull

# ----------------------------------------------------------------------------

pull <- function(a,n,k)
{
	b=0
	b=a
	l=1
	lr=n
	while (l<lr)
	{
		ax=b[k]
		jnc=l
		j=lr
		while (jnc<=j)
		{
			while (b[jnc]<ax)
			{
				jnc=jnc+1
			}
			while (b[j]>ax)
			{
				j=j-1
			}
			if (jnc<=j)
			{
				buffer=b[jnc]
				b[jnc]=b[j]
				b[j]=buffer
				jnc=jnc+1
				j=j-1
			}
		}
		if (j<k)
		{
			l=jnc
		}
		if (k<jnc)
		{
			lr=j
		}
	}
	outp=b[k]
	outp
}




# ----------------------------------------------------------------------------

# push

# ----------------------------------------------------------------------------

push<-function(mat){
#
# For every column of mat, move entry down 1
#
matn<-matrix(NA,nrow=nrow(mat),ncol=ncol(mat))
Jm<-nrow(mat)-1
for (k in 1:ncol(mat)){
temp<-mat[,k]
vec<-0
vec[2:nrow(mat)]<-temp[1:Jm]
matn[,k]<-vec
}
matn
}

PVALS<-function(x,vals){
#
#for each value	in vals, determine
# proportion in	x <= vals
#
x=elimna(x)
v=NA
for(i in 1:length(vals))v[i]=mean(x<=vals[i])
v
}




# ----------------------------------------------------------------------------

# pxly

# ----------------------------------------------------------------------------

pxly<-function(x,y,iter=100,SEED=TRUE){
#
#  x and y independent
#  Estimate P(x<y)
#
est=bmp(x,y)$phat
est
}


q2by2<-function(x,q = c(0.1, 0.25, 0.5, 0.75, 0.9), nboot = 2000,SEED=TRUE){
#
# For a 2 by 2 ANOVA, independent groups, test main effects
# and interaction for all quantiles indicated by argument q
#
if(SEED)set.seed(2)
if(is.matrix(x))x<-listm(x)
if(length(x)!=4)stop('Current version is for a 2-by-2 ANOVA only. Should have four groups.')
A=matrix(NA,nrow=length(q),6)
B=matrix(NA,nrow=length(q),6)
AB=matrix(NA,nrow=length(q),6)
dimnames(A)=list(NULL,c('q','psihat','p.value','ci.lower','ci.upper','p.hoch'))
dimnames(B)=list(NULL,c('q','psihat','p.value','ci.lower','ci.upper','p.hoch'))
dimnames(AB)=list(NULL,c('q','psihat','p.value','ci.lower','ci.upper','p.hoch'))
con=con2way(2,2)

for(i in 1:length(q)){
A[i,1]=q[i]
B[i,1]=q[i]
AB[i,1]=q[i]
a=linconpb(x,nboot=nboot,est=hd,con=con$conA,SEED=FALSE,q=q[i])
b=linconpb(x,nboot=nboot,est=hd,con=con$conB,SEED=FALSE,q=q[i])
ab=linconpb(x,nboot=nboot,est=hd,con=con$conAB,SEED=FALSE,q=q[i])
A[i,2:5]=a$output[,c(2,3,5,6)]
B[i,2:5]=b$output[,c(2,3,5,6)]
AB[i,2:5]=ab$output[,c(2,3,5,6)]
}
A[,6]=p.adjust(A[,3],method='hoch')
B[,6]=p.adjust(B[,3],method='hoch')
AB[,6]=p.adjust(AB[,3],method='hoch')
list(A=A,B=B,AB=AB)
}




# ----------------------------------------------------------------------------

# q2gci

# ----------------------------------------------------------------------------

q2gci<-function(x,y,q=c(.1,.25,.5,.75,.9),nboot=2000,plotit=TRUE,SEED=TRUE,xlab='Group 1',ylab='Est.1-Est.2',alpha=.05){
#
# Compare quantiles using pb2gen
# via hd estimator. Tied values are allowed.
# When comparing lower or upper quartiles, both power and the probability of Type I error
# compare well to other methods that have been derived.
# q: can be used to specify the quantiles to be compared
# q defaults to comparing the .1,.25,.5,.75, and .9 quantiles
#   Function returns p-values and critical p-values based on Hochberg's method.
#
x=elimna(x)
y=elimna(y)
if(sum(duplicated(x)>0))stop('Duplicate values were detected; use qcomhd or medpb2')
if(sum(duplicated(y)>0))stop('Duplicate values were detected; use qcomhd or medpb2')
if(SEED)set.seed(2)
pv=NULL
output=matrix(NA,nrow=length(q),ncol=10)
dimnames(output)<-list(NULL,c('q','n1','n2','est.1','est.2','est.1_minus_est.2','ci.low','ci.up','p_crit','p-value'))
for(i in 1:length(q)){
output[i,1]=q[i]
output[i,2]=length(elimna(x))
output[i,3]=length(elimna(y))
output[i,4]=qest(x,q=q[i])
output[i,5]=qest(y,q=q[i])
output[i,6]=output[i,4]-output[i,5]
temp=pb2gen(x,y,nboot=nboot,est=qest,q=q[i],SEED=FALSE,alpha=alpha,pr=FALSE)
output[i,7]=temp$ci[1]
output[i,8]=temp$ci[2]
output[i,10]=temp$p.value
}
temp=order(output[,10],decreasing=TRUE)
zvec=alpha/c(1:length(q))
output[temp,9]=zvec
#print(output)
output <- data.frame(output)
output$signif=rep('YES',nrow(output))
for(i in 1:nrow(output)){
if(output[temp[i],10]>output[temp[i],9])output$signif[temp[i]]='NO'
if(output[temp[i],10]<=output[temp[i],9])break
}
if(plotit){
xax=rep(output[,4],3)
yax=c(output[,6],output[,7],output[,8])
plot(xax,yax,xlab=xlab,ylab=ylab,type='n')
points(output[,4],output[,6],pch='*')
lines(output[,4],output[,6])
points(output[,4],output[,7],pch='+')
points(output[,4],output[,8],pch='+')
}
output
}
qci<-function(x,q=.5,alpha=.05,op=3){
#
# Compute a confidence interval for qth quantile
#  using an  estimate of standard error based on
#  adaptive kernel density estimator.
# The qth quantile is estimated with a single order statistic.
#
# For argument op, see the function qse.
#
if(sum(duplicated(x)>0))stop("Duplicate values detected; use hdpb")
n<-length(x)
xsort<-sort(x)
iq <- floor(q * n + 0.5)
qest<-xsort[iq]
se<-qse(x,q,op=op)
crit<-qnorm(1-alpha/2)
ci.low<-qest-crit*se
ci.up<-qest+crit*se
list(ci.low=ci.low,ci.up=ci.up,q.est=qest)
}




# ----------------------------------------------------------------------------

# qcipb

# ----------------------------------------------------------------------------

qcipb<-function(x,q=.5,alpha=.05,nboot=2000,SEED=TRUE,nv=0,...){
#
#   Compute a bootstrap, .95 confidence interval for the
#   qth quantile via the Harrell--Davis estimator.
#
#   Default is q=.5, meaning a confidence interval for the median is
#   computed.
#
#   Appears to be best method when there are tied values
#
#    nv=null value when  computing a p-value
#
if(SEED)set.seed(2) # set seed of random number generator so that
#             results can be duplicated.
x=elimna(x)
data<-matrix(sample(x,size=length(x)*nboot,replace=TRUE),nrow=nboot)
bvec<-apply(data,1,hd,q=q)
bvec<-sort(bvec)
low<-round((alpha/2)*nboot)
up<-nboot-low
low<-low+1
pv=mean(bvec>nv)+.5*mean(bvec==nv)
pv=2*min(c(pv,1-pv))
estimate=hd(x,q=q)
list(ci=c(bvec[low],bvec[up]),n=length(x),estimate=estimate,p.value=pv)
}
qcmul<-function(x,y,q=.5,nboot=599,alpha=.05,SEED=FALSE,xout=FALSE,
outfun=outpro,method='BH'){
#
#
# For each independent variable, compute a confidence interval for a
#  quantile regression correlation.
#
if(SEED)set.seed(2)
 xy=elimna(cbind(x,y))
p1=ncol(xy)
p=p1-1
x=xy[,1:p]
x=as.matrix(x)
y=xy[,p1]
n=length(y)
if(xout){
x<-as.matrix(x)
flag<-outfun(x,plotit=FALSE)$keep
x<-x[flag,]
y<-y[flag]
n=length(y)
xy=cbind(x,y)
}
x=as.matrix(x)
e=matrix(NA,p,5)
dimnames(e)=list(NULL,c('Est.','ci.low','ci.up','p-value','adj.p.value'))
for(j in 1:p)e[j,1]=qcorp1(x[,j],y,q=q)$cor
 v=matrix(NA,nboot,p)
 n=nrow(xy)
 for(j in 1:p){
 for(i in 1:nboot){
 id=sample(n,replace=TRUE)
v[i,j]=qcorp1(xy[id,j],xy[id,p1],q=q)$cor
 }}
 v=apply(v,2,sort)
 ilow<-round((alpha/2) * nboot)
ihi<-nboot - ilow
ilow<-ilow+1
 e[,2]=v[ilow,]
 e[,3]=v[ihi,]
 pv=NA
 for(j in 1:p){
 pv[j]=mean(v[,j]<0)
 if(pv[j]>=.5)pv[j]=1-pv[j]
 }
 e[,4]=2*pv
 e[,5]=p.adjust(e[,4],method=method)
 list(n=n,results=e)
}


qcomthd<-function(x,y,q=c(.1,.25,.5,.75,.9),nboot=2000,plotit=TRUE,SEED=TRUE,xlab='Group 1',ylab='Est.1-Est.2',alpha=.05,ADJ.CI=TRUE,MC=FALSE){
#
# Compare quantiles using pb2gen using trimmed version of the Harrell-Davis estimator
#Tied values are allowed.
#
#  ADJ.CI=TRUE means that the confidence intervals are adjusted based on the level used by the corresponding
#  test statistic. If a test is performed with at the .05/3 level, for example, the confidence returned has
#  1-.05/3 probability coverage.
#
# When comparing lower or upper quartiles, both power and the probability of Type I error
# compare well to other methods that have been derived.
# q: can be used to specify the quantiles to be compared
# q defaults to comparing the .1,.25,.5,.75, and .9 quantiles
#
#   Function returns p-values and critical p-values based on Hochberg's method.
#

if(SEED)set.seed(2)
pv=NULL
output=matrix(NA,nrow=length(q),ncol=10)
dimnames(output)<-list(NULL,c('q','n1','n2','est.1','est.2','est.1_minus_est.2','ci.low','ci.up','p-value','adj.p.value'))
for(i in 1:length(q)){
output[i,1]=q[i]
output[i,2]=length(elimna(x))
output[i,3]=length(elimna(y))
output[i,4]=thd(x,q=q[i])
output[i,5]=thd(y,q=q[i])
output[i,6]=output[i,4]-output[i,5]
temp=qcomthd.sub(x,y,nboot=nboot,q=q[i],SEED=FALSE,alpha=alpha,MC=MC)
output[i,7]=temp$ci[1]
output[i,8]=temp$ci[2]
output[i,9]=temp$p.value
}
temp=order(output[,9],decreasing=TRUE)
zvec=alpha/c(1:length(q))
zvec[temp]=zvec
if(ADJ.CI){
for(i in 1:length(q)){
if(!MC)temp=pb2gen(x,y,nboot=nboot,est=thd,q=q[i],SEED=FALSE,alpha=zvec[i],pr=FALSE)
else
temp=pb2genMC(x,y,nboot=nboot,est=thd,q=q[i],SEED=FALSE,alpha=zvec[i],pr=FALSE)
output[i,7]=temp$ci[1]
output[i,8]=temp$ci[2]
output[i,9]=temp$p.value
}
temp=order(output[,10],decreasing=TRUE)
}
output[,10]=p.adjust(output[,9],method='hoch')


if(plotit){
xax=rep(output[,4],3)
yax=c(output[,6],output[,7],output[,8])
plot(xax,yax,xlab=xlab,ylab=ylab,type='n')
points(output[,4],output[,6],pch='*')
lines(output[,4],output[,6])
points(output[,4],output[,7],pch='+')
points(output[,4],output[,8],pch='+')
}
output
}

qcomthd.sub<-function(x,y,q,alpha=.05,nboot=2000,SEED=TRUE,MC=TRUE){
#
x<-x[!is.na(x)] # Remove any missing values in x
y<-y[!is.na(y)] # Remove any missing values in y
if(SEED)set.seed(2) # set seed of random number generator so that
#             results can be duplicated.
datax<-matrix(sample(x,size=length(x)*nboot,replace=TRUE),nrow=nboot)
datay<-matrix(sample(y,size=length(y)*nboot,replace=TRUE),nrow=nboot)
datax=listm(t(datax))
datay=listm(t(datay))
if(MC){
bvecx<-mclapply(datax,thd,q,mc.preschedule=TRUE)
bvecy<-mclapply(datay,thd,q,mc.preschedule=TRUE)
}
else{
bvecx<-lapply(datax,thd,q)
bvecy<-lapply(datay,thd,q)
}
bvecx=as.vector(matl(bvecx))
bvecy=as.vector(matl(bvecy))
bvec<-sort(bvecx-bvecy)
low<-round((alpha/2)*nboot)+1
up<-nboot-low
temp<-sum(bvec<0)/nboot+sum(bvec==0)/(2*nboot)
sig.level<-2*(min(temp,1-temp))
se<-var(bvec)
list(est.1=thd(x,q),est.2=thd(y,q),ci=c(bvec[low],bvec[up]),p.value=sig.level,sq.se=se,n1=length(x),n2=length(y))
}




# ----------------------------------------------------------------------------

# qdec

# ----------------------------------------------------------------------------

qdec<-function(x){
#
# compute deciles using single order statistics
# (function deciles uses Harrell-Davis estimator)
#
vals<-NA
for(i in 1:9){
vals[i]<-qest(x,i/10)
}
vals
}




# ----------------------------------------------------------------------------

# qdec2ci

# ----------------------------------------------------------------------------

qdec2ci<-function(x,y=NA,nboot=500,alpha=.05,pr=FALSE,SEED=TRUE,plotit=TRUE){
#
# Compare the deciles of two dependent groups
# with quantiles estimated with a single order statistic
#
#  x: can  be a matrix with two columns in which case
#  y  is ignored.
#
if(SEED)set.seed(2)
if(is.na(y[1])){
y<-x[,2]
x<-x[,1]
}
xy=elimna(cbind(x,y))
x=xy[,1]
y=xy[,2]
if(sum(duplicated(x))>0)stop('Tied values detected, use Dqcomhd')
if(sum(duplicated(y))>0)stop('Tied values detected, use Dqcomhd')
bvec<-matrix(NA,nrow=nboot,ncol=9)
if(pr)print("Taking bootstrap samples. Please Wait.")
data<-matrix(sample(n,size=n*nboot,replace=TRUE),nrow=nboot)
for(i in 1:nboot)bvec[i,]<-qdec(x[data[i,]])-qdec(y[data[i,]])
pval<-NA
m<-matrix(0,9,5)
dimnames(m)<-list(NULL,c("lower","upper","Delta.hat","p.values",'p.crit'))
crit <- alpha/2
icl <- round(crit * nboot) + 1
icu <- nboot - icl
for(i in 1:9){
pval[i]<-(sum(bvec[,i]<0)+.5*sum(bvec[,i]==0))/nboot
pval[i]<-2*min(pval[i],1-pval[i])
temp<-sort(bvec[,i])
m[i,1]<-temp[icl]
m[i,2]<-temp[icu]
}
m[,3]<-qdec(x)-qdec(y)
m[,4]<-pval
temp=order(pval,decreasing=TRUE)
zvec=alpha/c(1:9)
m[temp,5]=zvec
if(plotit){
xaxis<-c(qdec(x),qdec(x))
par(pch="+")
yaxis<-c(m[,1],m[,2])
plot(xaxis,yaxis,ylab="delta",xlab="x (first group)")
par(pch="*")
points(qdec(x),m[,3])
}
m
}




qdtest<-function(x,y=NA,q=.5,bop=FALSE,nboot=100,se.val=NA){
#
# Test hypothesis of equal q quantiles for
# two dependent groups.
#
# x is either a matrix with 2 columns or a vector
# If x is a vector, must specify y
#
# Appears to adequately control type I error when n>=20
#
if(!is.na(y[1]))x<-cbind(x,y)
if(!is.matrix(x))stop("Something is wrong, with x or y")
x<-elimna(x)
y<-x[,2]
x<-x[,1]
n<-length(y)
df<-n-1
if(is.na(se.val[1])){
if(!bop)se.val<-sedm(x,y,q=q)
if(bop)se.val<-bootdse(x,y,est=qest,q=q,pr=FALSE,nboot=nboot)
}
test<-(qest(x,q)-qest(y,q))/se.val
sig.level<-2*(1-pt(abs(test),df))
list(test.stat=test,p.value=sig.level,se=se.val)
}




# ----------------------------------------------------------------------------

# qest.meth

# ----------------------------------------------------------------------------

qest.meth<-function(x,q=.5,method=c('HD','NO','TR','SO')){

type=match.arg(method)
switch(type,
HD=hd(x,q=q),
NO=qno.est(x,q=q),
TR=thd(x,q=q),
SO=qest(x,q=q))
}




# ----------------------------------------------------------------------------

# qfun

# ----------------------------------------------------------------------------

qfun<-function(x,y,coef,q){
x=as.matrix(x)
p1=ncol(x)+1
 r=y-coef[1]-x%*%coef[2:p1]
 rhoq=sum(r*(q-as.numeric((r<0))))
 s=sum(rhoq)
 s
 }




# ----------------------------------------------------------------------------

# qghdist

# ----------------------------------------------------------------------------

qghdist<-function(q=.5,g=0,h=0){
#
# Determine quantile of a g-and-h distribution
#
e=qnorm(q)
v=ghtransform(e,g=g,h=h)
v
}

qhomt<-function(x,y,nboot=100,alpha=.05,qval=c(.2,.8),plotit=TRUE,SEED=TRUE,pch='*',
xlab="X",ylab="Y",xout=FALSE,outfun=outpro,pr=TRUE,WARN=FALSE,...){
#
#   Test hypothesis that the error term is  homogeneous by
#   computing a confidence interval for beta_1-beta_2, the
#   difference between the slopes of the qval[2] and qval[1]
#   regression slopes, where qval[1] and qval[2] are
#   the quantile regression slopes
#   estimated via the Koenker-Bassett method.
#   So by default, use the .8 quantile slope minus the
#   the .2 quantile slope.
#
if(length(qval)!=2)stop("Argument qval should have 2 values exactly")
x<-as.matrix(x)
if(ncol(x)!=1)stop("Only one predictor is allowed; use qhomtv2")
xy<-elimna(cbind(x,y))
x<-xy[,1]
x<-as.matrix(x)
y<-xy[,2]
if(xout){
flag<-outfun(x,...)$keep
x<-as.matrix(x)
x<-x[flag,]
y<-y[flag]
x<-as.matrix(x)
}
if(SEED)set.seed(2) # set seed of random number generator so that
#             results can be duplicated.
if(!WARN)options(warn=-1)
if(pr)print("Taking bootstrap samples. Please wait.")
data<-matrix(sample(length(y),size=length(y)*nboot,replace=TRUE),nrow=nboot)
bvec<-apply(data,1,qhomtsub,x,y,qval) # An nboot vector.
se<-sqrt(var(bvec))
temp<-qplotreg(x,y,qval=qval,plotit=plotit,xlab=xlab,ylab=ylab,pch=pch)
crit<-qnorm(1-alpha/2)
crit.ad<-NA
dimnames(temp)=c(NULL,NULL)
dif<-temp[2,2]-temp[1,2]
regci<-NA
regci[1]<-dif-crit*se
regci[2]<-dif+crit*se
sig.level<-2*(1-pnorm(abs(dif)/se))
regci.ad<-NA
if(alpha==.05 && qval[1]==.2 && qval[2]==.8)crit.ad<-qnorm(0-.09/sqrt(length(y))+.975)
ci.ad<-c(dif-crit.ad*se,dif+crit.ad*se)
if(!WARN)options(warn=0)
list(slope.bottom=temp[1,2],slope.top=temp[2,2],
dif.est=dif,dif.ci=regci,p.value=sig.level,se=se,adjusted.ci=ci.ad)
}



qhomtsub<-function(isub,x,y,qval){
#
#  Perform quantile regression using x[isub] to predict y[isub]
#  isub is a vector of length n,
#  a bootstrap sample from the sequence of integers
#  1, 2, 3, ..., n
#
#  This function is used by other functions when computing
#  bootstrap estimates.
#
#  regfun is some regression method already stored in R
#  It is assumed that regfun$coef contains the  intercept and slope
#  estimates produced by regfun.  The regression methods written for
#  this  book, plus regression functions in R, have this property.
#
#  x is assumed to be a matrix containing values of the predictors.
#
xmat<-matrix(x[isub,],nrow(x),ncol(x))
temp<-qplotreg(xmat,y[isub],qval=qval,plotit=FALSE)
regboot<-temp[1,2]-temp[2,2]
regboot
}

qhomtsub2<-function(isub,x,y,qval){
#
#  Perform quantile regression using x[isub] to predict y[isub]
#  isub is a vector of length n,
#  a bootstrap sample from the sequence of integers
#  1, 2, 3, ..., n
#
#  This function is used by other functions when computing
#  bootstrap estimates.
#
#  regfun is some regression method already stored in R
#  It is assumed that regfun$coef contains the  intercept and slope
#  estimates produced by regfun.  The regression methods written for
#  this  book, plus regression functions in R, have this property.
#
#  x is assumed to be a matrix containing values of the predictors.
#
xmat<-matrix(x[isub,],nrow(x),ncol(x))
pp<-ncol(x)+1
temp<-rqfit(xmat,y[isub],qval)$coef[2:pp]
temp
}




# ----------------------------------------------------------------------------

# qhomtv2

# ----------------------------------------------------------------------------

qhomtv2<-function(x,y,nboot=100,alpha=.05,qval=c(.2,.8),SEED=TRUE,WARN=FALSE){
#
#   Test hypothesis of homoscedasticiy  by
#   computing a confidence interval for beta_1-beta_2, the
#   difference between the slopes of the qval[2] and qval[1]
#   regression slopes, where qval[1] and qval[2] are
#   the quantile regression slopes
#   estimated via the Koenker-Bassett method.
#   So by default, use the .8 quantile slope minus the
#   the .2 quantile slope.
#
print("Note: adjusted confidence intervals are used;")
print("they can differ from p-values")
print("FWE is not controlled")
if(length(qval)!=2)stop("Argument qval should have 2 values")
x<-as.matrix(x)
p<-ncol(x)
xy<-elimna(cbind(x,y))
x<-xy[,1:p]
x<-as.matrix(x)
pp<-p+1
y<-xy[,pp]
if(!WARN)options(warn=-1)
if(SEED)set.seed(2) # set seed of random number generator so that
#             results can be duplicated.
print("Taking bootstrap samples. Please wait.")
data<-matrix(sample(length(y),size=length(y)*nboot,replace=TRUE),nrow=nboot)
bvec1<-apply(data,1,qhomtsub2,x,y,qval[1]) # A p by nboot matrix
bvec2<-apply(data,1,qhomtsub2,x,y,qval[2])
bvec1<-as.matrix(bvec1)
bvec2<-as.matrix(bvec2)
if(p==1){
bvec1<-t(bvec1)
bvec2<-t(bvec2)
}
se<-NA
for(j in 1:p)se[j]<-sqrt(var(bvec1[j,]-bvec2[j,]))
temp1<-rqfit(x,y,qval[1])$coef[2:pp]
temp2<-rqfit(x,y,qval[2])$coef[2:pp]
crit<-qnorm(1-alpha/2)
crit.ad<-NA
dif<-temp2-temp1
regci<-NA
regci1<-dif-crit*se
regci2<-dif+crit*se
sig.level<-2*(1-pnorm(abs(dif)/se))
regci.ad<-NA
if(alpha==.05 && qval[1]==.2 && qval[2]==.8)
crit.ad<-qnorm(0-.09/sqrt(length(y))+.975)
output<-matrix(NA,ncol=7,nrow=p)
dimnames(output)<-list(NULL,c("Est 1","Est 2","Dif","SE",
"ci.lower","ci.upper","p.value"))
output[,1]<-temp1
output[,2]<-temp2
output[,3]<-dif
output[,4]<-se
output[,5]<-regci1
output[,6]<-regci2
output[,7]<-sig.level
ci.ad<-c(dif-crit.ad*se,dif+crit.ad*se)
if(!WARN)options(warn=0)
output
}




# ----------------------------------------------------------------------------

# qint

# ----------------------------------------------------------------------------

qint<-function(x,q=.5,alpha=.05,pr=FALSE){
#
# Compute a 1-alpha confidence interval for the qth quantile
# The function returns the exact probability coverage.
#
if(pr){
if(sum(duplicated(x)>0))print("Duplicate values detected; use hdpb")
}
n<-length(x)
ii<-floor(q*n+.5)
jj<-ii+1
if(ii<=0)stop("Cannot compute a confidence interval for this q")
if(jj>n)stop("Cannot compute a confidence interval for this q")
jjm<-jj-1
iim<-ii-1
cicov<-pbinom(jjm,n,q)-pbinom(iim,n,q)
while(cicov<1-alpha){
iim<-max(iim-1,0)
jjm<-min(jjm+1,n)
if(iim==0 && jjm==n)break
cicov<-pbinom(jjm,n,q)-pbinom(iim,n,q)
}
xsort<-sort(x)
low<-xsort[iim+1]
hi<-xsort[jjm+1]
if(cicov<1-alpha){
if(pr)print("Warning: Desired probability coverage could not be achieved")
}
list(ci.low=low,ci.up=hi,ci.coverage=cicov)
}


qloc.dif<-function(x,y,est=tmean,...){
#
#  Compute a measure of	location for each group.
#  Using the data in first group, determine what quantiles
#  these measure of location correspond	to.
#  The difference is used as a measure of effect size.
#
m1=est(x)
m2=est(y)
q1=mean(x<=m1)
q2=mean(x<=m2)
delta=q1-q2
delta
}

Qmcp<-function(x,q=.5,con=0,SEED=TRUE,THD=FALSE,nboot=NA,alpha=.05,HOCH=FALSE){
#
#  Multiple comparisons among independent groups
#  based on the quantile indicated by the argument
#  q
#
# THD=TRUE would use the trimmed Harrell--Davis estimator
# The default is the Harrell--Davis estimator
#   Familywise error is controlled with the Hochberg's method

#
# The Harrell--Davis estimator is used in order to deal with tied values
#
est=hd
if(THD)est=thd
res=linconpb(x,est=est,q=q,nboot=nboot,SEED=SEED,con=con,method='hoch')
res
}


qmjci<-function(x,q=.5,alpha=.05,op=1,pr=TRUE){
#
#   Compute a 1-alpha confidence for qth quantile using the
#   Maritz-Jarrett estimate of the standard error.
#
#   The default quantile is .5.
#   The default value for alpha is .05.
#
x=elimna(x)
if(pr){
if(sum(duplicated(x)>0))print("Duplicate values detected; use hdpb")
}
if(q <= 0 || q>=1)stop("q must be between 0 and 1")
y<-sort(x)
m<-floor(q*length(x)+.5)
crit<-qnorm(1-alpha/2)
qmjci<-vector(mode="numeric",2)
se<-NA
if(op==1)se<-mjse(x)
if(op==2){
if(q!=.5)stop("op=2 works only with q=.5")
se<-msmedse(x)
}
if(op==3)se<-qse(x,q)
if(is.na(se))stop("Something is wrong, op should be 1, 2 or 3")
qmjci[1]<-y[m]-crit*se
qmjci[2]<-y[m]+crit*se
qmjci
}


qrchk<-function(x,y,qval=.5,q=NULL,nboot=1000,com.pval=FALSE,SEED=TRUE,alpha=.05,pr=TRUE,
xout=FALSE,outfun=out,chk.table=FALSE,MC=FALSE,...){
#
# Test of a linear fit based on quantile regression
# The method stems from He and Zhu 2003, JASA, 98, 1013-1022.
# Here, resampling is avoided using approximate critical values if
# com.pval=F
#
#  To get a p-value, via simulations, set  com.pval=T
#  nboot is number of simulations used to determine p-value.
#  Execution time can be quite high
#
#  This function quickly determines .1, .05, .025 and .01
#  critical values for n<=400 and p<=6 (p= number of predictors)
#  and when dealing with the .5 quantile.
#  Otherwise, critical values are determined via simulations, which
#  can have high execution time.
#
if(!is.null(q))qval=q
if(pr){
if(!com.pval)print('To get a p-value, set com.pval=T and use MC=T if a multicore processor is available')
print('Reject if test statistic is >= critical value')
}
x<-as.matrix(x)
p<-ncol(x)
pp1<-p+1
yx<-elimna(cbind(y,x)) #Eliminate missing values.
y<-yx[,1]
x<-yx[,2:pp1]
store.it=F
x<-as.matrix(x)
p.val<-NULL
crit.val<-NULL
x<-as.matrix(x)
if(xout){
flag<-outfun(x,...)$keep
x<-x[flag,]
y<-y[flag]
}
# shift the  marginal x values so that the test statistic is
# invariant under changes in location
n<-length(y)
x=standm(x)
if(p<=6){
if(qval==.5){
aval<-c(.1,.05,.025,.01)
aokay<-duplicated(c(alpha,aval))
aokay<-sum(aokay)
if(aokay>0){
crit10<-matrix(c(.0254773,.008372,.00463254,.0023586,.000959315,.00042248,
.00020069,
.039728,.012163,.0069332,.0036521,.001571,.0006882, .0003621,
.055215,.0173357,.009427,.004581,.0021378,.00093787,.00045287,
.075832,.0228556,.0118571,.005924,.00252957,.0011593,.00056706,
.103135,.0298896,.0151193,.0073057,.00305456,.0014430,.000690435,
.12977,.03891,.018989,.009053,.0036326,.001617,.000781457),ncol=6,nrow=7)
crit05<-matrix(c(.031494,.010257,.00626,.00303523,.0012993,.000562247,
.00025972,
.046296,.015066,.00885556,.0045485,.0110904,.00086946,.000452978,
.063368,.0207096546,.010699,.005341,.0025426,.0011305,.000539873,
.085461,.027256,.014067,.0071169,.002954,.0013671,.000660338,
.11055,.03523,.017511,.0084263,.0036533,.0016338,.00081289,
.13692,.043843,.0222425,.0102265,.004283,.0019,.000907241),ncol=6,nrow=7)
crit025<-matrix(c(.0361936,.012518,.007296,.0036084,.00172436,.000725365,
.000327776,
.05315,.017593,.0102389,.0055043,.00227459,.0010062,.000523526,
.07214,.023944,.013689,.0060686,.0028378,.00136379,.000635645,
.093578,.0293223,.0156754,.0086059,.0035195,.001694,.00074467,
.118414,.03885,.0201468,.0094298,.0040263,.00182437,.000916557,
.14271,.047745,.0253974,.011385,.004725,.00207588,.0010191),ncol=6,nrow=7)
crit01<-matrix(c(.0414762,.0146553,.0098428,.0045274,.00219345,.00096244,
.000443827,
.058666,.020007,.01129658,.0063092,.002796,.0011364,.000628054,
.079446,.0267958,.015428,.0071267,.0034163,.0015876,.000734865,
.102736,.0357572,.017786,.0093682,.0042367,.0019717,.000868506,
.125356,.041411,.0234916,.0106895,.0047028,.0020759,.00101052,
.14837,.053246,.027759,.012723,.00528,.002437,.00116065),ncol=6,nrow=7)
if(alpha==.1)critit<-crit10
if(alpha==.05)critit<-crit05
if(alpha==.025)critit<-crit025
if(alpha==.01)critit<-crit01
nvec<-c(10,20,30,50,100,200,400)
nval<-duplicated(c(n,nvec))
nval<-nval[2:7]
if(sum(nval)>0)crit.val<-critit[nval,p]
if(is.null(crit.val)){
if(n<=400){
loc<-rank(c(n,nvec))
xx<-c(1/nvec[loc[1]-1]^1.5,1/nvec[loc[1]]^1.5)
yy<-c(critit[loc[1]-1,p],critit[loc[1],p])
}
icoef<-lsfit(xx,yy)$coef
crit.val<-icoef[1]+icoef[2]/n^1.5
}}}}
if(is.null(crit.val)){
# no critical value found
if(!com.pval){
print('Critical values not available, will set com.pval=T')
print('and compute a p-value')
com.pval<-T
}}
gdot<-cbind(rep(1,n),x)
gdot<-ortho(gdot)
x<-gdot[,2:pp1]
x<-as.matrix(x)
temp<-rqfit(x,y,qval=qval,res=TRUE)
coef<-temp$coef
psi<-NA
psi<-ifelse(temp$residuals>0,qval,qval-1)
rnmat<-matrix(0,nrow=n,ncol=pp1)
ran.mat<-apply(x,2,rank)
flagvec<-apply(ran.mat,1,max)
for(j in 1:n){
flag<-ifelse(flagvec<=flagvec[j],TRUE,FALSE)
flag<-as.numeric(flag)
rnmat[j,]<-apply(flag*psi*gdot,2,sum)
}
rnmat<-rnmat/sqrt(n)
temp<-matrix(0,pp1,pp1)
for(i in 1:n)temp<-temp+rnmat[i,]%*%t(rnmat[i,])
temp<-temp/n
test<-max(eigen(temp)$values)
if(com.pval){
if(SEED)set.seed(2)
xy=list()
p1=p+1
for(i in 1:nboot)xy[[i]]=rmul(n,p=p1)
if(MC)temp3=mclapply(xy,qrchkv2.sub2,qval=qval,mc.preschedule=TRUE)
if(!MC)temp3=lapply(xy,qrchkv2.sub2,qval=qval)
rem=matl(temp3)
p.val=sum(test>=rem)
rem<-sort(rem)
p.val<-1-p.val/nboot
ic<-round((1-alpha)*nboot)
crit.val<-rem[ic]
}
de='Fail to reject'
if( test>=crit.val)de='Reject'
list(test.stat=test,crit.value=crit.val,p.value=p.val,Decision=de)
}




# ----------------------------------------------------------------------------

# qrchkv2

# ----------------------------------------------------------------------------

qrchkv2<-function(x,y,qval=.5,...){
#
# Test of a linear fit based on quantile regression
# The method stems from He and Zhu 2003, JASA, 98, 1013-1022.
# Here, resampling is avoided using approximate critical values if
# com.pval=F
#
#  To get a p-value, via simulations, set  com.pval=T
#  nboot is number of simulations used to determine p-value.
#  Execution time can be quite high
#
#  This function quickly determines .1, .05, .025 and .01
#  critical values for n<=400 and p<=6 (p= number of predictors)
#  and when dealing with the .5 quantile.
#  Otherwise, critical values are determined via simulations, which
#  can have high execution time.
#
#  But, once critical values are determined for a given n, p and
#  quantile qval, the function will remember these values and use them
# in the future. They are stored in a file called qrchk.crit
# Currently, however, when you source the Rallfun files, these values
#  will be lost. You might save the file qrchk.crit in another file,
# source Rallfun, then copy the save file back to qrchk.crit
#
x=as.matrix(x)
p<-ncol(x)
pp1<-p+1
yx<-elimna(cbind(y,x)) #Eliminate missing values.
y<-yx[,1]
x<-yx[,2:pp1]
store.it=F
x<-as.matrix(x)
p.val<-NULL
crit.val<-NULL
x<-as.matrix(x)
# shift the  marginal x values so that the test statistic is
# invariant under changes in location
n<-length(y)
x=standm(x)
gdot<-cbind(rep(1,n),x)
gdot<-ortho(gdot)
x<-gdot[,2:pp1]
x<-as.matrix(x)
temp<-rqfit(x,y,qval=qval,res=TRUE)
coef<-temp$coef
psi<-NA
psi<-ifelse(temp$residuals>0,qval,qval-1)
rnmat<-matrix(0,nrow=n,ncol=pp1)
ran.mat<-apply(x,2,rank)
flagvec<-apply(ran.mat,1,max)
for(j in 1:n){
flag<-ifelse(flagvec<=flagvec[j],TRUE,FALSE)
flag<-as.numeric(flag)
rnmat[j,]<-apply(flag*psi*gdot,2,sum)
}
rnmat<-rnmat/sqrt(n)
temp<-matrix(0,pp1,pp1)
for(i in 1:n)temp<-temp+rnmat[i,]%*%t(rnmat[i,])
temp<-temp/n
test<-max(eigen(temp)$values)
test
}




# ----------------------------------------------------------------------------

# qrchkv2.sub2

# ----------------------------------------------------------------------------

qrchkv2.sub2<-function(xy,qval){
p1=ncol(xy)
p=p1-1
val=qrchkv2(xy[,1:p],xy[,p1],qval=qval)
val
}

regciCV.sub<-function(xy,regfun,null.value=0,xout=FALSE,...){
 pv=regYci(xy[,1],xy[,2],SEED=FALSE,regfun=regfun,null.value=null.value,xout=xout,...)[,5]
 min(pv)
}

Qregci<-function(x,y,nboot=100,alpha=.05,
qval=.5,q=NULL,SEED=TRUE,pr=TRUE,xout=FALSE,outfun=outpro,...){
#
#  Test the hypothesis that the quantile regression slopes are zero.
#  Can use the .5 quantile regression line only,
#  the .2 and .8 quantile regression lines, or
#  the .2, .5 and .8 quantile regression lines.
#  In the latter two cases, FWE is controlled for alpha=.1, .05, .025 and .01.
#
if(!is.null(q))qval=q
xx<-elimna(cbind(x,y))
np<-ncol(xx)
p<-np-1
y<-xx[,np]
x<-xx[,1:p]
x<-as.matrix(x)
if(xout){
x<-as.matrix(x)
flag<-outfun(x,plotit=FALSE,...)$keep
x<-x[flag,]
y<-y[flag]
}
x<-as.matrix(x)
n<-length(y)
if(SEED)set.seed(2) # set seed of random number generator so that
#             results can be duplicated.
if(pr)print("Taking bootstrap samples. Please wait.")
data<-matrix(sample(length(y),size=length(y)*nboot,replace=TRUE),nrow=nboot)
# determine critical value.
crit<-NA
if(alpha==.1)crit<-1.645-1.19/sqrt(n)
if(alpha==.05)crit<-1.96-1.37/sqrt(n)
if(alpha==.025)crit<-2.24-1.18/sqrt(n)
if(alpha==.01)crit<-2.58-1.69/sqrt(n)
crit.fwe<-crit
if(length(qval)==2 || p==2){
if(alpha==.1)crit.fwe<-1.98-1.13/sqrt(n)
if(alpha==.05)crit.fwe<-2.37-1.56/sqrt(n)
if(alpha==.025)crit.fwe<-2.60-1.04/sqrt(n)
if(alpha==.01)crit.fwe<-3.02-1.35/sqrt(n)
}
if(length(qval)==3 || p==3){
if(alpha==.1)crit.fwe<-2.145-1.31/sqrt(n)
if(alpha==.05)crit.fwe<-2.49-1.49/sqrt(n)
if(alpha==.025)crit.fwe<-2.86-1.52/sqrt(n)
if(alpha==.01)crit.fwe<-3.42-1.85/sqrt(n)
}
if(is.na(crit.fwe)){
print("Could not determine a critical value")
print("Only alpha=.1, .05, .025 and .01 are allowed")
}
if(p==1){
bvec<-apply(data,1,Qindbt.sub,x,y,q=qval)
estsub<-NA
for(i in 1:length(qval)){
estsub[i]<-Qreg(x,y,q=qval[i])$coef[2]
}
if(is.matrix(bvec))se.val<-sqrt(apply(bvec,1,FUN=var))
if(!is.matrix(bvec))se.val<-sqrt(var(bvec))
test<-abs(estsub)/se.val
ci.mat<-matrix(nrow=length(qval),ncol=3)
dimnames(ci.mat)<-list(NULL,c("Quantile","ci.lower","ci.upper"))
ci.mat[,1]<-qval
ci.mat[,2]<-estsub-crit*se.val
ci.mat[,3]<-estsub+crit*se.val
}
if(p>1){
if(length(qval)>1){
print("With p>1 predictors,only the first qval value is used")
}
bvec<-apply(data,1,regboot,x,y,regfun=Qreg,qval=qval[1])
se.val<-sqrt(apply(bvec,1,FUN=var))
estsub<-Qreg(x,y,q=qval[1])$coef
test<-abs(estsub)/se.val
ci.mat<-matrix(nrow=np,ncol=3)
dimnames(ci.mat)<-list(NULL,c("Predictor","ci.lower","ci.upper"))
ci.mat[,1]<-c(0:p)
ci.mat[,2]<-estsub-crit*se.val
ci.mat[,3]<-estsub+crit*se.val
}
list(n=length(y),test=test,se.val=se.val,crit.val=crit,crit.fwe=crit.fwe,est.values=estsub,ci=ci.mat)
}




Qindbt.sub<-function(isub,x,y,qval){
#
#  Perform regression using x[isub] to predict y[isub]
#  isub is a vector of length n,
#  a bootstrap sample from the sequence of integers
#  1, 2, 3, ..., n
#
#  This function is used by other functions when computing
#  bootstrap estimates.
#
#
#  x is assumed to be a matrix containing values of the predictors.
#
xmat<-matrix(x[isub,],nrow(x),ncol(x))
regboot<-NA
for(i in 1:length(qval)){
regboot[i]<-Qreg(xmat,y[isub],q=qval[i])$coef[2]
}
regboot
}




# ----------------------------------------------------------------------------

# Qreghat

# ----------------------------------------------------------------------------

corblp.sub<-function(xy,regfun=tsreg,varfun=pbvar){
X=xy[,1]
Y=xy[,2]
rest=corblp(X,Y,regfun=regfun,varfun=varfun)$cor
rest
}


qregsm<-function(x, y,est=hd,qval=.5,sm=TRUE,plotit=TRUE,pyhat=FALSE,fr=0.8,nboot=40,xlab="X",
ylab="Y",xout=FALSE,outfun=outpro,STAND=TRUE,...)
{
#
# Do a smooth of x versus the quantiles of y
#
# qval indicates quantiles of interest.
# Example: qval=c(.2,.8) will create two smooths, one for the
# .2 quantile and the other for the .8 quantile.
#
# est can be any quantile estimator having the argument qval, indicating
# the quantile to be used.
#
# est = hd uses Harrel Davis estimator,
# est = qest uses a single order statistic.
#
# sm=T, bagging will be used.
# pyhat=T returns the estimates
#
chk=FALSE
if(identical(est,hd))chk=TRUE
#if(identical(est,qest))chk=TRUE
if(!chk)stop('For current version, argument est must be hd')
x<-as.matrix(x)
X<-cbind(x,y)
X<-elimna(X)
np<-ncol(X)
p<-np-1
x<-X[,1:p]
x<-as.matrix(x)
y<-X[,np]
if(xout){
m<-cbind(x,y)
flag<-outfun(x,plotit=FALSE)$keep
m<-m[flag,]
x<-m[,1:p]
y<-m[,np]
}
vals<-matrix(NA,ncol=length(y),nrow=length(qval))
for(i in 1:length(qval)){
if(sm)vals[i,]<-rplotsm(x,y,est=est,q=qval[i],pyhat=TRUE,plotit=FALSE,fr=fr,nboot=nboot,
na.rm=FALSE,STAND=STAND)$yhat
#if(!sm)vals[i,]<-rungen(x,y,est=est,q=qval[i],pyhat=TRUE,plotit=FALSE,fr=fr,na.rm=FALSE)$output
if(!sm)vals[i,]<-rplot(x,y,est=est,q=qval[i],pyhat=TRUE,plotit=FALSE,fr=fr,na.rm=FALSE)$yhat
}
if(p==1){
if(plotit){
plot(x,y,xlab=xlab,ylab=ylab)
for(i in 1:length(qval)){
sx <- sort(x)
xorder <- order(x)
sysm <- vals[i,]
#lines(sx, sysm)
lines(sx, sysm[xorder])
}}}
output <- "Done"
if(pyhat)output <- vals
output
}

QS.inter.pbci<-function(x,locfun=median,alpha=.05,nboot=1000,SEED=TRUE,SW=FALSE){
#
# For a 2-by-2 design, characterize an interaction
#  in terms of a quantile shift measure of effect size
#
#  SW=TRUE, switches rows and column
#
if(SEED)set.seed(2)
if(is.matrix(x))x=listm(x)
if(length(x)!=4)stop('There should be exactly four groups')
for(j in 1:4)x[[j]]=elimna(x[[j]])
if(SW)x=x[c(1,3,2,4)]
v=list()
dif=NA
a1=NA
a2=NA
for(i in 1:nboot){
for(j in 1:4)v[[j]]=sample(x[[j]],replace=TRUE)
a1[i]=shiftQS(v[[1]],v[[2]],locfun=locfun)$Q.Effect
a2[i]=shiftQS(v[[3]],v[[4]],locfun=locfun)$Q.Effect
}
dif=a1-a2
dif=sort(dif)
ilow<-round((alpha/2) * nboot)
ihi<-nboot - ilow
ilow<-ilow+1
ci=dif[ilow]
ci[2]=dif[ihi]
pv=mean(dif<0)+.5*mean(dif==0)
pv=2*min(pv,1-pv)
a1=shiftQS(x[[1]],x[[2]],locfun=locfun)$Q.Effect
a2=shiftQS(x[[3]],x[[4]],locfun=locfun)$Q.Effect
Dif=a1-a2
list(Est.1=a1, Est.2=a2,Dif=Dif,ci=ci,p.value=pv)
}




# ----------------------------------------------------------------------------

# QS1way

# ----------------------------------------------------------------------------

QS1way<-function(x,locfun=median,alpha=0.05,SEED=TRUE,nboot=500,CI=TRUE){
#
#  Estimate quantile shift function when comparing all
#  pairs of groups in a one-way (independent) groups design
#
#  CI=TRUE: confidence intervals for the measure of effect size are computed.
#
if(is.matrix(x) || is.data.frame(x))x<-listm(x)
if(!is.list(x))stop('Data must be stored in a matrix or in list mode.')
J=length(x)
Jall=(J^2-J)/2
con1=con1way(J)
output=matrix(NA,nrow=Jall,ncol=5)
dimnames(output)=list(NULL,c('Group','Group','Effect.Size','low.ci','up.ci'))
ic=0
for(j in 1:J){
for(k in 1:J){
if(j<k){
ic=ic+1
output[ic,1]=j
output[ic,2]=k
if(!CI)output[ic,3]=shiftes(x[[j]],x[[k]])$Q.effect
if(CI){
ci=shiftPBci(x[[j]],x[[k]],alpha=alpha,locfun=locfun,SEED=SEED)
output[ic,3]=ci$Q.Effect
output[ic,4]=ci$ci[1]
output[ic,5]=ci$ci[2]
}}}}
output
}

bcmpQSci=QS1way




# ----------------------------------------------------------------------------

# QS2by2

# ----------------------------------------------------------------------------

QS2by2<-function(x,nreps=200,locfun=median,SEED=TRUE,FACTOR.A=TRUE){
#
# Quantile shift-type measure of effect size for main effect  in a 2-by-2 design
# Comparing level 1 of Factor A to level 2 of Factor A.
#
g=c(1:4)
if(!FACTOR.A){  # So doing factor B
g=c(1,3,2,4)
}
if(is.matrix(x) || is.data.frame(x))x=listm(x)
nv=as.vector(matl(lapply(x,FUN='length')))
nmin=min(nv)
M=matrix(NA,nrow=nmin,ncol=4)
ef=NA
nt=prod(n)
if(nt>10^3){
if(SEED)set.seed(2)
for(i in 1:nreps){
for(j in 1:4)M[,j]=sample(x[[j]],nmin)
L1=outer(M[,g[1]],M[,g[2]],FUN='+')
L2=outer(M[,g[3]],M[,g[4]],FUN='+')
ef[i]=shiftes(L1,L2,locfun=locfun)$Q.Effect
}}
else{
L1=outer(x[[g[1]]],x[[g[2]]],FUN='+')
L2=outer(x[[g[3]]],x[[g[4]]],FUN='+')
ef=shiftes(L1,L2,locfun=locfun)$Q.Effect
}
es=mean(ef)
list(Q.Effect=es)
}




# ----------------------------------------------------------------------------

# QS2ci

# ----------------------------------------------------------------------------

QS2ci<-function(x,y,locfun=median,alpha=.05,SEED=TRUE,...){
#
# confidence interval for the quantile shift measure of effect size.
#
x=elimna(x)
y=elimna(y)
n1=length(x)
n2=length(y)
L=outer(x,y,FUN='-')
L=as.vector(L)
est=locfun(L,...)
CI=loc2dif.ci(x,y,alpha=alpha,SEED=SEED)$conf.int
ci=mean(L-est<=CI[1])
ci[2]=mean(L-est<=CI[2])
est=shiftes(x,y)$Q.Effect
list(n1=n1,n2=n2,Q.effect=est,ci=ci)
}

hdpv=function(val,dat,obs){z=abs(hd(dat,val)-obs)
z
}

qse<-function(x,q=.5,op=3){
#
#  Compute the standard error of qth sample quantile estimator
#  based on the single order statistic, x sub ([qn+.5]) (See Ch 3)
#
#  Store the data in vector
#  x, and the desired quantile in q
#  The default value for q is .5
#
# op=1 Use Rosenblatt's shifted histogram
# op=2 Use expected frequency curve
# op=3 Use adaptive kernel density estimator
#
y <- sort(x)
n <- length(x)
iq <- floor(q * n + 0.5)
qest <- y[iq]
fhat<-NA
if(op==1)fhat<-kerden(x,q)
if(op==2)fhat<-rdplot(x,pts=qest,pyhat=TRUE,plotit=FALSE)
if(op==3)fhat<-akerd(x,pts=qest,pyhat=TRUE,plotit=FALSE)
if(is.na(fhat[1]))stop("Something wrong, op should be 1 or 2 or 3")
qse<-1/(2*sqrt(length(x))*fhat)
qse
}

qshift<-function(x,y,locfun=tmean,...){
#
# Quantile shift using x as the	reference group
#
m1=locfun(x,...)
m2=locfun(y,...)
q1=mean(m1<=x)
q2=mean(m2<=x)
del=q1-q2
del
}

qshiftpb<-function(x,y,locfun=tmean,alpha=.05,nboot=2000,SEED=TRUE,pr=FALSE,...){
#
# quantile shift for comparing a control group to an experimental group
# x is assumed to be the control group,
# null value=0
#
x<-x[!is.na(x)] # Remove any missing values in x
y<-y[!is.na(y)] # Remove any missing values in y
est=qshift(x,y,locfun=locfun,...)
if(SEED)set.seed(2) # set seed of random number generator so that
#             results can be duplicated.
datax<-matrix(sample(x,size=length(x)*nboot,replace=TRUE),nrow=nboot)
datay<-matrix(sample(y,size=length(y)*nboot,replace=TRUE),nrow=nboot)
v=NA
for(i in 1:nboot)v[i]=qshift(datax[i,],datay[i,],locfun=locfun,...)
sv=sort(v)
low<-round((alpha/2)*nboot)+1
up<-nboot-low
temp<-sum(v<0)/nboot+sum(v==0)/(2*nboot)
sig.level<-2*(min(temp,1-temp))
list(n1=length(x),n2=length(y),est=est,ci.low=sv[low],ci.up=sv[up],p.value=sig.level)
}




# ----------------------------------------------------------------------------

# QSinter.mcp

# ----------------------------------------------------------------------------

QSinter.mcp<-function(J,K,x,alpha=.05,nboot=999,SEED=TRUE,SW=FALSE){
#
#  Interactions based on  measure of effect size
#
if(is.matrix(x)  || is.data.frame(x))x=listm(x)
con=con2way(J,K)$conAB
if(SW){
JK=J*K
M=matrix(c(1:JK),nrow=J,byrow=TRUE)
M=as.vector(M)
x=x[M]
con=con2way(K,J)$conAB
}
num=ncol(con)
CON=matrix(NA,nrow=num,ncol=8)
dimnames(CON)=list(NULL,c('Con.num','Est.1','Est.2','Dif','ci.low','ci.up','p.value','p.adjusted'))
for(j in 1:ncol(con)){
id=which(con[,j]!=0)
dat=x[id]
temp=pool.a.list(QS.inter.pbci(dat,alpha=alpha,nboot=nboot,SEED=SEED))
CON[j,1]=j
CON[j,2:7]=temp
}
CON[,8]=p.adjust(CON[,7],method='hoch')
list(CON=CON,con=con)
}




# ----------------------------------------------------------------------------

# qsm

# ----------------------------------------------------------------------------

qsm<-function(x,y,qval=c(.2,.5,.8),fr=.8,plotit=TRUE,scat=TRUE,pyhat=FALSE,eout=FALSE,xout=FALSE,outfun=out,op=TRUE,LP=TRUE,tr=FALSE,
xlab='X',ylab='Y',pch='.'){
#
# running  interval smoother for the quantiles stored in
# qval
#
# fr controls amount of smoothing
# op=T, use Harrell-Davis estimator
# op=F, use single order statistic
#
#  LP=TRUE: The initial smooth is smoothed again using LOESS
#
plotit<-as.logical(plotit)
scat<-as.logical(scat)
m<-cbind(x,y)
if(ncol(m)!=2)stop("Must have exactly one predictor. For more than one, use qhdsm.")
m<-elimna(m)
x<-m[,1]
y<-m[,2]
if(eout && xout)stop("Not allowed to have eout=xout=T")
if(eout){
flag<-outfun(m,plotit=FALSE)$keep
m<-m[flag,]
}
if(xout){
flag<-outfun(x)$keep
m<-m[flag,]
}
x<-m[,1]
y<-m[,2]
rmd<-c(1:length(x))
if(pyhat)outval<-matrix(NA,ncol=length(qval),nrow=length(x))
if(scat)plot(x,y,xlab=xlab,ylab=ylab,pch=pch)
if(!scat)plot(x,y,type="n",xlab=xlab,ylab=ylab)
for(it in 1:length(qval)){
if(!op)for(i in 1:length(x))rmd[i]<-qest(y[near(x,x[i],fr)],q=qval[it])
if(op)for(i in 1:length(x))rmd[i]<-hd(y[near(x,x[i],fr)],q=qval[it],tr=tr)
if(pyhat)outval[,it]<-rmd
points(x,rmd,type="n")
sx<-sort(x)
xorder<-order(x)
sysm<-rmd[xorder]
if(LP)sysm=lplot(sx,sysm,pyhat=TRUE,plotit=FALSE,pr=FALSE)$yhat.values
lines(sx,sysm)
}
if(pyhat)output<-outval
if(!pyhat)output<-"Done"
list(output=output)
}
qsmcobs<-function(x,y,qval=.5,xlab="X",ylab="Y",FIT=TRUE,pc=".",plotit=TRUE,
xout=FALSE,outfun=out,q=NULL,lambda=0,...){
#
# Plots smooths of quantile regression lines using R package cobs
#
# qval is the quantile
#  qsmcobs(x,y,qval=c(.2,.5,.8)) will plot three smooths corresponding to
#  the .2, .5 and .8 quantile regression lines.
#
# FIT=T, uses the values returned by predict
# FIT=F, determines predicted Y for each X and plots the results
library(cobs)
if(!is.null(q))qval=q
x=as.matrix(x)
if(xout){
flag<-outfun(x,...)$keep
x<-x[flag,]
y<-y[flag]
}
yhat=NULL
res=NULL
if(plotit)plot(x,y,xlab=xlab,ylab=ylab,pch=pc)
if(FIT){
for(j in 1:length(qval)){
if(plotit)lines(predict(cobs(x,y,tau=qval[j],lambda=lambda,print.mesg=FALSE,print.warn=FALSE)))
}}
if(!FIT){
for(j in 1:length(qval)){
temp=cobs(x,y,tau=qval[j],print.mesg=FALSE,print.warn=FALSE,lambda=lambda)
xord=order(x)
if(plotit)lines(x[xord],temp$fitted[xord])
}
if(length(qval)==1){
yhat=temp$fitted
#res=y-yhat
  # yhat is only for the unique x values. If x has,say,
#  three tied values = 6, then
#  yhat contains only one predicted value for x=6, not three yhat values
#  all equal to the predicted value at x=6
}
}
list(yhat=yhat)
}


QSmcp.ci<-function(x,locfun=median,alpha=0.05,SEED=TRUE,nboot=500,CI=TRUE){
#
#  Estimate quantile shift function when comparing all
#  pairs of groups in a one-way (independent) groups design
#
#  CI=TRUE: confidence intervals for the measure of effect size are computed.
#
if(is.matrix(x) || is.data.frame(x))x<-listm(x)
if(!is.list(x))stop('Data must be stored in a matrix or in list mode.')
J=length(x)
Jall=(J^2-J)/2
con1=con1way(J)
output=matrix(NA,nrow=Jall,ncol=5)
dimnames(output)=list(NULL,c('Group','Group','Effect.Size','low.ci','up.ci'))
ic=0
for(j in 1:J){
for(k in 1:J){
if(j<k){
ic=ic+1
output[ic,1]=j
output[ic,2]=k
if(!CI)output[ic,3]=shiftes(x[[j]],x[[k]])$effect.size
if(CI){
ci=shiftPBci(x[[j]],x[[k]],alpha=alpha,locfun=locfun,nboot=nboot,SEED=SEED)
output[ic,3]=ci$effect.size
output[ic,4]=ci$ci[1]
output[ic,5]=ci$ci[2]
}}}}
output
}




# ----------------------------------------------------------------------------

# qsplit

# ----------------------------------------------------------------------------

qsplit<-function(x,y,split.val=NULL){
#
# x assumed to be a matrix or data frame
#
#  IF split.val=NULL,
#
# split the data in x into 3 groups:
# those for which y <= lower quartile
# those between lower and upper quartile
# those >= upper quartile
#
# IF split.val CONTAINS TWO VALUES, SPLIT THE DATA ACCORDING TO
# THE VALUES SPECIFIED.
#
if(!is.data.frame(x))x=as.matrix(x)
if(is.null(split.val)){
v=idealf(y)
flag1=(y<=v$ql)
flag2=(y>=v$qu)
}
if(!is.null(split.val)){
flag1=(y<=split.val[1])
flag2=(y>=split.val[2])
}
flag3=as.logical(as.numeric(!flag1)*as.numeric(!flag2))
d1=x[flag1,]
d2=x[flag2,]
d3=x[flag3,]
list(lower=d1,middle=d3,upper=d2)
}




# ----------------------------------------------------------------------------

# quant

# ----------------------------------------------------------------------------

quant<-function(x,q=.5,names=TRUE,na.rm=TRUE,type=8){
#
# For convenience, follow style of other functions in Rallfun
# when using the R function quanitle. Also, use by default the estimator
# recommended by Hyndman and  Fan (1996).
#
a=quantile(x,probs=q,names=names,type=type,na.rm=na.rm)
a
}




# ----------------------------------------------------------------------------

# Quart

# ----------------------------------------------------------------------------

Quart<-function(x){
# compute the quartiles
 temp=summary(x)
 Qv=c(temp[2],temp[3],temp[5])
 Qv
 }




# ----------------------------------------------------------------------------

# r.gauss.pro

# ----------------------------------------------------------------------------

r.gauss.pro<-function(n,C,M,t){
#
# generate data from a Gaussian Process
#
# n is the sample size
# C is the covariance function
# for example:
# C <- function(x, y) 0.01 * exp(-10000 * (x - y)^2) # covariance function
# M is the mean function. For example
#M <- function(x) sin(x) # mean function
# t is the interval over which the mean is computed
#t <- seq(0, 1, by = 0.01) # will sample the GP at these points
#
k <- length(t)
m <- M(t)
S <- matrix(nrow = k, ncol = k)
for (i in 1:k) for (j in 1:k) S[i, j] = C(t[i], t[j])
z=matrix(NA,nrow=n,ncol=k)
#for(i in 1:n)
#z[i,] <- mvrnorm(1, m, S)
z=mvrnorm(n, m, S)
z
}

r1mcp<-function(x,alpha=.05,bhop=FALSE){
#
# Do all pairwise comparisons using a modification of
# the Brunner, Dette and Munk (1997) rank-based method.
# FWE is controlled using Rom's technique.
#
#  Setting bhop=T, FWE is controlled using the
#  Benjamini-Hochberg Method.
#
#  The data are assumed to be stored in x in list mode or in a matrix.
#
#   Missing values are automatically removed.
#
        if(is.matrix(x))x <- listm(x)
        if(!is.list(x))
                stop("Data must be stored in list mode or a matrix.")
J<-length(x)
        for(j in 1:J) {
                xx <- x[[j]]
                x[[j]] <- xx[!is.na(xx)] # Remove missing values
        }
#
CC<-(J^2-J)/2
# Determine critical values
ncon<-CC
if(!bhop){
if(alpha==.05){
dvec<-c(.05,.025,.0169,.0127,.0102,.00851,.0073,.00639,.00568,.00511)
if(ncon > 10){
avec<-.05/c(11:ncon)
dvec<-c(dvec,avec)
}}
if(alpha==.01){
dvec<-c(.01,.005,.00334,.00251,.00201,.00167,.00143,.00126,.00112,.00101)
if(ncon > 10){
avec<-.01/c(11:ncon)
dvec<-c(dvec,avec)
}}
if(alpha != .05 && alpha != .01)dvec<-alpha/c(1:ncon)
}
if(bhop)dvec<-(ncon-c(1:ncon)+1)*alpha/ncon
output<-matrix(0,CC,5)
dimnames(output)<-list(NULL,c("Level","Level","test.stat","p.value","p.crit"))
ic<-0
for(j in 1:J){
for(jj in 1:J){
if(j < jj){
ic<-ic+1
output[ic,1]<-j
output[ic,2]<-jj
temp<-bdm(x[c(j,jj)])
output[ic,3]<-temp$F
output[ic,4]<-temp$p.value
}}}
temp2<-order(0-output[,4])
output[temp2,5]<-dvec[1:length(temp2)]
list(output=output)
}




# ----------------------------------------------------------------------------

# r2mcp

# ----------------------------------------------------------------------------

r2mcp<-function(J,K,x,grp=NA,alpha=.05,bhop=FALSE){
#
# Do all pairwise comparisons of
# main effects for Factor A and B and all interactions
# using a rank-based method that tests for equal distributions.
#
#  The data are assumed to be stored in x in list mode or in a matrix.
#  If grp is unspecified, it is assumed x[[1]] contains the data
#  for the first level of both factors: level 1,1.
#  x[[2]] is assumed to contain the data for level 1 of the
#  first factor and level 2 of the second factor: level 1,2
#  x[[j+1]] is the data for level 2,1, etc.
#  If the data are in the wrong order, grp can be used to rearrange the
#  groups. For example, for a two by two design, grp<-c(2,4,3,1)
#  indicates that the second group corresponds to level 1,1;
#  group 4 corresponds to level 1,2; group 3 is level 2,1;
#  and group 1 is level 2,2.
#
#   Missing values are automatically removed.
#
        JK <- J * K
        if(is.matrix(x))
                x <- listm(x)
        if(!is.na(grp[1])) {
                yy <- x
                x<-list()
                for(j in 1:length(grp))
                        x[[j]] <- yy[[grp[j]]]
        }
        if(!is.list(x))
                stop("Data must be stored in list mode or a matrix.")
        for(j in 1:JK) {
                xx <- x[[j]]
                x[[j]] <- xx[!is.na(xx)] # Remove missing values
        }
        #
if(JK != length(x)){
print("Warning: The number of groups does not match")
print("the number of contrast coefficients.")
}
for(j in 1:JK){
temp<-x[[j]]
temp<-temp[!is.na(temp)] # Remove missing values.
x[[j]]<-temp
}
#
CC<-(J^2-J)/2
# Determine critical values
ncon<-CC*(K^2-K)/2
if(!bhop){
if(alpha==.05){
dvec<-c(.05,.025,.0169,.0127,.0102,.00851,.0073,.00639,.00568,.00511)
if(ncon > 10){
avec<-.05/c(11:ncon)
dvec<-c(dvec,avec)
}}
if(alpha==.01){
dvec<-c(.01,.005,.00334,.00251,.00201,.00167,.00143,.00126,.00112,.00101)
if(ncon > 10){
avec<-.01/c(11:ncon)
dvec<-c(dvec,avec)
}}
if(alpha != .05 && alpha != .01)dvec<-alpha/c(1:ncon)
}
if(bhop)dvec<-(ncon-c(1:ncon)+1)*alpha/ncon
Fac.A<-matrix(0,CC,5)
dimnames(Fac.A)<-list(NULL,c("Level","Level","test.stat","p.value","p.crit"))
mat<-matrix(c(1:JK),ncol=K,byrow=TRUE)
ic<-0
for(j in 1:J){
for(jj in 1:J){
if(j < jj){
ic<-ic+1
Fac.A[ic,1]<-j
Fac.A[ic,2]<-jj
temp<-bdm2way(2,K,x[c(mat[j,],mat[jj,])])
#Fac.A[ic,3]<-temp$outputA$F
#Fac.A[ic,4]<-temp$outputA$sig
Fac.A[ic,3]<-temp$A.F
Fac.A[ic,4]<-temp$p.valueA
}}}
temp2<-order(0-Fac.A[,4])
Fac.A[temp2,5]<-dvec[1:length(temp2)]
CCB<-(K^2-K)/2
ic<-0
Fac.B<-matrix(0,CCB,5)
dimnames(Fac.B)<-list(NULL,c("Level","Level","test.stat","p.value","p.crit"))
for(k in 1:K){
for(kk in 1:K){
if(k<kk){
ic<-ic+1
Fac.B[ic,1]<-k
Fac.B[ic,2]<-kk
mat1<-cbind(mat[,k],mat[,kk])
temp<-bdm2way(J,2,x[c(mat1[,1],mat1[,2])])
#Fac.B[ic,3]<-temp$outputB$F
#Fac.B[ic,4]<-temp$outputB$sig
Fac.B[ic,3]<-temp$B.F
Fac.B[ic,4]<-temp$p.valueB
}}}
temp2<-order(0-Fac.B[,4])
Fac.B[temp2,5]<-dvec[1:length(temp2)]
CCI<-CC*CCB
Fac.AB<-matrix(0,CCI,7)
dimnames(Fac.AB)<-list(NULL,c("Lev.A","Lev.A","Lev.B","Lev.B","test.stat",
"p.value","p.crit"))
ic<-0
for(j in 1:J){
for(jj in 1:J){
if(j < jj){
for(k in 1:K){
for(kk in 1:K){
if(k<kk){
ic<-ic+1
Fac.AB[ic,1]<-j
Fac.AB[ic,2]<-jj
Fac.AB[ic,3]<-k
Fac.AB[ic,4]<-kk
val<-c(mat[j,k],mat[j,kk],mat[jj,k],mat[jj,kk])
temp<-bdm2way(2,2,x[val])
Fac.AB[ic,5]<-temp$AB.F
Fac.AB[ic,6]<-temp$p.valueAB
}}}}}}
temp2<-order(0-Fac.AB[,6])
Fac.AB[temp2,7]<-dvec[1:length(temp2)]
list(Factor.A=Fac.A,Factor.B=Fac.B,Factor.AB=Fac.AB)
}


radians.2.degrees<-function(rad)rad*180/pi




# ----------------------------------------------------------------------------

# RANGE

# ----------------------------------------------------------------------------

RANGE<-function(x){
#
#  compute the range (max - min) using data in x
#
r=diff(range(x))
r
}




# ----------------------------------------------------------------------------

# ranki

# ----------------------------------------------------------------------------

ranki<-function(J,K,x,grp=c(1:p),alpha=.05,p=J*K){
#
#  Compute a confidence interval for all interaction terms
#  in J by K (two-way) anova using the modified Patel-Hoel method.
#
#  This method is not recommended if there are tied observations among the
#  pooled data.
#
#  All JK groups are independent.
#
#  The R variable x is assumed to contain the raw
#  data stored in list mode.
#  If grp is unspecified, it is assumed x[[1]] contains the data
#  for the first level of both factors: level 1,1.
#  x[[2]] is assumed to contain the data for level 1 of the
#  first factor and level 2 of the second factor: level 1,2
#  x[[j+1]] is the data for level 2,1, etc.
#  If the data are in wrong order, grp can be used to rearrange the
#  groups. For example, for a two by two design, grp<-c(2,4,3,1)
#  indicates that the second group corresponds to level 1,1;
#  group 4 corresponds to level 1,2; group 3 is level 2,1;
#  and group 1 is level 2,2.
#
#  It is assumed that the input variable x has length JK, the total number of
#  groups being tested. If not, a warning message is printed.
#
#  The estimated standard error is based on Sen's Jackknife as used by
#  Mee (1990).
#
if(!is.list(x))stop("Data are not stored in list mode")
if(p!=length(x)){
print("Warning: The number of groups in your data is not equal to JK")
}
jtk<-J*K
tl<-0
com<-x[[1]]
for(i in 1:jtk)tl<-tl+length(x[[i]])
for(i in 2:jtk)com<-c(com,x[[i]])
tiex<-sum(abs(c(1:tl)-sort(rank(com))))
if(tiex > 0)
print("Tied values detected. Interchanging columns might give different results. That is, comparing rows based on P(X<Y) is not necessarily the same as comparing rows based on P(X>Y)")
ck<-(K^2-K)/2
cj<-(J^2-J)/2
tc<-ck*cj
if(tc>28){
print("Warning: The number of contrasts exceeds 28.")
print("The critical value being used is based on 28 contrasts")
tc<-28
}
idmat<-matrix(NA,nrow=tc,ncol=8)
dimnames(idmat)<-list(NULL,c("row","row","col","col","ci.lower","ci.upper","estimate","test.stat"))
crit<-smmcrit(300,tc)
if(alpha != .05){
crit<-smmcrit01(300,tc)
if(alpha != .01){print("Warning: Only alpha = .05 and .01 are allowed,")
print("alpha = .01 is being assumed.")
}
}
phatsqse<-0
phat<-0
allit<-0
jcount<-0-K
it<-0
for(j in 1:J){
for(jj in 1:J){
if(j < jj){
for(k in 1:K){
for(kk in 1:K){
if(k < kk){
it<-it+1
idmat[it,1:4]<-c(j,jj,k,kk)
}}}}}
jcount<-jcount+K
for(k in 1:K){
for(kk in 1:K){
if(k < kk){
allit<-allit+1
xx<-x[[grp[k+jcount]]]
yy<-x[[grp[kk+jcount]]]
temp<-rankisub(xx,yy)
phat[allit]<-temp$phat
phatsqse[allit]<-temp$sqse
}}}}
#
# Compute the contrast matrix. Each row contains a 1, -1 and the rest 0
#  That is, all pairwise comparisons among K groups.
#
con<-matrix(0,cj,J)
id<-0
Jm<-J-1
for (j in 1:Jm){
jp<-j+1
for (k in jp:J){
id<-id+1
con[id,j]<-1
con[id,k]<-0-1
}}
IK<-diag(ck)
B<-kron(con,IK)
ntest<-ck*(J^2-J)/2
test<-0
civecl<-0
civecu<-0
for (itest in 1:ntest){
temp1<-sum(B[itest,]*phat)
idmat[itest,7]<-temp1
idmat[itest,8]<-temp1/sqrt(sum(B[itest,]^2*phatsqse))
idmat[itest,5]<-temp1-crit*sqrt(sum(B[itest,]^2*phatsqse))
idmat[itest,6]<-temp1+crit*sqrt(sum(B[itest,]^2*phatsqse))
}
nsig<-sum((abs(idmat[,8])>crit))
list(phat=phat,ci=idmat,crit=crit,nsig=nsig)
}


rankisub<-function(x,y){
#
#  compute phat and an estimate of its variance
#
x<-x[!is.na(x)]  # Remove missing values from x
y<-y[!is.na(y)]  # Remove missing values from y
u<-outer(x,y,FUN="<")
p1<-0
p2<-0
for (j in 1:length(y)){
temp<-outer(u[,j],u[,j])
p1<-p1+sum(temp)-sum(u[,j]*u[,j])
}
for (i in 1: length(x)){
temp<-outer(u[i,],u[i,])
p2<-p2+sum(temp)-sum(u[i,]*u[i,])
}
p<-sum(u)/(length(x)*length(y))
pad<-p
if(p==0)pad<-.5/(length(x)*length(y))
if(p==1)pad<-(1-.5)/(length(x)*length(y))
p1<-p1/(length(x)*length(y)*(length(x)-1))
p2<-p2/(length(x)*length(y)*(length(y)-1))
var<-pad*(1.-pad)*(((length(x)-1)*(p1-p^2)/(pad*(1-pad))+1)/(1-1/length(y))+
((length(y)-1)*(p2-p^2)/(pad*(1-pad))+1)/(1-1/length(x)))
var<-var/(length(x)*length(y))
list(phat=p,sqse=var)
}

rbbinom<-function(n,nbin,r,s){
#
# Generate n values from a beta-binomial,
# r and s are the parameters of the beta distribution.
# nbin is for the binomial distribution,
#  Example: nbin=10 means the sample space=c(0:10)
#
x<-NA
for(i in 1:n){
pval<-rbeta(1,r,s)
x[i]<-rbinom(1,nbin,pval)
}
x
}

rbeta.binomial=rbbinom

rbin.mul<-function(n,N,prob=.5,p=2){
#
# p multivariate binomial, correlations are .5
#
p1=p+1
np1=n*p1
z=matrix(rbinom(np1,size=N,prob=prob),ncol=p1)
for(j in 1:p)z[,j]=z[,j]+z[,p1]
z[,1:p]
}

RCmcp<-function(J,K,x,alpha=.05, est=tmean, PB=FALSE,SEED=TRUE,pr=TRUE,...){
#
# For each level of Factor A, do all pairwise comparisons
#  among the levels of B
#
#  Do the same of Factor B
#
if(pr){
if(!PB)print('To use a percentile bootstrap, set PB=TRUE')
}
if(is.data.frame(x))x=as.matrix(x)
if(is.matrix(x))x<-listm(x)
JK=J*K
imat=matrix(c(1:JK),nrow=J,ncol=K,byrow=TRUE)
A=list()
B=list()
if(!PB){
for(j in 1:J)A[[j]]=lincon(x[imat[j,]],tr=tr,alpha=alpha,pr=FALSE)
for(k in 1:K)B[[k]]=lincon(x[imat[,k]],tr=tr,alpha=alpha,pr=FALSE)
}
if(PB){
for(j in 1:J)A[[j]]=linconpb(x[imat[j,]],alpha=alpha,SEED=SEED,...)
for(k in 1:K)B[[k]]=linconpb(x[imat[,k]],alpha=alpha,SEED=SEED,...)
}
list(A=A,B=B)
}

Rcoefalpha<-function(x,cov.fun=wincov,pr=FALSE,...){
# Compute coefficient alpha plus a robust analog)
#
# x is assumed to be a matrix
# output:
# coefficient alpha plus robust version
#
#  NOTE: now use cov.fun=wincov by default. Use skipcov in earlier version,
#  But  it might  not be computable.
#
# Possible choices for cov.fun:
# skipcov
# tbscov
# covout
# covogk
# mgvcov
# mvecov
# mcdcov
# wincov
# bicovM
#
x=elimna(x)
x=as.matrix(x)
mcor=winall(x,tr=0)$cov
term=sum(mcor)
diag(mcor)=0
term1=sum(mcor)
k=ncol(x)
lam=k*term1/(k-1)
res1=lam/term
#
mcor=cov.fun(x,...)
term=sum(mcor)
diag(mcor)=0
term1=sum(mcor)
k=ncol(x)
lam=k*term1/(k-1)
lam=lam/term
list(coef.alpha=res1,robust.alpha=lam)
}

regci.het.blp<-function(x,y,regfun=tsreg,nboot=599,alpha=.05,SEED=TRUE,pr=TRUE,null.val=NULL,
method='hoch',plotit=FALSE,xlab='Predictor 1',ylab='Predictor 2',WARNS=FALSE,LABELS=FALSE,...){
#
#   Compute a .95 confidence interval for each of the parameters of
#   a linear regression equation. The default regression method is
#   the Theil-Sen estimator.
#
# Use method HH to eliminate bad leverage points
#
#   When using the least squares estimator, and when n<250, use
#   lsfitci instead.
#
#   The predictor values are assumed to be in the n by p matrix x.
#   The default number of bootstrap samples is nboot=599
#
#   regfun can be any R function that returns the coefficients in
#   the vector regfun$coef, the first element of which contains the
#   estimated intercept, the second element contains the estimated of
#   the first predictor, etc.
#
#   plotit=TRUE: If there are two predictors, plot 1-alpha confidence region based
#  on the bootstrap samples.
#
x<-as.matrix(x)
p1<-ncol(x)+1
p<-ncol(x)
xy<-cbind(x,y)
xy<-elimna(xy)
x<-xy[,1:p]
y<-xy[,p1]
nrem=length(y)

flag<-regcon.out(x,y,plotit=FALSE,...)$keep
xy<-xy[flag,]
x<-xy[,1:p]
y<-xy[,p1]

estit=regfun(x,y,...)$coef
if(is.null(null.val))null.val=rep(0,p1)
flagF=FALSE
flagF=identical(regfun,tsreg)
if(flagF){if(pr){
if(sum(duplicated(y)>0))print('Duplicate values detected; tshdreg might have more power than tsreg')
}}
nv=length(y)
x<-as.matrix(x)
if(SEED)set.seed(2) # set seed of random number generator so that
#             results can be duplicated.
if(pr)print('Taking bootstrap samples. Please wait.')
data<-matrix(sample(length(y),size=length(y)*nboot,replace=TRUE),nrow=nboot)
if(!WARNS)options(warn=-1)
bvec<-apply(data,1,regboot,x,y,regfun,xout=FALSE,...)
options(warn=0)
#Leverage points already removed.
# bvec is a p+1 by nboot matrix. The first row
#                     contains the bootstrap intercepts, the second row
#                     contains the bootstrap values for first predictor, etc.
regci<-matrix(0,p1,6)
vlabs='Intercept'
for(j in 2:p1)vlabs[j]=paste('Slope',j-1)
if(LABELS)vlabs[2:p1]=labels(x)[[2]]
dimnames(regci)<-list(vlabs,c('ci.low','ci.up','Estimate','S.E.','p-value','p.adj'))
ilow<-round((alpha/2) * nboot)
ihi<-nboot - ilow
ilow<-ilow+1
se<-NA
pvec<-NA
for(i in 1:p1){
bsort<-sort(bvec[i,])
#pvec[i]<-(sum(bvec[i,]<0)+.5*sum(bvec[i,]==0))/nboot
pvec[i]<-(sum(bvec[i,]<null.val[i])+.5*sum(bvec[i,]==null.val[i]))/nboot
if(pvec[i]>.5)pvec[i]<-1-pvec[i]
regci[i,1]<-bsort[ilow]
regci[i,2]<-bsort[ihi]
se[i]<-sqrt(var(bvec[i,]))
}
if(p1==3){
if(plotit){
plot(bvec[2,],bvec[3,],xlab=xlab,ylab=ylab)
}}
regci[,3]=estit
pvec<-2*pvec
regci[,4]=se
regci[,5]=regci[,6]=pvec
regci[2:p1,6]=p.adjust(pvec[2:p1],method=method)
list(regci=regci,n=nrem,n.keep=nv)
}




# ----------------------------------------------------------------------------

# regciHH

# ----------------------------------------------------------------------------

regciHH<-function(x,y,regfun=tsreg,nboot=599,alpha=.05,SEED=TRUE,pr=TRUE,null.val=NULL,
method='hoch',plotit=FALSE,xlab='Predictor 1',ylab='Predictor 2',WARNS=FALSE,LABELS=FALSE,...){
#
#   Compute a .95 confidence interval for each of the parameters of
#   a linear regression equation. The default regression method is
#   the Theil-Sen estimator.
#
# Use method HH to eliminate bad leverage points
#
#   When using the least squares estimator, and when n<250, use
#   lsfitci instead.
#
#   The predictor values are assumed to be in the n by p matrix x.
#   The default number of bootstrap samples is nboot=599
#
#   regfun can be any R function that returns the coefficients in
#   the vector regfun$coef, the first element of which contains the
#   estimated intercept, the second element contains the estimated of
#   the first predictor, etc.
#
#   plotit=TRUE: If there are two predictors, plot 1-alpha confidence region based
#  on the bootstrap samples.
#
x<-as.matrix(x)
p1<-ncol(x)+1
p<-ncol(x)
xy<-cbind(x,y)
xy<-elimna(xy)
x<-xy[,1:p]
y<-xy[,p1]
nrem=length(y)

flag<-outblp.HH(x,y,plotit=FALSE,...)$keep
xy<-xy[flag,]
x<-xy[,1:p]
y<-xy[,p1]

estit=regfun(x,y,...)$coef
if(is.null(null.val))null.val=rep(0,p1)
flagF=FALSE
flagF=identical(regfun,tsreg)
if(flagF){if(pr){
if(sum(duplicated(y)>0))print('Duplicate values detected; tshdreg might have more power than tsreg')
}}
nv=length(y)
x<-as.matrix(x)
if(SEED)set.seed(2) # set seed of random number generator so that
#             results can be duplicated.
if(pr)print('Taking bootstrap samples. Please wait.')
data<-matrix(sample(length(y),size=length(y)*nboot,replace=TRUE),nrow=nboot)
if(!WARNS)options(warn=-1)
bvec<-apply(data,1,regboot,x,y,regfun,xout=FALSE,...)
options(warn=0)
#Leverage points already removed.
# bvec is a p+1 by nboot matrix. The first row
#                     contains the bootstrap intercepts, the second row
#                     contains the bootstrap values for first predictor, etc.
regci<-matrix(0,p1,6)
vlabs='Intercept'
for(j in 2:p1)vlabs[j]=paste('Slope',j-1)
if(LABELS)vlabs[2:p1]=labels(x)[[2]]
dimnames(regci)<-list(vlabs,c('ci.low','ci.up','Estimate','S.E.','p-value','p.adj'))
ilow<-round((alpha/2) * nboot)
ihi<-nboot - ilow
ilow<-ilow+1
se<-NA
pvec<-NA
for(i in 1:p1){
bsort<-sort(bvec[i,])
#pvec[i]<-(sum(bvec[i,]<0)+.5*sum(bvec[i,]==0))/nboot
pvec[i]<-(sum(bvec[i,]<null.val[i])+.5*sum(bvec[i,]==null.val[i]))/nboot
if(pvec[i]>.5)pvec[i]<-1-pvec[i]
regci[i,1]<-bsort[ilow]
regci[i,2]<-bsort[ihi]
se[i]<-sqrt(var(bvec[i,]))
}
if(p1==3){
if(plotit){
plot(bvec[2,],bvec[3,],xlab=xlab,ylab=ylab)
}}
regci[,3]=estit
pvec<-2*pvec
regci[,4]=se
regci[,5]=regci[,6]=pvec
regci[2:p1,6]=p.adjust(pvec[2:p1],method=method)
list(regci=regci,n=nrem,n.keep=nv)
}




# ----------------------------------------------------------------------------

# regIVcommcp

# ----------------------------------------------------------------------------

reglev.est<-function(x,y,regfun=tsreg,...){
#
# Use homoscedastic method to remove bad leverage points and estimate parameters using remaining data
#
x<-as.matrix(x)
p1<-ncol(x)+1
p<-ncol(x)
xy<-cbind(x,y)
xy<-elimna(xy)
x<-xy[,1:p]
y<-xy[,p1]
x=as.matrix(x)
nrem=length(y)
id=reglev.gen(x,y,plotit=FALSE,...)$keep
e=regfun(x[id,],y[id],...)$coef
e
}

wmw.det<-function(x,y,refp=NULL,plotit=FALSE,xlab='Difference',ylab='Density',
plotfun=kerSORT){
#
#
# If execution time  is an issue when plotting, try plotfun=skerd
#
#
# Compare the tails distribution of x-y based on a specified value
# indicated by the argument
# refp
# That is compare  P(x-y< -refp) vs P(x-y > refp)
#
#
if(is.null(refp))stop('No reference point was provided')
x<-x[!is.na(x)]
y<-y[!is.na(y)]
refp=abs(refp)
m<-outer(x,y,FUN='-')
if(refp!=0){
L=mean(m<=0-refp)
U=mean(m>=refp)
}
else{
L=mean(m<0)
U=mean(m>0)
}
if(plotit)plotfun(as.vector(m),xlab=xlab,ylab=ylab)
list(L=L,U=U,dif=U-L)
}

reglev.gen<-function(x,y,regfun=tsreg,outfun=outpro.depth,regout=outpro,crit=sqrt(qchisq(.975,1)),
plotit=TRUE,xlab='X',ylab='Y',outplot=FALSE,DIS=FALSE,...){
#
#  Search for good and bad leverage points using the regression method
#  indicated by
#  regfun
#
#  This is  a more general version of reglev.
# Here, can specify the regression estimator and outlier detection method.
#
#. plotit=TRUE. Point marked o are bad leverage points
#
#  When x is univariate and has a skewed distribution, suggest using outfun=outmc
#
#  x is an n by p matrix
#
#  Strategy: first determine whether there are any leverage points
#  If yes, remove them and estimate the slopes and intercept
#   Based on this fit, compute residuals using all of the data.
#   Check for outliers among the residuals using MAD-median rule
#   Bad leverage point is a leverage points for which the residual is an outlier.
#
#   VALUE:
#   keep indicates which points are not bad leverage points.
#
#  if DIS=TRUE, distances used to determine leverage points are returned.
#
xy=elimna(cbind(x,y))
nkeep=c(1:nrow(xy))
x=as.matrix(x)
p=ncol(x)
p1=p+1
x=xy[,1:p]
y=xy[,p1]
x<-as.matrix(x)
d=outfun(x,plotit=outplot,...)
iout=d$out.id          #leverage points
glp=iout
nlp=length(iout)
keep=d$keep
est=regfun(x[keep,],y[keep])$coef
yhat=est[1]+x%*%est[2:p1]
res=y-yhat
dis=abs(res-median(res))/mad(res)
chk<-ifelse(dis>crit,1,0)     #residuals outliers
vec<-c(1:nrow(x))
outid=resid=vec[chk==1]     # id which are residuals outliers
keep<-vec[chk==0]
both=c(iout,outid)
blp=duplicated(both)
if(sum(!blp)>0)blp=unique(both[blp])
else blp=NULL
if(length(blp)>0){
flag=NULL
for(k in 1:length(blp)){
flag=c(flag,which(iout==blp[k]))
}
glp=iout[-flag]
}
if(!is.null(blp))regout.n=length(blp)
nkeep=c(1:length(y))
if(length(blp)>0)nkeep=vec[-blp]
if(ncol(xy)==2){
if(plotit){
plot(x,y,xlab=xlab,ylab=ylab,type='n')
points(x[keep],y[keep],pch='.')
points(x[glp],y[glp],pch='*')
points(x[blp],y[blp],pch='o')
}}
list(n.lev=d$n.out,lev.pts=iout,good.lev=glp,bad.lev=blp,res.out.id=resid,keep=nkeep)
}


outblp=reglev.gen




# ----------------------------------------------------------------------------

# regtest.blp

# ----------------------------------------------------------------------------

regtest.blp<-function(x,y,regfun=tsreg,nboot=600,alpha=.05,plotit=TRUE,
grp=c(1:ncol(x)),nullvec=c(rep(0,length(grp))),SEED=TRUE,pr=TRUE,...){
#
#  Test the hypothesis that q of the p predictors are equal to
#  some specified constants. By default, the hypothesis is that all
#  p predictors have a coefficient equal to zero.
#  The method is based on a confidence ellipsoid.
#  The critical value is determined with the percentile bootstrap method
#  in conjunction with Mahalanobis distance.
#
x<-as.matrix(x)
p1<-ncol(x)+1
p<-ncol(x)
xy<-cbind(x,y)
xy<-elimna(xy)
x<-xy[,1:p]
y<-xy[,p1]
m<-cbind(x,y)
flag<-reglev.gen(x,y,regfun=regfun,plotit=FALSE)$keep
m<-m[flag,]
x<-m[,1:p]
y<-m[,p1]
x<-as.matrix(x)
if(length(grp)!=length(nullvec))stop('The arguments grp and nullvec must have the same length.')
if(SEED)set.seed(2) # set seed of random number generator so that
#             results can be duplicated.
#print('Taking bootstrap samples. Please wait.')
data<-matrix(sample(length(y),size=length(y)*nboot,replace=TRUE),nrow=nboot)
bvec<-apply(data,1,regboot,x,y,regfun) # A p+1 by nboot matrix. The first row
#                     contains the bootstrap intercepts, the second row
#                     contains the bootstrap values for first predictor, etc.
grp<-grp+1  #Ignore the intercept.
est<-regfun(x,y)$coef
estsub<-est[grp]
bsub<-t(bvec[grp,])
if(length(grp)==1){
m1<-sum((bvec[grp,]-est)^2)/(length(y)-1)
dis<-(bsub-estsub)^2/m1
}
if(length(grp)>1){
mvec<-apply(bsub,2,FUN=mean)
m1<-var(t(t(bsub)-mvec+estsub))
dis<-mahalanobis(bsub,estsub,m1)
}
dis2<-order(dis)
dis<-sort(dis)
critn<-floor((1-alpha)*nboot)
crit<-dis[critn]
test<-mahalanobis(t(estsub),nullvec,m1)
sig.level<-1-sum(test>dis)/nboot
print(length(grp))
if(length(grp)==2 && plotit){
plot(bsub,xlab='Parameter 1',ylab='Parameter 2')
points(nullvec[1],nullvec[2],pch=0)
xx<-bsub[dis2[1:critn],]
xord<-order(xx[,1])
xx<-xx[xord,]
temp<-chull(xx)
lines(xx[temp,])
lines(xx[c(temp[1],temp[length(temp)]),])
}
list(test=test,crit=crit,p.value=sig.level,nullvec=nullvec,est=estsub,n=length(y))
}




# ----------------------------------------------------------------------------

# regYsub

# ----------------------------------------------------------------------------

regYsub<-function(x,y,xr,p1,regfun=tsreg,...){
est=regfun(x,y,...)$coef
xr=as.matrix(xr)
yhat=est[1]+xr%*%est[2:p1]
yhat
}

regYval<-function(m,regfun=tsreg){
val=as.vector(regYhat(m[,1],m[,2],regfun=regfun))
val
}
FBplot=func.plot

regYvar<-function(x,y,regfun=tsreg,pts=x,nboot=100,xout=FALSE,outfun=out,SEED=TRUE,...){
#
#  Estimate standard error of predicted value of Y using regression estimator
#  corresponding to the points in the argument
#  pts
#  A bootstrap estimate is used
#  nboot=100 indicates that 100 bootstrap samples will be used.
#  regfun indicates the regression estimator that will be used.
#  Theil--Sen is used by default.
#
xy=elimna(cbind(x,y))
x<-as.matrix(x)
p=ncol(x)
p1=p+1
vals=NA
x<-xy[,1:p]
y<-xy[,p1]
if(xout){
m<-cbind(x,y)
if(identical(outfun,outblp))flag=outblp(x,y,plotit=FALSE)$keep
else
flag<-outfun(x,plotit=FALSE,...)$keep
m<-m[flag,]
x<-m[,1:p]
y<-m[,p1]
}
nv=length(y)
x<-as.matrix(x)
pts=as.matrix(pts)
nvpts=nrow(pts)
bvec=matrix(NA,nrow=nboot,ncol=nvpts)
if(SEED)set.seed(2) # set seed of random number generator so that
#             results can be duplicated.
data<-matrix(sample(length(y),size=length(y)*nboot,replace=TRUE),nrow=nboot)
for(ib in 1:nboot){
bvec[ib,]=regYsub(x[data[ib,],],y[data[ib,]],xr=pts,p1=p1,regfun=regfun,...)
}
sqsd=apply(bvec,2,var)
sqsd
}




# ----------------------------------------------------------------------------

# rejpt.bt.lim

# ----------------------------------------------------------------------------

relfun<-function(xv,yv,C=36,epsilon=.0001,plotit=TRUE,pch='*',xlab='X',ylab='Y'){
#   Compute the measures of location, scale and correlation used in the
#   bivariate boxplot of Goldberg and Iglewicz,
#   Technometrics, 1992, 34, 307-320.
#
#   The code in relplot plots the boxplot.
#
#   This code assumes the data are in xv and yv
#
#   This code uses the function biloc, stored in the file biloc.b7 and
#   bivar stored in bivar.b7
#
plotit<-as.logical(plotit)
#
# Do pairwise elimination of missing values
#
temp<-matrix(c(xv,yv),ncol=2)
temp<-elimna(temp)
xv<-temp[,1]
yv<-temp[,2]
tx<-biloc(xv)
ty<-biloc(yv)
sx<-sqrt(bivar(xv))
sy<-sqrt(bivar(yv))
z1<-(xv-tx)/sx+(yv-ty)/sy
z2<-(xv-tx)/sx-(yv-ty)/sy
ee<-((z1-biloc(z1))/sqrt(bivar(z1)))^2+
((z2-biloc(z2))/sqrt(bivar(z2)))^2
w<-(1-ee/C)^2
if(length(w[w==0])>=length(xv)/2)warning("More than half of the w values equal zero")
sumw<-sum(w[ee<C])
tempx<-w*xv
txb<-sum(tempx[ee<C])/sumw
tempy<-w*yv
tyb<-sum(tempy[ee<C])/sumw
tempxy<-w*(xv-txb)*(yv-tyb)
tempx<-w*(xv-txb)^2
tempy<-w*(yv-tyb)^2
sxb<-sum((tempx[ee<C]))/sumw
syb<-sum((tempy[ee<C]))/sumw
rb<-sum(tempxy[ee<C])/(sqrt(sxb*syb)*sumw)
z1<-((xv-txb)/sqrt(sxb)+(yv-tyb)/sqrt(syb))/sqrt(2*(1+rb))
z2<-((xv-txb)/sqrt(sxb)-(yv-tyb)/sqrt(syb))/sqrt(2*(1-rb))
wo<-w
ee<-z1^2+z2^2
w<-(1-ee/C)^2
sumw<-sum(w[ee<C])
tempx<-w*xv
txb<-sum(tempx[ee<C])/sumw
tempy<-w*yv
tyb<-sum(tempy[ee<C])/sumw
tempxy<-w*(xv-txb)*(yv-tyb)
tempx<-w*(xv-txb)^2
tempy<-w*(yv-tyb)^2
sxb<-sum((tempx[ee<C]))/sumw
syb<-sum((tempy[ee<C]))/sumw
rb<-sum(tempxy[ee<C])/(sqrt(sxb*syb)*sumw)
z1<-((xv-txb)/sqrt(sxb)+(yv-tyb)/sqrt(syb))/sqrt(2*(1+rb))
z2<-((xv-txb)/sqrt(sxb)-(yv-tyb)/sqrt(syb))/sqrt(2*(1-rb))
iter<-0
while(iter<=10){
iter<=iter+1
ee<-z1^2+z2^2
w<-(1-ee/C)^2
sumw<-sum(w[ee<C])
tempx<-w*xv
txb<-sum(tempx[ee<C])/sumw
tempy<-w*yv
tyb<-sum(tempy[ee<C])/sumw
tempxy<-w*(xv-txb)*(yv-tyb)
tempx<-w*(xv-txb)^2
tempy<-w*(yv-tyb)^2
sxb<-sum((tempx[ee<C]))/sumw
syb<-sum((tempy[ee<C]))/sumw
rb<-sum(tempxy[ee<C])/(sqrt(sxb*syb)*sumw)
z1<-((xv-txb)/sqrt(sxb)+(yv-tyb)/sqrt(syb))/sqrt(2*(1+rb))
z2<-((xv-txb)/sqrt(sxb)-(yv-tyb)/sqrt(syb))/sqrt(2*(1-rb))
wo<-w
ee<-z1^2+z2^2
w<-(1-ee/C)^2
dif<-w-wo
crit<-sum(dif^2)/(mean(w))^2
if(crit <=epsilon)break
}
if(plotit){
em<-median(sqrt(ee))
r1<-em*sqrt((1+rb)/2)
r2<-em*sqrt((1-rb)/2)
temp<-c(0:179)
thet<-2*3.141593*temp/180
theta1<-r1*cos(thet)
theta2<-r2*sin(thet)
xplot1<-txb+(theta1+theta2)*sqrt(sxb)
yplot1<-tyb+(theta1-theta2)*sqrt(syb)
emax<-max(sqrt(ee[ee<7*em^2]))
r1<-emax*sqrt((1+rb)/2)
r2<-emax*sqrt((1-rb)/2)
theta1<-r1*cos(thet)
theta2<-r2*sin(thet)
xplot<-txb+(theta1+theta2)*sqrt(sxb)
yplot<-tyb+(theta1-theta2)*sqrt(syb)
totx<-c(xv,xplot,xplot1)
toty<-c(yv,yplot,yplot1)
plot(totx,toty,type="n",xlab=xlab,ylab=ylab)
points(xv,yv,pch=pch)
points(xplot,yplot,pch=".")
points(xplot1,yplot1,pch=".")
}
list(mest=c(txb,tyb),mvar=c(sxb,syb),mrho=rb)
}

relplot<-relfun

remove.lab.vec<-function(a){
#
# Remove labels
#
a=as.matrix(a)
dimnames(a)=list(NULL,NULL)
a
}




# ----------------------------------------------------------------------------

# rexgauss

# ----------------------------------------------------------------------------

rexgauss<-function(n,mu=0,sigma=1,rate=1){
#
#  Generate data from an Ex-Gaussian distribution
#
x=rnorm(n,mean=mu,sd=sigma)
y=rexp(n,rate=rate)
z=x+y
z
}



Rfit<-function(x,y,xout=FALSE,outfun=outpro,...){
#
#  Fit regression line using rank-based method based
#  Jaeckel's dispersion function
#  via the R package Rfit
#
library(Rfit)
if(xout){
m<-cbind(x,y)
p1=ncol(m)
p=p1-1
flag<-outfun(x,plotit=FALSE,...)$keep
m<-m[flag,]
x<-m[,1:p]
y<-m[,p1]
}
fit=rfit(y~x)
output=summary(fit)
list(summary=output[1]$coefficients,coef=output[1]$coefficients[,1],Drop_test=output[2]$dropstat,
 Drop_test_p.value=output[3]$droppval,Mult_R_squared=output[4]$R2)
}

rfit.est<-function(x,y,xout=FALSE,outfun=outpro,...){
#
#  Fit regression line using rank-based method based
#  Jaeckel's dispersion function
#  via the R package Rfit
#
library(Rfit)
library(quantreg)
if(xout){
m<-cbind(x,y)
p1=ncol(m)
p=p1-1
flag<-outfun(x,plotit=FALSE,...)$keep
m<-m[flag,]
x<-m[,1:p]
y<-m[,p1]
}
fit=rfitv2(y~x)
fit=fit$coef
list(coef=fit)
}

Rfit.est=rfit.est




# ----------------------------------------------------------------------------

# rfitv2

# ----------------------------------------------------------------------------

rfitv2 <- function (formula, data, subset, yhat0 = NULL,
    scores = Rfit::wscores, symmetric = FALSE, TAU = 'F0',  ...) {

# Below is taken from quantreg (under GPL) #
  call<-match.call()
  mf<-match.call(expand.dots=FALSE)
  m<-match(c('formula','data','subset'),names(mf),0)
  mf<-mf[c(1,m)]
  mf[[1]]<-as.name("model.frame")
  mf<-eval.parent(mf)
#

  x <- model.matrix(attr(mf, "terms"), data = mf)
  if( abs(max(x) - min(x)) < .Machine$double.eps ^ 0.5 ) stop("x cannot only contain an intercept")
  x1 <- as.matrix(x[,colnames(x)!='(Intercept)'])
  x1 <- as.matrix(cbind(rep(1,nrow(x1)),x1))

  y <- model.response(mf)

  qrx <- qr(x1)
  Q<-as.matrix(qr.Q(qrx))
  q1<-Q[,1]
  xq<-as.matrix(Q[,2:qrx$rank])

  if( is.null(yhat0) ) {
    fit0<-suppressWarnings(rq(y~xq-1))
  } else {
    fit0 <- lsfit(xq, yhat0, intercept = FALSE)
  }
#  ord<-order(fit0$resid)

## 20141211: set initial fit to null model if it has lower dispersion
  betahat0 <- fit0$coef
  if( disp(betahat0, xq, y, scores) > disp(rep(0,length(betahat0)), xq, y, scores) ) {
    betahat0 <- rep(0, length(betahat0) )
  }
  ord <- order(y - xq%*%betahat0)
##

  fit <- jaeckel(as.matrix(xq[ord,]), y[ord], betahat0, scores=scores, ...)
  if( fit$convergence != 0 ) {
    fit2 <- jaeckel(as.matrix(xq[ord,]), y[ord], jitter(fit$par), scores=scores, ...)
    if( fit$convergence != 0 ) {
      warning("rfit: Convergence status not zero in jaeckel")
      if( fit2$value < fit$value ) fit <- fit2
    } else {
      fit <- fit2
    }
    rm(fit2)
  }
  rm(ord)
  betahat <- fit$par

  yhat <- xq %*% betahat
  ehat <- y - yhat
  alphahat <- ifelse(symmetric, signedrank(ehat), median(ehat))
  ehat <- ehat - alphahat
  yhat <- yhat+alphahat

  bhat <- lsfit(x,yhat,intercept=FALSE)$coef

  r.gettau <- switch(TAU,
    F0 = gettauF0,
    R = gettau,
    N = function(...) NA
  )

  tauhat <- r.gettau(ehat, ncol(xq), scores, ...)
  if (symmetric) {
    taushat <- tauhat
  } else {
    taushat <- taustar(ehat, qrx$rank)
  }

  res <- list( coefficients = bhat, residuals = ehat, fitted.values = yhat,
    scores = scores, x = x, y = y, tauhat = tauhat, qrx1=qrx,
    taushat = taushat, symmetric = symmetric, betahat = bhat,disp=fit$value)
  res$call <- call
  class(res) <- list("rfit")
  res

}
rgvar<-function(x,est=covmcd,...){
#
# compute a robust generalized variance
#
# choices for est are:
#  var
#  covmcd
#  covmve
#  skipcov with MM=F (boxplot) MM=T (MAD-MEDIAN), op=1 (MGV method)
#               op=2 (projection method for outliers)
#  covroc  (S+ only as of Dec, 2005)
#  Rocke's measure of scatter, this requires that the command
#          library(robust) has been executed.
#
val<-prod(eigen(est(x,...))$values)
val
}




# ----------------------------------------------------------------------------

# rgvar2g

# ----------------------------------------------------------------------------

rgvar2g<-function(x,y,nboot=100,est=covmcd,alpha=.05,cop=3,op=2,SEED=TRUE,...){
#
# Two independent groups.
# Test hypothesis of equal generalized variances.
#
#  Choices for est include:
#  var
#  covmcd
#  covmve
#  skipcov with MM=F (boxplot) MM=T (MAD-MEDIAN), op=1 (MGV method)
#               op=2 (projection method for outliers)
#  covroc  Rocke's measure of scatter, this requires that the command
#          library(robust) has been executed.
#
if(SEED)set.seed(2)
se1<-rgvarseb(x,nboot=nboot,est=est,SEED=SEED,...)
se2<-rgvarseb(y,nboot=nboot,est=est,SEED=SEED,...)
dif<-rgvar(x,est=est,...)-rgvar(y,est=est,...)
test.stat<-dif/sqrt(se1^2+se2^2)
test.stat
}

rgvarseb<-function(x,nboot=100,est=skipcov,SEED=TRUE,...){
#
n<-nrow(x)
val<-NA
for(i in 1:nboot){
data<-sample(n,n,replace=TRUE)
val[i]<-rgvar(x[data,],est=est,...)
}
se<-sqrt(var(val))
se
}
rho.bt <- function(x,c1,M)
{
    x1 <- (x-M)/c1
    ivec1 <- (x1 < 0)
    ivec2 <- (x1 >  1)
    return(ivec1*(x^2/2)
        +ivec2*(M^2/2+c1*(5*c1+16*M)/30)
        +(1-ivec1-ivec2)*(M^2/2-M^2*(M^4-5*M^2*c1^2+15*c1^4)/(30*c1^4)
            +(1/2+M^4/(2*c1^4)-M^2/c1^2)*x^2
            +(4*M/(3*c1^2)-4*M^3/(3*c1^4))*x^3
            +(3*M^2/(2*c1^4)-1/(2*c1^2))*x^4
            -4*M*x^5/(5*c1^4)+x^6/(6*c1^4)))
}




# ----------------------------------------------------------------------------

# rhoBY3

# ----------------------------------------------------------------------------

rhoBY3 <- function(t,c3)
{
  (t*exp(-sqrt(c3))*as.numeric(t <= c3))+
    (((exp(-sqrt(c3))*(2+(2*sqrt(c3))+c3))-(2*exp(-sqrt(t))*(1+sqrt(t))))*as.numeric(t >c3))
}




# ----------------------------------------------------------------------------

# ribmp

# ----------------------------------------------------------------------------

ribmp<-function(J,K,x,alpha=.05,p=J*K,grp=c(1:p),plotit=TRUE,op=4){
#
#  Rank-based multiple comparisons for all interactions
#  in J by K design. The method is based on an
#  extension of Cliff's heteroscedastic technique for
#  handling tied values and the Patel-Hoel definition of no interaction.
#
#  The familywise type I error probability is controlled by using
#  a critical value from the Studentized maximum modulus distribution.
#
#  It is assumed all groups are independent.
#
#  Missing values are automatically removed.
#
#  The default value for alpha is .05. Any other value results in using
#  alpha=.01.
#
#  Argument grp can be used to rearrange the order of the data.
#
if(is.matrix(x))x<-listm(x)
if(!is.list(x))stop("Data must be stored in a matrix or in list mode.")
CCJ<-(J^2-J)/2
CCK<-(K^2-K)/2
CC<-CCJ*CCK
test<-matrix(NA,CC,8)
test.p<-matrix(NA,CC,7)
for(j in 1:J){
xx<-!is.na(x[[j]])
val<-x[[j]]
x[[j]]<-val[xx]  # Remove missing values
}
mat<-matrix(grp,ncol=K,byrow=TRUE)
dimnames(test)<-list(NULL,c("Factor A","Factor A","Factor B","Factor B","delta","ci.lower","ci.upper","p.value"))
jcom<-0
crit<-smmcrit(200,CC)
if(alpha!=.05)crit<-smmcrit01(200,CC)
alpha<-1-pnorm(crit)
for (j in 1:J){
for (jj in 1:J){
if (j < jj){
for (k in 1:K){
for (kk in 1:K){
if (k < kk){
jcom<-jcom+1
test[jcom,1]<-j
test[jcom,2]<-jj
test[jcom,3]<-k
test[jcom,4]<-kk
temp1<-bmp(x[[mat[j,k]]],x[[mat[j,kk]]],plotit=FALSE,alpha=alpha)
temp2<-bmp(x[[mat[jj,k]]],x[[mat[jj,kk]]],plotit=FALSE,alpha=alpha)
delta<-temp1$phat-temp2$phat
sqse<-temp1$s.e.^2.+temp2$s.e.^2
test[jcom,5]<-delta
test[jcom,6]<-delta-crit*sqrt(sqse)
test[jcom,7]<-delta+crit*sqrt(sqse)
test[jcom,8]=2*(1-pnorm(abs((delta)/sqrt(sqse))))
}}}}}}
list(test=test)
}




# ----------------------------------------------------------------------------

# risk.ratio

# ----------------------------------------------------------------------------

risk.ratio<-function(x1,n1,x2,n2,alpha=.05){
#
#  Risk ratio confidence interval
#
p1=x1/n1
p2=x2/n2
rat=p1/p2
term1=((n1-x1)/x1)/n1
term2=((n2-x2)/x2)/n2
term=sqrt(term1+term2)
z=qnorm(1-alpha/2)
LL=log(rat)
v1=LL-z*term
v2=LL+z*term
ci=c(exp(v1),exp(v2))
# Compute p-value
pv=seq(.001,.999,.001)
for(j in 1:length(pv)){
pv.rem=pv[j]
z=qnorm(1-pv[j]/2)
v1=LL-z*term
v2=LL+z*term
chk=c(exp(v1),exp(v2))
if(chk[1]>1 || chk[2]<1)break
}
if(p1==p2)pv.rem=1
list(p1=p1,p2=p2,RR.est=rat,ci=ci,p.value=pv.rem)
}




# ----------------------------------------------------------------------------

# ritest

# ----------------------------------------------------------------------------

ritest<-function(x,y,adfun=adrun,plotfun=lplot,eout=FALSE,xout=TRUE,plotit=TRUE,flag=3,
nboot=500,alpha=.05,tr=.2,...){
#
# There are two methods for testing for regression interactions
# using robust smooths.
# The first, performed by this function, fits an additive model
# and test the hypothesis that the residuals, given x, is a
# horizontal plane.
#
# The second, which is done by function adtest, tests the hypothesis
# that a generalized additive model fits the data.
#
# Plot used to investigate regression interaction
# (the extent a generalized additive model does not fit data).
# Compute additive fit, plot residuals
# versus x, an n by 2 matrix.
#
if(!is.matrix(x))stop(" x must be a matrix")
if(ncol(x)!=2)stop(" x must have two columns only")
yhat<-adfun(x,y,pyhat=TRUE,eout=eout,xout=xout,plotit=FALSE)
res<-y-yhat
output<-indt(x,res,flag=flag,nboot=nboot)
if(plotit)plotfun(x,y-yhat,eout=eout,xout=xout,expand = 0.5,scale=FALSE,xlab="X",
ylab="Y",zlab="",theta=50,phi=25,...)
output
}




# ----------------------------------------------------------------------------

# rm.marg.es

# ----------------------------------------------------------------------------

rm.marg.es<-function(x,y=NULL,tr=0.2){
#
# Analog of robust version  KMS measure of effect size for two
# dependent groups
#
if(!is.null(y))x=cbind(x,y)
x=elimna(x)
if(ncol(x)>2)stop('Should have only two variables')
v1=winsdN(x[,1],tr=tr)^2
v2=winsdN(x[,2],tr=tr)^2
v3=wincovN(x,tr=tr)
a=v1+v2 -2*v3
e=sqrt(2)*(mean(x[,1],tr=tr)-mean(x[,2],tr=tr))/sqrt(a)
e
}




# ----------------------------------------------------------------------------

# rm.marg.esCI

# ----------------------------------------------------------------------------

rm.marg.esCI<-function(x,y=NULL,tr=.2,nboot=1000,SEED=TRUE,alpha=.05,
null.val=0,MC=FALSE,...){
#
# Two dependent groups.
# Confidence interval for effect size that takes into account heteroscedasticity as well as the
# association between X and Y based on the marginal distributions, not the
# difference scores. For robust estimators, these two approaches generally give
# different results.
#
if(!is.null(y))x=cbind(x,y)
x=elimna(x)
if(SEED)set.seed(2)
e=rm.marg.es(x,tr=tr)
n=nrow(x)
if(!MC){
v=NA
for(i in 1:nboot){
id=sample(n,replace=TRUE)
v[i]=rm.marg.es(x[id,],tr=tr)
}
}
if(MC){
d=list()
for(j in 1:nboot){
id=sample(n,replace=TRUE)
d[[j]]=x[id,]
}
v=mclapply(d,rm.marg.es,tr=tr)
v=matl(v)
}

v=sort(v)
ilow<-round((alpha/2) * nboot)
ihi<-nboot - ilow
ilow<-ilow+1
ci=v[ilow]
ci[2]=v[ihi]
pv=mean(v<null.val)
pv=2*min(pv,1-pv)
list(n=n,effect.size=e,ci=ci,p.value=pv)
}




# ----------------------------------------------------------------------------

# rm.margOM.es

# ----------------------------------------------------------------------------

rm.margOM.es<-function(x,y=NULL,locfun=onestep){
#
# Analog of robust version  KMS measure of effect size for two
# dependent groups using a one-step M-estimator by default coupled
# with the percentage bend variance.
#
if(!is.null(y))x=cbind(x,y)
x=elimna(x)
if(ncol(x)>2)stop('Should have only two variables')
v1=pbvar(x[,1])
v2=pbvar(x[,2])
v3=pbcor(x[,1],x[,2])$cor
a=v1+v2 -2*v3*sqrt(v1)*sqrt(v2)
e=sqrt(2)*(locfun(x[,1])-locfun(x[,2]))/sqrt(a)
e
}




# ----------------------------------------------------------------------------

# RM.PMD.PCD

# ----------------------------------------------------------------------------

RM.PMD.PCD<-function(x,tr=.2,delta=.5,alpha=.05,p.crit=NULL,iter=5000,nboot=500,SEED=TRUE){
#
#
#  Use an indifference zone. Given x
#  determine the
#  probability of making a decision and the probability of
#  of correct decision given that a decision is made
# within the context of an indiffernce zone
#
if(is.list(x))stop('x should be a matrix or a data frame')
PMD=0
PCD=0
x=elimna(x)
est=apply(x,2,tmean,tr=tr)
ID=which(est==max(est))
n=nrow(x)
# First, determine decision rule
#
A=cov(x)
J=ncol(x)
aval=c(seq(.001,.1,.001),seq(.11,.99,.01))
id=which(aval==alpha)
if(length(id)==0)stop('alpha be one one values .001(.001).1 or 11(.01).99')
p.crit=rmanc.best.crit(x,iter=iter,alpha=alpha,tr=tr,SEED=SEED)
#
# Now simulate indifference zone.
for(i in 1:nboot){
x=mvrnorm(n,mu=rep(0,J),Sigma=A)
x[,1]=x[,1]+delta*sqrt(A[ID,ID])
est=apply(x,2,tmean,tr=tr)
id=which(est==max(est))
a=rmanc.best.ex(x,tr=tr)
if(sum(a<=p.crit)){
PMD=PMD+1
if(id==1)PCD=PCD+1
}}
PMD.ci=binom.conf(PMD,nboot,pr=FALSE)$ci
PCD.ci=binom.conf(PCD,PMD,pr=FALSE)$ci
PCD=PCD/max(PMD,1)
PMD=PMD/nboot
list(PMD=PMD,PMD.ci=PMD.ci,PCD=PCD,PCD.ci=PCD.ci)
}




# ----------------------------------------------------------------------------

# rm2g.tests

# ----------------------------------------------------------------------------

rm2g.tests<-function(x,y=NULL,tr=.2, alpha=.05, REL.MAG=NULL,SEED=TRUE,nboot=1000,AUTO=FALSE){
#
#
# For two dependent groups,
# compute confidence intervals for six measures of effect size based on difference scores:
#
#  AKP: robust standardized difference similar to  Cohen's d
#  QS:  Quantile shift based on the median of the distribution of difference scores,
#  QStr: Quantile shift based on the trimmed mean of the distribution of X-Y
#  SIGN:  P(X<Y), probability that for a random pair, the first is less than the second.
#   TRIM: Trimmed mean based on
#  MED: median based on
#
#   If arg y is not null, x-y, difference scores are used
#
#  y=NULL: Assume difference scores stored in x or only a single distribution is being used.
#

if(!is.null(y))x=x-y
x=elimna(x)
n=length(x)
output=matrix(NA,ncol=4,nrow=6)
dimnames(output)=list(c('AKP','QS (median)','QStr','SIGN','TRIM','MEDIAN'),c('Est','ci.low','ci.up','p.value'))
a=D.akp.effect.ci(x,alpha=alpha,SEED=SEED,tr=tr,nboot=nboot)
a=pool.a.list(a)
output[1,]=a
a=depQSci(x,alpha=alpha,SEED=SEED,nboot=nboot)
a=pool.a.list(a)
output[2,]=a
a=depQSci(x,locfun=tmean,alpha=alpha, SEED=SEED,tr=tr,nboot=nboot)
a=pool.a.list(a)
output[3,]=a
Z=sum(x<0)
a=binom.conf.pv(Z,n,alpha=alpha,AUTO=AUTO,pr=FALSE)
a=pool.a.list(a[2:4])
output[4,]=a
a=trimci(x,pr=FALSE,tr=tr)
a=pool.a.list(a[c(1,2,5)])
output[5,]=a
a=hdpb(x,SEED=SEED)
a=pool.a.list(a[c(3,1,4)])
output[6,]=a
output
}




# ----------------------------------------------------------------------------

# rm2mcp

# ----------------------------------------------------------------------------

rm2mcp<-function(J,K,x,est=tmean,alpha=.05,grp=NA,dif=TRUE,nboot=NA,
plotit=FALSE,BA=FALSE,hoch=FALSE,...){
#
# This function performs multiple comparisons for
# dependent groups in a within by within designs.
# It creates the linear contrasts and call calls rmmcppb
# only it is assumed that main effects and interactions for a
# two-way design are to be tested.
#
        #   The data are assumed to be stored in x in list mode or in a matrix.
        #  If grp is unspecified, it is assumed x[[1]] contains the data
        #  for the first level of both factors: level 1,1.
        #  x[[2]] is assumed to contain the data for level 1 of the
        #  first factor and level 2 of the second factor: level 1,2
        #  x[[j+1]] is the data for level 2,1, etc.
        #  If the data are in wrong order, grp can be used to rearrange the
        #  groups. For example, for a two by two design, grp<-c(2,4,3,1)
        #  indicates that the second group corresponds to level 1,1;
        #  group 4 corresponds to level 1,2; group 3 is level 2,1;
        #  and group 1 is level 2,2.
        #
        #   Missing values are automatically removed.
        #
        JK <- J * K
        if(is.matrix(x))
                x <- listm(x)
        if(!is.na(grp[1])) {
                yy <- x
                for(j in 1:length(grp))
                        x[[j]] <- yy[[grp[j]]]
        }
        if(!is.list(x))
                stop("Data must be stored in list mode or a matrix.")
        for(j in 1:JK) {
                xx <- x[[j]]
               # xx[[j]] <- xx[!is.na(xx)]
               x[[j]] <- xx[!is.na(xx)]
        }
        #
        # Create the three contrast matrices
        #
temp<-con2way(J,K)
conA<-temp$conA
conB<-temp$conB
conAB<-temp$conAB
        ncon <- max(nrow(conA), nrow(conB), nrow(conAB))
FacA<-rmmcppb(x,con=conA,est=est,plotit=plotit,dif=dif,grp=grp,
nboot=nboot,BA=TRUE,hoch=FALSE,...)
FacB<-rmmcppb(x,con=conB,est=est,plotit=plotit,dif=dif,grp=grp,
nboot=nboot,BA=TRUE,hoch=FALSE,...)
FacAB<-rmmcppb(x,con=conAB,est=est,plotit=plotit,dif=dif,grp=grp,
nboot=nboot,BA=TRUE,hoch=FALSE,...)
list(Factor.A=FacA,Factor.B=FacB,Factor.AB=FacAB)

}

rm2miss<-function(x,y=NULL,tr=0,nboot=1000,alpha=.05,SEED=TRUE){
#
#   Compare the marginal trimmed means of two dependent groups
#   using a bootstrap t method that allows missing values
#
# If y is not supplied, this function assumes x is a matrix with 2 columns.
#
#  NOTE: This function can fail if there are too many missing values
# get the error: incorrect number of dimensions
#
#
if(SEED)set.seed(2)
if(is.null(y)){
if(!is.matrix(x))stop("y is null and x is not a matrix")
}
if(!is.null(y))x=cbind(x,y)
if(ncol(x)!=2)
print("warning: x has more than one column; columns 1 and 2 are used")
n=nrow(x)
test=yuendna(x,tr=tr)
cen=x
cen[,1]=cen[,1]-mean(x[,1],na.rm=TRUE,tr=tr)
cen[,2]=cen[,2]-mean(x[,2],na.rm=TRUE,tr=tr)
data=matrix(sample(n,n*nboot,replace=TRUE),ncol=nboot)
tval=apply(data,2,FUN=rm2miss.sub,x=cen,tr=tr)
tval=sort(abs(tval))
icrit<-floor((1-alpha)*nboot+.5)
ci=test$est-tval[icrit]*test$se
ci[2]=test$est+tval[icrit]*test$se
pv=mean(abs(test$test)<=abs(tval))
list(est.dif=test$est,ci=ci,p.value=pv)
}
rm2miss.sub<-function(data,x,tr){
n=nrow(x)
m=x[data,]
ans=yuendna(m,tr=tr)$test
ans
}




# ----------------------------------------------------------------------------

# rmanog

# ----------------------------------------------------------------------------

rmanog<-function(x,alpha=.05,est=onestep,grp=NA,nboot=NA,...){
#
#   Using the percentile bootstrap method,
#   test the hypothesis that all differences among J
#   dependent groups have a
#   measure of location equal to zero.
#   That is, if
#   Dij is the difference between ith observations
#   in groups j and j+1,
#   and Dij has measure of location  muj
#   the goal is to test
#   H0: mu1=mu2=...=0
#
#   By default, an M-estimator is used.
#
#   nboot is the bootstrap sample size. If not specified, a value will
#   be chosen depending on the number of groups
#
#   x can be an n by J matrix or it can have list mode
#   grp can be used to specify a subset of the groups for analysis
#
#   the argument ... can be used to specify options associated
#   with the argument est.
#
if(!is.list(x) && !is.matrix(x))stop("Data must be stored in a matrix or in list mode.")
if(is.list(x)){
# put the data in an n by J matrix
mat<-matrix(0,length(x[[1]]),length(x))
for (j in 1:length(x))mat[,j]<-x[[j]]
}
if(is.matrix(x))mat<-x
mat<-elimna(mat) # Remove rows with missing values.
J<-ncol(mat)
Jm<-J-1
jp<-0
dif<-matrix(NA,nrow=nrow(mat),ncol=Jm)
for(j in 1:Jm){
jp<-j+1
dif[,j]<-mat[,j]-mat[,jp]
}
if(is.na(nboot)){
nboot<-5000
if(Jm <= 4)nboot<-1000
}
print("Taking bootstrap samples. Please wait.")
data <- matrix(sample(nrow(mat), size = nrow(mat) * nboot, replace = T),
                nrow = nboot)
bvec <- matrix(NA, ncol = ncol(dif), nrow = nboot)
        for(j in 1:ncol(dif)) {
                temp <- dif[, j]
                bvec[, j] <- apply(data, 1., rmanogsub, temp, est)
        }  #bvec is an nboot by Jm matrix
testvec<-NA
for(j in 1:Jm){
testvec[j]<-sum(bvec[,j]>0)/nboot
if(testvec[j] > .5)testvec[j]<-1-testvec[j]
}
critvec<-alpha/c(1:Jm)
#testvec<-2*testvec[order(-1*testvec)]
test<-2*testvec
test.sort<-order(-1*test)
chk<-sum((test.sort <= critvec))
if(chk > 0)print("Significant difference found")
output<-matrix(0,Jm,6)
dimnames(output)<-list(NULL,c("con.num","psihat","sig","crit.sig","ci.lower","ci.upper"))
tmeans<-apply(dif,2,est,...)
psi<-1
output[,2]<-tmeans
for (ic in 1:Jm){
output[ic,1]<-ic
output[ic,3]<-test[ic]
crit<-critvec[ic]
output[test.sort[ic],4]<-crit
}
for(ic in 1:Jm){
icrit<-output[ic,4]
icl<-round(icrit*nboot/2)+1
icu<-round((1-icrit/2)*nboot)
temp<-sort(bvec[,ic])
output[ic,5]<-temp[icl]
output[ic,6]<-temp[icu]
}
list(output=output)
}

rmanogsub<-function(isub,x,est=onestep,...){
tsub <- est(x[isub],...)
tsub
}




# ----------------------------------------------------------------------------

# rmaseq

# ----------------------------------------------------------------------------

rmaseq<-function(x,est=onestep,alpha=.05,grp=NA,nboot=NA,...){
#
#   Using the percentile bootstrap method,
#   test hypothesis that all marginal distributions
#   among J dependent groups
#   have a common measure of location.
#   This is done by using a sequentially rejective method
#   of J-1 pairs of groups.
#   That is, compare group 1 to group 2, group 2 to group 3, etc.
#
#   By default, onestep M-estimator is used.
#
#   nboot is the bootstrap sample size. If not specified, a value will
#   be chosen depending on the number of groups
#
#   x can be an n by J matrix or it can have list mode
#   grp can be used to specify a subset of the groups for analysis
#
#   the argument ... can be used to specify options associated
#   with the argument est.
#
if(!is.list(x) && !is.matrix(x)){
stop("Data must be stored in a matrix or in list mode.")
}
if(is.list(x)){
# put the data in an n by J matrix
mat<-matrix(0,length(x[[1]]),length(x))
for (j in 1:length(x))mat[,j]<-x[[j]]
}
if(is.matrix(x))mat<-x
mat<-elimna(mat) # Remove rows with missing values.
J<-ncol(mat)
Jm<-J-1
con<-matrix(0,ncol=Jm,nrow=J)
for(j in 1:Jm){
jp<-j+1
for(k in j:jp){
con[j,j]<-1
con[jp,j]<-0-1
}}
rmmcp(x,est=est,alpha=alpha,con=con,nboot=nboot,...)
}




# ----------------------------------------------------------------------------

# rmblo

# ----------------------------------------------------------------------------

rmblo<-function(x,y){
#
# Remove only bad leverage points and return the
# remaining data
#
x=as.matrix(x)
p=ncol(x)
p1=p+1
xy=elimna(cbind(x,y))
x=xy[,1:p]
y=xy[,p1]
temp1=reglev(x,y,plotit=FALSE)
ad1=c(temp1$levpoints,temp1$regout)
flag1=duplicated(ad1)
if(sum(flag1)>0){
flag1=ad1[flag1]
x=as.matrix(x)
x1=x[-flag1,]
y1=y[-flag1]
xy=cbind(x1,y1)
}
list(x=xy[,1:p],y=xy[,p1])
}




# ----------------------------------------------------------------------------

# rmc2list

# ----------------------------------------------------------------------------

rmc2list<-function(x,grp.col,lev.col,pr=TRUE){
#
#  for a between by within design
#  grp.col is column indicating levels of between factor.
#  lev.col indicates the columns where repeated measures are contained
#
#  Example:  column 2 contains information on levels of between factor
#  have a 3 by 2 design, column 3 contains time 1 data,
#  column 7 contains time 2
#  rmc2list(x,2,c(3,7)) will store data in list mode that can be
#  bw2list(x,2,c(3,7)) also can be used.
#  used by rmanova and related functions
#
res=selbybw(x,grp.col,lev.col)
if(pr){
print("Levels for between factor:")
print(unique(x[,grp.col]))
}
res$x
}




# ----------------------------------------------------------------------------

# rmdat2mat

# ----------------------------------------------------------------------------

rmdat2mat<-function(m,id.col=NULL,dv.col=NULL){
#
# This function helps manipulate data when dealing with repeated measures
#
# Have data stored in R in a matrix or data.frame.
# One of the columns indicates subject id. So for a repeated measures
# at times 1, 2 and 3, say, Subject one's id will appear 3 times
# subject two's id will appear 3 times, etc.
#
# convert the data to a matrix where time 1 times 2 and time 3 data are
# in columns 1, 2, and 3.
#
x<-vector("list")
grpn<-sort(unique(m[,id.col]))
it<-0
for (ig in 1:length(grpn)){
for (ic in 1:length(dv.col)){
it<-it+1
flag<-(m[,id.col]==grpn[ig])
x[[it]]<-m[flag,dv.col[ic]]
}}
x=t(matl(x))
x
}




# ----------------------------------------------------------------------------

# rmdzD

# ----------------------------------------------------------------------------

rmdzD<-function(x,est=skipSPR,grp=NA,nboot=500,SEED=TRUE,MC=FALSE,...){
#
#   Do ANOVA on dependent groups
#   using depth of zero among  bootstrap values
#   based on difference scores.
#
#    Projection distances
#    rather than Mahalanobis distances are used  when computing a p-value
#
#   The data are assumed to be stored in x in list mode
#   or in a matrix. In the first case
#   x[[1]] contains the data for the first group, x[[2]] the data
#   for the second group, etc. Length(x)=the number of groups = J.
#   If stored in a matrix, columns correspond to groups.
#
#   grp is used to specify some subset of the groups, if desired.
#   By default, all J groups are used.
#
#   The default number of bootstrap samples is nboot=500
#
if(!is.list(x) && !is.matrix(x))stop('Data must be stored in a matrix or in list mode.')
if(is.list(x)){
# put the data in an n by J matrix
mat<-matrix(0,length(x[[1]]),length(x))
for (j in 1:length(x))mat[,j]<-x[[j]]
}
if(is.matrix(x))mat<-x
if(!is.na(grp[1])){
mat<-mat[,grp]
}
mat<-elimna(mat) # Remove rows with missing values.
center=est(mat,...)$center
if(SEED)set.seed(2) # set seed of random number generator so that
#             results can be duplicated.
data <- matrix(sample(nrow(mat), size = nrow(mat) * nboot, replace = T),
                nrow = nboot)
data=listm(t(data))
if(MC)bvec<-mclapply(data,rmdG_sub,mat,est,mc.preschedule=TRUE,...)
if(!MC) bvec<-lapply(data,rmdG_sub,mat,est,...)
bvec=t(matl(bvec)) # nboot by J matrix
J<-ncol(mat)
jp<-0
Jall<-(J^2-J)/2
dif<-matrix(NA,nrow=nboot,ncol=Jall)
ic<-0
cdif=NA
for(j in 1:J){
for(k in 1:J){
if(j<k){
ic<-ic+1
dif[,ic]<-bvec[,j]-bvec[,k]
cdif[ic]=center[j]-center[k]
}}}
if(J==2){
pv=mean(bvec[,1]<bvec[,2])+.5*mean(bvec[,1]==bvec[,2])
sig.level=2*min(c(pv,1-pv))
}
if(J>2){
zvec<-rep(0,Jall)
m1<-rbind(dif,zvec)
bplus<-nboot+1
cmat=var(dif)
dv<-pdisMC(m1,center=cdif)
bplus<-nboot+1
sig.level<-1-sum(dv[bplus]>=dv[1:nboot])/nboot
}
list(p.value=sig.level,center=center)
}


rmdzero<-function(x,est=mom,grp=NA,nboot=500,SEED=TRUE,...){
#
#   Do ANOVA on dependent groups
#   using #   depth of zero among  bootstrap values
#   based on difference scores.
#
#   The data are assumed to be stored in x in list mode
#   or in a matrix. In the first case
#   x[[1]] contains the data for the first group, x[[2]] the data
#   for the second group, etc. Length(x)=the number of groups = J.
#   If stored in a matrix, columns correspond to groups.
#
#   grp is used to specify some subset of the groups, if desired.
#   By default, all J groups are used.
#
#   The default number of bootstrap samples is nboot=500
#
if(!is.list(x) && !is.matrix(x))stop("Data must be stored in a matrix or in list mode.")
if(is.list(x)){
# put the data in an n by J matrix
mat<-matrix(0,length(x[[1]]),length(x))
for (j in 1:length(x))mat[,j]<-x[[j]]
}
if(is.matrix(x))mat<-x
if(!is.na(grp[1])){
mat<-mat[,grp]
}
mat<-elimna(mat) # Remove rows with missing values.
J<-ncol(mat)
jp<-0
Jall<-(J^2-J)/2
dif<-matrix(NA,nrow=nrow(mat),ncol=Jall)
ic<-0
for(j in 1:J){
for(k in 1:J){
if(j<k){
ic<-ic+1
dif[,ic]<-mat[,j]-mat[,k]
}}}
dif<-as.matrix(dif)
if(SEED)set.seed(2) # set seed of random number generator so that
#             results can be duplicated.
#print("Taking bootstrap samples. Please wait.")
data <- matrix(sample(nrow(mat), size = nrow(mat) * nboot, replace = T),
                nrow = nboot)
bvec <- matrix(NA, ncol = ncol(dif), nrow = nboot)
        for(j in 1:ncol(dif)) {
                temp <- dif[, j]
                bvec[, j] <- apply(data, 1., rmanogsub, temp, est)
        }  #bvec is an nboot by Jm matrix
center<-apply(dif,2,est,...)
bcen<-apply(bvec,2,mean)
cmat<-var(bvec-bcen+center)
zvec<-rep(0,Jall)
m1<-rbind(bvec,zvec)
bplus<-nboot+1
discen<-mahalanobis(m1,center,cmat)
sig.level<-sum(discen[bplus]<=discen)/bplus
list(p.value=sig.level,center=center)
}

rmdzeroG<-function(x,est=skipSPR,grp=NA,nboot=500,SEED=TRUE,...){
#
#   Do ANOVA on dependent groups
#   using #   depth of zero among  bootstrap values
#   based on difference scores.
#
#    Like rmdzero, only designed for multivariate  estimators such as
#    computed by the R functions, skip and dmean for example.
#
#   The data are assumed to be stored in x in list mode
#   or in a matrix. In the first case
#   x[[1]] contains the data for the first group, x[[2]] the data
#   for the second group, etc. Length(x)=the number of groups = J.
#   If stored in a matrix, columns correspond to groups.
#
#   grp is used to specify some subset of the groups, if desired.
#   By default, all J groups are used.
#
#   The default number of bootstrap samples is nboot=500
#
if(!is.list(x) && !is.matrix(x))stop('Data must be stored in a matrix or in list mode.')
if(is.list(x)){
# put the data in an n by J matrix
mat<-matrix(0,length(x[[1]]),length(x))
for (j in 1:length(x))mat[,j]<-x[[j]]
}
if(is.matrix(x))mat<-x
if(!is.na(grp[1])){
mat<-mat[,grp]
}
FLAG=FALSE
if(ncol(mat)<3)FLAG=TRUE
#if(ncol(mat)<3)stop('This function is for three or more measures of location')
mat<-elimna(mat) # Remove rows with missing values.
J<-ncol(mat)
jp<-0
Jall<-(J^2-J)/2
dif<-matrix(NA,nrow=nrow(mat),ncol=Jall)
ic<-0
for(j in 1:J){
for(k in 1:J){
if(j<k){
ic<-ic+1
dif[,ic]<-mat[,j]-mat[,k]
}}}
dif<-as.matrix(dif)
if(SEED)set.seed(2) # set seed of random number generator so that
#             results can be duplicated.
data <- matrix(sample(nrow(mat), size = nrow(mat) * nboot, replace = T),
                nrow = nboot)
if(!FLAG){
bvec <- matrix(NA, ncol = ncol(dif), nrow = nboot)
        for(i in 1:nboot) {
                bvec[i, ] <- est(dif[data[i,],],...)$center
        }  #bvec is an nboot by Jm matrix
center<-est(dif,...)$center
bcen<-apply(bvec,2,mean)
cmat<-var(bvec-bcen+center)
zvec<-rep(0,Jall)
m1<-rbind(bvec,zvec)
bplus<-nboot+1
discen<-mahalanobis(m1,center,cmat)
sig.level<-sum(discen[bplus]<=discen)/bplus
}
if(FLAG){
bvec=matrix(NA,ncol=ncol(mat),nrow=nboot)
for(i in 1:nboot)bvec[i, ]=est(mat[data[i,],],...)$center
pv=mean(bvec[,1]<bvec[,2])+.5*mean(bvec[,1]==bvec[,2])
sig.level=2*min(c(pv,1-pv))
center=est(mat,...)$center
}
list(p.value=sig.level,center=center)
}
rmdzeroGMC<-function(x,est=skipSPR,grp=NA,nboot=500,SEED=TRUE,...){
#
#   Do ANOVA on dependent groups
#   using #   depth of zero among  bootstrap values
#   based on difference scores.
#
#    Like rmdzeroG, only designed to used a multicore processor
#
#   The data are assumed to be stored in x in list mode
#   or in a matrix. In the first case
#   x[[1]] contains the data for the first group, x[[2]] the data
#   for the second group, etc. Length(x)=the number of groups = J.
#   If stored in a matrix, columns correspond to groups.
#
#   grp is used to specify some subset of the groups, if desired.
#   By default, all J groups are used.
#
#   The default number of bootstrap samples is nboot=500
#
if(!is.list(x) && !is.matrix(x))stop('Data must be stored in a matrix or in list mode.')
if(is.list(x)){
# put the data in an n by J matrix
mat<-matrix(0,length(x[[1]]),length(x))
for (j in 1:length(x))mat[,j]<-x[[j]]
}
if(is.matrix(x))mat<-x
if(!is.na(grp[1])){
mat<-mat[,grp]
}
FLAG=FALSE
if(ncol(mat)<3)FLAG=TRUE
#stop('This function is for three or more measures of location')
mat<-elimna(mat) # Remove rows with missing values.
J<-ncol(mat)
jp<-0
Jall<-(J^2-J)/2
dif<-matrix(NA,nrow=nrow(mat),ncol=Jall)
ic<-0
for(j in 1:J){
for(k in 1:J){
if(j<k){
ic<-ic+1
dif[,ic]<-mat[,j]-mat[,k]
}}}
dif<-as.matrix(dif)
if(SEED)set.seed(2) # set seed of random number generator so that
#             results can be duplicated.
data <- matrix(sample(nrow(mat), size = nrow(mat) * nboot, replace = T),
                nrow = nboot)
data=listm(t(data))
if(!FLAG){
bvec<-mclapply(data,rmdG_sub,dif,est,mc.preschedule=TRUE,...)
bvec=t(matl(bvec))
center<-est(dif,...)$center
bcen<-apply(bvec,2,mean)
cmat<-var(bvec-bcen+center)
zvec<-rep(0,Jall)
m1<-rbind(bvec,zvec)
bplus<-nboot+1
discen<-mahalanobis(m1,center,cmat)
sig.level<-sum(discen[bplus]<=discen)/bplus
}
if(FLAG){
bvec<-mclapply(data,rmdG_sub,mat,est,mc.preschedule=TRUE,...)
bvec=t(matl(bvec))
pv=mean(bvec[,1]<bvec[,2])+.5*mean(bvec[,1]==bvec[,2])
sig.level=2*min(c(pv,1-pv))
center=est(mat,...)$center
}
list(p.value=sig.level,center=center)
}

rmdG_sub<-function(data,dif,est,...){
v=est(dif[data,],...)$center
v
}
rmdzeroOP<-function(x, nboot = 500, cop=3,MC = FALSE,xlab="",ylab="",STAND=TRUE,SEED=TRUE,...){
#
#   Form all pairwise differences of scores then test
#   the hypothesis that all differences have OP measure of location = 0
#
#   The data are assumed to be stored in x in list mode
#   or in a matrix. In the first case
#   x[[1]] contains the data for the first group, x[[2]] the data
#   for the second group, etc. Length(x)=the number of groups = J.
#   If stored in a matrix, columns correspond to groups.
#
#   grp is used to specify some subset of the groups, if desired.
#   By default, all J groups are used.
#
#   The default number of bootstrap samples is nboot=500
#
if(!is.list(x) && !is.matrix(x))
stop("Data must be stored in a matrix or in list mode.")
if(is.list(x)){
# put the data in an n by J matrix
mat<-matrix(0,length(x[[1]]),length(x))
for (j in 1:length(x))mat[,j]<-x[[j]]
}
if(is.matrix(x))mat<-x
mat<-elimna(mat) # Remove rows with missing values.
J<-ncol(mat)
jp<-0
Jall<-(J^2-J)/2
dif<-matrix(NA,nrow=nrow(mat),ncol=Jall)
ic<-0
for(j in 1:J){
for(k in 1:J){
if(j<k){
ic<-ic+1
dif[,ic]<-mat[,j]-mat[,k]
}}}
dif<-as.matrix(dif)
res=smeancrv2(dif, nboot = nboot, cop=cop,plotit = plotit,
 MC = MC, xlab = xlab,ylab = ylab,STAND=STAND,SEED=SEED)
res
}




# ----------------------------------------------------------------------------

# rmES.dif.pro

# ----------------------------------------------------------------------------

rmES.dif.pro<-function(x,est=tmean,...){
#
#  Global measure of effect size,
#   based on difference scores,
#  relative to the  null distribution
#
if(is.list(x))x=matl(x)
x=elimna(x)
n=nrow(x)
n1=n+1
J=ncol(x)
ALL=(J^2-J)/2
M=matrix(NA,nrow=n,ncol=ALL)
ic=0
for(j in 1:J){
for(k in 1:J){
if(j<k){
ic=ic+1
M[,ic]=x[,j]-x[,k]
}}}
ES=apply(M,2,est,...)
Z=rep(0,ALL)
dis=pdis(M,ES,center=Z)
dis
}




# ----------------------------------------------------------------------------

# rmES.pro

# ----------------------------------------------------------------------------

rmES.pro<-function(x,est=tmean,PV=FALSE,
                   SEED=TRUE, ND=NULL,iter=2000,...){
  #
  #  Measure of effect size based on the projection distance
  #  between the estimate of location and the estimate of the grand mean
  #
  if(is.list(x))x=matl(x)
  x=elimna(x)
  n=nrow(x)
  E=apply(x,2,est,...)
  GM=mean(E)
  J=ncol(x)
  GMvec=rep(GM,J)
      GMvec=rep(GM,J)
      DN=pdis(x,E,center=GMvec)
      pv=NA
      if(PV){
  if(is.null(ND))ND=rmESPRO.null(n,J,est=est,nboot=iter,SEED=SEED,...)
      pv=mean(DN<=ND)
      }
list(effect.size=DN, p.value=pv)
}




# ----------------------------------------------------------------------------

# rmESPRO.null

# ----------------------------------------------------------------------------

rmESPRO.null<-function(n,J,est=tmean,nboot=1000,SEED=TRUE,...){
  #
  # Determine null distribution
  # for rmES.pro
  #
  if(SEED)set.seed(2)
    v=NA
    for(i in 1:nboot){
        x=rmul(n,J,g=0,h=0,rho=0)
E=apply(x,2,est,...)
  GM=mean(E)
  GMvec=rep(GM,J)
 DN=pdis(x,GMvec,center=E)
v[i]=DN
}
v
}




# ----------------------------------------------------------------------------

# rmlinQS

# ----------------------------------------------------------------------------

rmlinQS<-function(x, con = NULL, locfun=median){
#
#  Dependent groups:
#  For each linear contrast, compute quantile shift effect size based on the linear sum
#
if(!is.matrix(x))x<-matl(x)
if(!is.matrix(x))stop('Data must be stored in a matrix or in list mode.')
J=ncol(x)
if(is.null(con)){
C=(J^2-J)/2
con=matrix(0,ncol=C,nrow=J)
ic=0
for(j in 1:J){
for(k in 1:J){
if(j<k){
ic=ic+1
con[j,ic]=1
con[k,ic]=-1
}}}}
x=elimna(x)
n=nrow(x)
ES=NA
for (d in 1:ncol(con)){
ES[d]=lindQS(x,con=con[,d],locfun=locfun)$Q.effect
}
list(con=con,Effect.Size=ES)
}


rmm.dif<-function(x, tr = 0.2, alpha = 0.05,BH=FALSE,ADJ.CI=FALSE){
#
#  Dependent groups
# Pairwise comparisons,  trimmed means based on difference scores
# ADJ.CI=TRUE: Confidence interval adjusted based on Hochberg  or, if BH=TRUE, Benjamini--Hochberg
#
if(!is.matrix(x))x<-matl(x)
if(!is.matrix(x))stop('Data must be stored in a matrix or in list mode.')
J<-ncol(x)
L=(J^2-J)/2
psihat<-matrix(0,L,5)
testt<-matrix(0,L,4)
dimnames(psihat)<-list(NULL,c('Group','Group','est','ci.lower','ci.upper'))
test<-matrix(NA,L,4)
dimnames(test)<-list(NULL,c('Group','Group','p.value','p.adjust'))
jcom<-0
for (j in 1:J){
for (k in 1:J){
if (j < k){
jcom<-jcom+1
test[jcom,1]<-j
test[jcom,2]<-k
psihat[jcom,1]<-j
psihat[jcom,2]<-k
a=trimci(x[,j]-x[,k],tr=tr,pr=FALSE)
psihat[jcom,3]=a$estimate
test[jcom,1]<-j
test[jcom,2]<-k
test[jcom,3]<-a$p.value
psihat[jcom,4]=a$ci[1]
psihat[jcom,5]=a$ci[2]
}}}
if(ADJ.CI){
ior=rev(rank(test[,3]))
adj=alpha/c(1:L) #Hoch
if(BH)adj=alpha*(L-1:L+1)
#
#    Adjust confidence intervals
#
jcom<-0
for (j in 1:J){
for (k in 1:J){
if (j < k){
jcom<-jcom+1
a=trimci(x[,j]-x[,k],alpha=adj[ior[[jcom]]],tr=tr,pr=FALSE)
psihat[jcom,4]=a$ci[1]
psihat[jcom,5]=a$ci[2]
}}}
}
test[,4]=p.adjust(test[,3],method='hoch')
if(BH)test[,4]=p.adjust(test[,3],method='BH')
nval=nrow(x)
list(n=nval,test=test,psihat=psihat)
}




# ----------------------------------------------------------------------------

# rmm.difpb

# ----------------------------------------------------------------------------

rmm.difpb<-function(x, est=tmean, alpha = 0.05,nboot=NA,SEED=TRUE,BH=FALSE,ADJ.CI=FALSE,...){
#
#  Dependent groups
# Pairwise comparisons,  trimmed means based on difference scores
# ADJ.CI=TRUE: Confidence interval adjusted based on Hochberg  or, if BH=TRUE, Benjamini--Hochberg
#
if(!is.matrix(x))x<-matl(x)
if(!is.matrix(x))stop('Data must be stored in a matrix or in list mode.')
J<-ncol(x)
L=(J^2-J)/2
psihat<-matrix(0,L,5)
testt<-matrix(0,L,4)
dimnames(psihat)<-list(NULL,c('Group','Group','est','ci.lower','ci.upper'))
test<-matrix(NA,L,4)
dimnames(test)<-list(NULL,c('Group','Group','p.value','p.adjust'))
jcom<-0
for (j in 1:J){
for (k in 1:J){
if (j < k){
jcom<-jcom+1
test[jcom,1]<-j
test[jcom,2]<-k
psihat[jcom,1]<-j
psihat[jcom,2]<-k
a=rmmcppbd(x[,j],x[,k],est=est,alpha=alpha,nboot=nboot,SEED=SEED,plotit=FALSE,...)$output
psihat[jcom,3]=a[2]
test[jcom,1]<-j
test[jcom,2]<-k
test[jcom,3]<-a[3]
psihat[jcom,4]=a[5]
psihat[jcom,5]=a[6]
}}}
if(ADJ.CI){
ior=rev(rank(test[,3]))
adj=alpha/c(1:L) #Hoch
if(BH)adj=alpha*(L-1:L+1)
jcom<-0
for (j in 1:J){
for (k in 1:J){
if (j < k){
jcom<-jcom+1
print(adj[ior[[jcom]]])
#a=trimci(x[,j]-x[,k],alpha=adj[ior[[jcom]]],tr=tr)
#a=two.dep.pb(x[,j],x[,k],dif=TRUE,est=est,alpha=adj[ior[[jcom]]],nboot=nboot,SEED=SEED,pr=FALSE,...)
a=rmmcppbd(x[,j],x[,k],est=est,alpha=adj[ior[[jcom]]],nboot=nboot,SEED=SEED,plotit=FALSE,...)$output
psihat[jcom,4]=a[5]
psihat[jcom,5]=a[6]
}}}
}
test[,4]=p.adjust(test[,3],method='hoch')
if(BH)test[,4]=p.adjust(test[,3],method='BH')
nval=nrow(x)
list(n=nval,test=test,psihat=psihat)
}




# ----------------------------------------------------------------------------

# rmm.mar

# ----------------------------------------------------------------------------

rmm.mar<-function(x, tr = 0.2, alpha = 0.05,BH=FALSE,ADJ.CI=FALSE){
#
# Dependent groups
# Pairwise comparisons based on trimmed means of the marginal distributions
# ADJ.CI=TRUE: Confidence interval adjusted based on Hochberg  or, if BH=TRUE, Benjamini--Hochberg

if(!is.matrix(x))x<-matl(x)
if(!is.matrix(x))stop('Data must be stored in a matrix or in list mode.')
J<-ncol(x)
L=(J^2-J)/2
psihat<-matrix(0,L,7)
testt<-matrix(0,L,4)
dimnames(psihat)<-list(NULL,c('Group','Group','est 1','est 2','dif','ci.lower','ci.upper'))
test<-matrix(NA,L,4)
dimnames(test)<-list(NULL,c('Group','Group','p.value','p.adjust'))
jcom<-0
for (j in 1:J){
for (k in 1:J){
if (j < k){
jcom<-jcom+1
test[jcom,1]<-j
test[jcom,2]<-k
psihat[jcom,1]<-j
psihat[jcom,2]<-k
a=yuend(x[,j],x[,k],tr=tr)
psihat[jcom,3]=a$est1
psihat[jcom,4]=a$est2
psihat[jcom,5]=a$dif
test[jcom,1]<-j
test[jcom,2]<-k
test[jcom,3]<-a$p.value
psihat[jcom,6]=a$ci[1]
psihat[jcom,7]=a$ci[2]
}}}
if(ADJ.CI){
ior=order(0-test[,3])
adj=alpha/c(1:L) #Hoch
if(BH)adj=alpha*(L-1:L+1)
#
#    Adjust confidence intervals
#
jcom<-0
for (j in 1:J){
for (k in 1:J){
if (j < k){
jcom<-jcom+1
print(adj[ior[[jcom]]])
a=yuend(x[,j],x[,k],alpha=adj[ior[[jcom]]],tr=tr)
psihat[jcom,6]=a$ci[1]
psihat[jcom,7]=a$ci[2]
}}}
}
test[,4]=p.adjust(test[,3],method='hoch')
if(BH)test[,4]=p.adjust(test[,3],method='BH')
nval=nrow(x)
list(n=nval,test=test,psihat=psihat)
}




# ----------------------------------------------------------------------------

# rmm.marpb

# ----------------------------------------------------------------------------

rmm.marpb<-function(x, est=tmean, alpha = 0.05,nboot=NA,BH=FALSE,SEED=TRUE,ADJ.CI=FALSE,...){
#
# Dependent groups
# Pairwise comparisons based on trimmed means of the marginal distributions
# ADJ.CI=TRUE: Confidence interval adjusted based on Hochberg  or, if BH=TRUE, Benjamini--Hochberg
#
if(!is.matrix(x))x<-matl(x)
if(!is.matrix(x))stop('Data must be stored in a matrix or in list mode.')
J<-ncol(x)
if(nrow(x)<80){
if(identical(est,mom))stop('Use rmmcppb with argument BA=TRUE')
if(identical(est,onestep))stop('Use rmmcppb with argument BA=TRUE')
}
L=(J^2-J)/2
psihat<-matrix(0,L,7)
testt<-matrix(0,L,4)
dimnames(psihat)<-list(NULL,c('Group','Group','est 1','est 2','dif','ci.lower','ci.upper'))
test<-matrix(NA,L,4)
dimnames(test)<-list(NULL,c('Group','Group','p.value','p.adjust'))
jcom<-0
for (j in 1:J){
for (k in 1:J){
if (j < k){
jcom<-jcom+1
test[jcom,1]<-j
test[jcom,2]<-k
psihat[jcom,1]<-j
psihat[jcom,2]<-k
#a=yuend(x[,j],x[,k],tr=tr)
a=two.dep.pb(x[,j],x[,k],dif=FALSE,est=est,nboot=nboot,SEED=SEED,pr=FALSE,...)
psihat[jcom,3]=a[1]
psihat[jcom,4]=a[2]
psihat[jcom,5]=a[3]
test[jcom,1]<-j
test[jcom,2]<-k
test[jcom,3]<-a[4]
psihat[jcom,6]=a[5]
psihat[jcom,7]=a[6]
}}}
if(ADJ.CI){
ior=order(0-test[,3])
adj=alpha/c(1:L) #Hoch
if(BH)adj=alpha*(L-1:L+1)
#
#. Next, adjust the confidence intervals
#
jcom<-0
for (j in 1:J){
for (k in 1:J){
if (j < k){
jcom<-jcom+1
#a=yuend(x[,j],x[,k],alpha=adj[ior[[jcom]]],tr=tr)
a=two.dep.pb(x[,j],x[,k],dif=FALSE,est=est,alpha=adj[ior[[jcom]]],nboot=nboot,SEED=SEED,pr=FALSE,...)
psihat[jcom,6]=a[5]
psihat[jcom,7]=a[6]
}}}
}
test[,4]=p.adjust(test[,3],method='hoch')
if(BH)test[,4]=p.adjust(test[,3],method='BH')
nval=nrow(x)
list(n=nval,test=test,psihat=psihat)
}




# ----------------------------------------------------------------------------

# rmmest

# ----------------------------------------------------------------------------

rmmest<-function(x,y=NA,alpha=.05,con=0,est=onestep,plotit=TRUE,dif=FALSE,grp=NA,
hoch=FALSE,nboot=NA,BA=TRUE,xlab="Group 1",ylab="Group 2",pr=TRUE,...){
#
#   Use a percentile bootstrap method to  compare dependent groups.
#   By default,
#   compute a .95 confidence interval for all linear contasts
#   specified by con, a J by C matrix, where  C is the number of
#   contrasts to be tested, and the columns of con are the
#   contrast coefficients.
#   If con is not specified, all pairwise comparisons are done.
#
#   By default, a one-step M-estimator is used
#   and a sequentially rejective method
#   is used to control the probability of at least one Type I error.
#
#   dif=T indicates that difference scores are to be used
#   dif=F indicates that measure of location associated with
#   marginal distributions are used instead.
#
#   nboot is the bootstrap sample size. If not specified, a value will
#   be chosen depending on the number of contrasts there are.
#
#   x can be an n by J matrix or it can have list mode
#   for two groups, data for second group can be put in y
#   otherwise, assume x is a matrix (n by J) or has list mode.
#
#   A sequentially rejective method is used to control alpha.
#
#   Argument BA: When using dif=F, BA=T uses a correction term
#  that is recommended when using MOM.
#
if(dif){
if(pr)print("dif=T, so analysis is done on difference scores")
temp<-rmmcppbd(x,y=y,alpha=.05,con=con,est,plotit=plotit,grp=grp,
nboot=nboot,hoch=hoch,...)
output<-temp$output
con<-temp$con
}
if(!dif){
if(pr)print("dif=F, so analysis is done on marginal distributions")
if(!is.na(y[1]))x<-cbind(x,y)
if(!is.list(x) && !is.matrix(x))stop("Data must be stored in a matrix or
in list mode.")
if(is.list(x)){
if(is.matrix(con)){
if(length(x)!=nrow(con))stop("The number of rows in con is not equal to the
number of groups.")
}}
if(is.list(x)){
# put the data in an n by J matrix
mat<-matl(x)
}
if(is.matrix(x) && is.matrix(con)){
if(ncol(x)!=nrow(con))stop("The number of rows in con is not equal to the
number of groups.")
mat<-x
}
n=nrow(x)
if(is.matrix(x))mat<-x
if(!is.na(sum(grp)))mat<-mat[,grp]
mat<-elimna(mat) # Remove rows with missing values.
x<-mat
J<-ncol(mat)
xcen<-x
for(j in 1:J)xcen[,j]<-x[,j]-est(x[,j])
Jm<-J-1
if(sum(con^2)==0){
d<-(J^2-J)/2
con<-matrix(0,J,d)
id<-0
for (j in 1:Jm){
jp<-j+1
for (k in jp:J){
id<-id+1
con[j,id]<-1
con[k,id]<-0-1
}}}
d<-ncol(con)
if(is.na(nboot)){
if(d<=4)nboot<-1000
if(d>4)nboot<-5000
}
n<-nrow(mat)
crit.vec<-alpha/c(1:d)
connum<-ncol(con)
set.seed(2) # set seed of random number generator so that
#             results can be duplicated.
xbars<-apply(mat,2,est)
psidat<-NA
for (ic in 1:connum)psidat[ic]<-sum(con[,ic]*xbars)
psihat<-matrix(0,connum,nboot)
psihatcen<-matrix(0,connum,nboot)
bvec<-matrix(NA,ncol=J,nrow=nboot)
bveccen<-matrix(NA,ncol=J,nrow=nboot)
print("Taking bootstrap samples. Please wait.")
data<-matrix(sample(n,size=n*nboot,replace=TRUE),nrow=nboot)
for(ib in 1:nboot){
bvec[ib,]<-apply(x[data[ib,],],2,est,...)
bveccen[ib,]<-apply(xcen[data[ib,],],2,est,...)
}
#
# Now have an nboot by J matrix of bootstrap values.
#
test<-1
bias<-NA
for (ic in 1:connum){
psihat[ic,]<-apply(bvec,1,bptdpsi,con[,ic])
psihatcen[ic,]<-apply(bveccen,1,bptdpsi,con[,ic])
bias[ic]<-sum((psihatcen[ic,]>0))/nboot-.5
if(BA){
test[ic]<-sum((psihat[ic,]>0))/nboot-.1*bias[ic]
if(test[ic]<0)test[ic]<-0
}
if(!BA)test[ic]<-sum((psihat[ic,]>0))/nboot
test[ic]<-min(test[ic],1-test[ic])
}
test<-2*test
ncon<-ncol(con)
if(alpha==.05){
dvec<-c(.025,.025,.0169,.0127,.0102,.00851,.0073,.00639,.00568,.00511)
dvecba<-c(.05,.025,.0169,.0127,.0102,.00851,.0073,.00639,.00568,.00511)
if(ncon > 10){
avec<-.05/c(11:ncon)
dvec<-c(dvec,avec)
}}
if(alpha==.01){
dvec<-c(.005,.005,.00334,.00251,.00201,.00167,.00143,.00126,.00112,.00101)
dvecba<-c(.01,.005,.00334,.00251,.00201,.00167,.00143,.00126,.00112,.00101)
if(ncon > 10){
avec<-.01/c(11:ncon)
dvec<-c(dvec,avec)
}}
if(alpha != .05 && alpha != .01){
dvec<-alpha/c(1:ncon)
dvecba<-dvec
dvec[1]<-alpha/2
}
if(n>=80)hoch=T
if(hoch)dvec<-alpha/(c(1:ncon))
if(plotit && ncol(bvec)==2){
z<-c(0,0)
one<-c(1,1)
plot(rbind(bvec,z,one),xlab=xlab,ylab=ylab,type="n")
points(bvec)
totv<-apply(x,2,est,...)
cmat<-var(bvec)
dis<-mahalanobis(bvec,totv,cmat)
temp.dis<-order(dis)
ic<-round((1-alpha)*nboot)
xx<-bvec[temp.dis[1:ic],]
xord<-order(xx[,1])
xx<-xx[xord,]
temp<-chull(xx)
lines(xx[temp,])
lines(xx[c(temp[1],temp[length(temp)]),])
abline(0,1)
}
temp2<-order(0-test)
ncon<-ncol(con)
zvec<-dvec[1:ncon]
if(BA)zvec<-dvecba[1:ncon]
sigvec<-(test[temp2]>=zvec)
output<-matrix(0,connum,6)
dimnames(output)<-list(NULL,c("con.num","psihat","sig.level","crit.sig",
"ci.lower","ci.upper"))
tmeans<-apply(mat,2,est,...)
psi<-1
for (ic in 1:ncol(con)){
output[ic,2]<-sum(con[,ic]*tmeans)
output[ic,1]<-ic
output[ic,3]<-test[ic]
output[temp2,4]<-zvec
temp<-sort(psihat[ic,])
icl<-round(output[ic,4]*nboot/2)+1
icu<-nboot-(icl-1)
output[ic,5]<-temp[icl]
output[ic,6]<-temp[icu]
}
}
num.sig<-sum(output[,3]<=output[,4])
list(output=output,con=con,num.sig=num.sig)
}

RMPB.PMD.PCD<-function(x,est=tmean,delta=.5,alpha=.05,iter=500,nboot=1000,SEED=TRUE,...){
#
#
#  Use an indifference zone. Given x
#  determine the
#  probability of making a decision and the probability of
#  of correct decision given that a decision is made
# within the context of an indifference zone
#
if(SEED)set.seed(2)
if(is.list(x))stop('x should be a matrix or a data frame')
PMD=0
PCD=0
x=elimna(x)
n=nrow(x)
E=apply(x,2,est,...)
ID=which(E==max(E))
#
A=cov(x)
J=ncol(x)
#
# Now simulate indifference zone.
for(i in 1:iter){
x=mvrnorm(n,mu=rep(0,J),Sigma=A)
x[,1]=x[,1]+delta*sqrt(A[ID,ID])
e=apply(x,2,est,...)
id=which(e==max(e))
a=rmanc.best.PB(x,est=est,nboot=nboot,SEED=FALSE,...)
if(sum(a$p.value<=alpha)){
PMD=PMD+1
if(id==1)PCD=PCD+1
}}
PMD.ci=binom.conf(PMD,iter,pr=FALSE)$ci
PCD.ci=binom.conf(PCD,PMD,pr=FALSE)$ci
PCD=PCD/max(PMD,1)
PMD=PMD/iter
list(PMD=PMD,PMD.ci=PMD.ci,PCD=PCD,PCD.ci=PCD.ci)
}




# ----------------------------------------------------------------------------

# rmrvar

# ----------------------------------------------------------------------------

rmrvar<-function(x,y=NA,alpha=.05,con=0,est=pbvar,plotit=FALSE,grp=NA,
hoch=TRUE,nboot=NA,xlab="Group 1",ylab="Group 2",pr=TRUE,SEED=TRUE,...){
#
#   Use a percentile bootstrap method to compare dependent groups.
#   based on some robust measure of variation indicated by the argument
#   est
#   By default, est=pbvar, the percentage bend midvariance.
#
#   The function computes a .95 confidence interval for all linear contrasts
#   specified by con, a J by C matrix, where  C is the number of
#   contrasts to be tested, and the columns of con are the
#   contrast coefficients.
#   If con is not specified, all pairwise comparisons are done.
#
#   nboot is the bootstrap sample size. If not specified, a value will
#   be chosen depending on the number of contrasts there are.
#
#   x can be an n by J matrix or it can have list mode
#   for two groups, data for second group can be put in y
#   otherwise, assume x is a matrix (n by J) or has list mode.
#
#   Hochberg's  sequentially rejective method is used to control alpha.
#
if(!is.na(y[1]))x=cbind(x,y)
if(is.list(x)){
# put the data in an n by J matrix
mat<-matl(x)
}
if(is.matrix(x) && is.matrix(con)){
if(ncol(x)!=nrow(con))stop("The number of rows in con is not equal to the
number of groups.")
mat<-x
}
if(is.matrix(x))mat<-x
if(!is.na(sum(grp)))mat<-mat[,grp]
mat<-elimna(mat) # Remove rows with missing values.
x<-mat
J<-ncol(mat)
Jm<-J-1
if(sum(con^2)==0){
d<-(J^2-J)/2
con<-matrix(0,J,d)
id<-0
for (j in 1:Jm){
jp<-j+1
for (k in jp:J){
id<-id+1
con[j,id]<-1
con[k,id]<-0-1
}}}
d<-ncol(con)
if(is.na(nboot)){
if(d<=4)nboot<-1000
if(d>4)nboot<-5000
}
n<-nrow(mat)
crit.vec<-alpha/c(1:d)
connum<-ncol(con)
if(SEED)set.seed(2) # set seed of random number generator so that
#             results can be duplicated.
xbars<-apply(mat,2,est)
psidat<-NA
for (ic in 1:connum)psidat[ic]<-sum(con[,ic]*xbars)
psihat<-matrix(0,connum,nboot)
bvec<-matrix(NA,ncol=J,nrow=nboot)
print("Taking bootstrap samples. Please wait.")
data<-matrix(sample(n,size=n*nboot,replace=TRUE),nrow=nboot)
for(ib in 1:nboot){
bvec[ib,]<-apply(x[data[ib,],],2,est,...)
}
#
# Now have an nboot by J matrix of bootstrap values.
#
test<-1
bias<-NA
for (ic in 1:connum){
psihat[ic,]<-apply(bvec,1,bptdpsi,con[,ic])
test[ic]<-sum((psihat[ic,]>0))/nboot
test[ic]<-min(test[ic],1-test[ic])
}
test<-2*test
ncon<-ncol(con)
if(alpha==.05){
dvec<-c(.025,.025,.0169,.0127,.0102,.00851,.0073,.00639,.00568,.00511)
dvecba<-c(.05,.025,.0169,.0127,.0102,.00851,.0073,.00639,.00568,.00511)
if(ncon > 10){
avec<-.05/c(11:ncon)
dvec<-c(dvec,avec)
}}
if(alpha==.01){
dvec<-c(.005,.005,.00334,.00251,.00201,.00167,.00143,.00126,.00112,.00101)
dvecba<-c(.01,.005,.00334,.00251,.00201,.00167,.00143,.00126,.00112,.00101)
if(ncon > 10){
avec<-.01/c(11:ncon)
dvec<-c(dvec,avec)
}}
#if(hoch)dvec<-alpha/(2* c(1:ncon))
#dvec<-2*dvec
if(alpha != .05 && alpha != .01){
dvec<-alpha/c(1:ncon)
dvecba<-dvec
dvec[1]<-alpha/2
}
if(hoch)dvec<-alpha/(c(1:ncon))
if(plotit && ncol(bvec)==2){
z<-c(0,0)
one<-c(1,1)
plot(rbind(bvec,z,one),xlab=xlab,ylab=ylab,type="n")
points(bvec)
totv<-apply(x,2,est,...)
cmat<-var(bvec)
dis<-mahalanobis(bvec,totv,cmat)
temp.dis<-order(dis)
ic<-round((1-alpha)*nboot)
xx<-bvec[temp.dis[1:ic],]
xord<-order(xx[,1])
xx<-xx[xord,]
temp<-chull(xx)
lines(xx[temp,])
lines(xx[c(temp[1],temp[length(temp)]),])
abline(0,1)
}
temp2<-order(0-test)
ncon<-ncol(con)
zvec<-dvec[1:ncon]
sigvec<-(test[temp2]>=zvec)
output<-matrix(0,connum,6)
dimnames(output)<-list(NULL,c("con.num","est.var","p.value","crit.p.value",
"ci.lower","ci.upper"))
tmeans<-apply(mat,2,est,...)
psi<-1
for (ic in 1:ncol(con)){
output[ic,2]<-sum(con[,ic]*tmeans)
output[ic,1]<-ic
output[ic,3]<-test[ic]
output[temp2,4]<-zvec
temp<-sort(psihat[ic,])
icl<-round(output[ic,4]*nboot/2)+1
icu<-nboot-(icl-1)
output[ic,5]<-temp[icl]
output[ic,6]<-temp[icu]
}
num.sig<-sum(output[,3]<=output[,4])
list(output=output,con=con,num.sig=num.sig)
}




# ----------------------------------------------------------------------------

# RMSAE

# ----------------------------------------------------------------------------

RMSAE<-function(x) sqrt(sum(x^2)/(length(x)))




# ----------------------------------------------------------------------------

# rmul.MAR

# ----------------------------------------------------------------------------

rmul.MAR<-function(n,p=2,g=rep(0,p),h=rep(0,p),rho=0,cmat=NULL){
#
# Generate multivariate normal data and transform the marginal
#  distributions to g-and-h distributions
#
if(!is.null(cmat)){
if(ncol(cmat)!=p)stop('cmat: number of  columns must equal the value in the argument p')
}
if(abs(rho)>1)stop('rho must be between -1 and 1')
if(is.null(cmat)){
cmat<-matrix(rho,p,p)
diag(cmat)<-1
}
if(length(g)!=p)stop('Length of g should equal p')
if(length(h)!=p)stop('Length of h should equal p')
x=mvrnorm(n,rep(0,p),cmat)
for(j in 1:p){
if(g[j]==0)x[,j]=x[,j]*exp(h[j]*x[,j]^2/2)
if(g[j]>0)x[,j]=(exp(g[j]*x[,j])-1)*exp(h[j]*x[,j]^2/2)/g[j]
}
x
}

lnormsd=function()sqrt(exp(1))*sqrt(exp(1)-1)  #standard deviation of a lognormal distribution.




# ----------------------------------------------------------------------------

# rmulnorm

# ----------------------------------------------------------------------------

rmulnorm<-function(n,p,cmat,SEED=FALSE){
#
# Generate data from a multivariate normal
# n= sample size
# p= number of variables
# cmat is the covariance (or correlation) matrix
#
# Method (e.g. Browne, M. W. (1968) A comparison of factor analytic
# techniques. Psychometrika, 33, 267-334.
#  Let U'U=R be the Cholesky decomposition of R. Generate independent data
#  from some dist yielding X. Then XU has population correlation matrix R
#
if(SEED)set.seed(2)
y<-matrix(rnorm(n*p),ncol=p)
rval<-matsqrt(cmat)
y<-t(rval%*%t(y))
y
}

 matsqrt <- function(x) {
       xev1<-NA
         xe <- eigen(x)
         xe1 <- xe$values
         if(all(xe1 >= 0)) {
             xev1 <- diag(sqrt(xe1))
         }
if(is.na(xev1[1]))stop("The matrix has negative eigenvalues")
         xval1 <- cbind(xe$vectors)
         xval1i <- solve(xval1)
         y <- xval1 %*% xev1 %*% xval1i
y
 }




# ----------------------------------------------------------------------------

# rmVARcom

# ----------------------------------------------------------------------------

rmVARcom<-function(x,y=NULL,alpha=.05,est=bivar,plotit=TRUE,nboot=500,SEED=TRUE,...){
#
#   Use a percentile bootstrap method to  compare dependent groups.
#   based on some robust measure of variation.
#
#  if y=NULL, assume x is a matrix or data frame with two columns.
#
#
#   nboot is the number of bootstrap samples.
#
#
if(!is.null(y[1]))x<-cbind(x,y)
if(ncol(x)>2)stop('x should have at most two columns')
x=elimna(x)
n=nrow(x)
if(SEED)set.seed(2) # set seed of random number generator so that
#             results can be duplicated.
data<-matrix(sample(n,size=n*nboot,replace=TRUE),nrow=nboot)
bvec=matrix(NA,nboot,2)
for(ib in 1:nboot){
bvec[ib,]<-apply(x[data[ib,],],2,est,...)
}
#
# Now have an nboot by 2 matrix of bootstrap values.
#
pstar=mean(bvec[,1]<bvec[,2])+.5*mean(bvec[,1]==bvec[,2])
pv=2*min(c(pstar,1-pstar))
EST=apply(x,2,est,...)
dif=bvec[,1]-bvec[,2]
dif=sort(dif)
low=round(alpha*nboot/2)
up=nboot-low
low=low+1
ci=c(dif[low],dif[up])
list(n=n,estimates=EST,dif=EST[1]-EST[2],ci=ci,p.value=pv)
}




# ----------------------------------------------------------------------------

# rmVARcom.mcp

# ----------------------------------------------------------------------------

rmVARcom.mcp<-function(x,est=winvar,alpha=.05,nboot=500,method='hoch',SEED=TRUE){
#
#  Compare the variances of J dependent variables.
#  Perform all pairwise comparisons using the HC4 extension of the Morgan-Pitman test
#
if(is.null(dim(x)))stop('x should be a matrix or data frame')
J=ncol(x)
CC=(J^2-J)/2
output<-matrix(0,CC,9)
dimnames(output)<-list(NULL,c('Var','Var','Est. 1','Est 2','Dif','cilow','ci.up','p.value','Adj.p.value'))
ic=0
for (j in 1:J){
for (k in 1:J){
if (j < k){
ic=ic+1
a=rmVARcom(x[,j],x[,k],est=est,alpha=alpha,nboot=nboot,plotit=FALSE,SEED=SEED)
output[ic,1]=j
output[ic,2]=k
output[ic,3:4]=a$estimate
output[ic,5]=a$dif
output[ic,6]=a$ci[1]
output[ic,7]=a$ci[2]
output[ic,8]=a$p.value
}}}
output[,9]=p.adjust(output[,8],method=method)
output
}

rngh<-function(n,rho=0,p=2,g=0,h=0,ADJ=TRUE,pr=TRUE){
#
# Generate data from a multivariate distribution where the marginal distributions
#  are g-and-h distributions that have common correlation rho.
# Strategy: adjust the correlation when generating data from multivariate normal  normal distribution so that
# when transforming the marginal distributions to a g-and-h distribution, the correlation is rho.
#
#
if(ADJ){
adjrho=rngh.sub(n,g,h,rho)$rho.adjusted
rho=adjrho
if(pr)print(paste('Adjusted rho',rho))
}
cmat<-matrix(rho,p,p)
diag(cmat)<-1
x=mvrnorm(n=n, mu=rep(0,p), Sigma=cmat)
for(i in 1:p){
if (g>0){
x[,i]<-(exp(g*x[,i])-1)*exp(h*x[,i]^2/2)/g
}
if(g==0)x[,i]<-x[,i]*exp(h*x[,i]^2/2)
}
x
}


rngh.sub<-function(n,g,h,rho,ntest=1000000){
#
# Determine adjusted value for rho so that
# the actual correlation is some desired value
#
#  rho: desired correlation
vals=seq(rho,.99,.01)
for(i in 1:length(vals)){
adj=vals[i]
cmat<-matrix(vals[i],2,2)
diag(cmat)<-1
x=mvrnorm(ntest,mu=c(0,0),Sigma=cmat)
for(i in 1:2){
if (g>0){
x[,i]<-(exp(g*x[,i])-1)*exp(h*x[,i]^2/2)/g
}
if(g==0)x[,i]<-x[,i]*exp(h*x[,i]^2/2)
}
chk=cor(x)
if(abs(chk[1,2]-rho)<.01)break
if(chk[1,2]>=rho)break
}
list(rho.adjusted=adj,rho.actual=chk[1,2])
}
rob.ridge<-function(x,y,Regfun=tsreg,k=NULL,xout=FALSE,outfun=outpro,plotit=FALSE,MSF=TRUE,
STAND=TRUE,INT=FALSE,locfun=median,...){
#
# Do robust regression based on the robust estimator indicated by
#   Regfun
# which defaults to Theil--Sen
#
#  When MSF=FALSE, the bias parameter, k, is estimated based on results in
#  Kidia, G. (2003).
# Performance of Some New Ridge
# Regression Estimators Communications in  Statistics
  # Simulation and Computation
  #Vol. 32, No. 2, pp. 419-435 (in file CommB2003.pdf)
#  The method was derived when using OLS.
# 
# MSF=TRUE, use the method in Shabbir et al.(2032). doi.org/10.1080/03610918.2023.2186874
#
#  For results on the LTS version see
# Kan et al. 2013, J Applied Statistics, 40, 644-655
#  However, suggest using this function when testing hypotheses in conjunction with regci or regciMC
#
x=as.matrix(x)
xy=elimna(cbind(x,y))
x=as.matrix(x)
n=length(y)
p=ncol(x)
p1=p+1
x=xy[,1:p]
y=xy[,p1]
x=as.matrix(x)
if(xout){
if(identical(outfun,outblp))flag=outblp(x,y,plotit=FALSE)$keep
else flag<-outfun(x,plotit=plotit)$keep                            
x<-x[flag,]                                             
x<-as.matrix(x)                                                    
y<-y[flag]                                                                    
}
if(STAND){
x=standm(x)
y=y-mean(y)
}       
if(is.null(k)){
if(!MSF)k=ridge.est.k(x,y,regfun=Regfun,...)
else{ 
ires=Regfun(x,y)$residuals
sigh=sqrt(sum(ires^2)/(n-p-1))
k=p^(1+1/p)*sigh
}
}
n=nrow(x)
init=Regfun(x,y,...)$coef
y=as.matrix(y)
if(!INT){
kbeta=diag(k,nrow=p,ncol=p)
slopes=as.matrix(init[2:p1])
xtx=t(x)%*%x
beta=solve(xtx+kbeta)%*%xtx
beta=beta%*%slopes 
res=y-x%*%beta
b0=locfun(res)
beta=as.vector(beta)
beta=c(b0,beta)
}
if(INT){
kbeta=diag(k,nrow=p1,ncol=p1)
slopes=as.matrix(init)
x1=cbind(rep(1,n),x)
xtx=t(x1)%*%x1
beta=solve(xtx+kbeta)%*%xtx
beta=beta%*%slopes 
}
res<-y-x%*%beta[2:p1]-beta[1]
list(coef=as.vector(beta),k=k,residuals=res)
}




# ----------------------------------------------------------------------------

# rob.ridge.Liu

# ----------------------------------------------------------------------------

rob.ridge.Liu<-function(x,y,k=NULL,d=NULL,OP=TRUE,Regfun=tsreg,locfun=median,xout=FALSE,outfun=outpro,
STAND=FALSE,plotit=TRUE,...){
#
# See
#Ertas, H., Kaciranlar, S. \& Guer (2017). Robust Liu-type estimator for regression based on M-estimator.
# Communications in Statistics--Simulation and Computation, 46, 3907--3932.
#
#  But here, k and d are estimated using the R function ridge.est and ridge.Liu, respectively. These estimates differ from the estimates
#  used by Ertas et al.
#
#   OP=TRUE is W2 in Ertas paper
#   OP=FALSE is W1
#
x=as.matrix(x)
xy=elimna(cbind(x,y))
x=as.matrix(x)
p=ncol(x)
p1=p+1
x=xy[,1:p]
y=xy[,p1]
x=as.matrix(x)
if(xout){
flag<-outfun(x,plotit=plotit)$keep
x<-x[flag,]
x<-as.matrix(x)
y<-y[flag]
}
if(STAND){
x=standm(x)
y=standm(y)
}
n=nrow(x)
C=t(x)%*%x
cr=eigen(C)
D=as.matrix(cr$vectors)
LAM.m=diag(cr$values)
LAM=cr$values
k=ridge.est.k(x,y)
ident=diag(1,p,p)
bm=as.matrix(Regfun(x,y)$coef[2:p1],...)
dtil=ridge.Liu(x,y)$d

term1=solve(C+k*ident)
if(!OP)est=term1%*%(C-dtil*ident)%*%bm

if(OP){
term3=C-dtil*term1%*%C
est=term1%*%term3%*%bm
}
res=y-x%*%est
Inter=locfun(res)
est=c(Inter,as.vector(est))
list(coef=est)
}

rrl.est=rob.ridge.Liu

rob.ridge.liu=rob.ridge.Liu




# ----------------------------------------------------------------------------

# rob.ridge.test

# ----------------------------------------------------------------------------

rob.ridge.test<-function(x,y,regfun=tsreg,xout=FALSE,outfun=outpro,MC=FALSE,method='hoch',
nboot=599,alpha = 0.05,MSF=TRUE,SEED=TRUE,...){
#
#
#   Test  the global hypothesis that all slopes are equal to zero,
#   If it  rejects, it also suggests  which slope is significant, but it cannot reject more than one slope.
#
#    A robust ridge estimator is used  base on the robust estimator indicated by the argument
#   regfun
#
#   MC=TRUE: takes advantage of a multicore processor.
#
p=ncol(x)
if(p==1)stop('Should have two or more independent variables')
p1=p+1
if(!MC)a=regci(x,y,regfun=rob.ridge,Regfun=regfun,xout=xout,
alpha=alpha,SEED=SEED,MSF=MSF)$regci
if(MC)a=regciMC(x,y,regfun=rob.ridge,Regfun=regfun,xout=xout,
alpha=alpha,SEED=SEED,MSF=MSF)$regci
pv=a[2:p1,5]
padj=min(p.adjust(pv,method=method))
id=NULL
if(padj<=alpha)id=which(pv==min(pv))
list(p.value=padj,a.sig.slope=id)
}

robci <- function(x, alpha = 0.05, trmp = 0.25, ka = 6, ks = 3.5
	)
{
#Gets several robust  100 (1-alpha)% CI's for data x.
#defaults are alpha = .05
   n <- length(x)
   up <- 1 - alpha/2
   	med <- median(x)
	madd <- mad(x, constant = 1)
	d <- sort(x)
	dtem <- d	## get the CI for T_A,
	LM <- sum(x < (med - ka * madd))
	nmUM <- sum(x > (med + ka * madd))
	# ll (hh) is the percentage to be trimmed to the left (right)
	ll <- ceiling((100 * LM)/n)
	hh <- ceiling((100 * (nmUM))/n)
	ln <- floor((ll * n)/100)
	un <- floor((n * (100 - hh))/100)
	low <- ln + 1
	val1 <- dtem[low]
	val2 <- dtem[un]
	tstmn <- mean(x[(x >= val1) & (x <= val2)])
	#have obtained the two stage asymmetrically trimmed mean
	if(ln > 0) {
		d[1:ln] <- d[low]
	}
	if(un < n) {
		d[(un + 1):n] <- d[un]
	}
	den <- ((un - ln)/n)^2
	swv <- var(d)/den
	#got the scaled Winsorized variance
	rdf <- un - low
	rval <- qt(up, rdf) * sqrt(swv/n)
	talo <- tstmn - rval
	tahi <- tstmn + rval
	##got low and high endpoints of robust T_A,n CI
##get robust T_S,n CI
	d <- dtem
	lo <- sum(x < (med - ks * madd))
	hi <- sum(x > (med + ks * madd))
	low <- ceiling((100 * lo)/n)
	high <- ceiling((100 * hi)/n)
	tp <- min(max(low, high)/100, 0.5)
	tstmn <- mean(x, trim = tp)
	#have obtained the two stage symetrically trimmed mean
	ln <- floor(n * tp)
	un <- n - ln
	if(ln > 0) {
		d[1:ln] <- d[(ln + 1)]
	}
	if(un < n) {
		d[(un + 1):n] <- d[un]
	}
	den <- ((un - ln)/n)^2
	swv <- var(d)/den
	#got the scaled Winsorized variance
	rdf <- un - ln - 1
	rval <- qt(up, rdf) * sqrt(swv/n)
	tslo <- tstmn - rval
	tshi <- tstmn + rval
	##got low and high endpoints of robust T_S,n CI
##get median CI that uses a scaled Winsorized variance
	d <- dtem
	lnbg <- floor(n/2) - ceiling(sqrt(n/4))
	unbg <- n - lnbg
	lowbg <- lnbg + 1
	if(lnbg > 0) {
		d[1:lnbg] <- d[(lowbg)]
	}
	if(unbg < n) {
		d[(unbg + 1):n] <- d[unbg]
	}
	den <- ((unbg - lnbg)/n)^2
	swv <- var(d)/den
	#got the scaled Winsorized variance
	rdf <- unbg - lnbg - 1
	cut <- qt(up, rdf)
	rval <- cut * sqrt(swv/n)
	rlo <- med - rval
	rhi <- med + rval
	##got median CI that uses a scaled Winsorized variance
##get BG CI
	se2 <- 0.5 * (d[unbg] - d[lowbg])
	rval <- cut * se2
	rlo2 <- med - rval
	rhi2 <- med + rval
	#got low and high endpoints of BG CI
## get classical CI
	mn <- mean(x)
	v <- var(x)
	se <- sqrt(v/n)
	val <- qt(up, n - 1) * se
	lo <- mn - val
	hi <- mn + val	##got classical CI endpoints
## get trimmed mean CI
	d <- dtem
	ln <- floor(n * trmp)
	un <- n - ln
	trmn <- mean(x, trim = trmp)
	if(ln > 0) {
		d[1:ln] <- d[(ln + 1)]
	}
	if(un < n) {
		d[(un + 1):n] <- d[un]
	}
	den <- ((un - ln)/n)^2
	swv <- var(d)/den
	#got the scaled Winsorized variance
	rdf <- un - ln - 1
	rval <- qt(up, rdf) * sqrt(swv/n)
	trlo <- trmn - rval
	trhi <- trmn + rval
	##got trimmed mean CI endpoints
	list(tint = c(lo, hi), taint = c(talo, tahi),
		tsint = c(tslo, tshi), bgint = c(rlo2,
		rhi2), mint = c(rlo, rhi), trint = c(
		trlo, trhi))
}


rrplot<-
function(x, y, nsamps = 7)
{
# Makes an RR plot. Needs the mbareg function.
	n <- length(y)
	rmat <- matrix(nrow = n, ncol = 5)
	lsres <- lsfit(x, y)$residuals
	print("got OLS")
	l1res <- l1fit(x, y)$residuals
	print("got L1")
	almsres <- lmsreg(x, y)$resid
	print("got ALMS")
	altsres <- ltsreg(x, y)$residuals
	print("got ALTS")
	out <- mba$coef
	mbacoef <- mbareg(x, y, nsamp = nsamps)$coef
	MBARES <- y - mbacoef[1] - x %*% mbacoef[-1]
	print("got MBA")
	rmat[, 1] <- lsres
	rmat[, 2] <- l1res
	rmat[, 3] <- almsres
	rmat[, 4] <- altsres
	rmat[, 5] <- MBARES
	pairs(rmat, labels = c("OLS residuals",
		"L1 residuals", "ALMS residuals",
		"ALTS residuals", "MBA residuals"))
}

rrplot2<-
function(x, y, nsamps = 7)
{
# Makes an RR plot. Needs the mbareg function.
	n <- length(y)
	rmat <- matrix(nrow = n, ncol = 4)
	lsres <- lsfit(x, y)$residuals
	print("got OLS")
	almsres <- lmsreg(x, y)$resid
	print("got ALMS")
	altsres <- ltsreg(x, y)$residuals
	print("got ALTS")
	out <- mba$coef
	mbacoef <- mbareg(x, y, nsamp = nsamps)$coef
	MBARES <- y - mbacoef[1] - x %*% mbacoef[-1]
	print("got MBA")
	rmat[, 1] <- lsres
	rmat[, 2] <- almsres
	rmat[, 3] <- altsres
	rmat[, 4] <- MBARES
	pairs(rmat, labels = c("OLS residuals",
		 "ALMS residuals",
		"ALTS residuals", "MBA residuals"))
}

rstmn<-
function(x, k1 = 5, k2=5)
{
#robust symmetically trimmed 2 stage mean
#truncates too many cases when the contamination is asymmetric
	madd <- mad(x, constant = 1)
	med <- median(x)
	LM <- sum(x < (med - k1 * madd))
	nmUM <- sum(x > (med + k2 * madd))
	n <- length(x)	#ll (hh) is the percentage trimmed to the left (right)
# tp is the trimming proportion
	ll <- ceiling((100 * LM)/n)
	hh <- ceiling((100 * nmUM)/n)
	tp <- min(max(ll, hh)/100, 0.5)
	mean(x, trim = tp)
}

sir<-
function(x, y, h)
{
#   Obtained from STATLIB. Contributed by Thomas Koetter.
#   Calculates the effective dimension-reduction (e.d.r.)
#   directions by Sliced Inverse Regression (K.C. Li 1991, JASA 86, 316-327)
#
#  Input:   x     n x p matrix, explanatory variable
#           y     n x 1 vector, dependent variable
#           h     scalar:  if h >=  2   number of slices
#                          if h <= -2   number of elements within a slice
#                          0 < h < 1    width of a slice:  h = slicewidth /
# range
#
#  Output:  list(edr, evalues)
#           edr      p x p matrix, estimates for the e.d.r. directions
#           evalues  p x 1 vector, the eigenvalues to the directions
#
# written by Thomas Koetter (thomas@wiwi.hu-berlin.de) 1995
# last modification: 7/18/95
# based on the implementation in XploRe
# a full description of the XploRe program can be found in (chapter 11)
# 'XploRe: An interactive statistical computing environment',
#  W. Haerdle, S. Klinke, B.A. Turlach, Springer, 1995
#
# This software can be freely used for non-commercial purposes and freely
# distributed.
#+-----------------------------------------------------------------------------+
#|  Thomas Koetter                                                             |
#|  Institut fuer Statistik und Oekonometrie                                   |
#|  Fakultaet Wirtschaftswissenschaften                                        |
#|  Humboldt-Universitaet zu Berlin, 10178 Berlin, GERMANY                     |
#+-----------------------------------------------------------------------------+
#|  Tel. voice:   +49 30  2468-321                                             |
#|  Tel. FAX:     +49 30  2468-249                                             |
#|  E-mail:       thomas@wiwi.hu-berlin.de                                     |
#+-----------------------------------------------------------------------------+
	n <- nrow(x)
	ndim <- ncol(x)
	if(n != length(c(y))) {
		stop("length of y doesn't match to number of rows of x !!")
	}
	if( - h > n) {
		stop("Number of elements within slices can't exceed number of data !!"
			)
	}
# stanardize the x variable to z (mean 0 and cov I)
	xb <- apply(x, 2, mean)
	si2 <- solve(chol(var(x)))
	xt <- (x - matrix(xb, nrow(x), ncol(x), byrow = T)) %*% si2
	# sort the data regarding y. x values are now packed into slices
	ord1 <- order(y)
	data <- cbind(y[ord1], xt[ord1,  ])	# determine slicing strategy
	if(h <= -2) {
# abs(h) is number of elements per slice
		h <- abs(h)
		ns <- floor(n/h)
		condit <- 1:n
		choice <- (1:ns) * h
	# if there are observations left, add them to the first and last slice
		if(h * ns != n) {
			hk <- floor((n - h * ns)/2)
			choice <- choice + hk
			choice[ns] <- n	# to aviod numerical problems
		}
	}
	else if(h >= 2) {
# h is number of slices
		ns <- h
		slwidth <- (data[n, 1] - data[1, 1])/ns
		slend <- seq(data[1, 1] + slwidth, length = ns, by = slwidth)
		slend[ns] <- data[n, 1]
		condit <- c(data[, 1])
		choice <- slend
	}
	else if((0 < h) && (h < 1)) {
# h is widht of a slice divides by the range of y
		ns <- floor(1/h)
		slwidth <- (data[n, 1] - data[1, 1]) * h
		slend <- seq(data[1, 1] + slwidth, length = ns, by = slwidth)
		slend[ns] <- data[n, 1]	# to aviod numerical problems
		condit <- c(data[, 1])
		choice <- slend
	}
	else stop("values of third parameter not valid")
	v <- matrix(0, ndim, ndim)	# estimate for Cov(E[z|y])
	ind <- rep(TRUE, n)	# index for already sliced elements
	ndim <- ndim + 1
	j <- 1	# loop counter
	while(j <= ns) {
		sborder <- (condit <= choice[j]) & ind	# index of slice j
		if(any(sborder)) {
# are there elements in slice j ?
			ind <- ind - sborder
			xslice <- data[sborder, 2:ndim]
			if(sum(sborder) == 1) {
# xslice is a vector !
				xmean <- xslice
				v <- v + outer(xmean, xmean, "*")
			}
			else {
				xmean <- apply(xslice, 2, mean)
				v <- v + outer(xmean, xmean, "*") * nrow(xslice
				  )
			}
		}
		j <- j + 1
	}
	if(any(ind)) {
		print("Error:  elements unused !!")
		print(ind)
	}
	v <- (v + t(v))/(2 * n)	# to prevent numerical errors (v is symmetric)
	eig <- eigen(v)
	b <- si2 %*% eig$vectors	# estimates for e.d.r. directions
	data <- sqrt(apply(b * b, 2, sum))
	b <- t(b)/data
	return(list(edr = t(b), evalues = eig$values))
}

sirviews<-
function(x, Y, ii = 1)
{
# Uses the function "sir" from STATLIB.
# Trimmed views for 90, 80, ... 0 percent
# trimming. Allows visualization of m
# and crude estimation of c beta in models
# of the form y = m(x^T beta) + e.
# beta is obtained from SIR.
# Workstation need to activate a graphics
# device with command "X11()" or "motif()."
# R needs command "library(lqs)."
# Advance the view with the right mouse button.
# In R, highlight "stop."
	x <- as.matrix(x)
        q <- dim(x)[2]
	out <- cov.mcd(x)	# or use out <- cov.mve(x)
	center <- out$center
	cov <- out$cov
	rd2 <- mahalanobis(x, center, cov)
	labs <- c("90%", "80%", "70%", "60%", "50%", "40%", "30%", "20%", "10%",
		"0%")
	tem <- seq(0.1, 1, 0.1)
	h <- q + 7
	for(i in ii:10) {
		val <- quantile(rd2, tem[i])
		b <- sir(x[rd2 <= val,  ], Y[rd2 <= val], h)$edr[, 1]
		ESP <- x %*% b
		plot(ESP, Y)
		title(labs[i])
		identify(ESP, Y)
		print(b)
	}
}

stmci<-
function(x, alpha = 0.05, ks = 3.5)
{
#gets se for sample median and the corresponding robust  100 (1-alpha)% CI
#defaults are alpha = .05
	n <- length(x)
	up <- 1 - alpha/2
	med <- median(x)
	madd <- mad(x, constant = 1)
	lo <- sum(x < (med - ks * madd))
	hi <- sum(x > (med + ks * madd))
	low <- ceiling((100 * lo)/n)
	high <- ceiling((100 * hi)/n)
	tp <- min(max(low, high)/100, 0.5)
	tstmn <- mean(x, trim = tp)
	#have obtained the two stage symetrically trimmed mean
	ln <- floor(n * tp)
	un <- n - ln
	d <- sort(x)
	if(ln > 0) {
		d[1:ln] <- d[(ln + 1)]
		d[(un + 1):n] <- d[un]
	}
	den <- ((un - ln)/n)^2
	swv <- var(d)/den
	#got the scaled Winsorized variance
	rdf <- un - ln - 1
	rval <- qt(up, rdf) * sqrt(swv/n)
	tslo <- tstmn - rval
	tshi <- tstmn + rval
	list(int = c(tslo, tshi), tp = tp)
}

symviews<-
function(x, Y)
{
# Makes trimmed views for 90, 80, ..., 0
# percent trimming and sometimes works even if m
# is symmetric about E(x^t beta) where
# y = m(x^T beta ) + e.
# For work stations, activate a graphics
# device with command "X11()" or "motif()."
# For R, use "library(lqs)."
# Use the rightmost mouse button to advance
# the view. In R, highlight ``stop."
	x <- as.matrix(x)
	tem <- seq(0.1, 1, 0.1)
	bols <- lsfit(x, Y)$coef
	fit <- x %*% bols[-1]
	temx <- x[fit > median(fit),  ]
	temy <- Y[fit > median(fit)]
	out <- cov.mcd(temx)	# or use out <- cov.mve(temx)
	center <- out$center
	cov <- out$cov
	rd2 <- mahalanobis(temx, center, cov)
	for(i in 1:10) {
		val <- quantile(rd2, tem[i])
		bhat <- lsfit(temx[rd2 <= val,  ], temy[rd2 <= val])$coef
		ESP <- x %*% bhat[-1]
		plot(ESP, Y)
		identify(ESP, Y)
		print(bhat)
	}
}


tmci<-
function(x, alpha = 0.05, tp = 0.25)
{
#gets se for the tp trimmed mean and the corresponding robust  100 (1-alpha)% CI
#defaults are alpha = .05
	n <- length(x)
	up <- 1 - alpha/2
	tmn <- mean(x, trim = tp)
	ln <- floor(n * tp)
	un <- n - ln
	d <- sort(x)
	if(ln > 0) {
		d[1:ln] <- d[(ln + 1)]
			d[(un + 1):n] <- d[un]
	}
	den <- ((un - ln)/n)^2
	swv <- var(d)/den
	#got the scaled Winsorized variance
	rdf <- un - ln - 1
	rval <- qt(up, rdf) * sqrt(swv/n)
	tmlo <- tmn - rval
	tmhi <- tmn + rval
	list(int = c(tmlo, tmhi), tp = tp)
}

Tplt<-
function(x, y)
{
# For Unix, use X11() to turn on the graphics device before using this function.
# This function plots y^L vs OLS fit. If plot is linear for L, use y^L instead of y.
# This is a graphical method for a response transform.
	olsfit <- y - lsfit(x, y)$resid
	lam <- c(-1, -2/3, -1/2, -1/3, -1/4, 0, 1/4, 1/
		3, 1/2, 2/3, 1)
	xl <- c("Y**(-1)", "Y**(-2/3)", "Y**(-0.5)",
		"Y**(-1/3)", "Y**(-1/4)", "LOG(Y)",
		"Y**(1/4)", "Y**(1/3)", "Y**(1/2)",
		"Y**(2/3)", "Y")
	for(i in 1:length(lam)) {
		if(lam[i] == 0)
			ytem <- log(y)
		else if(lam[i] == 1)
			ytem <- y
		else ytem <- (y^lam[i] - 1)/lam[i]
		plot(olsfit, ytem, xlab = "YHAT", ylab
			 = xl[i])
		abline(lsfit(olsfit, ytem)$coef)
		identify(olsfit, ytem)
	}
}

trviews<-
function(x, Y, ii = 1)
{
# Trimmed views for 90, 80, ... 0 percent
# trimming.   Increase ii if 90% trimming is too harsh.
# Allows visualization of  m and crudely estimation of
# c beta in models of the form y = m(x^T beta) + e.
# Workstation: activate a graphics device
# with commands "X11()" or "motif()."
# R needs command "library(lqs)."
# Advance the view with the right mouse button and
# in R, highight "stop."
        x <- as.matrix(x)
	out <- cov.mcd(x)
	center <- out$center
	cov <- out$cov
	rd2 <- mahalanobis(x, center, cov)
	labs <- c("90%", "80%", "70%", "60%", "50%", "40%", "30%",
     "20%","10%","0%")
	tem <- seq(0.1, 1, 0.1)
	for(i in ii:10) {
		val <- quantile(rd2, tem[i])
		b <- lsfit(x[rd2 <= val,  ], Y[rd2 <= val])$coef
		ESP <- x %*% b[-1]
		plot(ESP, Y)
		title(labs[i])
		identify(ESP, Y)
		print(b)
	}
}

tvreg<-
function(x, Y, ii = 1)
{
# Trimmed views (TV) regression for 90, 80, ..., 0 percent
# trimming.  Increase ii if 90% trimming is too harsh.
# Workstation: activate a graphics device
# with commands "X11()" or "motif()."
# R needs command "library(lqs)."
# Advance the view with the right mouse button and
# in R, highight "stop."
        x <- as.matrix(x)
	out <- cov.mcd(x)
	center <- out$center
	cov <- out$cov
	rd2 <- mahalanobis(x, center, cov)
	labs <- c("90%", "80%", "70%", "60%", "50%",
		"40%", "30%", "20%", "10%", "0%")
	tem <- seq(0.1, 1, 0.1)
	for(i in ii:10) {
		val <- quantile(rd2, tem[i])
		b <- lsfit(x[rd2 <= val,  ], Y[rd2 <=
			val])$coef
		FIT <- x %*% b[-1] + b[1]
		plot(FIT, Y)
		abline(0, 1)
		title(labs[i])
		identify(FIT, Y)
		print(b)
	}
}

tvreg2<-
function(X, Y, M = 0)
{
# Trimmed views regression for M percent trimming.
# Workstation: activate a graphics device
# with commands "X11()" or "motif()."
# R needs command "library(lqs)."
        X <- as.matrix(X)
	out <- cov.mcd(X)
	center <- out$center
	cov <- out$cov
	rd2 <- mahalanobis(X, center, cov)
	tem <- (100 - M)/100
	val <- quantile(rd2, tem)
	b <- lsfit(X[rd2 <= val,  ], Y[rd2 <= val])$coef
	FIT <- X %*% b[-1] + b[1]
	plot(FIT, Y)
	abline(0, 1)
	identify(FIT, Y)
	list(coef = b)
}


wddplot<-
function(x)
{# Shows the southwest corner of the DD plot.
	n <- dim(x)[1]
	wt <- 0 * (1:n)
	p <- dim(x)[2]
	center <- apply(x, 2, mean)
	cov <- var(x)
	md2 <- mahalanobis(x, center, cov)
	out <- cov.mcd(x)
	center <- out$center
	cov <- out$cov
	rd2 <- mahalanobis(x, center, cov)
	md <- sqrt(md2)
	rd <- sqrt(rd2)
	const <- sqrt(qchisq(0.5, p))/median(rd)
	rd <- const * rd
	wt[rd < sqrt(qchisq(0.975, p))] <- 1
	MD <- md[wt > 0]
	RD <- rd[wt > 0]
	plot(MD, RD)
}

robpca<-function(x, pval=ncol(x), kmax=10, alpha=0.75, h, mcd=1,
plots=1, labsd=3, labod=3, classic=0,plotit=FALSE,pr=TRUE,SEED=TRUE,
STAND=TRUE,est=tmean,varfun=winvar,scree=TRUE,xlab="Principal Component",ylab="Proportion of Variance"){
x<-elimna(x)
#
# This is a slightly modified version of the code in robpca.SSC that
# was downloade from M. Hubert's web page.
#
if(pval!=ncol(x))scree=FALSE
if(STAND)x=standm(x,est=est,scat=varfun)
if(SEED)set.seed(2) # so cov.mve will always return same result
k<-pval # k=0 generates an error when using the original code.
if(pr)print(paste("Number of principal components specified is",pval))
#
# ROBPCA is a 'ROBust method for Principal Components Analysis'.
# It is resistant to outliers in the data. The robust loadings are computed using
# projection-pursuit techniques and the MCD method. Therefore ROBPCA can be applied
# to both low and high-dimensional data sets.In low dimensions, the MCD method is applied (see cov.mcd).
# The ROBPCA method is described in
#   Hubert, M., Rousseeuw, P.J., Vanden Branden K. (2005),
#   "ROBPCA: a new approach to robust principal components analysis",
#   to appear in Technometrics.
# For the up-to-date reference, please consult the website:
#     www.wis.kuleuven.ac.be/stat/robust.html

# Required input arguments:
#            x : Data matrix (observations in the rows, variables in the
#                columns)
#
# Optional input arguments:
#            pval (which was k) in original code by Hubert
#              : Number of principal components to compute. If k is missing,
#                or k = 0, a screeplot is drawn which allows you to select
#                the number of principal components. If k = 0 and plots = 0,
#                the algorithm itself will determine the number of components.
#                This is not recommended.
#         kmax : Maximal number of principal components to compute (default = 10).
#                If k is provided, kmax does not need to be specified, unless k is larger
#                than 10.
#        alpha : (1-alpha) measures the fraction of outliers the algorithm should
#                resist. Any value between 0.5 and 1 may be specified (default = 0.75).
#            h : (n-h+1) measures the number of outliers the algorithm should
#                resist. Any value between n/2 and n may be specified. (default = 0.75*n)
#                Alpha and h may not both be specified.
#          mcd : If equal to one: when the number of variables is sufficiently small,
#                the loadings are computed as the eigenvectors of the MCD covariance matrix,
#                hence the function 'cov.mcd' is automatically called. The number of
#                principal components is then taken as k = rank(x). (default)
#                If equal to zero, the robpca algorithm is always applied.
#        plots : If equal to one, a scree plot, and a robust score outlier map are
#                drawn (default). If the input argument 'classic' is equal to one,
#                the classical plots are drawn as well.
#                If 'plots' is equal to zero, all plots are suppressed.
#        labsd : The 'labsd' observations with largest score distance are
#                labeled on the outlier map. (default = 3)
#        labod : The 'labod' observations with largest orthogonal distance are
#                labeled on the outlier map. default = 3)
#      classic : If equal to one, the classical PCA analysis will be performed. (default = 0)
#
# I/O: result<-robpca(x,k=2,kmax=10,alpha=0.75,h=50,mcd=1,plots=1,labsd=3,labod=3,classic=0)
#  The user should only give the input arguments that have to change their default value.
#  The name of the input arguments needs to be followed by their value.
#  The order of the input arguments is of no importance.
#
# Examples:
#    result<-robpca(x,k=3,alpha=0.65,plots=0)
#    result<-robpca(x,alpha=0.80,kmax=15,labsd=5)
# plotit=FALSE, is the same as using plots=0
#
# The output of ROBPCA is a structure containing
#
#    result$P        : Robust loadings (eigenvectors)
#    result$L        : Robust eigenvalues
#    result$M        : Robust center of the data
#    result$T        : Robust scores
#    result$k        : Number of (chosen) principal components
#    result$h        : The quantile h used throughout the algorithm
#    result$sd       : Robust score distances within the robust PCA subspace
#    result$od       : Orthogonal distances to the robust PCA subspace
#    result$cutoff   : Cutoff values for the robust score and orthogonal distances
#    result$flag     : The observations whose score distance is larger than result.cutoff.sd
#                      or whose orthogonal distance is larger than result$cutoff$od
#                      can be considered as outliers and receive a flag equal to zero.
#                      The regular observations receive a flag 1.
#    result$class    : 'ROBPCA'
#    result$classic  : If the input argument 'classic' is equal to one, this structure
#                     contains results of the classical PCA analysis.

# Short description of the method: Let n denote the number of observations, and p the number of original variables,
# then ROBPCA finds a robust center (p x 1) of the data M and a loading matrix P which is (p x k) dimensional.
# Its columns are orthogonal and define a new coordinate system. The scores (n x k) are the coordinates of the centered
# observations with respect to the loadings: T=(X-M)*P. Note that ROBPCA also yields a robust covariance matrix (often singular)
# which can be computed as  cov<-out$P*out$L*t(out$P). The scree plot shows the eigenvalues and is helpful to select the number
# of principal components. The outlier map visualizes the observations by plotting their orthogonal distance to the robust PCA subspace
# versus their robust distances within the PCA subspace. This allows to classify the data points into 4 types: regular observations,
# good leverage points, bad leverage points and orthogonal outliers.
#
# robpca.ssc was written by Jan Wijfels
#				  adapted by Karlien Vanden Branden.
# Last Update: 14/01/2005
if(!plotit)plots<-0
	if(missing(x)){
		stop("Error in robpca: You have to provide at least some data")
	}
	data <- as.matrix(x)
	n <- nrow(data)
	p <- ncol(data)

	if(n < p) {
		X.svd <- kernelEVD(data)
	}
	else {
		X.svd <- classSVD(data)
	}
	if(X.svd$rank == 0) {
		stop("All data points collapse!")
	}
	kmax <- max(min(floor(kmax), floor(n/2), X.svd$rank),1)
	k <- floor(k)
	if(k < 0) {
		k <- 0
	}
	else if(k > kmax) {
		warning("Attention robpca: The number of principal components k = ", k, " is larger then kmax = ", kmax, "; k is set to ", kmax,".")
		k <- kmax
	}
	if(!missing(h) & !missing(alpha)) {
		stop("Error in robpca: Both inputarguments alpha and h are provided. Only one is required.")
	}
	if(missing(h) & missing(alpha)) {
		h <- min(floor(2*floor((n+kmax+1)/2)-n+2*(n-floor((n+kmax+1)/2))*alpha),n)
	}
	if(!missing(h) & missing(alpha)) {
		alpha <- h/n
		if(k==0) {
			if(h < floor((n+kmax+1)/2)) {
				h <- floor((n+kmax+1)/2)
				alpha <- h/n
				warning("Attention robpca: h should be larger than (n+kmax+1)/2. It is set to its minimum value ", h, ".")
			}
		}
		else {
			if(h < floor((n+k+1)/2)) {
				h <- floor((n+k+1)/2)
				alpha <- h/n
				warning("Attention robpca: h should be larger than (n+k+1)/2. It is set to its minimum value ", h, ".")
			}
		}
		if(h > n) {
			alpha <- 0.75
			if(k==0) {
				h <- floor(2*floor((n+kmax+1)/2)-n+2*(n-floor((n+kmax+1)/2))*alpha)
			}
			else {
				h <- floor(2*floor((n+k+1)/2)-n+2*(n-floor((n+k+1)/2))*alpha)
			}
			warning("Attention robpca: h should be smaller than n = ", n, ". It is set to its default value ", h, ".")
		}
	}
	if(missing(h) & !missing(alpha)) {
		if(alpha < 0.5) {
			alpha <- 0.5
			warning("Attention robpca: Alpha should be larger then 0.5. It is set to 0.5.")
		}
		if(alpha >= 1) {
			alpha <- 0.75
			warning("Attention robpca: Alpha should be smaller then 1. It is set to its default value 0.75.")


		}
		if(k==0) {
			h <- floor(2*floor((n+kmax+1)/2)-n+2*(n-floor((n+kmax+1)/2))*alpha)
		}
		else {
			h <- floor(2*floor((n+k+1)/2)-n+2*(n-floor((n+k+1)/2))*alpha)
		}
	}
	labsd <- floor(max(0,min(labsd,n)))
	labod <- floor(max(0,min(labod,n)))

	out <- list()

	Xa <- X.svd$scores
	center <- X.svd$centerofX
	rot <- X.svd$loadings
	p1 <- ncol(Xa)
	if( (p1 <= min(floor(n/5), kmax)) & (mcd == 1 ) ) {
		if(k != 0) {
			k <- min(k, p1)
		}
		else {
			k <- p1
#			cat("Message from robpca: The number of principal
# components is defined by the algorithm. It is set to ", k,".\n", sep="")
		}
		if(h < floor((nrow(Xa) + ncol(Xa) +1)/2)) {
			h <- floor((nrow(Xa) + ncol(Xa) +1)/2)
			cat("Message from robpca: The number of non-outlying observations h is set to ", h," in order to make the mcd algorithm function.\n", sep="")
		}
# 		Xa.mcd <- cov.mcd(as.data.frame(Xa), quan=h, print=FALSE)
Xa.mcd <- cov.mcd(as.data.frame(Xa), quan=h) # R version
#print(Xa.mcd$method)
#if(length(grep("equation", Xa.mcd$method)) == 1) {
#			print(Xa.mcd$method)
#			stop("The ROBPCA algorithm can not deal with this
#   result from the FAST-MCD algorithm. The algorithm is aborted.")
#		}
#print("OUT")
		Xa.mcd.svd <- svd(Xa.mcd$cov)
		scores <- (Xa - matrix(data=rep(Xa.mcd$center, times=nrow(Xa)), nrow=nrow(Xa), ncol=ncol(Xa), byrow=TRUE)) %*% Xa.mcd.svd$u
		out$M <- center + as.vector(Xa.mcd$center %*% t(rot))
		out$L <- Xa.mcd.svd$d[1:k]
#
if(scree){
pv=out$L
cs=pv/sum(pv)
cm=cumsum(cs)
plot(rep(c(1:ncol(x)),2),c(cs,cm),type="n",xlab=xlab,ylab=ylab)
points(c(1:ncol(x)),cs,pch="*")
lines(c(1:ncol(x)),cs,lty=1)
points(c(1:ncol(x)),cm,pch=".")
lines(c(1:ncol(x)),cm,lty=2)
}

		out$P <- X.svd$loadings %*% Xa.mcd.svd$u[,1:k]
		out$T <- as.matrix(scores[,1:k])
		if(is.list(dimnames(data))) {
			dimnames(out$T)[[1]] <- dimnames(data)[[1]]
		}
		out$h <- h
		out$k <- k
		out$alpha <- alpha
	}
	else {
		directions <- choose(n,2)
		ndirect <- min(250, directions)
		all <- (ndirect == directions)
		seed <- 0
		B <- extradir(Xa, ndirect, seed, all)
		Bnorm <- vector(mode="numeric", length=nrow(B))
		Bnorm<-apply(B,1,vecnorm)
		Bnormr <- Bnorm[Bnorm > 1.E-12]
		B <- B[Bnorm > 1.E-12,]
		A <- diag(1/Bnormr) %*% B
		Y <- Xa %*% t(A)
		Z <- matrix(data=0, nrow=n, ncol=length(Bnormr))
		for(i in 1:ncol(Z)) {
			univ <- unimcd(Y[,i],quan = h)
			if(univ$smcd < 1.E-12) {
				r2 <- qr(data[univ$weights==1,])$rank
				if(r2 == 1) {
					stop("Error in robpca: At least ", sum(univ$weights), " observations are identical.")
				}
			}
			else {
				Z[,i] <- abs(Y[,i] - univ$tmcd) / univ$smcd
			}
		}
		H0 <- order(apply(Z, 1, max))

		Xh <- Xa[H0[1:h],]
		Xh.svd <- classSVD(Xh)

		kmax <- min(Xh.svd$rank, kmax)
		if( (k == 0) & (plots == 0) ) {
			test <- which((Xh.svd$eigenvalues/Xh.svd$eigenvalues[1]) <= 1.E-3)
			if(length(test) != 0) {
				k <- min(min(Xh.svd$rank, test[1]), kmax)
			}
			else {
				k <- min(Xh.svd$rank, kmax)
			}
			cumulative <- cumsum(Xh.svd$eigenvalues[1:k]) / sum(Xh.svd$eigenvalues)
			if(cumulative[k] > 0.8) {
				k <- which(cumulative >= 0.8)[1]
			}
			cat("Message from robpca: The number of principal components is set by the algorithm. It is set to ", k, ".\n", sep="")
		}
		else {
			if( (k==0) & (plots != 0) ) {
				loc <- 1:kmax
				plot(loc, Xh.svd$eigenvalues[1:kmax], type='b', axes= FALSE, xlab="Component", ylab="Eigenvalue")
				axis(2)
				axis(1, at=loc)
				cumv <- cumsum(Xh.svd$eigenvalues)/sum(Xh.svd$eigenvalues)
				text(loc, Xh.svd$eigenvalues[1:kmax] + par("cxy")[2], as.character(signif(cumv[1:kmax], 2)))
				box <- dialogbox(title="ROBPCA", controls=list(),buttons = c("OK"))
				box <- dialogbox.add.control(box, where=1, statictext.control(paste("How many principal components would you like to retain?\nMaximum = ", kmax, sep=""), size=c(200,20)))
				box <- dialogbox.add.control(box, where=2, editfield.control(label="Your choice:", size=c(30,10)))
				input <- as.integer(dialogbox.display(box)$values$"Your choice:")
				k <- max(min(min(Xh.svd$rank, input), kmax), 1)
			}
			else {
				k <- min(min(Xh.svd$rank, k), kmax)
			}
		}
		if(k!=X.svd$rank){
			XRc <- Xa-matrix(data=rep(Xh.svd$centerofX, times=nrow(Xa)), nrow=nrow(Xa), ncol=ncol(Xa), byrow=TRUE)
			Xtilde <- XRc%*%Xh.svd$loadings[,1:k]%*%t(Xh.svd$loadings[,1:k])
			Rdiff <- XRc-Xtilde
          odh <- apply(Rdiff,1,vecnorm)
          ms <- unimcd(odh^(2/3),h)
        	cutoffodh <- sqrt(qnorm(0.975,ms$tmcd,ms$smcd)^3)
          indexset <- (odh<=cutoffodh)
          Xh.svd <- classSVD(Xa[indexset,])
			kmax <- min(Xh.svd$rank, kmax)
		}

		center <- center + Xh.svd$centerofX %*% t(rot)
		rot <- rot %*% Xh.svd$loadings
		Xstar<- (Xa - matrix(data=rep(Xh.svd$centerofX, times=nrow(Xa)), nrow=nrow(Xa), ncol=ncol(Xa), byrow=TRUE)) %*% Xh.svd$loadings
		Xstar <- as.matrix(Xstar[,1:k])
		rot <- as.matrix(rot[,1:k])
		mah <- mahalanobis(Xstar, center=rep(0, ncol(Xstar)), cov=diag(Xh.svd$eigenvalues[1:k], nrow=k))
		oldobj <- prod(Xh.svd$eigenvalues[1:k])
		niter <- 100
		for(j in 1:niter) {
			mah.order <- order(mah)
			Xh <- as.matrix(Xstar[mah.order[1:h],])
			Xh.svd <- classSVD(Xh)
			obj <- prod(Xh.svd$eigenvalues)
			Xstar <- (Xstar - matrix(data=rep(Xh.svd$centerofX, times=nrow(Xstar)), nrow=nrow(Xstar), ncol=ncol(Xstar), byrow=TRUE)) %*% Xh.svd$loadings
			center <- center + Xh.svd$centerofX %*% t(rot)
			rot <- rot %*% Xh.svd$loadings
			mah <- mahalanobis(Xstar, center=rep(0, ncol(Xstar)), cov=diag(x=Xh.svd$eigenvalues, nrow=length(Xh.svd$eigenvalues)))
			if( (Xh.svd$rank == k) & ( abs(oldobj - obj) < 1.E-12) ) {
				break
			}
			else {
				oldobj <- obj
				if(Xh.svd$rank < k) {
					j <- 1
					k <- Xh.svd$rank
				}
			}
		}
Xstar.mcd <- cov.mcd(as.data.frame(Xstar), quan=h) # R version
#		if(Xstar.mcd$raw.objective < obj) {
			covf <- Xstar.mcd$cov
			centerf <- Xstar.mcd$center
#		}
#		else {
#			consistencyfactor <- median(mah)/qchisq(0.5,k)
#			mah <- mah/consistencyfactor
#			weights <- ifelse(mah <= qchisq(0.975, k), T, F)
#			noMCD <- weightmecov(Xstar, weights, n, k)
#			centerf <- noMCD$center
#			covf <- noMCD$cov
#		}

		covf.eigen <- eigen(covf)
		covf.eigen.values.sort <- greatsort(covf.eigen$values)
		P6 <- covf.eigen$vectors
		P6 <- covf.eigen$vectors[,covf.eigen.values.sort$index]

out$T <- (Xstar - matrix(data=rep(centerf, times=n), nrow=n, ncol=ncol(Xstar), byrow=TRUE)) %*% covf.eigen$vectors[,covf.eigen.values.sort$index]

		if(is.list(dimnames(data))) {
			dimnames(out$T)[[1]] <- dimnames(data)[[1]]
		}
		out$P <- rot %*% covf.eigen$vectors[,covf.eigen.values.sort$index]
		out$M <- as.vector(center + centerf %*% t(rot))
		out$L <- as.vector(covf.eigen$values)
		out$k <- k
		out$h <- h

		out$alpha <- alpha
	}
	oldClass(out) <- "robpca"
	out <- CompRobustDist(data, X.svd$rank, out, classic)
	if(classic == 1) {
		out <- CompClassicDist(X.svd, out)
	}
	if(plots == 1) {
		plot(out, classic, labod=labod, labsd=labsd)
	}
	return(out)
}
"greatsort"<-function(vec){
	x <- vec * (-1)
	index <- order(x)
	return(list(sortedvector=rev(sort(vec)), index=index))
}
"classSVD"<-function(x){
	if(!is.matrix(x)) {
		stop("The function classSVD requires input of type 'matrix'.")
	}
	n <- nrow(x)
	p <- ncol(x)
	if(n == 1) {
		stop("The sample size is 1. No singular value decomposition can be performed.")
	}
	if(p < 5) {
		tolerance <- 1E-12
	}
	else {
		if(p <= 8) {
			tolerance <- 1E-14
		}
		else {
			tolerance <- 1E-16
		}
	}
	centerofX <- apply(x, 2, mean)
	Xcentered <- scale(x, center=TRUE, scale=FALSE)
	XcenteredSVD <- svd(Xcentered/sqrt(n-1))
	rank <- sum(XcenteredSVD$d > tolerance)
	eigenvalues <- (XcenteredSVD$d[1:rank])^2
	loadings <- XcenteredSVD$v[,1:rank]
	scores <- Xcentered %*% loadings
	return(list(loadings=as.matrix(loadings), scores=as.matrix(scores), eigenvalues=as.vector(eigenvalues), rank=rank,
					Xcentered=as.matrix(Xcentered), centerofX=as.vector(centerofX)))
}
"kernelEVD"<-function(x){
	if(!is.matrix(x)) {
		stop("The function kernelEVD requires input of type 'matrix'.")
	}
	n <- nrow(x)
	p <- ncol(x)
	if(n > p) {
		return(classSVD(x))
	}
	else {
		centerofX <- apply(x, 2, mean)
		Xcentered <- scale(x, center=TRUE, scale=FALSE)
		if(n == 1) {
			stop("The sample size is 1. No singular value decomposition can be performed.")
		}
		eigen <- eigen(Xcentered %*% t(Xcentered)/(n-1))
		eigen.descending <- greatsort(eigen$values)
		loadings <- eigen$vectors[,eigen.descending$index]
		tolerance <- n * max(eigen$values) * .Machine$double.eps
		rank <- sum(eigen.descending$sortedvector > tolerance)
		eigenvalues <- eigen.descending$sortedvector[1:rank]
		loadings <- t((Xcentered/sqrt(n-1))) %*% loadings[,1:rank] %*% diag(1/sqrt(eigenvalues), nrow=length(eigenvalues), ncol=length(eigenvalues))
		scores <- Xcentered %*% loadings
		return(list(loadings=as.matrix(loadings), scores=as.matrix(scores), eigenvalues=as.vector(eigenvalues), rank=rank,
						Xcentered=as.matrix(Xcentered), centerofX=as.vector(centerofX)))
	}
}
"extradir"<-function(data, ndirect, seed=0, all=TRUE){
	n <- nrow(data)
	p <- ncol(data)
	B2 <- matrix(data=0, nrow = ndirect, ncol = p)
	rowindex <- 1
	i <- 1
	if(all == T) {
		while( (i < n) & (rowindex <= ndirect) ) {
			j <- i + 1
			while( (j <= n) & (rowindex <= ndirect) ) {
				B2[rowindex,] <- data[i,] - data[j,]
				j <- j + 1
				rowindex <- rowindex + 1
			}
			i <- i + 1
		}
	}
	else {
		while(rowindex <= ndirect) {
			sseed<-randomset(n,2,seed)
			seed<-sseed$seed
			B2[rowindex,] <- data[sseed$ranset[1],] - data[sseed$ranset[2],]
			rowindex <- rowindex + 1
		}
	}
	return(B2)
}
"randomset"<-function(tot,nel,seed){
out<-list()
for(j in 1:nel){
   randseed<-uniran(seed)
	seed<-randseed$seed
   num<-floor(randseed$random*tot)+1
   if(j > 1){
      while(any(out$ranset==num)){
         	randseed<-uniran(seed)
			seed<-randseed$seed
        	num<-floor(randseed$random*tot)+1

      }
   }
   out$ranset[j]<-num
	}
	out$seed<-seed
	return(out)
}
"uniran"<-function(seed = 0){
	out <- list()
	seed<-floor(seed*5761)+999
	quot<-floor(seed/65536)
	out$seed<-floor(seed)-floor(quot*65536)
	out$random<-out$seed/65536
	return(out)
}
"unimcd"<-function(y,quan){
	out<-list()
	ncas<-length(y)
	len<-ncas-quan+1
	if(len==1){
   	 	out$tmcd<-mean(y)
    	out$smcd<-sqrt(var(y))
		}
	else {
		ay<-c()
		I<-order(y)
		y<-y[I]
		ay[1]<-sum(y[1:quan])
	    for(samp in 2:len){
			ay[samp]<-ay[samp-1]-y[samp-1]+y[samp+quan-1]
		}
   	 ay2<-ay^2/quan
	 sq<-c()
    sq[1]<-sum(y[1:quan]^2)-ay2[1]
    for(samp in 2:len){
		 sq[samp]<-sq[samp-1]-y[samp-1]^2+y[samp+quan-1]^2-ay2[samp]+ay2[samp-1]
		}
	 sqmin<-min(sq)
	 Isq<-order(sq)
	 ndup<-sum(sq == sqmin)
	 ii<-Isq[1:ndup]
	 slutn<-c()
    slutn[1:ndup]<-ay[ii]
    initmean<-slutn[floor((ndup+1)/2)]/quan
    initcov<-sqmin/(quan-1)
    res<-(y-initmean)^2/initcov
    sortres<-sort(res)
    factor<-sortres[quan]/qchisq(quan/ncas,1)
    initcov<-factor*initcov
    res<-(y-initmean)^2/initcov
    quantile<-qchisq(0.975,1)
    out$weights<-(res<quantile)
    out$tmcd<-sum(y*out$weights)/sum(out$weights)
    out$smcd<-sqrt(sum((y-out$tmcd)^2*out$weights)/(sum(out$weights)-1))
	Iinv<-order(I)
	out$weights<-out$weights[Iinv]
}
	return(out)
}
"weightmecov"<-function(data, weights, n, nvar) {
	weightedcov <- cov.wt(x=data, wt=weights, center=TRUE)
	return(list(center=weightedcov$center, cov=weightedcov$cov*sum(weights)/(sum(weights)-1)))
}
"CompRobustDist"<-function(data, r, out, classic) {
	n <- nrow(data)
	p <- ncol(data)
	out$sd <- sqrt(mahalanobis(out$T, center=rep(0, length=ncol(out$T)), cov=diag(x=out$L, nrow=length(out$L))))
	out$cutoff$sd <- sqrt(qchisq(0.975, out$k))
	XRc <- data - matrix(data=rep(out$M, times=n), nrow=n, ncol=p, byrow=TRUE)
	Xtilde <- out$T %*% t(out$P)
	Rdiff <- XRc - Xtilde
	out$od <- vector(mode="numeric", length=n)
	if(is.list(dimnames(out$T))) {
		names(out$od) <- dimnames(out$T)[[1]]
	}
	out$od<-apply(Rdiff,1,vecnorm)
	if(out$k != r) {
		ms <- unimcd(out$od^(2/3), quan=out$h)
		out$cutoff$od <- sqrt(qnorm(0.975, ms$tmcd, ms$smcd)^3)
		out$flag <- (out$od <= rep(x=out$cutoff$od, times=length(out$od))) & (out$sd <= rep(x=out$cutoff$sd, times=length(out$sd)))
	}
	else {
		out$cutoff$od <- 0
		out$flag <- out$sd <= rep(x=out$cutoff$sd, times=length(out$sd))
	}
	if(classic == 0) {
		out$classic <- 0
	}
	out$class <- "ROBPCA"

	return(out)
}
"CompClassicDist"<-function(svd, out) {
	out$classic$P <- as.matrix(svd$loadings[,1:out$k])
	out$classic$L <- as.vector(svd$eigenvalues[1:out$k])
	out$classic$M <- as.vector(svd$centerofX)
	out$classic$T <- as.matrix(svd$scores[,1:out$k])

	out$classic$k <- out$k
	out$classic$Xc <- as.matrix(svd$Xcentered)
	Tclas <- out$classic$Xc %*% out$classic$P
	out$classic$sd <- sqrt(mahalanobis(Tclas, center=rep(0, length=ncol(Tclas)), cov=diag(x=out$classic$L, nrow=out$classic$k)))
	out$classic$cutoff$sd <- sqrt(qchisq(0.975, out$classic$k))
	Xtilde <- Tclas %*% t(out$classic$P)
	Cdiff <- out$classic$Xc - Xtilde
	out$classic$od <- vector(mode="numeric", length=nrow(out$classic$Xc))
	if(is.list(dimnames(out$classic$T))) {
		names(out$classic$od) <- dimnames(out$classic$T)[[1]]
	}
	out$classic$od<-apply(Cdiff,1,vecnorm)
	if(out$k != svd$rank) {
		m <- mean(out$classic$od^(2/3))
		s <- sqrt(var(out$classic$od^(2/3)))
		out$classic$cutoff$od <- sqrt((qnorm(0.975, m, s))^3)
	}
	else {
		out$classic$cutoff$od <- 0
	}
	out$classic$flag <- (out$classic$od <= rep(x=out$classic$cutoff$od, times=length(out$classic$od))) &
								(out$classic$sd <= rep(x=out$classic$cutoff$sd, times=length(out$classic$sd)))
	out$classic$class <- "CPCA"
	return(out)
}
robpcaS<-function(x,pval=ncol(x),SCORES=FALSE,STAND=TRUE,est=tmean,varfun=winvar,SEED=TRUE){
#
# An abbreviated form of robpca.
#
# compute eigen values to determine proportion of scatter.
# Goal is to see how many components are needed
#
#  pval indicates the number of principal components.
#
x=elimna(x)
if(STAND)x=standm(x,est=est,scat=varfun)
v=robpca(x,pval=pval,pr=FALSE,plotit=FALSE,SEED=SEED)
cumsum(v$L/sum(v$L))
val=matrix(NA,ncol=length(v$L),nrow=4)
scores=NULL
if(SCORES)scores=v$T
dimnames(val)=list(c("Number of Comp.","Robust Stand Dev","Proportion Robust var","Cum. Proportion"),
NULL)
val[1,]=c(1:length(v$L))
val[2,]=sqrt(v$L)
val[3,]=v$L/sum(v$L)
val[4,]=cumsum(v$L/sum(v$L))
list(summary=val,scores=scores)
}




# ----------------------------------------------------------------------------

# RobRsq

# ----------------------------------------------------------------------------

RobRsq<-function(x,y){
library(robust)
z=lmRob(y~x)
res=robR2w(z)
res
}

robR2w = function (rob.obj, correc=1.2076) {
  ## R2 in robust regression, see
  ## Renaud, O. & Victoria-Feser, M.-P. (2010). A robust coefficient of determination for regression.
  ## Journal of Statistical Planning and Inference, 140, 1852-1862.
  ## rob.obj is an lmRob object. correc is the correction for consistancy. Call:
  ##
  ## library(robust)
  ## creat.lmRob = lmRob(original1 ~ approprie1+approprie2+creativite1+creativite2, data=creatif)
  ## summary(creat.lmRob)
  ## robR2w(creat.lmRob)

  ## Weights in robust regression
  wt.bisquare = function(u, c = 4.685) {
    U <- abs(u/c)
    w <- ((1. + U) * (1. - U))^2.
    w[U > 1.] <- 0.
    w
  }
  weight.rob=function(rob.obj){
    resid.rob=rob.obj$resid
    scale.rob=(rob.obj$scale)*rob.obj$df.residual/length(resid.rob)
    resid.rob= resid.rob/scale.rob
    weight=wt.bisquare(resid.rob)
  }

  if (attr(rob.obj, "class") !="lmRob")
    stop("This function works only on lmRob objects")
  pred = rob.obj$fitted.values
  resid = rob.obj$resid
  resp = resid+pred
  wgt = weight.rob(rob.obj)
  scale.rob = rob.obj$scale
  resp.mean = sum(wgt*resp)/sum(wgt)
  pred.mean = sum(wgt*pred)/sum(wgt)
  yMy = sum(wgt*(resp-resp.mean)^2)
  rMr = sum(wgt*resid^2)
  r2 = (yMy-rMr) / yMy
  r2correc= (yMy-rMr) / (yMy-rMr +rMr*correc)
  r2adjcor = 1-(1-r2correc) * (length(resid)-1) / (length(resid)-length(rob.obj$coefficients)-1)
  return(list(robR2w.NoCorrection=r2, robR2w.WithCorrection=r2correc, robR2w.AdjustedWithCorrection=r2adjcor))
}




# ----------------------------------------------------------------------------

# robVARcom.mcp

# ----------------------------------------------------------------------------

robVARcom.mcp<-function(x,est=winvar,alpha=.05,nboot=2000,method='hoch',SEED=TRUE){
#
#  Compare the robust variances of J indepenent variables.
#  Perform all pairwise comparisons using the HC4 extension of the Morgan-Pitman test
#
if(is.null(dim(x)))stop('x should be a matrix or data frame')
J=ncol(x)
CC=(J^2-J)/2
output<-matrix(0,CC,9)
dimnames(output)<-list(NULL,c('Var','Var','Est. 1','Est 2','Dif','cilow','ci.up','p.value','Adj.p.value'))
ic=0
for (j in 1:J){
for (k in 1:J){
if (j < k){
ic=ic+1
a=b2ci(x[,j],x[,k],est=est,SEED=SEED,nboot=nboot,alpha=alpha)
output[ic,1]=j
output[ic,2]=k
output[ic,3]=a$est1
output[ic,4]=a$est2
output[ic,5]=a$est1-a$est2
output[ic,6]=a$ci[1]
output[ic,7]=a$ci[2]
output[ic,8]=a$p.value
}}}
output[,9]=p.adjust(output[,8],method=method)
output
}

#' Compare Variances - Independent Groups (Ophthalmology)
#'
#' @description
#' Compares variances of prediction errors for intraocular lens power calculation
#' formulas using independent samples. Uses HC4 version of the Morgan-Pitman test.
#'
#' @param x Matrix, data frame, or list with J columns/elements representing different formulas
#' @param y Optional second variable; if provided, x is treated as a vector and compared to y
#' @param method Multiple comparison method for p-value adjustment (default: 'hommel')
#' @param invalid Maximum valid absolute value in diopters (default: 4, note: different from default 3)
#' @param SEED Logical; if TRUE, set random seed for reproducibility
#' @param STOP Logical; if TRUE, stop on detection of invalid values
#'
#' @details
#' Performs all pairwise comparisons using a slight extension of the HC4 version
#' of the Morgan-Pitman test for comparing variances. Reports standard deviations
#' but tests are based on variances.
#'
#' @return
#' Matrix with columns: Var, Var, SD 1, SD 2, Dif, p.value, Adj.p.value
#'
#' @seealso \code{\link{oph.dep.comvar}}, \code{\link{varcom.IND.MP}}
#'
#' @keywords ophthalmology
#' @export
oph.ind.comvar<-function(x,y=NULL,method='hommel',invalid=4,SEED=TRUE,STOP=TRUE){
#
#  This function is designed specifically for dealing with
#  Prediction Error for Intraocular Lens Power Calculation
#  It is assumed that any value less than -3 diopters or greater than 3 diopters
#  is invalid.  The argument invalid can be used to change this decision rule.
#
#  Goal: compare the variances of J independent measures.
#  All pairwise comparisons are performed using
#   a slight extension of the  HC4 vesion of the Morgan-Pitman test
#
#   x can be a matrix, a data frame or it can have list mode.
#   if y is not NULL, the function assumes x is a vector
#   and the goal is to compare the variances of the data in x and y.
#
#  By default, Hochberg's method is used to control the probability of one
#  or more TypeI errors
#
if(!is.null(y))x=list(x,y)
if(is.matrix(x) || is.data.frame(x))x=listm(x)
J=length(x)
for(j in 1:J)x[[j]]=elimna(x[[j]])
for(j in 1:J){
flag=abs(elimna(x[[j]]))>invalid
if(sum(flag,na.rm=TRUE)>0){
print(paste('The value of argument invalid indicates that any value greater in absolute value than', invalid,' is invalid'))
print(paste('Variable', j, 'has one  or more invalid values'))
print('They occur in the following positions')
nr=c(1:length(x[[j]]))
print(nr[flag])
if(STOP)stop()
}
}
CC=(J^2-J)/2
output<-matrix(0,CC,7)
dimnames(output)<-list(NULL,c('Var','Var','SD 1','SD 2','Dif','p.value','Adj.p.value'))
ic=0
for (j in 1:J){
for (k in 1:J){
if (j < k){
ic=ic+1
a=varcom.IND.MP(x[[j]],x[[k]],SEED=SEED)
a=pool.a.list(a)
output[ic,1]=j
output[ic,2]=k
output[ic,3:4]=a[1:2]
output[ic,3:4]=sqrt(output[ic,3:4])
output[ic,5]=output[ic,3]-output[ic,4]
output[ic,6]=a[3]
}}}
output[,7]=p.adjust(output[,6],method=method)
output
}

#' Compare Variances - Dependent Groups (Ophthalmology)
#'
#' @description
#' Compares variances of prediction errors for intraocular lens power calculation
#' formulas using dependent (paired) samples. Uses HC4 version of the Morgan-Pitman test.
#'
#' @param x Matrix, data frame, or list with J columns/elements representing different formulas
#' @param y Optional second variable; if provided, x is treated as a vector and compared to y
#' @param invalid Maximum valid absolute value in diopters (default: 4, note: different from default 3)
#' @param method Multiple comparison method for p-value adjustment (default: 'hommel')
#' @param STOP Logical; if TRUE, stop on detection of invalid values
#'
#' @details
#' Performs all pairwise comparisons of variances for dependent variables using
#' the HC4 extension of the Morgan-Pitman test. Reports standard deviations but
#' tests are based on variances.
#'
#' @return
#' Matrix with columns: Var, Var, SD 1, SD 2, Dif, p.value, Adj.p.value
#'
#' @seealso \code{\link{oph.ind.comvar}}, \code{\link{comdvar}}
#'
#' @keywords ophthalmology
#' @export
oph.dep.comvar<-function(x, y=NULL, invalid=4, method='hommel',STOP=TRUE){
#
#  This function is designed specifically for dealing with
#  Prediction Error for Intraocular Lens Power Calculation
#  It is assumed that any value less than -3 diopters or greater than 3 diopters
#  is invalid.  The argument invalid can be used to change this decision rule.
#
#  Goal: compare the variances of J dependent measures.
#  All pairwise comparisons are performed using
#   a slight extension of the  HC4 version of the Morgan-Pitman test
#  Compare the variances of J dependent variables.
#  Perform all pairwise comparisons using the HC4 extension of the Morgan-Pitman test

#   x can be a matrix, a data frame or it can have list mode.
#   if y is not NULL, the function assumes x is a vector
#   and the goal is to compare the variances of the data in x and y.
#
#  By default, Hochberg's method is used to control the probability of one
#  or more Type I errors
#
if(!is.null(y))x=cbind(x,y)
if(is.list(x)){
n=pool.a.list(lapply(x,length))
if(var(n)!=0)stop('lengths have different values')
x=matl(x)
}
J=ncol(x)
flag=abs(elimna(x))>invalid
if(sum(flag,na.rm=TRUE)>0){
nr=c(1:nrow(x))
if(sum(flag)>1){
print(paste('The value of argument invalid indicates that any value greater in absolute value than', invalid,' is invalid'))
print('The following rows have invalid values')
}
if(sum(flag)==1){
print(paste('The value of argument invalid indicates that any value greater in absolute value than', invalid,' is invalid'))
print('The following row has an  invalid value')
}
irow=NA
ic=0
N=nrow(x)
for(i  in 1:N){
iflag=abs(x[i,])>invalid
if(sum(iflag,na.rm=TRUE)>0){
ic=ic+1
irow[ic]=i
}}
print(irow)
if(STOP)stop()
}
CC=(J^2-J)/2
output<-matrix(0,CC,7)
dimnames(output)<-list(NULL,c('Var','Var','SD 1','SD 2','Dif','p.value','Adj.p.value'))
ic=0
for (j in 1:J){
for (k in 1:J){
if (j < k){
ic=ic+1
a=comdvar(x[,j],x[,k])
output[ic,1]=j
output[ic,2]=k
output[ic,3]=a$est1
output[ic,4]=a$est2
output[ic,3]=sqrt(a$est1)
output[ic,4]=sqrt(a$est2)
output[ic,5]=sqrt(a$est1)- sqrt(a$est2)
output[ic,6]=a$p.value
}}}
output[,7]=p.adjust(output[,6],method=method)
output
}


ROCmul.curve<-function(train=NULL,g=NULL,x1=NULL,x2=NULL,method=c('KNN','DIS'),pro.p=.8,
SEED=TRUE,reps=10,POS=TRUE,...){
#
#  Required ROCR
#
#  Using cross validation
#
#  Plot ROC curves based on two or more methods
# Current choices available:
#  KNN: Nearest neighbor using robust depths
#  DIS:  Points classified based on their depths
#  DEP: Uses depths as suggested by Makinde and Fasoranbaku (2018). JAS
#  SVM: support vector machine
#  RF: Random forest
#  NN: neural network
#  ADA: ada boost
#  PRO: project the points onto a line connecting the centers of the data clouds.
#       Then use estimate of the pdf for each group to make a decision about future points.
#
#  reps number of resamples, the resulting ROC curves are averaged and the average is plotted.
#  pro.p controls the proportion used in the training, the rest are used in the test set
#
library(ROCR)
n.meth=length(method)
UBROC(train=train,g=g,x1=x1,x2=x2,method=method[1],reps=reps,pro.p=pro.p,SEED=SEED,POS=POS,...)
if(!is.null(train)){
if(is.null(g))stop('Argument g, group ids, must be specified')
if(is.matrix(g)){
if(dim(g)>1)stop('Argument g should be a vector')
}
flag=g==min(g)
x1=train[flag,]
x2=train[!flag,]
if(length(unique(g))>2)stop('Only two groups allowed, g has more than two unique values')
}
x1=elimna(x1)
x2=elimna(x2)
n1=nrow(x1)
n2=nrow(x2)
g=c(rep(0,n1),rep(1,n2))
k=1
if(n.meth>1){
for(k in 2:n.meth){
CHK=TRUE
if(!CHK){
if(is.null(g))stop('The argument g should contain the group id values')
xg=elimna(cbind(train,g))
p=ncol(train)
p1=p+1
train=xg[,1:p]
g=xg[,p1]
if(length(unique(g))!=2)stop('Should have only two unique values in g')
flag=g==min(g)
x1=train[flag,]
x2=train[!flag,]
}
ns1=round(pro.p*n1)
ns2=round(pro.p*n2)
PRED=NULL
LABS=NULL
for(j in 1:reps){
N1=sample(n1,ns1)
N2=sample(n2,ns2)
test1=x1[-N1,]
test2=x2[-N2,]
a1=CLASS.fun(x1=x1[N1,],x2=x2[N2,],test=test1,method=method[k],...)
a2=CLASS.fun(x1=x1[N1,],x2=x2[N2,],test=test2,method=method[k],...)
PRED=c(PRED,c(a1,a2))
LABS=c(LABS,c(rep(1,length(a1)),rep(2,length(a2))))
pred=prediction(PRED,LABS)
}
if(POS)perf <- performance(pred,'tpr','fpr')
if(!POS)perf <- performance(pred,'tnr','fnr')
plot(perf,lty=k,add=TRUE)
}}
}
# perf=performance(pred, "spec")
# auroc<- perf@y.values




# ----------------------------------------------------------------------------

# rotate.points

# ----------------------------------------------------------------------------

rotate.points<-function(x,y,deg=NULL,rad=NULL){
#
#  Rotate points deg degrees or rad radians
#
if(!is.null(deg))rad=degrees.2.radians(deg)
if(is.null(rad))stop(' deg or rad need to be indicated')
xp=x*cos(rad)-y*sin(rad)
yp=y*cos(rad)+x*sin(rad)
d=cbind(xp,yp)
d
}




# ----------------------------------------------------------------------------

# Rpca

# ----------------------------------------------------------------------------

Rpca<-function(x,p=ncol(x)-1,locfun=llocv2,loc.val=NULL,iter=100,SCORES=FALSE,
gvar.fun=cov.mba,SEED=TRUE,...){
#
# Robust PCA using random orthogonal matrices and
# robust generalized variance method
#
#  locfun, by default, use the marginal medians
#  alternatives are mcd, tauloc, spat,...
#
if(SEED)set.seed(2)
x<-elimna(x)
n<-nrow(x)
m<-ncol(x)
if(is.null(loc.val))info<-locfun(x,...)$center
if(!is.null(loc.val))info<-loc.val
for(i in 1:n)x[i,]<-x[i,]-info
vals<-NA
z<-matrix(nrow=n,ncol=p)
bval<-array(NA,c(p,m,iter))
for(it in 1:iter){
B<-matrix(runif(p*m),nrow=p,ncol=m)
B <- t(ortho(t(B))) # so rows are orthogonal
bval[,,it]<-B
for(i in 1:n)z[i,]<-B%*%as.matrix(x[i,])
#vals[it]<-gvar(z)
vals[it]<-gvarg(z,var.fun=gvar.fun)
}
iord<-order(vals)
Bop<-0-bval[,,iord[iter]]
zval<-NULL
if(SCORES){
for(i in 1:n)z[i,]<-Bop%*%as.matrix(x[i,])
zval<-z
}
list(B=Bop,gen.var=vals[iord[iter]],scores=zval)
}




# ----------------------------------------------------------------------------

# rqfit

# ----------------------------------------------------------------------------

rqfit<-function(x,y,qval=0.5,alpha=0.05,xout=FALSE,outfun=outpro,res=FALSE,method='br',...){
#
# Do a quantile regression fit
#
if(alpha!=.05)stop("This function only allows alpha=0.05. Use qregci")
library(quantreg)
xx<-cbind(x,y)
p<-ncol(xx)-1
xx<-elimna(xx)
x<-xx[,1:p]
y<-xx[,ncol(xx)]
x=as.matrix(x)
if(xout){
flag<-outfun(x,...)$keep
x<-x[flag,]
y<-y[flag]
}
residuals<-NA
if(res)residuals<-rq(y~x)$residuals
temp<-summary(rq(y~x,tau=qval,alpha=alpha,method=method))
temp0<-temp[[4]]
if(is.matrix(temp[[3]]))temp0<-temp[[3]] #Newer R version
temp<-temp0
coef<-temp[,1]
ci<-temp[,2:3]
list(coef=coef,ci=ci,residuals=residuals)
}
rqtest.sub<-function(isub,x,y,qval=.5){
#
#  Perform regression using x[isub] to predict y[isub]
#  isub is a vector of length n,
#  a bootstrap sample from the sequence of integers
#  1, 2, 3, ..., n
#
#  This function is used by other functions when computing
#  bootstrap estimates.
#
#  x is assumed to be a matrix containing values of the predictors.
#
xmat<-matrix(x[isub,],nrow(x),ncol(x))
#regboot<-rqfit(xmat,y[isub],qval=qval)$coef
regboot<-qreg(xmat,y[isub],qval=qval)$coef
regboot
}




# ----------------------------------------------------------------------------

# rqfitpv

# ----------------------------------------------------------------------------

rqfitpv<-function(x,y,alpha=.05,nullval=rep(0,ncol(cbind(x,y))),
qval=.5,xout=FALSE,outfun=out,...){
#
#   Compute a p-value for slope parameter when fitting a
#   quantile regression model to data.
#
stop("This function has been removed. Use qregci")
x<-as.matrix(x)
p<-ncol(x)
np<-p+1
output<-matrix(NA,ncol=4,nrow=np)
dimnames(output)<-list(NULL,c("Param.","ci.low","ci.up","p.value"))
for(j in 1:np){
output[j,1]<-j-1
ci<-rqfit(x,y,qval=qval,xout=xout,outfun=outfun,...)$ci[j,]
output[j,2]<-ci[1]
output[j,3]<-ci[2]
alph<-c(1:99)/100
for(i in 1:99){
irem<-i
chkit<-rqfit(x,y,alpha=alph[i],qval=qval,xout=xout,outfun=outfun,...)$ci[j,]
if(chkit[1]>nullval || chkit[2]<nullval)break
}
p.value<-irem/100
if(p.value<=.1){
iup<-(irem+1)/100
alph<-seq(.001,iup,.001)
for(i in 1:length(alph)){
p.value<-alph[i]
chkit<-rqfit(x,y,alpha=alph[i],qval=qval,xout=xout,outfun=outfun,...)$ci[j,]
if(chkit[1]>nullval || chkit[2]<nullval)break
}}
output[j,4]<-p.value
}
output
}




# ----------------------------------------------------------------------------

# rqtest

# ----------------------------------------------------------------------------

rqtest<-function(x,y,qval=.5,nboot=200,alpha=.05,SEED=TRUE,xout=FALSE,outfun=outpro,...){
#
#   Omnibus test when using  a quantile regression estimator
#
x<-as.matrix(x)
if(xout){
x<-as.matrix(x)
flag<-outfun(x,...)$keep
x<-x[flag,]
y<-y[flag]
}
x<-as.matrix(x)
if(SEED)set.seed(2) # set seed of random number generator so that
#             results can be duplicated.
print("Taking bootstrap samples. Please wait.")
data<-matrix(sample(length(y),size=length(y)*nboot,replace=TRUE),nrow=nboot)
bvec<-apply(data,1,rqtest.sub,x,y,qval=qval)
# bvec is a p+1 by nboot matrix. The first row
#                     contains the bootstrap intercepts, the second row
#                     contains the bootstrap values for first predictor, etc.
p<-ncol(x)
if(p==1)stop("Use qregci when p=1")
n<-length(y)
np<-p+1
bvec<-t(bvec)
semat<-var(bvec[,2:np])

#temp<-rqfit(x,y,qval=qval)$coef[2:np]
temp<-qreg(x,y,qval=qval)$coef[2:np]
temp<-as.matrix(temp)
test<-t(temp)%*%solve(semat)%*%temp
test<-test*(n-p)/((n-1)*p)
p.value<-1-pf(test,p,n-p)
# Determine adjusted critical level, if possible.
adjusted.alpha=NULL
b1=NULL
if(n<=60){
if(alpha==.1){
if(p==2){
b1<-0-0.001965
b0<-.2179
}
if(p==3){
b1<-0-.003
b0<-.2814
}
if(p==4){
b1<-0-.0058
b0<-.4478
}
if(p==5){
b1<-0-.00896
b0<-.6373
}
if(p>=6){
b1<-0-.0112
b0<-.7699
}}
if(alpha==.05){
if(p==2){
b1<-0-0.001173
b0<-.1203
}
if(p==3){
b1<-0-.00223
b0<-.184
}
if(p==4){
b1<-0-.00476
b0<-.3356
}
if(p==5){
b1<-0-.0063
b0<-.425
}
if(p==6){
b1<-0-.00858
b0<-.5648
}}
if(alpha==.025){
if(p==2){
b1<-0-0.00056
b0<-.05875
}
if(p==3){
b1<-0-.00149
b0<-.1143
}
if(p==4){
b1<-0-.00396
b0<-.2624
}
if(p==5){
b1<-0-.00474
b0<-.3097
}
if(p==6){
b1<-0-.0064
b0<-.4111
}}
if(alpha==.01){
if(p==2){
b1<-0-0.00055
b0<-.043
}
if(p==3){
b1<-0-.00044
b0<-.0364
}
if(p==4){
b1<-0-.0024
b0<-.1546
}
if(p==5){
b1<-0-.00248
b0<-.159
}
if(p==6){
b1<-0-.00439
b0<-.2734
}}
if(!is.null(b1))adjusted.alpha<-b1*n+b0
adjusted.alpha<-max(alpha,adjusted.alpha)
}
list(test.stat=test,p.value=p.value,adjusted.alpha=adjusted.alpha)
}




# ----------------------------------------------------------------------------

# RS.LOC.IZ

# ----------------------------------------------------------------------------

RS.LOC.IZ<-function(n,J=NULL,locfun=tmean,delta,iter=10000,SEED=TRUE,...){
#
#  Estimate probability of a correct decision based on an indifference zone
#
if(SEED)set.seed(2)
PCD=0
if(length(n)==1){
if(is.null(J))stop('Number of groups, J, was not specified')
n=rep(n,J)
}
if(length(n)>1){
x=list()
J=length(n)
for(i in 1:iter){
for(j in 1:J)x[[j]]=rnorm(n[j])
x[[1]]=x[[1]]+delta
est=lapply(x,locfun,...)
est=as.vector(list2mat(est))
id=which(est==max(est))
if(id==1)PCD=PCD+1
}}
PCD.ci=binom.conf(PCD,iter,pr=FALSE)$ci
PCD=PCD/iter
list(PCD=PCD,PCD.ci=PCD.ci)
}




# ----------------------------------------------------------------------------

# rslope

# ----------------------------------------------------------------------------

rslope<-function(x,y,fr=1,est=tmean,nmin=10,pts=x,plotit=FALSE,xlab="X",
ylab="Y",...){
#
# Estimate slope at points in pts.
#
# fr controls amount of smoothing
#
# Missing values are automatically removed.
#
temp<-cbind(x,y)
temp<-elimna(temp) # Eliminate any rows with missing values
x<-temp[,1]
y<-temp[,2]
vals<-rep(NA,length(pts))
for(i in 1:length(pts)){
flagl<-nearl(x,fr=fr,pts[i])
flagr<-nearr(x,fr=fr,pts[i])
flagr<-as.logical(flagr)
flagl<-as.logical(flagl)
if(sum(flagl)>=nmin && sum(flagr)>=nmin){
yl<-est(y[flagl],...)
yr<-est(y[flagr],...)
xl<-est(x[flagl],...)
xr<-est(x[flagr],...)
vals[i]<-(yr-yl)/(xr-xl)
}}
if(plotit){
plot(c(x,x[1],x[2]),c(vals,-5,5),xlab=xlab,ylab=ylab)
xord<-order(x)
lines(x[xord],vals[xord])
}
vals
}




# ----------------------------------------------------------------------------

# rslopesm

# ----------------------------------------------------------------------------

rslopesm<-function(x,y,fr=1,est=tmean,nmin=10,pts=x,plotit=FALSE,xlab="X",
ylab="Y",SEED=TRUE,nboot=40,xout=FALSE,RNA=TRUE,atr=.2,scat=TRUE,pyhat=TRUE,...){
#
#  For a regression line predicting Y given X
# Estimate slope at points in pts with bagging
# followed by a smooth.
#
# pyhat=T, returns estimated slopes corresponding to the sorted
# x values.
# fr controls amount of smoothing
# atr controls the amount of trimming.
#
# OUTPUT: by default, the estimated  slopes at
# X_1<=X_2<=...<=X_n
# That is, for the x values written in ascending order, the
# slope is estimated for each value. If the slope is not considered
# estimable, the estimate is set to NA.
#
# pts is used if the goal is to estimate the slope for some
# other collection of points.
#
# nmin controls how many points close to x are required when
# deciding that the slope is estimable.
# plotit=TRUE will plot the estimates.
#
# The plotted points are the estimates using rslope and
# the solid line gives the estimated values reported by this function
#
# Missing values are automatically removed.
#
if(SEED) set.seed(2)
temp<-cbind(x,y)
if(ncol(temp)!=2)stop("One predictor only is allowed")
temp<-elimna(temp) # Eliminate any rows with missing values
if(xout) {
                flag <- outfun(temp[, 1], plotit = FALSE)$keep
                temp <- temp[flag,  ]
x<-temp[,1]
y<-temp[,2]
}
flag<-order(x)
x<-x[flag]
y<-y[flag]
mat<-matrix(NA,nrow=nboot,ncol=length(pts))
vals<-NA
       for(it in 1:nboot) {
                idat <- sample(c(1:length(y)), replace = T)
                xx <- temp[idat, 1]
                yy <- temp[idat, 2]
#                mat[it,  ] <- runhat(xx, yy, pts = x, est = est, fr = fr, ...)
mat[it,]<-rslope(xx,yy,fr=fr,est=est,nmin=nmin,pts=x,plotit=FALSE)
        }
rmd<-apply(mat,2,mean,na.rm=RNA,tr=atr)
flag<-is.na(rmd)
rmdsm<-lplot(x,rmd,pyhat=TRUE,plotit=plotit)
output<-"Done"
if(pyhat){
temp<-rep(NA,length(x))
temp[!flag]<-rmdsm$yhat.values
output<-temp
}
output
}




# ----------------------------------------------------------------------------

# Rsq.ols

# ----------------------------------------------------------------------------

Rsq.ols<-function(x,y){
res=lsfit(x,y)$residuals
yhat=y-res
rsq=var(yhat)/var(y)
rsq
}

rtdep<-function(pts,m,nsamp=100,SEED=NA){
#
#  Determine Tukey depth by randomly sampling
#  p-1 points from m (which has p columns),
#  combine this with pt, fit a plane, check
#  the residuals, and repeat many times.
#  Count how many positive residuals
#  there are, say pr, how many negative residuals, nr.
#  The approximate depth is min (pr,nr) over all samples.
#
set.seed(2)
if(!is.na(SEED))set.seed(SEED)
if(!is.matrix(m))stop("Second argument is not a matrix")
if(ncol(m)==2)tdep<-depth(pts[1],pts[2],m)
if(ncol(m)>2){
n<-nrow(m)
pts<-matrix(pts,ncol=ncol(m))
mold<-m
p<-ncol(m)
pm1<-p-1
mdup<-matrix(rep(pts,nrow(m)),ncol=ncol(m),byrow=TRUE)
dif<-abs(m-mdup)
chk<-apply(dif,1,sum)
flag<-(chk!=0)
m<-m[flag,]
m<-as.matrix(m)
dmin<-sum(chk==0)
m3<-rbind(m,pts)
tdep<-nrow(m)+1
for(i in 1:nsamp){
mat<-sample(nrow(m),pm1,T)
if(p>2)x<-rbind(m[mat,2:p],pts[,2:p])
y<-c(m[mat,1],pts[1])
if(prod(eigen(var(x))$values) >10^{-8}){
#print(prod(eigen(var(x))$values))
temp<-qr(x)
if(temp$rank[1]==ncol(x)){
temp<-lsfit(x,y)$coef
m2<-cbind(rep(1,nrow(m3)),m3[,2:p])
res<-m3[,1]-temp%*%t(m2)
p1<-sum((res>0))
p2<-sum((res<0))
tdep<-min(c(tdep,p1,p2))
if(tdep<dmin)tdep<-dmin
}}}
tdep<-tdep/n
}
tdep
}
sband<-function(x,y,plotit=TRUE,CI=TRUE,alpha=.05,crit=NULL,
sm=TRUE,op=1,xlab='First Group',ylab='Est. 2 - Est. 1'){
#
#  Compute a confidence band for the shift function.
#  Assuming two independent groups are being compared
#
#  The default critical value is the approximate .05 critical value.
#
#  If flag=TRUE, the exact simultaneous  probability coverage isomputed
#             based on the critical value indicated the the argument
#   crit. The default value yields, approximately, a .95 confidence band.
#
#  If plotit=TRUE, a plot of the shift function is created, assuming that
#  the graphics window has already been activated.
#
#  This function removes all missing observations.
#
#  When plotting, the median of x is marked with a + and the two
#  quaratiles are marked with o.
#
#  sm=TRUE, shift function is smoothed using:
#  op!=1, running interval smoother,
#  otherwise use lowess.
#
# Note: which group is the reference group matters.
# sband(x,y)  often gives different results than sband(y,x).
#
x<-x[!is.na(x)]  # Remove missing values from x.
y<-y[!is.na(y)]  # Remove missing values from y.
n1=length(x)
n2=length(y)
if(is.null(crit))crit=ks.crit(n1=n1,n2=n2,alpha=alpha)
plotit<-as.logical(plotit)
pc<-NA
pc<-1-kssig(length(x),length(y),crit)
chk=sum(duplicated(x,y))
if(chk>0){
crit=ksties.crit(x,y,alpha)
pc=1-kstiesig(x,y,crit)
}
xsort<-sort(x)
ysort<-c(NA,sort(y))
l<-0
u<-0
ysort[length(y)+1+1]<-NA
for(ivec in 1:length(x))
{
isub<-max(0,ceiling(length(y)*(ivec/length(x)-crit)))
l[ivec]<-ysort[isub+1]-xsort[ivec]
isub<-min(length(y)+1,floor(length(y)*(ivec/length(x)+crit))+1)
u[ivec]<-ysort[isub+1]-xsort[ivec]
}
id.sig.greater=NULL
id.sig.less.than=NULL
num<-length(l[l>0 & !is.na(l)])+length(u[u<0 & !is.na(u)])
id.sig.greater=which(l>0)
id.sig.less.than=which(u<0)
qhat<-c(1:length(x))/length(x)
m<-matrix(c(qhat,l,u),length(x),3)
dimnames(m)<-list(NULL,c('qhat','lower','upper'))
if(plotit){
xsort<-sort(x)
ysort<-sort(y)
del<-0
for (i in 1:length(x)){
ival<-round(length(y)*i/length(x))
if(ival<=0)ival<-1
if(ival>length(y))ival<-length(y)
del[i]<-ysort[ival]-xsort[i]
}
xaxis<-c(xsort,xsort)
yaxis<-c(m[,1],m[,2])
allx<-c(xsort,xsort,xsort)
ally<-c(del,m[,2],m[,3])
temp2<-m[,2]
temp2<-temp2[!is.na(temp2)]
plot(allx,ally,type='n',ylab=ylab,xlab=xlab)
ik<-rep(F,length(xsort))
if(sm){
if(op==1){
ik<-duplicated(xsort)
del<-lowess(xsort,del)$y
}
if(op!=1)del<-runmean(xsort,del,pyhat=TRUE)
}
lines(xsort[!ik],del[!ik])
lines(xsort,m[,2],lty=2)
lines(xsort,m[,3],lty=2)
temp<-summary(x)
text(temp[3],min(temp2),"+")
text(temp[2],min(temp2),"o")
text(temp[5],min(temp2),"o")
}
flag=is.na(m[,2])
m[flag,2]=-Inf
flag=is.na(m[,3])
m[flag,3]=Inf
q.greater=NULL
if(length(id.sig.greater)>0)q.greater=m[id.sig.greater,1]
q.less=NULL
if(length(id.sig.less.than)>0)q.less=m[id.sig.less.than,1]
if(!CI)m=NULL
list(m=m,crit=crit,numsig=num,q.sig.greater=q.greater,q.sig.less=q.less,prob.coverage=pc)
}


scat2d2g<-function(x1,x2,xlab='X1',ylab='X2',ticktype='detailed',pch1='+',
pch2='*'){
#
#  Create a scatterplot marking data from the first group with the symbol
#  indicated by pch1 and the symbol indicated by pch2 for group 2.
#
if(ncol(x1)!=2)stop('x1 should be a matrix with 2 columns')
if(ncol(x2)!=2)stop('x2 should be a matrix with 2 columns')
plot(rbind(x1,x2),type='n',xlab=xlab,ylab=ylab)
points(x1,pch=pch1)
points(x2,pch=pch2)
}
scat3d2g<-function(x1,x2,pch1='+',pch2='*',tick.marks=TRUE,
xlab='X1', ylab='X1', zlab='X3'){
#
#  plot data in x1 and x2
#  marking points in x1 with symbol indicated by pch1
#  marking points in x2 with symbol indicated by pch2
#
if(ncol(x1)!=3)stop('x1 should be a matrix with 3 columns')
if(ncol(x2)!=3)stop('x2 should be a matrix with 3 columns')
library(scatterplot3d)
temp=scatterplot3d(x=c(x1[,1],x2[,1]),y=c(x1[,2],x2[,2]),
z=c(x1[,3],x2[,3]),type='n',tick.marks=tick.marks,
xlab=xlab, ylab=ylab, zlab=zlab)
temp$points(x1,pch=pch1)
temp$points(x2,pch=pch2)
}




# ----------------------------------------------------------------------------

# Scheffe

# ----------------------------------------------------------------------------

Scheffe<-function(x,con=0,alpha=.05,WARN=TRUE){
#
#  Scheffe's MCP
#
#  The data are assumed to be stored in $x$ in list mode, a matrix
#  or a data frame. If in list mode,
#  length(x) is assumed to correspond to the total number of groups.
#  It is assumed all groups are independent.
#
#  con is a J by d matrix containing the contrast coefficients that are used.
#  If con is not specified, all pairwise comparisons are made.
#
#  Missing values are automatically removed.
#
#
if(WARN)print('WARNING: Suggest using lincon instead')
if(is.data.frame(x))x=as.matrix(x)
if(is.matrix(x))x<-listm(x)
if(!is.list(x))stop('Data must be stored in a matrix or in list mode.')
con<-as.matrix(con)
J<-length(x)
n=NA
xbar<-NA
for(j in 1:J){
xx<-!is.na(x[[j]])
val<-x[[j]]
x[[j]]<-val[xx]  # Remove missing values
n[j]=length(x[[j]])
xbar[j]<-mean(x[[j]])
}
N=sum(n)
df2=N-J
AOV=anova1(x)
if(sum(con^2)==0){
CC<-(J^2-J)/2
df1=J-1
psihat<-matrix(0,CC,6)
dimnames(psihat)<-list(NULL,c('Group','Group','psihat','ci.lower','ci.upper',
'p.value'))
test<-matrix(NA,CC,5)
dimnames(test)<-list(NULL,c('Group','Group','test','crit','se'))
jcom<-0
for (j in 1:J){
for (k in 1:J){
if (j < k){
jcom<-jcom+1
test[jcom,3]<-abs(xbar[j]-xbar[k])/sqrt((J-1)*AOV$MSWG*(1/n[j]+1/n[k]))
sejk<-sqrt((CC-1)*AOV$MSWG*(1/n[j]+1/n[k]))
test[jcom,5]<-sqrt(AOV$MSWG*(1/n[j]+1/n[k]))
psihat[jcom,1]<-j
psihat[jcom,2]<-k
test[jcom,1]<-j
test[jcom,2]<-k
psihat[jcom,3]<-(xbar[j]-xbar[k])
psihat[jcom,6]<-1-pf(test[jcom,3]^2,df1,df2)
crit=sqrt(qf(1-alpha,df1,df2))
test[jcom,4]<-crit
psihat[jcom,4]<-(xbar[j]-xbar[k])-crit*sejk
psihat[jcom,5]<-(xbar[j]-xbar[k])+crit*sejk
}}}}
if(sum(con^2)>0){
if(nrow(con)!=length(x)){
stop('The number of groups does not match the number of contrast coefficients.')
}
psihat<-matrix(0,ncol(con),5)
dimnames(psihat)<-list(NULL,c('con.num','psihat','ci.lower','ci.upper',
'p.value'))
test<-matrix(0,ncol(con),4)
dimnames(test)<-list(NULL,c('con.num','test','crit','se'))
df1<-nrow(con)-1
df2=N-nrow(con)
crit=sqrt(qf(1-alpha,df1,df2))
for (d in 1:ncol(con)){
psihat[d,1]<-d
psihat[d,2]<-sum(con[,d]*xbar)
sejk<-sqrt(df1*AOV$MSWG*sum(con[,d]^2/n))
test[d,1]<-d
test[d,2]<-sum(con[,d]*xbar)/sejk
test[d,3]<-crit
test[d,4]=sqrt(AOV$MSWG*sum(con[,d]^2/n))
psihat[d,3]<-psihat[d,2]-crit*sejk
psihat[d,4]<-psihat[d,2]+crit*sejk
psihat[d,5]<-(1-pf(test[d,2]^2,df1,df2))
}
}
list(n=n,test=test,psihat=psihat)
}

sdwe<-function(m,K=3){
#
# Stahel-Donoho W-estimator implemented as suggested by
# Zuo, Cui and He 2004, Annals of Statistics, 32, 167--188
#
m=elimna(m)
pd=1/(1+zdepth(m)) # projection depth
MPD=median(pd) # C in Zuo et al. notation
flag=(pd<MPD)
W=rep(1,rep(length(pd)))
W[flag]=(exp(0-K*(1-pd[flag]/MPD)^2)-exp(0-K))/(1-exp(0-K))
bot=sum(W)
temp=m
for(i in 1:length(pd))temp[i,]=W[i]*m[i,]
val=apply(temp,2,sum)/bot
dif=m
for(i in 1:length(pd))dif[i,]=m[i,]-val
V=matrix(0,ncol(m),ncol(m))
for(i in 1:length(pd))V=V+W[i]*(as.matrix(dif[i,])%*%t(as.matrix(dif[i,])))
V=V/bot
list(center=val,cov=V)
}




# ----------------------------------------------------------------------------

# sedm

# ----------------------------------------------------------------------------

sedm<-function(x,y=NA,q=.5){
#
# Let D=X_m-Y_m be the difference between
# mth order statistics where X and Y are dependent.
# Estimate standard error D with m=[qn+.5]
# using adaptive kernel method
#
# This function is used by qdtest
#
x<-as.matrix(x)
if(is.na(y[1]))y<-x[,2]
x<-x[,1]
n<-length(x)
m<-floor(q*n+.5)
yord<-sort(y)
flag<-(y<=yord[m])
xord<-sort(x)
xq<-xord[m]
yord<-sort(y)
yq<-yord[m]
flag1<-(x<=xq)
flag2<-(y<=yq)
A<-mean(flag1*flag2)
flag1<-(x<=xq)
flag2<-(y>yq)
B<-mean(flag1*flag2)
flag1<-(x>xq)
flag2<-(y<=yq)
C1<-mean(flag1*flag2)
flag1<-(x>xq)
flag2<-(y>yq)
D1<-mean(flag1*flag2)
fx<-akerd(x,pts=xq,plotit=FALSE,pyhat=TRUE)
fy<-akerd(y,pts=yq,plotit=FALSE,pyhat=TRUE)
v1<-(q-1)^2*A
v2<-(q-1)*q*B
v3<-(q-1)*q*C1
v4<-q*q*D1
temp<-0-2*(v1+v2+v3+v4)/(fx*fy)+q*(1-q)/fx^2+q*(1-q)/fy^2
val<-sqrt(temp/n)
val
}




# ----------------------------------------------------------------------------

# SEQ_PE

# ----------------------------------------------------------------------------

SEQ_PE<-function(dependent=T){

#library("readxl")
#library("xlsx")
library("reshape")
library("tidyverse")
library("viridis")
library("ragg")

#src<-choose.files(caption="Please specify source R code file: Rallfun-v40")
#src<-file.choose()
#source(src)


##filein<-choose.files(caption="Please specify your data input file")
filein<-file.choose()
x=read.table(filein,header=T,sep="\t")

## dir of the inputfile:
## dir<-dirname(filein)

#x=read_excel(filein)
#dim(x)
#print(x)

#x=read.table(file.choose(),header=T,sep="\t")

f_d<-dependent
max<-as.integer(max(abs(x),na.rm=T))+1
if(max>7) {
#print("A maxium value is > 6 in your data. Stop running.");
stop("A maxium magnitude is > 6 in your data. Stop running.",call.=F);}
cmbn<-names(x)

## fileout<-file.choose()


##dir<-choose.dir(caption="Please specify where to save your results")
dir=dirname(filein)

box_plot1(x,paste0(dir,"/","PE_boxplot.tiff"))


## if dependent is False
if(f_d==F) {
## SD:

e1<-try({
name="02.SD"
res<-as.data.frame(oph.ind.comvar(x,invalid=max))
colnames(res)[1]="Formula_1"
colnames(res)[2]="Formula_2"
res$Formula_1<-cmbn[match(res$Formula_1,c(1:length(cmbn)))]
res$Formula_2<-cmbn[match(res$Formula_2,c(1:length(cmbn)))]
write.table(res,file=paste0(dir,"/",name,".xls"),sep="\t", quote=F,row.names=T,append=F,col.names=NA)
}
)

name="04.MAD"
res<-as.data.frame(oph.indep.comMAD(x,invalid=max))
colnames(res)[1]="Formula_1"
colnames(res)[2]="Formula_2"
res$Formula_1<-cmbn[match(res$Formula_1,c(1:length(cmbn)))]
res$Formula_2<-cmbn[match(res$Formula_2,c(1:length(cmbn)))]
write.table(res,file=paste0(dir,"/",name,".xls"),sep="\t", quote=F,row.names=T,append=F,col.names=NA)

name="06.Intervals"
resList<-oph.indepintervals(x,invalid=max)
resL<-do.call(rbind,resList)
resL1<-as.data.frame(resL)
resL1$D<-row.names(resL)
colnames(resL1)[1]="Formula_1"
colnames(resL1)[2]="Formula_2"
resL1$Formula_1<-cmbn[match(resL1$Formula_1,c(1:length(cmbn)))]
resL1$Formula_2<-cmbn[match(resL1$Formula_2,c(1:length(cmbn)))]
resL1<-resL1[,c(dim(resL1)[2],1:dim(resL1)[2]-1)]
write.table(resL1,file=paste0(dir,"/",name,".xls"),sep="\t", quote=F,row.names=F,append=F)

e1<-try({
name="08.Mean"
res<-as.data.frame(oph.indep.commean(x,invalid=max))
colnames(res)[1]="Formula_1"
colnames(res)[2]="Formula_2"
res$Formula_1<-cmbn[match(res$Formula_1,c(1:length(cmbn)))]
res$Formula_2<-cmbn[match(res$Formula_2,c(1:length(cmbn)))]
write.table(res,file=paste0(dir,"/",name,".xls"),sep="\t", quote=F,row.names=T,append=F,col.names=NA)
}
)

name="10.Median"
res<-as.data.frame(oph.indep.commedian(x,invalid=max))
colnames(res)[1]="Formula_1"
colnames(res)[2]="Formula_2"
res$Formula_1<-cmbn[match(res$Formula_1,c(1:length(cmbn)))]
res$Formula_2<-cmbn[match(res$Formula_2,c(1:length(cmbn)))]
write.table(res,file=paste0(dir,"/",name,".xls"),sep="\t", quote=F,row.names=T,append=F,col.names=NA)

name="12.MeanAE"
res<-as.data.frame(oph.indep.comMeanAE(x,invalid=max))
colnames(res)[1]="Formula_1"
colnames(res)[2]="Formula_2"
res$Formula_1<-cmbn[match(res$Formula_1,c(1:length(cmbn)))]
res$Formula_2<-cmbn[match(res$Formula_2,c(1:length(cmbn)))]
write.table(res,file=paste0(dir,"/",name,".xls"),sep="\t", quote=F,row.names=T,append=F,col.names=NA)

name="14.MedianAE"
res<-as.data.frame(oph.indep.comMedAE(x,invalid=max))
colnames(res)[1]="Formula_1"
colnames(res)[2]="Formula_2"
res$Formula_1<-cmbn[match(res$Formula_1,c(1:length(cmbn)))]
res$Formula_2<-cmbn[match(res$Formula_2,c(1:length(cmbn)))]
write.table(res,file=paste0(dir,"/",name,".xls"),sep="\t", quote=F,row.names=T,append=F,col.names=NA)


#oph.indep.comRMSAE(x,invalid=max)
name="16.RMSAE"
res<-as.data.frame(oph.indep.comRMSAE(x,invalid=max))
colnames(res)[1]="Formula_1"
colnames(res)[2]="Formula_2"
res$Formula_1<-cmbn[match(res$Formula_1,c(1:length(cmbn)))]
res$Formula_2<-cmbn[match(res$Formula_2,c(1:length(cmbn)))]
write.table(res,file=paste0(dir,"/",name,".xls"),sep="\t", quote=F,row.names=T,append=F,col.names=NA)

}

else {

name="01.SD"
res<-as.data.frame(oph.dep.comvar(x,invalid=max))
colnames(res)[1]="Formula_1"
colnames(res)[2]="Formula_2"
res$Formula_1<-cmbn[match(res$Formula_1,c(1:length(cmbn)))]
res$Formula_2<-cmbn[match(res$Formula_2,c(1:length(cmbn)))]
write.table(res,file=paste0(dir,"/",name,".xls"),sep="\t", quote=F,row.names=T,append=F,col.names=NA)

name="07.Mean"
res<-as.data.frame(oph.dep.commean(x,invalid=max))
colnames(res)[1]="Formula_1"
colnames(res)[2]="Formula_2"
res$Formula_1<-cmbn[match(res$Formula_1,c(1:length(cmbn)))]
res$Formula_2<-cmbn[match(res$Formula_2,c(1:length(cmbn)))]
write.table(res,file=paste0(dir,"/",name,".xls"),sep="\t", quote=F,row.names=T,append=F,col.names=NA)

name="09.Median"
res<-as.data.frame(oph.dep.commedian(x,invalid=max))
colnames(res)[1]="Formula_1"
colnames(res)[2]="Formula_2"
res$Formula_1<-cmbn[match(res$Formula_1,c(1:length(cmbn)))]
res$Formula_2<-cmbn[match(res$Formula_2,c(1:length(cmbn)))]
write.table(res,file=paste0(dir,"/",name,".xls"),sep="\t", quote=F,row.names=T,append=F,col.names=NA)

name="11.MeanAE"
res<-as.data.frame(oph.dep.comMeanAE(x,invalid=max))
colnames(res)[1]="Formula_1"
colnames(res)[2]="Formula_2"
res$Formula_1<-cmbn[match(res$Formula_1,c(1:length(cmbn)))]
res$Formula_2<-cmbn[match(res$Formula_2,c(1:length(cmbn)))]
write.table(res,file=paste0(dir,"/",name,".xls"),sep="\t", quote=F,row.names=T,append=F,col.names=NA)

name="13.MedianAE"
res<-as.data.frame(oph.dep.comMedAE(x,invalid=max))
colnames(res)[1]="Formula_1"
colnames(res)[2]="Formula_2"
res$Formula_1<-cmbn[match(res$Formula_1,c(1:length(cmbn)))]
res$Formula_2<-cmbn[match(res$Formula_2,c(1:length(cmbn)))]
write.table(res,file=paste0(dir,"/",name,".xls"),sep="\t", quote=F,row.names=T,append=F,col.names=NA)

e1<-try({
name="03.MAD"
res<-as.data.frame(oph.dep.comMAD(x,invalid=max))
colnames(res)[1]="Formula_1"
colnames(res)[2]="Formula_2"
res$Formula_1<-cmbn[match(res$Formula_1,c(1:length(cmbn)))]
res$Formula_2<-cmbn[match(res$Formula_2,c(1:length(cmbn)))]
write.table(res,file=paste0(dir,"/",name,".xls"),sep="\t", quote=F,row.names=T,append=F,col.names=NA)
}
)

name="15.RMSAE"
res<-as.data.frame(oph.dep.comRMSAE(x,invalid=max))
colnames(res)[1]="Formula_1"
colnames(res)[2]="Formula_2"
res$Formula_1<-cmbn[match(res$Formula_1,c(1:length(cmbn)))]
res$Formula_2<-cmbn[match(res$Formula_2,c(1:length(cmbn)))]
write.table(res,file=paste0(dir,"/",name,".xls"),sep="\t", quote=F,row.names=T,append=F,col.names=NA)

name="05.Intervals"
resList<-oph.mcnemar(x,invalid=max)
resL<-do.call(rbind,resList)
resL<-as.data.frame(resL)
colnames(resL)[2]="Formula_1"
colnames(resL)[5]="Formula_2"
resL$Formula_1<-cmbn[match(resL$Formula_1,c(1:length(cmbn)))]
resL$Formula_2<-cmbn[match(resL$Formula_2,c(1:length(cmbn)))]
write.table(resL,file=paste0(dir,"/",name,".xls"),sep="\t", quote=F,row.names=T,append=F,col.names=NA)

}
}

shiftci<-function(x,y,locfun=median,alpha=.05,pr=TRUE,...){
#
# confidence interval for the quantile shift measure of effect size.
#
#  OLD VERSION USE shiftesci
#
x=elimna(x)
y=elimna(y)
if(pr){
if(sum(duplicated(x)>0))print('Duplicate values detected; suggest using shiftPBci')
if(sum(duplicated(y)>0))print('Duplicate values detected; suggest using shiftPBci')
}
n1=length(x)
n2=length(y)
if(pr){
if(min(c(n1,n2)<40))print('Minimum sample size is less than 40; suggest using shiftPBci')
}
L=outer(x,y,FUN='-')
L=as.vector(L)
est=locfun(L,...)
ef=shiftes(x,y,locfun=locfun)$Q.Effect
ci=cidv2(x-2*est,y,alpha=alpha)$p.ci
list(n1=n1,n2=n2,effect.size=ef,conf.int=ci)
}


shiftes<-function(x,y,locfun=median,iter=100,SEED=TRUE,...){
#
#  Probabilistic measure of effect size: shift of the median.
#
ef.size=NA
ef.sizeND=NA
Q.size=NA
n1=length(x)
n2=length(y)
if(n1>=10 & n2>=10){
x=elimna(x)
y=elimna(y)
nt=n1*n2
if(nt<10^6)L=outer(x,y,FUN='-')
if(nt>=10^6){
if(SEED)set.seed(2)
L=NULL
nmin=min(c(n1,n2,100))
vef=NA
vefND=NA

for(i in 1:iter){
id1=sample(n1,nmin)
id2=sample(n2,nmin)
L=outer(x[id1],y[id2],FUN='-')
est=locfun(L,...)
vef[i]=mean(L-est<=est)
if(est<0)ef.sizeND=mean(L-est>=est)
}
ef.size=mean(vef)
}
if(nt<10^6){
est=locfun(L,...)
ef.size=mean(L-est<=est)
}}

list(Q.Effect=ef.size)
}


shiftQS=shiftes




# ----------------------------------------------------------------------------

# shiftesci

# ----------------------------------------------------------------------------

shiftesci<-function(x,y,locfun=median,alpha=.05,nboot=500,SEED=TRUE,...){
#
# confidence interval for the quantile shift measure of effect size.
#
if(SEED)set.seed(2)
x=elimna(x)
y=elimna(y)
n1=length(x)
n2=length(y)
v=NA
for(i in 1:nboot){
X=sample(x,n1,replace=TRUE)
Y=sample(y,n2,replace=TRUE)
v[i]=shiftes(X,Y,locfun=locfun)$Q.Effect
}
v=sort(v)
ilow<-round((alpha/2) * nboot)
ihi<-nboot - ilow
ilow<-ilow+1
ci=v[ilow]
ci[2]=v[ihi]
ci
}




# ----------------------------------------------------------------------------

# shifthd

# ----------------------------------------------------------------------------

shifthd<-function(x,y,nboot=200,plotit=TRUE,plotop=FALSE,SEED=TRUE){
#
#   Compute confidence intervals for the difference between deciles
#   of two independent groups. The simultaneous probability coverage is .95.
#   The Harrell-Davis estimate of the qth quantile is used.
#   The default number of bootstrap samples is nboot=200
#
#   The results are stored and returned in a 9 by 3 matrix,
#   the ith row corresponding to the i/10 quantile.
#   The first column is the lower end of the confidence interval.
#   The second column is the upper end.
#   The third column is the estimated difference between the deciles
#   (second group minus first).
#
plotit<-as.logical(plotit)
x<-x[!is.na(x)]
y<-y[!is.na(y)]
if(SEED)set.seed(2) # set seed of random number generator so that
#   results can be duplicated.
crit<-80.1/(min(length(x),length(y)))^2+2.73
m<-matrix(0,9,3)
for (i in 1:9){
q<-i/10
data<-matrix(sample(x,size=length(x)*nboot,replace=TRUE),nrow=nboot)
bvec<-apply(data,1,hd,q)
sex<-var(bvec)
data<-matrix(sample(y,size=length(y)*nboot,replace=TRUE),nrow=nboot)
bvec<-apply(data,1,hd,q)
sey<-var(bvec)
dif<-hd(y,q)-hd(x,q)
m[i,3]<-dif
m[i,1]<-dif-crit*sqrt(sex+sey)
m[i,2]<-dif+crit*sqrt(sex+sey)
}
dimnames(m)<-list(NULL,c("ci.lower","ci.upper","Delta.hat"))
if(plotit){
if(plotop){
xaxis<-c(1:9)/10
xaxis<-c(xaxis,xaxis)
}
if(!plotop)xaxis<-c(deciles(x),deciles(x))
par(pch="+")
yaxis<-c(m[,1],m[,2])
if(!plotop)plot(xaxis,yaxis,ylab="delta",xlab="x (first group)")
if(plotop)plot(xaxis,yaxis,ylab="delta",xlab="Deciles")
par(pch="*")
if(!plotop)points(deciles(x),m[,3])
if(plotop)points(c(1:9)/10,m[,3])
}
m
}

shiftPBci<-function(x,y,locfun=median,alpha=.05,null.val=.5,nboot=500,SEED=TRUE,...){
#
# confidence interval for the quantile shift measure of effect size.
# (Same as shiftQSci)
#
if(SEED)set.seed(2)
x=elimna(x)
y=elimna(y)
n1=length(x)
n2=length(y)
v=NA
ef=shiftes(x,y,locfun=locfun,SEED=FALSE)$Q.Effect
for(i in 1:nboot){
X=sample(x,n1,replace=TRUE)
Y=sample(y,n2,replace=TRUE)
v[i]=shiftes(X,Y,locfun=locfun,SEED=FALSE)$Q.Effect
}
v=sort(v)
ilow<-round((alpha/2) * nboot)
ihi<-nboot - ilow
ilow<-ilow+1
ci=v[ilow]
ci[2]=v[ihi]
pv=mean(v<null.val)+.5*mean(v==null.val)
pv=2*min(c(pv,1-pv))
list(n1=n1,n2=n2,effect.size=ef,ci=ci,p.value=pv)
}

QSci=shiftPBci
shiftQSci=shiftPBci




# ----------------------------------------------------------------------------

# sigmaBY3

# ----------------------------------------------------------------------------

sigmaBY3<-function(sigma,s,y,c3) {mean(phiBY3(s/sigma,y,c3))}

derphiBY3=function(s,y,c3)
{
  Fs= exp(-(log(1+exp(-abs(s)))+abs(s)*(s<0)))
  ds=Fs*(1-Fs)
  dev=log(1+exp(-abs(s)))+abs(s)*((y-0.5)*s<0)
  Gprim1=log(1+exp(-abs(s)))+abs(s)*(s<0)
  Gprim2=log(1+exp(-abs(s)))+abs(s)*(s>0)
  return(-psiBY3(dev,c3)*(y-Fs)+((psiBY3(Gprim1,c3)-psiBY3(Gprim2,c3))*ds))
}

der2phiBY3=function(s,y,c3)
{
  s=as.double(s)
  Fs= exp(-(log(1+exp(-abs(s)))+abs(s)*(s<0)))
  ds=Fs*(1-Fs)
  dev=log(1+exp(-abs(s)))+abs(s)*((y-0.5)*s<0)
  Gprim1=log(1+exp(-abs(s)))+abs(s)*(s<0)
  Gprim2=log(1+exp(-abs(s)))+abs(s)*(s>0)
  der2=(derpsiBY3(dev,c3)*(Fs-y)^2)+(ds*psiBY3(dev,c3))
  der2=der2+(ds*(1-2*Fs)*(psiBY3(Gprim1,c3)-psiBY3(Gprim2,c3)))
  der2=der2-(ds*((derpsiBY3(Gprim1,c3)*(1-Fs))+(derpsiBY3(Gprim2,c3)*Fs)))
  der2
}




# ----------------------------------------------------------------------------

# simp.break

# ----------------------------------------------------------------------------

simp.break<-function(x,y,pts){
#
# Estimate the break point
#
#. Using a method derived by
#  Muggeo 2003
# STATISTICS IN MEDICINE
# Statist. Med. 2003; 22:3055-3071 (DOI: 10.1002/sim.1545)
#
 library(segmented)
sati=data.frame(xx=x,yy=y)
M.lm=lm(y~x,data=dati)
a=segmented(ut.lm,psi=pts)
a
}

sint<-function(x,alpha=.05,pr=FALSE){
#
#   Compute a 1-alpha confidence interval for the median using
#   the Hettmansperger-Sheather interpolation method.
#
#   The default value for alpha is .05.
#
x=elimna(x)
if(pr){
if(sum(duplicated(x)>0))print("Duplicate values detected; hdpb might have more power")
}
k<-qbinom(alpha/2,length(x),.5)
gk<-pbinom(length(x)-k,length(x),.5)-pbinom(k-1,length(x),.5)
if(gk >= 1-alpha){
gkp1<-pbinom(length(x)-k-1,length(x),.5)-pbinom(k,length(x),.5)
kp<-k+1
}
if(gk < 1-alpha){
k<-k-1
gk<-pbinom(length(x)-k,length(x),.5)-pbinom(k-1,length(x),.5)
gkp1<-pbinom(length(x)-k-1,length(x),.5)-pbinom(k,length(x),.5)
kp<-k+1
}
xsort<-sort(x)
nmk<-length(x)-k
nmkp<-nmk+1
ival<-(gk-1+alpha)/(gk-gkp1)
lam<-((length(x)-k)*ival)/(k+(length(x)-2*k)*ival)
low<-lam*xsort[kp]+(1-lam)*xsort[k]
hi<-lam*xsort[nmk]+(1-lam)*xsort[nmkp]
sint<-c(low,hi)
sint
}




# ----------------------------------------------------------------------------

# sintv2

# ----------------------------------------------------------------------------

sintv2<-function(x,y=NULL,alpha=.05,nullval=0,null.value=NULL,pr=TRUE){
#
#   Compute a 1-alpha confidence interval for the median using
#   the Hettmansperger-Sheather interpolation method.
#   (See section 4.5.2.)
#
#   The default value for alpha is .05.
#
#  If y is not null, the function uses x-y, as might be done when comparing dependent variables.
#
if(!is.null(y))x=x-y
x=elimna(x)
if(!is.null(null.value))nullval=null.value
if(pr){
if(sum(duplicated(x)>0))print("Duplicate values detected; hdpb might have more power")
}
ci<-sint(x,alpha=alpha,pr=FALSE)
alph<-c(1:99)/100
for(i in 1:99){
irem<-i
chkit<-sint(x,alpha=alph[i],pr=FALSE)
if(chkit[1]>nullval || chkit[2]<nullval)break
}
p.value<-irem/100
if(p.value<=.01){
iup<-(irem+1)/100
alph<-seq(.001,iup,.001)
for(i in 1:length(alph)){
p.value<-alph[i]
chkit<-sint(x,alpha=alph[i],pr=FALSE)
if(is.na(chkit[1]))break
if(is.na(chkit[2]))break
if(chkit[1]>nullval || chkit[2]<nullval)break
}}
if(p.value<=.001){
alph<-seq(.0001,.001,.0001)
for(i in 1:length(alph)){
p.value<-alph[i]
chkit<-sint(x,alpha=alph[i],pr=FALSE)
if(is.na(chkit[1]))break
if(is.na(chkit[2]))break
if(chkit[1]>nullval || chkit[2]<nullval)break
if(chkit[1]>nullval || chkit[2]<nullval)break
}}
list(median=median(elimna(x)),n=length(elimna(x)),ci.low=ci[1],ci.up=ci[2],p.value=p.value)
}



sintv2mcp<-function(x, con=0, alpha=0.05,method='hoch'){
#
#  Dependent groups
#  Multiple comparisons using medians on difference scores
#
# Note: some obsolete code has not yet been removed
#
flagcon=FALSE
if(!is.matrix(x))x<-matl(x)
if(!is.matrix(x))stop('Data must be stored in a matrix or in list mode.')
con<-as.matrix(con)
J<-ncol(x)
x<-elimna(x)  # Remove missing values
nval<-nrow(x)
if(sum(con^2!=0))CC<-ncol(con)
if(sum(con^2)==0)CC<-(J^2-J)/2
ncon<-CC
if(alpha==.05){
dvec<-c(.05,.025,.0169,.0127,.0102,.00851,.0073,.00639,.00568,.00511)
if(ncon > 10){
avec<-.05/c(11:ncon)
dvec<-c(dvec,avec)
}}
if(alpha==.01){
dvec<-c(.01,.005,.00334,.00251,.00201,.00167,.00143,.00126,.00112,.00101)
if(ncon > 10){
avec<-.01/c(11:ncon)
dvec<-c(dvec,avec)
}}
if(alpha != .05 && alpha != .01)dvec<-alpha/c(1:ncon)
if(sum(con^2)==0){
flagcon<-TRUE
psihat<-matrix(0,CC,7)
dimnames(psihat)<-list(NULL,c('Group','Group','psihat','ci.lower','ci.upper','p.value','adj.p.value'))
temp1<-0
jcom<-0
for (j in 1:J){
for (k in 1:J){
if (j < k){
jcom<-jcom+1
dv<-x[,j]-x[,k]
temp=sintv2(dv,pr=FALSE)
temp1[jcom]<-temp$p.value
psihat[jcom,1]<-j
psihat[jcom,2]<-k
psihat[jcom,3]<-median(dv)
psihat[jcom,4]<-temp$ci.low
psihat[jcom,5]<-temp$ci.up
psihat[jcom,6]<-temp$p.value
}}}
temp2<-order(0-temp1)
zvec<-dvec[1:ncon]
sigvec<-(psihat[temp2,6]>=zvec)
dd=0
if(sum(sigvec)<ncon){
dd<-ncon-sum(sigvec) #number that are sig.
ddd<-sum(sigvec)+1
}
psihat[,7]=p.adjust(psihat[,6],method=method)
dd=sum(psihat[,7]<=alpha)
}

if(sum(con^2)>0){
if(nrow(con)!=ncol(x))warning('The number of groups does not match the number
 of contrast coefficients.')
ncon<-ncol(con)
psihat<-matrix(0,ncol(con),6)
dimnames(psihat)<-list(NULL,c('con.num','psihat','ci.lower','ci.upper','p.value','adj.p.value'))
temp1<-NA
for (d in 1:ncol(con)){
psihat[d,1]<-d
for(j in 1:J){
if(j==1)dval<-con[j,d]*x[,j]
if(j>1)dval<-dval+con[j,d]*x[,j]
}
temp=sintv2(dval,pr=FALSE)
temp1[d]=temp$p.value
psihat[d,5]=temp$p.value
psihat[d,2]<-median(dval)
psihat[d,3]<-temp$ci.low
psihat[d,4]<-temp$ci.up
}
temp2<-order(0-temp1)
zvec<-dvec[1:ncon]
sigvec<-(psihat[temp2,5]>=zvec)
psihat[temp2,6]<-zvec
dd=0
if(sum(sigvec)<ncon){
dd<-ncon-sum(sigvec) #number that are sig.
ddd<-sum(sigvec)+1
}
psihat[,6]=p.adjust(psihat[,5],method=method)
dd=sum(psihat[,6]<=alpha)
}
list(output=psihat,con=con,num.sig=dd)
}

sisplit<-function(J,K,x){
#
# Check for interactions by comparing binomials
# Here, have J by K (between by within) design
# Only alpha=.05 is allowed.
#
p<-J*K
connum<-(J^2-J)*(K^2-K)/4
if(is.matrix(x))x<-listm(x)
if(!is.list(x))stop("Data must be stored in a matrix or in list mode")
imap<-matrix(c(1:p),J,K,byrow=TRUE)
outm<-matrix(NA,ncol=8,nrow=connum)
dimnames(outm)<-list(NULL,c("Fac.A","Fac.A","Fac.B","Fac.B","p1","p2","ci.low","ci.up"))
ic<-0
if(connum <= 28)qval<-smmcrit(999,connum)
if(connum > 28)qval<-2.383904*connum^.1-.202
aval<-4*(1-pnorm(qval))
if(J==2 && K==2)aval<-.05
if(J==5 && K==2)aval<-2*(1-pnorm(qval))
if(J==3 && K==2)aval<-3*(1-pnorm(qval))
if(J==4 && K==2)aval<-3*(1-pnorm(qval))
if(J==2 && K==3)aval<-3*(1-pnorm(qval))
for (j in 1:J){
for (jj in 1:J){
if(j<jj){
for (k in 1:K){
for (kk in 1:K){
if(k<kk){
dif<-x[[imap[j,k]]]-x[[imap[j,kk]]]
dif<-dif[dif!=0]
dif1<-(dif<0)
dif<-(x[[imap[jj,k]]]-x[[imap[jj,kk]]])
dif<-dif[dif!=0]
dif2<-(dif<0)
ic<-ic+1
outm[ic,1]<-j
outm[ic,2]<-jj
outm[ic,3]<-k
outm[ic,4]<-kk
temp<-twobici(x=dif1,y=dif2,alpha=aval)
outm[ic,5]<-temp$p1
outm[ic,6]<-temp$p2
outm[ic,7]<-temp$ci[1]
outm[ic,8]<-temp$ci[2]
}}}}}}
outm
}


skerd<-function(x,op=TRUE,kernel="gaussian",xlab='X',ylab=''){
#
# Compute kernel density estimate
# for univariate data using S+ function density
#
# kernel=epanechnikov will use the Epanechnikov kernel.
#
if(!op)temp<-density(x,na.rm=TRUE,width=bandwidth.sj(x,method="dpi"),n=256)
if(op)temp<-density(x)
plot(temp$x,temp$y,type="n",ylab=ylab,xlab=xlab)
lines(temp$x,temp$y)
}


skew<-function(x){
#
# Compute skew and kurtosis
#
x=elimna(x)
m1<-mean(x)
m2<-var(x)
m3<-sum((x-m1)^3)/length(x)
m4<-sum((x-m1)^4)/length(x)
sk<-m3/m2^1.5
ku<-m4/m2^2
list(skew=sk,kurtosis=ku)
}




# ----------------------------------------------------------------------------

# skip

# ----------------------------------------------------------------------------

skip<-function(m,cop=6,MM=FALSE,op=1,mgv.op=0,outpro.cop=3,STAND=TRUE,pr=TRUE){
#
# m is an n by p matrix
#
# Compute skipped location and covariance matrix
#
# op=1:
# Eliminate outliers using a projection method
# That is, first determine center of data using:
#
# cop=1 Donoho-Gasko median,
# cop=2 MCD,
# cop=3 marginal medians.
#  cop=4 uses MVE center
#  cop=5 uses TBS
#  cop=6 uses rmba (Olive's median ball algorithm)
#
# For each point
# consider the line between it and the center,
# project all points onto this line, and
# check for outliers using
#
# MM=F, a boxplot rule.
# MM=T, rule based on MAD and median
#
# Repeat this for all points. A point is declared
# an outlier if for any projection it is an outlier
#
# op=2 use mgv (function outmgv) method to eliminate outliers
#
# Eliminate any outliers and compute means
#  using remaining data.
# mgv.op=0, mgv uses all pairwise distances to determine center of the data
# mgv.op=1 uses MVE
# mgv.op=2 uses MCD
#
temp<-NA
m<-elimna(m)
if(op==2)temp<-outmgv(m,plotit=FALSE,op=mgv.op)$keep
if(op==1)temp<-outpro(m,plotit=FALSE,MM=MM,cop=outpro.cop,STAND=STAND,pr=pr)$keep
val<-var(m[temp,])
loc<-apply(m[temp,],2,mean)
list(center=loc,cov=val)
}

skip.gen.cor<-function(m,y=NULL,outfun=outpro.depth,...){
#
#  Eliminate outliers using
#  outfun
#  estimate correlation using remaining data
#
m=cbind(m,y)
id=outfun(m,...)$keep
if(ncol(m)==2)val=cor(m[id,])[1,2]
else val=cor(m[id,])
val
}




# ----------------------------------------------------------------------------

# skip.o.lap

# ----------------------------------------------------------------------------

skip.o.lap<-function(m,MC=FALSE,corfun=wincor,outfun=outpro,...){
#
# For two explanatory variables,
# Compute skipped corrlations with y
# return the diference
#
# x matrix with 2 columns
#
#m=cbind(x,y)
m=elimna(m)
if(!MC)temp<-outfun(m,plotit=FALSE)$keep
if(MC)temp<-outproMC(m,plotit=FALSE)$keep
m=m[temp,]
r1=corfun(m[,1],m[,3],...)$cor
r2=corfun(m[,2],m[,3],...)$cor
dif=r1-r2
dif
}

skipreg<-function(x,y,outfun=outpro.depth,Regfun=ols,...){
#
# Skipped regression: remove outliers from cbind(x,y) using a method
# that takes into account the overall structure of the data cloud.
#
# other choices for outfun:
#  outpro
#  outmgv
#  out
#
x<-as.matrix(x)
xx<-cbind(x,y)
xx<-elimna(xx)
temp<-NA
x<-as.matrix(x)
n=nrow(x)
a=outfun(xx)
id=a$keep
x<-xx[id,1:ncol(x)]
x<-as.matrix(x)
y<-xx[id,ncol(x)+1]
b=Regfun(x,y)
list(n=a$n,n.keep=a$n.keep,coef=b$coef)
}




# ----------------------------------------------------------------------------

# skipSPR

# ----------------------------------------------------------------------------

skipSPR<-function(x,cop=6,MM=FALSE,op=1,mgv.op=0,outpro.cop=3,pr=FALSE){
v=skip(x,pr=pr,STAND=TRUE,cop=cop,op=op,mgv.op=mgv.op,outpro.cop=outpro.cop)
v
}




# ----------------------------------------------------------------------------

# skiptbs

# ----------------------------------------------------------------------------

skiptbs<-function(x,y=NA,plotit=FALSE){
#
# Remove outliers and compute correlations
#
if(!is.na(y[1]))x<-cbind(x,y)
x<-elimna(x)
n<-nrow(x)
keep<-outtbs(x,plotit=plotit)$keep
val<-cor(x[keep,])
p.value<-NA
test<-NA
crit.05<-30.41/n+2.21
vat<-val
diag(vat)<-0
test<-abs(vat*sqrt((n-2)/(1-vat^2)))
diag(test)<-NA
if(ncol(val)==2){
p.value<-c("Greater than .1")
val<-val[1,2]
test<-abs(val*sqrt((n-2)/(1-val^2)))
p.value<-c("Greater than .1")
crit<-20.20/n+1.89
if(test>=crit)p.value<-c("Less than .1")
crit<-30.41/n+2.21
if(test>=crit)p.value<-c("Less than .05")
crit<-39.72/n+2.5
if(test>=crit)p.value<-c("Less than .025")
crit<-58.55/n+2.80
if(test>=crit)p.value<-c("Less than .01")
}
list(cor=val,test.stat=test,p.value=p.value,crit.05=crit.05)
}
sm2str<-function(xx,y,iv=c(1,2),nboot=100,SEED=TRUE,xout=FALSE,outfun=outpro,
STAND=TRUE,...){
#
# Compare robust measures of association of two predictors
# based on a smooth
#
if(!is.matrix(xx))stop("x should be a matrix with 2 or more columns")
if(ncol(xx)<2)stop("x should be a matrix with 2 or more columns")
val1=NA
val2=NA
x=xx[,iv]
xy=elimna(cbind(x,y))
x=xy[,1:2]
y=xy[,3]
if(xout){
x<-as.matrix(x)
flag<-outfun(x,...)$keep
x<-x[flag,]
y<-y[flag]
x<-as.matrix(x)
}
if(SEED)set.seed(2)
data1<-matrix(sample(length(y),size=length(y)*nboot,replace=TRUE),nrow=nboot)
data2<-matrix(sample(length(y),size=length(y)*nboot,replace=TRUE),nrow=nboot)
bvec1=apply(data1,1,sm2str.sub,x[,1],y) #  2 by nboot matrix
bvec2=apply(data2,1,sm2str.sub,x[,2],y) #  2 by nboot matrix
bvecd=bvec1-bvec2
pv=akerdcdf(bvecd,pts=0)
vcor=cor(x,method="kendall")
pv=2*min(c(pv,1-pv))
p.crit=.25*abs(vcor[1,2])+.05+(100-length(y))/10000
p.crit=max(c(.05,p.crit))
list(p.value=pv,p.crit=p.crit)
}

sm2str.sub<-function(isub,x,y){
xmat<-x[isub]
val1<-lplot(xmat,y[isub],plotit=FALSE)$Explanatory.power
val1
}




# ----------------------------------------------------------------------------

# sm2strv7

# ----------------------------------------------------------------------------

sm2strv7<-function(xx,y,iv=c(1,2),nboot=100,SEED=TRUE,xout=FALSE,outfun=outpro,
STAND=TRUE,...){
#
# Compare robust measures of association of two predictors
# based on a smooth
#
# x is a matrix with two columns
# robust explanatory of  x[,1] with y is compared to x[,2] with y.
# xout=T eliminates any leverage points found with outfun, which
# defaults to outpro, a projecion method for detecting outliers.
#
#  iv: indicates the two columns of x that will be used. By default, col 1 and 2 are used.
#
if(!is.matrix(xx))stop("x should be a matrix with 2 or more columns")
if(ncol(xx)<2)stop("x should be a matrix with 2 or more columns")
val1=NA
val2=NA
x=xx[,iv]
xy=elimna(cbind(x,y))
x=xy[,1:2]
y=xy[,3]
if(xout){
x<-as.matrix(x)
flag<-outfun(x,...)$keep
x<-x[flag,]
y<-y[flag]
x<-as.matrix(x)
}
if(SEED)set.seed(2)
data1<-matrix(sample(length(y),size=length(y)*nboot,replace=TRUE),nrow=nboot)
data2<-matrix(sample(length(y),size=length(y)*nboot,replace=TRUE),nrow=nboot)
bvec1=apply(data1,1,sm2str.sub,x[,1],y) #  2 by nboot matrix
bvec2=apply(data2,1,sm2str.sub,x[,2],y) #  2 by nboot matrix
bvecd=bvec1-bvec2
pv=akerdcdf(bvecd,pts=0)
vcor=cor(x,method="kendall")
pv=2*min(c(pv,1-pv))
p.crit=.25*abs(vcor[1,2])+.05+(100-length(y))/10000
p.crit=max(c(.05,p.crit))
list(p.value=pv,p.crit=p.crit)
}

smgvcr<-function(m,nullv=rep(0,ncol(m)),SEED=TRUE,op=0,
nboot=500,plotit=TRUE){
#
# m is an n by p matrix
#
# Test hypothesis that estimand of the MGV estimator
# is equal to the null value, which defaults to zero vector.
# The level of the test is .05.
#
# Argument op: See function outmgv
#
if(SEED)set.seed(2)
m<-elimna(m)
n<-nrow(m)
crit.level<-.05
if(n<=120)crit.level<-.045
if(n<=80)crit.level<-.04
if(n<=60)crit.level<-.035
if(n<=40)crit.level<-.03
if(n<=30)crit.level<-.025
if(n<=20)crit.level<-.02
data<-matrix(sample(n,size=n*nboot,replace=TRUE),nrow=nboot)
val<-matrix(NA,ncol=ncol(m),nrow=nboot)
for(j in 1: nboot){
mm<-m[data[j,],]
temp<-outmgv(mm,plotit=FALSE,op=op)$keep
val[j,]<-apply(mm[temp,],2,mean)
}
temp<-mgvar(rbind(val,nullv),op=op)
flag2<-is.na(temp)
if(sum(flag2)>0)temp[flag2]<-0
sig.level<-sum(temp[nboot+1]<temp[1:nboot])/nboot
if(ncol(m)==2 && plotit){
plot(val[,1],val[,2],xlab="VAR 1",ylab="VAR 2")
temp3<-mgvmean(m,op=op)
points(temp3[1],temp3[2],pch="+")
ic<-round((1-crit.level)*nboot)
temp<-mgvar(val)
temp.dis<-order(temp)
xx<-val[temp.dis[1:ic],]
xord<-order(xx[,1])
xx<-xx[xord,]
temp<-chull(xx)
lines(xx[temp,])
lines(xx[c(temp[1],temp[length(temp)]),])
}
list(p.value=sig.level,crit.level=crit.level)
}


lts.sub<-function(X,theta,h){
np<-ncol(X)
p<-np-1
x<-X[,1:p]
y<-X[,np]
temp<-t(t(x)*theta[2:np])
yhat<-apply(temp,1,sum)+theta[1]
res<-(y-yhat)^2
res<-sort(res)
val<-sum(res[1:h])
val
}

SMpre<-function(x,y,est=tmean,fr=NA,varfun=pbvar,model=NULL,adz=TRUE,
xout=FALSE,outfun=out,...){
#
#  Estimate prediction error for all of the models specified by the
#  the argument model, which has list mode.
# Leave-one-out cross-validation is used in conjunction with a running interval smoother
#
x=as.matrix(x)
p=ncol(x)
p1=p+1
xy=elimna(cbind(x,y))
x=xy[,1:p]
y=xy[,p1]
n=nrow(x)
if(xout){
x<-as.matrix(x)
flag<-outfun(x,...)$keep
x<-x[flag,]
y<-y[flag]
}
if(p>5)stop("Can have at most 5 predictors")
if(is.null(model))model=modgen(p)
mout<-matrix(NA,length(model),3,dimnames=list(NULL,c("error",
"var.used","rank")))
for(imod in 1:length(model)){
nmod=length(model[[imod]])-1
temp=c(nmod:0)
mout[imod,2]=sum(model[[imod]]*10^temp)
mout[imod,1]=rplotCV(x[,model[[imod]]],y,fr=fr,est=est,varfun=varfun)$VAR.Y.HAT
}
if(adz){
va=0
 for(i in 1:n)va[i]=y[i]-tmean(y[-i])
no=pbvar(va)
mout=rbind(mout,c(no,0,NA))
}
mout[,3]=rank(mout[,1])
list(estimates=mout)
}




# ----------------------------------------------------------------------------

# snmregv2

# ----------------------------------------------------------------------------

snmregv2<-function(x,y,SEED=TRUE){
#
# Compute regression S-estimator
# remove points for which residuals are outliers
# then recompute the estimated slopes and intercept
#
res=snmreg(x,y,SEED=SEED)$residuals
chk<-abs(res-median(res))/mad(res)
x=as.matrix(x)
xx<-x[chk<=2,]
yy<-y[chk<=2]
temp<-snmreg(xx,yy,SEED=SEED)
list(coef=temp$coef,residuals=temp$residuals)
}




# ----------------------------------------------------------------------------

# spat

# ----------------------------------------------------------------------------

spat<-function(x){
#
# compute spatial median
# x is an n by p matrix
#
if(!is.matrix(x))stop("x must be a matrix")
x<-elimna(x)
START<-apply(x,2,median)
val=optim(START,spat.sub,x=x,method='BFGS')$par
val
}




# ----------------------------------------------------------------------------

# spatcen

# ----------------------------------------------------------------------------

spatcen<-function(x){
#
# compute spatial median
# x is an n by p matrix
#
if(!is.matrix(x))stop("x must be a matrix")
x<-elimna(x)
START<-apply(x,2,median)
val=optim(START,spat.sub,x=x,method='BFGS')$par
list(center=val)
}




# ----------------------------------------------------------------------------

# spca

# ----------------------------------------------------------------------------

spca<-function(x,var.fun=pbvar,SCORES=FALSE,pval=1){
#
# Spherical PCA (Locantore et al., 1999, Test, 8, 1-28)
# cf. Marrona, 2005, Technometrics
#
scores<-NULL
mvec<-spat(x)
n<-nrow(x)
m<-ncol(x)
y<-matrix(ncol=m,nrow=n)
xdif<-matrix(ncol=m,nrow=n)
for(i in 1:n){
y[i,]<-x[i,]-mvec
xdif[i,]<-y[i,]
y[i,]<-y[i,]/sqrt(sum(y[i,]^2))
}
e.val<-eigen(var(y))
b<-e.val$vectors
lam<-NA
for(j in 1:ncol(x)){
val<-NA
for(i in 1:n)val[i]<-sum(b[,j]*x[i,])
lam[j]<-var.fun(val)
}
if(SCORES){
ord.val<-order(lam)
mn1<-m-pval+1
Bp<-e.val$vectors[,ord.val[mn1:m]] #m by m
scores<-matrix(ncol=pval,nrow=n)
for(i in 1:n)scores[i,]<-t(as.matrix(Bp))%*%as.matrix(xdif[i,])
}
list(eigen.val=lam,B=b,spat.mu=mvec,scores=scores)
}




# ----------------------------------------------------------------------------

# SPCA

# ----------------------------------------------------------------------------

SPCA<-function(x, k = 0, kmax = ncol(x), delta = 0.001,
    na.action = na.fail, scale = FALSE, signflip = TRUE, trace=FALSE, ...){
library(rrcov)
res=PcaLocantore(x,k=k,kmax=kmax,delta=delta,na.action = na.action,
scale=scale,signflip=signflip,trace=trace, ...)
res
}




# ----------------------------------------------------------------------------

# spear

# ----------------------------------------------------------------------------

spear<-function(x,y=NULL){
# Compute Spearman's rho
#
if(!is.null(y[1])){
m=elimna(cbind(x,y))
n=nrow(m)
x=m[,1]
y=m[,2]
corv<-cor(rank(x),rank(y))
}
if(is.null(y[1])){
x=elimna(x)
n=nrow(x)
m<-apply(x,2,rank)
corv<-cor(m)
}
test <-corv * sqrt((n - 2)/(1. - corv^2))
sig <- 2 * (1 - pt(abs(test), length(x) - 2))
if(is.null(y[1]))sig<-matrix(sig,ncol=sqrt(length(sig)))
list(cor=corv,p.value = sig)
}


spearci<-function(x,y,nboot=1000,alpha=.05,SEED=TRUE,MC=FALSE){
if(!MC)res=corb(x,y,corfun=spear,nboot=nboot,alpha=alpha,SEED=SEED)
if(MC)res=corbMC(x,y,corfun=spear,nboot=nboot,alpha=alpha,SEED=SEED)
res
}
split.mat<-function(x,y=NULL,xcol=1,q=c(.25,.5,.75),vals=NULL){
#
#  x is a matrix  with two columns or  a vector when y=NULL
#
#  Split the data in y into groups based on values in x.
#  If x is a matrix, use col
#  xcol
#
#  vals=NULL means quantiles of x will be used, quantiles indicated by argument
#  q
#
if(!is.null(y))x=cbind(x,y)
p=ncol(x)
v=NA
L=min(x[,xcol])
M=max(x[,xcol])
if(is.null(vals)){
for(j in 1:length(q))v[j]=qest(x[,xcol],q[j])
v=c(L,v,M)
}
else v=c(L,vals,M)
J=length(v)-1
res=list()
for(j  in 1:J){
j1=j+1
flag=(x[,xcol]>=v[j] & x[,xcol]<v[j1])
res[[j]]=x[flag,p]
}
res
}

sqfun<-function(y,na.rm=FALSE){
#
sqfun<-sum(y^2,na.rm=na.rm)
sqfun
}




# ----------------------------------------------------------------------------

# sqmad

# ----------------------------------------------------------------------------

sqmad<-function(x){
val<-mad(x)^2
val
}




# ----------------------------------------------------------------------------

# srg1.vs.2

# ----------------------------------------------------------------------------

srg1.vs.2<-function(n,m,alpha=.05){
#
#  Goal: Compare proportions for each of 8 intervals.
#
#  m has two columns.
#   Col1: frequencies for first person
#   Col2:  frequencies for second person
#  n: indicates the totals (number of cases) for each
#  So n=c(100,120) would indicate 100 surgeries for the first
#   and 120 for the second.

INT=c(
'<= 0.25',
'<= 0.50',
'<= 0.75',
'<= 1.00',
'<= 1.25',
'<= 1.50',
'<= 1.75',
'<= 2.00')
output<-matrix(NA,ncol=3,nrow=8)
n1=n[1]
n2=n[2]
for(j in 1:8){
r1=m[j,1]
r2=m[j,2]
a=binom2g(r1,n1,r2,n2,alpha=alpha)
output[j,]=c(a$p1,a$p2,a$p.value)
}
dimnames(output)=list(INT,c('p1','p2','p-value'))
output
}




# ----------------------------------------------------------------------------

# stackit

# ----------------------------------------------------------------------------

stackit<-function(x,jval){
#
# Take a matrix having p columns and convert
# it to a matrix having jval columns and np/jval rows
# So take first jval columns, and rbind this with
# next jval columns, etc.
#
x<-as.matrix(x)
chkit<-ncol(x)%%jval
if(chkit!=0)stop("ncol(x) is not a multiple of jval")
xval<-x[,1:jval]
xval<-as.matrix(xval)
iloop<-ncol(x)/jval-1
il<-1
iu<-jval
for(i in 1:iloop){
il<-il+jval
iu<-iu+jval
temp<-x[,il:iu]
temp<-as.matrix(temp)
xval<-rbind(xval,temp)
}
xval
}
stacklist<-function(x){
#
# Assumes x has list mode with each entry a
# matrix having p columns.
#
# Goal: stack the data into a matrix having p columns.
#
p<-ncol(x[[1]])
xx<-as.matrix(x[[1]])
for(j in 2:length(x)){
temp<-as.matrix(x[[j]])
xx<-rbind(xx,temp)
}
xx
}




# ----------------------------------------------------------------------------

# standmar

# ----------------------------------------------------------------------------

standmar<-function(x,locfun=lloc,est=mean,scat=var,...){
# standardize a matrix x
#
x=as.matrix(x)
m1=apply(x,2,est,na.rm=TRUE)
v1=apply(x,2,scat,na.rm=TRUE)
p=ncol(x)
for(j in 1:p)x[,j]=(x[,j]-m1[j])/sqrt(v1[j])
x
}




# ----------------------------------------------------------------------------

# sterby3

# ----------------------------------------------------------------------------

sterby3 <- function(x0,y,const,estim)
{
  n=nrow(x0)
  p=ncol(x0)+1

  z=cbind(matrix(1,nrow=n),x0)
  argum=z %*% estim

  matM=matrix(data=0,nrow=p,ncol=p)
  IFsquar=matrix(data=0,nrow=p,ncol=p)
  for (i in 1:n)
{
myscalar=as.numeric(der2phiBY3(argum[i],y[i],const))
matM=matM+myscalar * (z[i,] %*% t(z[i,]))
IFsquar=IFsquar+myscalar^2 * (z[i,] %*% t(z[i,]))
}
  matM=matM/n
  matMinv=solve(matM)
  IFsquar=IFsquar/n
  asvBY=matMinv %*% IFsquar %*% t(matMinv)
  sqrt(diag(asvBY))/sqrt(n)
}




# ----------------------------------------------------------------------------

# sum.T

# ----------------------------------------------------------------------------

sum.T<-function(df,reps=100000,con){
#
# Estimate distribution of T=sum T_j, T_j independent T distribution with degree of stored in
# df
#
# Return dist.
n=df+1
v=NA
K=length(df)
for(j in 1:reps){
e=0
for(k  in 1:K)e=e+con[k]*rt(1,df=df[k])
v[j]=e
}
v
}




# ----------------------------------------------------------------------------

# SVM

# ----------------------------------------------------------------------------

SVM<-function(train=NULL,test=NULL,g=NULL,x1=NULL,x2=NULL,kernel='radial',SEED=NULL,...){
#
# Do SVM classification
#  g=class id
#  if there are two classes and the training data are stored in  separate variables, can enter
#  the data for each class via the arguments
#  x1 and x2.
#  The function will then create appropriate labels and store them in g.
#
# SEED=NULL, for convenience when this function is called by other functions
#
# Regarding  cross validation, see
# Shao (1993). Linear Model Selection by Cross-Validation, JASA, 88, 486--494
#
library(e1071)
if(!is.null(train)){
train=elimna(train)
if(is.null(g))stop('Argument g, group ids, must be specified')
if(is.matrix(g)){
if(dim(g)>1)stop('Argument g should be a vector')
}}
if(!is.null(x1)){
if(!is.null(x2)){
if(ncol(x1)!=ncol(x2))stop('x1 and x2 have different number of columns')
x1=elimna(x1)
x2=elimna(x2)
n1=nrow(x1)
n2=nrow(x2)
g=c(rep(0,n1),rep(1,n2))
train=rbind(x1,x2)
}}
g=as.numeric(as.vector(g))
flag=g==min(g)
gnum=g
gnum[flag]=0
gnum[!flag]=1
g=gnum
test=as.matrix(test)
FLAG=FALSE
if(ncol(test)==1){
FLAG=TRUE
test=t(test)  # If test is a vector, a single point, transpose to get correct number of columns.
test=rbind(test,test)   #avoid R from aborting.
}
svm_model=svm(train,as.factor(g),kernel=kernel)
dec=predict(svm_model,as.matrix(test))
dec=as.vector(as.numeric(dec))
if(FLAG)dec=dec[1]
dec
}




# ----------------------------------------------------------------------------

# T.HSD

# ----------------------------------------------------------------------------

T.HSD<-function(x,alpha=.05,plotit=FALSE){
#
# Perform Tukey--Kramer MCP
#
z=dat2form(x)
temp=aov(z$x~as.factor(z$g))
v=TukeyHSD(temp,conf.level=1-alpha)
if(plotit)plot(v)
v
}




# ----------------------------------------------------------------------------

# t1waysub

# ----------------------------------------------------------------------------

t1waysub<-function(tm,sqse,hval){
#
#  Used by t1waybt to compute Welch test statistic based on trimmed means
#  and squared standard errors stored in tm and sqse
#
w<-1/sqse
uval<-sum(w)
xtil<-sum(w*tm)/uval
A<-sum(w*(tm-xtil)^2)/(length(tm)-1)
B<-sum((1-w/uval)^2/(hval-1))
t1waysub<-A/(B+1)
t1waysub
}

t3pval<-function(cmat,tmeans,v,h){
alph<-c(1:99)/100
for(i in 1:99){
irem<-i
chkit<-johan(cmat,tmeans,v,h,alph[i])
if(chkit$teststat>chkit$crit)break
}
p.value <- irem/100
        if(p.value <= 0.1) {
                iup <- (irem + 1)/100
                alph <- seq(0.001, iup, 0.001)
                for(i in 1:length(alph)) {
                        p.value <- alph[i]
                        chkit<-johan(cmat,tmeans,v,h,alph[i])
if(chkit$teststat>chkit$crit)break
                }
        }
  if(p.value <= 0.001) {
                alph <- seq(0.0001, 0.001, 0.0001)
                for(i in 1:length(alph)) {
                        p.value <- alph[i]
chkit<-johan(cmat,tmeans,v,h,alph[i])
if(chkit$teststat>chkit$crit)break
                }
        }
p.value
}

tailci<-function(x,y,pts=NULL,q=.25, nboot=1000,estfun=hdmq,alpha=.05,SEED=TRUE){
#
#   If pts is specified, the goal is to compute a confidence interval
# P(x-y< pts). If pts is not specified, it is taken to be an estimate of the qth quantile,
# q=.25 is the default.
#
if(SEED)set.seed(2)
x<-x[!is.na(x)]
y<-y[!is.na(y)]
n1=length(x)
n2=length(y)
qe=NA
m=as.vector(outer(x,y,FUN='-'))
if(is.null(pts))e=estfun(m,q)
else e=pts
est=mean(m<=e)
V=NA
for(i in 1:nboot){
id1=sample(n1,replace=TRUE)
id2=sample(n2,replace=TRUE)
M=as.vector(outer(x[id1],y[id2],FUN='-'))
V[i]=mean(M<=e)
}
crit<-alpha/2
icl<-round(crit*nboot)+1
icu<-nboot-icl
a=sort(V)
ci=a[icl]
ci[2]=a[icu]
list(Est=est,ci=ci)
}

difqci=tailci


tailci.reverse<-function(x,y,pts=NULL,q=.25, nboot=1000,estfun=hdmq,alpha=.05,SEED=TRUE){
  #
  #   If pts is specified, the goal is to compute a confidence interval 
  # P(x-y> pts). If pts is not specified, it is taken to be an estimate of the qth quantile,
  # q=.25 is the default.
  #
  if(SEED)set.seed(2)
  x<-x[!is.na(x)]
  y<-y[!is.na(y)]
  n1=length(x)
  n2=length(y)
  qe=NA
  m=as.vector(outer(x,y,FUN='-'))
  if(is.null(pts))e=estfun(m,q)
  else e=pts
  est=mean(m>=e)
  V=NA
  for(i in 1:nboot){
    id1=sample(n1,replace=TRUE)
    id2=sample(n2,replace=TRUE)
    M=as.vector(outer(x[id1],y[id2],FUN='-'))
    V[i]=mean(M>=e) 
  }
  crit<-alpha/2
  icl<-round(crit*nboot)+1
  icu<-nboot-icl
  a=sort(V)
  ci=a[icl]
  ci[2]=a[icu]
  list(Est=est,ci=ci)
}

tamhane<-function(x,x2=NA,cil=NA,crit=NA){
#
# First stage of Tamhane's method
#
# x contains first stage data
# x2 contains second stage data
#
# cil is the desired length of the confidence intervals.
# That is, cil is the distance between the upper and lower
# ends of the confidence intervals.
#
if(is.matrix(x))x<-listm(x)
if(!is.list(x))stop("Data must be stored in list mode or in matrix mode.")
J<-length(x)
tempn<-0
svec<-NA
for(j in 1:J){
temp<-x[[j]]
temp<-temp[!is.na(temp)] # Remove missing values.
tempn[j]<-length(temp)
x[[j]]<-temp
svec[j]<-var(temp)
}
A<-sum(1/(tempn-1))
df<-J/A
paste("The degrees of freedom are:",df)
if(is.na(crit))stop("Enter a critical value and reexecute this function")
if(is.na(cil))stop("To proceed, you must specify the length of the confidence intervals.")
d<-(cil/(2*crit))^2
n.vec<-NA
for(j in 1:J){
n.vec[j]<-max(tempn[j]+1,floor(svec[j]/d)+1)
}
ci.mat<-NA
if(!is.na(x2[1])){
if(is.matrix(x2))x2<-listm(x2)
if(!is.list(x2))stop("Data must be stored in list mode or in matrix mode.")
TT<-NA
U<-NA
J<-length(x)
nvec2<-NA
for(j in 1:length(x)){
nvec2[j]<-length(x2[[j]])
if(nvec2[j] <n.vec[j]-tempn[j]){
paste("The required number of observations for group",j," in the second stage is ")
paste(n.vec[j]-tempn[j]," but only ",nvec2[j]," are available")
stop()
}
TT[j]<-sum(x[[j]])
U[j]<-sum(x2[[j]])
print(c(TT[j],U[j],nvec2[j]))
}
b<-sqrt(tempn*((tempn+nvec2)*d-svec)/(nvec2*svec))
b<-(b+1)/(tempn+nvec2)
print(c(b,svec))
xtil<-TT*(1-nvec2*b)/tempn+b*U
print(xtil)
jall<-(J^2-J)/2
ci.mat<-matrix(0,ncol=4,nrow=jall)
dimnames(ci.mat)<-list(NULL,c("Group","Group","ci.low","ci.high"))
ic<-0
for(j in 1:J){
for(k in 1:J){
if(j<k){
ic<-ic+1
ci.mat[ic,1]<-j
ci.mat[ic,2]<-k
ci.mat[ic,3]<-xtil[j]-xtil[k]-cil/2
ci.mat[ic,4]<-xtil[j]-xtil[k]+cil/2
}}}}
list(n.vec=n.vec,ci.mat=ci.mat)
}




# ----------------------------------------------------------------------------

# tbs

# ----------------------------------------------------------------------------

tbs<- function(x,eps=1e-3,maxiter=20,r=.45,alpha=.05,init.est=OGK){
#        Rocke's contrained s-estimator
#
#      r=.45 is the breakdown point
#      alpha=.05 is the asymptotic rejection probability.
#
x<-elimna(x)
x=as.matrix(x)
    n <- nrow(x)
    p <- ncol(x)
LIST=FALSE
if(p==1){
LIST=T
p=2
x=cbind(x,rnorm(nrow(x)))
# Yes, this code is odd, but for moment easiest way of handling p=1
}
temp<-init.est(x)
#  very poor outside rate per obs under normality.
t1<-temp$center
s<-temp$cov
c1M<-cgen.bt(n,p,r,alpha,asymp=FALSE)
c1<-c1M$c1
if(c1==0)c1<-.001 #Otherwise get division by zero
M<-c1M$M
    b0 <- erho.bt(p,c1,M)
    crit <- 100
    iter <- 1
    w1d <- rep(1,n)
    w2d <- w1d
    while ((crit > eps)&(iter <= maxiter))
    {
        t.old <- t1
        s.old <- s
        wt.old <- w1d
        v.old <- w2d
        d2 <- mahalanobis(x,center=t1,cov=s)
        d <- sqrt(d2)
        k <- ksolve.bt(d,p,c1,M,b0)
        d <- d/k
        w1d <- wt.bt(d,c1,M)
        w2d <- v.bt(d,c1,M)
        t1 <- (w1d %*% x)/sum(w1d)
        s <- s*0
        for (i in 1:n)
        {
            xc <- as.vector(x[i,]-t1)
            s <- s + as.numeric(w1d[i])*(xc %o% xc)
        }
        s <- p*s/sum(w2d)
        mnorm <- sqrt(as.vector(t.old) %*% as.vector(t.old))
        snorm <- eigen(s.old)$values[1]
        crit1 <- max(abs(t1 - t.old))
#        crit <- max(crit1,crit2)
        crit <- max(abs(w1d-wt.old))/max(w1d)
        iter <- iter+1
    }
if(LIST){
v1=t1[1]
v2=s[1,1]
return(list(center=v1,var=v2))
}
if(!LIST)return(list(center=t1,cov=s))
}

test.4.break<-function(x,y,int=NULL,xout=TRUE,regfun=tsreg,outfun=outpro,
plotit=TRUE,SEED=TRUE,...){
#
# Estimate the breakpoint and then compare the resulting slopes and intercepts
# int is the search  interval, see the function reg.break
#
# Rejecting provides evidence that a breakpoint exists.
#
FLAG=TRUE
x<-as.matrix(x)
p1<-ncol(x)+1
p<-ncol(x)
if(p!=1)stop('Current version limited to a single independent variable')
xy<-cbind(x,y)
xy<-elimna(xy)
x<-xy[,1:p]
x=as.vector(x)
y<-xy[,p1]
e=reg.break(x,y,int,xout=xout,regfun=regfun,outfun=outfun,...)
id=x<=e$break.est
a=reg2ci(x[id],y[id],x[!id],y[!id],regfun=regfun,xout=xout,SEED=SEED,plotit=plotit)
output=matrix(NA, 2,5)
dimnames(output)=list(c('Inter','slope'),c('ci.low,dif','ci.upper.dif','p-value',
'Est.below break','Est.above break'))
output[1,]=a$output[1,2:6]
output[2,]=a$output[2,2:6]
list(n=e$n,break.est=e$break.est,test.results=output)
}




# ----------------------------------------------------------------------------

# thd

# ----------------------------------------------------------------------------

thd<- function(x, q=.5, width = 1 / sqrt(length(x)))
sapply(q, function(p) {
#
# q =quantiles to be estimated
#
n <- length(x)
if (n == 0) return(NA)
if (n == 1) return(x)
x <- sort(x)
a <- (n + 1) * p
b <- (n + 1) * (1 - p)
hdi <- getBetaHdi(a, b, width)
hdiCdf <- pbeta(hdi, a, b)
cdf <- function(xs) {
xs[xs <= hdi[1]] <- hdi[1]
xs[xs >= hdi[2]] <- hdi[2]
(pbeta(xs, a, b) - hdiCdf[1]) / (hdiCdf[2] - hdiCdf[1])
}
iL <- floor(hdi[1] * n)
iR <- ceiling(hdi[2] * n)
cdfs <- cdf(iL:iR/n)
W <- tail(cdfs, -1) - head(cdfs, -1)
sum(x[(iL+1):iR] * W)
})

TKmeans<-function(x,k,trim=0.1,scaling=FALSE,runs=100,points=NULL,
countmode=runs+1,printcrit=FALSE,maxit = 2 * nrow(as.matrix(data))){
#
# Cluster analysis using trimmed k means method.
# This function merely accesses the R package trimcluster
# It uses the same default values for the arguments and it
# performs casewise deletion of missing values
#
x=elimna(x)
library(trimcluster)
res=trimkmeans(x,k=k,trim=trim,scaling=scaling,runs=runs,points=points,
countmode=countmode,printcrit=printcrit,maxit=maxit)
res
}




# ----------------------------------------------------------------------------

# TKmeans.grp

# ----------------------------------------------------------------------------

TKmeans.grp<-function(x,k,y){
#
# Create k groups based on data in x using trimmed k means method
#  Then sort the data in y into k groups based on the groups indicated
#
x=elimna(x)
x=as.matrix(x)
y=as.matrix(y)
res=Kmeans(x,k)
grpid=res[[1]]
grpdat=list()
for(i in 1:k){
flag=(grpid==i)
grpdat[[i]]=y[flag,]
}
grpdat
}


tlist<-function(x){
#
# x is assumed to have list mode
# check for tied values
#
#  if(!list(x))stop('x should have list mode')
chk=0
for(j in 1:length(x))chk=chk+length(x[[j]])-length(unique(x[[j]]))
chk
}




# ----------------------------------------------------------------------------

# TM

# ----------------------------------------------------------------------------

TM<-function(x,bend=1.28){

   ## x has  list mode
   ## Computes TM test statistic.
   ## "mestse" is used as the standard error of one-step M-estimator and
   ## "mad" is used as a measure of scale.

  X<-lapply(x,na.omit)
  f1<-function(t){length(t[abs((t-median(t))/mad(t))>bend])}
  alist<-X
  f<-(sapply(alist,length))-(sapply(alist,f1))
  s=sapply(alist,mestse)^2
  wden=sum(1/s)
  w=(1/s)/wden
  xplus<-sum(w*(sapply(alist,onestep)))
  tt<-((sapply(alist,onestep))-xplus)/sqrt(s)
  TM<-sum(tt^2)
  list(TM=TM)
}

trange<-function(dfvec,iter=10000,alpha=.05,SEED=TRUE){
if(SEED)set.seed(1)
dfv<-length(dfvec)/sum(1/dfvec)
vals<-NA
tvals<-NA
J<-length(dfvec)
for(i in 1:iter){
for(j in 1:J){
tvals[j]<-rt(1,dfvec[j])
}
vals[i]<-max(tvals)-min(tvals)
}
vals<-sort(vals)
ival<-round((1-alpha)*iter)
qval<-vals[ival]
qval
}




# ----------------------------------------------------------------------------

# trqreg

# ----------------------------------------------------------------------------

trqreg<-function(x,y,a1=.1,a2,xout=FALSE,outfun=outpro){
library(quantreg)
x<-as.matrix(x)
p1<-ncol(x)+1
p<-ncol(x)
xy<-cbind(x,y)
xy<-elimna(xy)
x<-xy[,1:p]
y<-xy[,p1]
if(xout){
m<-cbind(x,y)
flag<-outfun(x,plotit=FALSE)$keep
m<-m[flag,]
x<-m[,1:p]
y<-m[,p1]
}
n=length(y)
xx=cbind(rep(1,n),x)
z <- rq.fit.br(xx, y, tau = -1)
res=trq.fit(x,y,z=z,a1=a1,a2)
coef=res$coef[1:p1]
list(coef=coef)
}




# ----------------------------------------------------------------------------

# tshd

# ----------------------------------------------------------------------------

tshd<-function(x,y,HD=TRUE,plotit=FALSE,xlab='X',ylab='Y',OPT=FALSE,tr=FALSE){
#
# Compute the Theil-Sen regression estimator.
# Only a single predictor is allowed in this version
#
#  HD=TRUE, use Harrell-Davis for slopes
#  HD=FALSE, use usual median
#
#  OPT=TRUE, compute the intercept using median(y)-b_1median(X)
#  OPT=FALSE compute the intercept using median of y-b_1X
#
#
temp<-matrix(c(x,y),ncol=2)
temp<-elimna(temp)     # Remove any pairs with missing values
x<-temp[,1]
y<-temp[,2]
ord<-order(x)
xs<-x[ord]
ys<-y[ord]
vec1<-outer(ys,ys,'-')
vec2<-outer(xs,xs,'-')
v1<-vec1[vec2>0]
v2<-vec2[vec2>0]
if(!HD)slope<-median(v1/v2,na.rm=TRUE)
if(HD)slope<-hd(v1/v2,na.rm=TRUE,tr=tr)
res=y-slope*x
if(!OPT)int=hd(res,tr=tr)
if(OPT)int=hd(y,na.rm=TRUE)-slope*hd(x,na.rm=TRUE,tr=tr)
coef=c(int,slope)
if(plotit){
plot(x,y,xlab=xlab,ylab=ylab)
abline(coef)
}
list(coef=coef)
}

tsplit<-function(J,K,data,tr=.2,grp=c(1:p),p=J*K){
#  Perform a J by K anova on trimmed means with
#  repeated measures on the second factor. That is, a split-plot design
#  is assumed, with the first factor consisting of independent groups.
#
#  The  variable data is assumed to contain the raw
#  data stored in list mode or a matrix.
#  If in list mode, data[[1]] contains the data
#  for the first level of both factors: level 1,1.
#  data[[2]] is assumed to contain the data for level 1 of the
#  first factor and level 2 of the second: level 1,2
#  data[[K]] is the data for level 1,K
#  data[[K+1]] is the data for level 2,1, data[2K] is level 2,K, etc.
#
#  The default amount of trimming is tr=.2
#
#  It is assumed that data has length JK, the total number of
#  groups being tested, but a subset of the data can be analyzed
#  using grp
#
if(is.data.frame(data))data=as.matrix(data)
x<-data
       if(is.matrix(x)) {
                y <- list()
                for(j in 1:ncol(x))
                        y[[j]] <- x[, j]
                data <- y
        }
if(!is.list(data))stop("Data are not stored in list mode or a matrix")
if(p!=length(data)){
print("The total number of groups, based on the specified levels, is")
print(p)
print("The number of groups in data is")
print(length(data))
print("Warning: These two values are not equal")
}
if(p!=length(grp)){
print("Apparently a subset of the groups was specified")
print("that does not match the total number of groups ")
stop("indicated by the values for J and K.")
}
tmeans<-0
h<-0
v<-matrix(0,p,p)
klow<-1-K
kup<-0
for (i in 1:p)tmeans[i]<-mean(data[[grp[i]]],tr,na.rm=TRUE)
for (j in 1:J){
h[j]<-length(data[[grp[j]]])-2*floor(tr*length(data[[grp[j]]]))
#    h is the effective sample size for the jth level of factor A
#   Use covmtrim to determine blocks of squared standard errors and
#   covariances.
klow<-klow+K
kup<-kup+K
sel<-c(klow:kup)
v[sel,sel]<-covmtrim(data[grp[klow:kup]],tr)
}
ij<-matrix(c(rep(1,J)),1,J)
ik<-matrix(c(rep(1,K)),1,K)
jm1<-J-1
cj<-diag(1,jm1,J)
for (i in 1:jm1)cj[i,i+1]<-0-1
km1<-K-1
ck<-diag(1,km1,K)
for (i in 1:km1)ck[i,i+1]<-0-1
#  Do test for factor A
cmat<-kron(cj,ik)  # Contrast matrix for factor A
Qa<-johansp(cmat,tmeans,v,h,J,K)
# Do test for factor B
cmat<-kron(ij,ck)  # Contrast matrix for factor B
Qb<-johansp(cmat,tmeans,v,h,J,K)
# Do test for factor A by B interaction
cmat<-kron(cj,ck)  # Contrast matrix for factor A by B
Qab<-johansp(cmat,tmeans,v,h,J,K)
list(Qa=Qa$teststat,Qa.p.value=Qa$p.value,
Qb=Qb$teststat,Qb.p.value=Qb$p.value,
Qab=Qab$teststat,Qab.p.value=Qab$p.value)
}




# ----------------------------------------------------------------------------

# tsplitbt

# ----------------------------------------------------------------------------

tsplitbt<-function(J,K,x,tr=.2,alpha=.05,JK=J*K,grp=c(1:JK),nboot=599,
SEED=TRUE,monitor=FALSE){
#
# A bootstrap-t for performing a split-plot design
# with trimmed means.
# By default, 20% trimming is used with B=599 bootstrap samples.
#
#
#  The R variable x is assumed to contain the raw
#  data stored in list mode or in a matrix.
#  If in list mode, x[[1]] contains the data
#  for the first level of both factors: level 1,1.
#  x[[2]] is assumed to contain the data for level 1 of the
#  first factor and level 2 of the second: level 1,2
#  x[[K]] is the data for level 1,K
#  x[[K+1]] is the data for level 2,1, x[[2K]] is level 2,K, etc.
#
#  If the data are in a matrix, column 1 is assumed to
#  correspond to x[[1]], column 2 to x[[2]], etc.
#
#  When in list mode x is assumed to have length JK, the total number of
#  groups being tested, but a subset of the data can be analyzed
#  using grp
#
if(SEED)set.seed(2)
if(is.data.frame(x))x=as.matrix(x)
if(is.matrix(x)) {
y <- list()
ik=0
il=c(1:K)-K
for(j in 1:J){
il=il+K
zz=x[,il]
zz=elimna(zz)
for(k in 1:K){
ik=ik+1
y[[ik]]=zz[,k]
}}
                x <- y
}
JK<-J*K
data<-list()
xcen<-list()
for(j in 1:length(x)){
data[[j]]<-x[[grp[j]]] # Now have the groups in proper order.
xcen[[j]]<-data[[j]]-mean(data[[j]],tr) # Centered data
}
x<-data
#
set.seed(2) # set seed of random number generator so that
#             results can be duplicated.
# Next determine the n_j values
nvec<-NA
jp<-1-K
for(j in 1:J){
jp<-jp+K
nvec[j]<-length(x[[j]])
}
if(min(nvec)<10){
print("Warning: with small sample sizes, a bootstrap-t method can")
print("result in estimated variances equal to zero, resulting in")
print("this function terminating and giving no results and peculiar error messages.")
}
blist<-list()
print("Taking bootstrap samples. Please wait.")
testmat<-matrix(NA,ncol=3,nrow=nboot)
for(iboot in 1:nboot){
iv<-0
for(j in 1:J){
temp<-sample(nvec[j],replace = T)
for(k in 1:K){
iv<-iv+1
tempx<-xcen[[iv]]
blist[[iv]]<-tempx[temp]
}}
if(monitor)print(paste("Bootstrap iteration" ,iboot, "is complete"))
btest<-tsplit(J,K,blist,tr)
testmat[iboot,1]<-btest$Qa
testmat[iboot,2]<-btest$Qb
testmat[iboot,3]<-btest$Qab
}
lcrit<-round((1-alpha)*nboot)
temp3<-sort(testmat[,1])
crit.Qa<-temp3[lcrit]
temp3<-sort(testmat[,2])
crit.Qb<-temp3[lcrit]
temp3<-sort(testmat[,3])
crit.Qab<-temp3[lcrit]
temp4<-tsplit(J,K,x,tr=tr)
list(Qa=temp4$Qa,Qb=temp4$Qb,Qab=temp4$Qab,crit.Qa=crit.Qa,crit.Qb=crit.Qb,crit.Qab=crit.Qab)
}
tsregF<-function(x,y,xout=FALSE,outfun=out,iter=10,varfun=pbvar,
corfun=pbcor,plotit=FALSE,tol=.0001,...){
#
#  Compute Theil-Sen regression estimator
#
#  Use Gauss-Seidel algorithm
#  when there is more than one predictor
#
#
x<-as.matrix(x)
xx<-cbind(x,y)
xx<-elimna(xx)
x<-xx[,1:ncol(x)]
x<-as.matrix(x)
y<-xx[,ncol(x)+1]
temp<-NA
x<-as.matrix(x)
if(xout){
x<-as.matrix(x)
flag<-outfun(x,plotit=plotit,...)$keep
x<-x[flag,]
y<-y[flag]
x<-as.matrix(x)
}
if(ncol(x)==1){
temp1<-tsp1reg(x,y)
coef<-temp1$coef
res<-temp1$res
}
if(ncol(x)>1){
for(p in 1:ncol(x)){
temp[p]<-tsp1reg(x[,p],y)$coef[2]
}
res<-y-x%*%temp
alpha<-median(res)
r<-matrix(NA,ncol=ncol(x),nrow=nrow(x))
tempold<-temp
for(it in 1:iter){
for(p in 1:ncol(x)){
r[,p]<-y-x%*%temp-alpha+temp[p]*x[,p]
temp[p]<-tsp1reg(x[,p],r[,p],plotit=FALSE)$coef[2]
}
if(max(abs(temp-tempold))<tol)break
alpha<-median(y-x%*%temp)
tempold<-temp
}
coef<-c(alpha,temp)
res<-y-x%*%temp-alpha
}
yhat<-y-res
stre=NULL
temp=varfun(y)
if(temp==0)print('Warning: When computing strength of association, measure of variation=0')
e.pow=NULL
if(temp>0){
e.pow<-varfun(yhat)/varfun(y)
if(!is.na(e.pow)){
if(e.pow>=1)e.pow<-corfun(yhat,y)$cor^2
e.pow=as.numeric(e.pow)
stre=sqrt(e.pow)
}}
res=NULL
list(coef=coef,residuals=res,Strength.Assoc=stre,Explanatory.Power=e.pow)
}


twocor<-function(x1,y1,x2,y2,corfun=pbcor,nboot=599,alpha=.05,SEED=TRUE,...){
#
#  Compute a .95 confidence interval for the
#  difference between two correlation coefficients
#  corresponding to two independent groups.
#
#   the function corfun is any R function that returns a
#   correlation coefficient in corfun$cor. The functions pbcor and
#   wincor follow this convention.
#
#   For Pearson's correlation, use
#   the function twopcor instead.
#
#   The default number of bootstrap samples is nboot=599
#
if(SEED)set.seed(2) # set seed of random number generator so that
#             results can be duplicated.
#print("Taking bootstrap samples. Please wait.")
data1<-matrix(sample(length(y1),size=length(y1)*nboot,replace=TRUE),nrow=nboot)
bvec1<-apply(data1,1,corbsub,x1,y1,corfun,...) # A 1 by nboot matrix.
data2<-matrix(sample(length(y2),size=length(y2)*nboot,replace=TRUE),nrow=nboot)
bvec2<-apply(data2,1,corbsub,x2,y2,corfun,...) # A 1 by nboot matrix.
bvec<-bvec1-bvec2
bsort<-sort(bvec)
term<-alpha/2
ilow<-round((alpha/2) * nboot)
ihi<-nboot - ilow
ilow<-ilow+1
corci<-1
corci[1]<-bsort[ilow]
corci[2]<-bsort[ihi]
pv<-(sum(bvec<0)+.5*sum(bvec==0))/nboot
pv=2*min(c(pv,1-pv))
r1<-corfun(x1,y1)$cor
r2<-corfun(x2,y2)$cor
reject<-"NO"
if(corci[1]>0 || corci[2]<0)reject="YES"
list(r1=r1,r2=r2,ci.dif=corci,p.value=pv)
}


UB.class<-function(train=NULL,test=NULL,g=NULL,x1=NULL,x2=NULL,
method=c('DIS','DEP','PRO'),depthfun=prodepth,...){
#
# A collection of classification methods  for which the error rate is not
#  impacted by unequal sample sizes.
#  Bagged version are available  in class.bag
#
#  DIS:  Points classified based on their depths
#  DEP: Uses depths as suggested by Makinde and Fasoranbaku (2018). JAS
#  PRO: project the points onto a line connecting the centers of the data clouds.
#       Then use estimate of the pdf for each group to make a decision about future points.
#
type=match.arg(method)
switch(type,
    DIS=discdepth(train=train,test=test,g=g,x1=x1,x2=x2),
    DEP=Depth.class(train=train,test=test,g=g,x1=x1,x2=x2),
    PRO=pro.classPD(train=train,test=test,g=g,x1=x1,x2=x2),
    )
}




# ----------------------------------------------------------------------------

# UB.class.error

# ----------------------------------------------------------------------------

UB.class.error<-function(train=NULL,g=NULL,x1=NULL,x2=NULL,method=c('KNN','SVM','DIS','DEP','PRO','PROBAG'),
alpha=.05,pro.p=.8,SEED=TRUE,...){
#
#  Use cross validation to estimate error rates associated with the classification method indicated by the argument
#  method
#
#  By default, estimates are computed for each method listed in the argument'
#  method
#
#  To include a neural net method, included 'NNbag' in methods; not included automatically to avoid high execution  time.
#
#  Also reports estimates of a false positive or false negative, but no confidence interval is included. The obvious approach performs poorly
#
#  pro.p = proportion used from each of the  two training groups; remainder used as test data.
#
#  g=class id
#  if there are two classes and the training data are stored in  separate variables, can enter
#  the data for each class via the arguments
#  x1 and x2.
#
#  FN: False negative, assign to group 2 by mistake  e.g., NULL predict no fracture  but Non-null gets a fracture
#  FP: False positive, assign to group 1 by mistake like NULL, e.g, will not have a fracture, but did
#   TE: Overall all error rate.
#
#
n.est=length(method)
if(SEED)set.seed(2)
if(!is.null(train)){
if(is.null(g))stop('Argument g, group ids, must be specified')
if(is.matrix(g)){
if(dim(g)>1)stop('Argument g should be a vector')
}
Train=cbind(train,g)
Train=elimna(Train)
p=ncol(train)
p1=p+1
train=Train[,1:p]
g=Train[,p1]
flag=g==min(g)
x1=Train[flag,1:p]
x2=Train[!flag,1:p]
}
if(is.null(x1))stop('Something is wrong, no data in x1')
if(is.null(x2))stop('Something is wrong, no data in x2')
if(ncol(x1)!=ncol(x2))stop('x1 and x2 have different number of columns')
x1=elimna(x1)
x2=elimna(x2)
n1=nrow(x1)
n2=nrow(x2)
ns1=round(pro.p*n1)
ns2=round(pro.p*n2)
ciFN=matrix(NA,nrow=reps,ncol=2)
ciFP=matrix(NA,nrow=reps,ncol=2)
ciTE=matrix(NA,nrow=reps,ncol=2)
phat.FN=NA
phat.FP=NA
phat.TE=NA
RES=matrix(NA,nrow=n.est,ncol=3)
dimnames(RES)=list(method,c('phat.FN','phat.FP','phat.TE'))
for(k in 1:n.est){
N1=sample(n1,ns1)
N2=sample(n2,ns2)
test1=x1[-N1,]
test2=x2[-N2,]
a1=UB.class(x1=x1[N1,],x2=x2[N2,],test=test1,method=method[k],SEED=SEED,...)
a2=UB.class(x1=x1[N1,],x2=x2[N2,],test=test2,method=method[k],SEED=SEED,...)
flag1=a1!=1 # ID  False negatives e.g., predict no fracture but fracture occurred.
flag2=a2!=2 # ID  False positives e.g., predict fracture but no fracture occurred.
FN=binom.conf(y=flag1,alpha=alpha,pr=FALSE)
FP=binom.conf(y=flag2,alpha=alpha,pr=FALSE)  #CI does not work well when P(FP)=.5
TE=binom.conf(y=c(flag1,flag2),alpha=alpha,pr=FALSE) #total error
RES[k,]=c(FN$phat,FP$phat,TE$phat)
}
RES
}

UBROC<-function(train=NULL,g=NULL,x1=NULL,x2=NULL,method='KNN',reps=10,pro.p=.8,SEED=TRUE,POS=TRUE,EN=TRUE,...){
#
#  Compute ROC curve based on an 'unbiased' classification method if  EN=TRUE, possible bias if EN=FALSE
#  method indicates the method to be used
#
# Current choices available:
#  KNN: Nearest neighbor using robust depths
#  DIS:  Points classified based on their depths
#  DEP: Uses depths as suggested by Makinde and Fasoranbaku (2018). JAS
#  SVM: support vector machine
#  RF: Random forest
#  NN: neural network
#  ADA: ada boost
#  PRO: project the points onto a line connecting the centers of the data clouds.
#       Then use estimate of the pdf for each group to make a decision about future points.
#
#
#  reps: number of resamples resamples
#  pro.p controls the proportion used in the training, the rest are used in the test set
#
library(ROCR)
CHK=FALSE
if(!is.null(x1)){
if(!is.null(x2)){
if(ncol(x1)!=ncol(x2))stop('x1 and x2 have different number of columns')
x1=elimna(x1)
x2=elimna(x2)
n1=nrow(x1)
n2=nrow(x2)
g=c(rep(0,n1),rep(1,n2))
CHK=TRUE
}}
if(!CHK){
if(is.null(g))stop('The argument g should contain the group id values')
xg=elimna(cbind(train,g))
p=ncol(train)
p1=p+1
train=xg[,1:p]
g=xg[,p1]
if(length(unique(g))!=2)stop('Should have only two unique values in g')
flag=g==min(g)
x1=train[flag,]
x2=train[!flag,]
}
n1=nrow(x1)
n2=nrow(x2)
ns1=round(pro.p*n1)
ns2=round(pro.p*n2)
if(EN)ns1=ns2=min(ns1,ns2)
PRED=NULL
LABS=NULL
for(j in 1:reps){
N1=sample(n1,ns1)
N2=sample(n2,ns2)
test1=x1[-N1,]
test2=x2[-N2,]
a1=CLASS.fun(x1=x1[N1,],x2=x2[N2,],test=test1,method=method,...)
a2=CLASS.fun(x1=x1[N1,],x2=x2[N2,],test=test2,method=method,...)
PRED=c(PRED,c(a1,a2))
LABS=c(LABS,c(rep(1,length(a1)),rep(2,length(a2))))
pred=prediction(PRED,LABS)
}
if(POS)perf <- performance(pred,'tpr','fpr')
if(!POS)perf <- performance(pred,'tnr','fnr')
plot(perf)
}




# ----------------------------------------------------------------------------

# v.bt

# ----------------------------------------------------------------------------

v.bt <- function(x,c1,M) return(x*psi.bt(x,c1,M))




# ----------------------------------------------------------------------------

# varcom.IND.MP

# ----------------------------------------------------------------------------

varcom.IND.MP<-function(x,y,SEED=TRUE){
#
#
#  Compare the variances of two independent variables.
#  Uses an updated Morgan-Pitman test based on a random
#  permutations of the data.
#
#  Returns a p-value and estimates of the variances
#  No confidence interval
#
if(SEED)set.seed(2)
x=elimna(x)
y=elimna(y)
e1=var(x)
e2=var(y)
if(length(x)>length(y)){
tempx=x
tempy=y
x=tempy
y=tempx
}
n1=length(x)
n2=length(y)
n=min(n1,n2)
nmax=max(n1,n2)
X=sample(x,n1)
Y=sample(y,n2)
p1=comdvar(X[1:n],Y[1:n])$p.value
PV=p1
if(n1!=n2){
neq=floor(nmax/n)
EQ=neq*n1
A=matrix(c(1:EQ),nrow=n1)
PV=NA
J=ncol(A)
for(j in 1:J)PV[j]=comdvar(X[1:n],Y[A[,j]])$p.value
if(nmax>EQ){
d=n2-n1+1
Y2=Y[n2:d]  #deliberately reversed the order.
p2=comdvar(X[1:n],Y2)$p.value
PV=c(PV,p2)
}}
PV=min(p.adjust(PV,method='hoch'))
list(est1=e1,est2=e2,p.value=PV)
}




# ----------------------------------------------------------------------------

# vecnorm

# ----------------------------------------------------------------------------

vecnorm<-function(x, p=2) sum(x^p)^(1/p)




# ----------------------------------------------------------------------------

# wAKP.avg

# ----------------------------------------------------------------------------

wAKP.avg<-function(x,tr=.2){
#
# Have J dependent groups. For each pair of groups, compute
# AKP type measure of effect size and average the results
#
#  For tr=0, get Cohen d type measure
#
#
a=wmcpAKP(x,tr=tr)
e=mean(a[,3])
e
}




# ----------------------------------------------------------------------------

# wband

# ----------------------------------------------------------------------------

wband<-function(x,y,alpha=.05,plotit=FALSE)
{
#  Compute a confidence band for the shift function
#  Assuming two independent groups are being compared
#
# Note: which group is the reference group matters.
# wband(x,y)  often gives different results than wband(y,x).
#
x<-x[!is.na(x)]  # Remove missing values from x.
y<-y[!is.na(y)]  # Remove missing values from y.
n1=length(x)
n2=length(y)
crit=ksw.crit(n1,n2,alpha)
plotit<-as.logical(plotit)
flag<-as.logical(flag)
pc<-NA
xsort<-sort(x)
ysort<-sort(y)
l<-0
u<-0
ysort[0]<-NA
ysort[length(y)+1]<-NA
m<-length(x)*length(y)/(length(x)+length(y))
lambda<-length(x)/(length(x)+length(y)) #M in book
cc<-crit^2/m
temp1<-1+cc*(1-lambda)^2
for(ivec in 1:length(x)){
uu<-ivec/length(x)
temp<-.5*sqrt(cc^2*(1-lambda)^2+4*cc*uu*(1-uu))
hminus<-(uu+.5*cc*(1-lambda)*(1-2*lambda*uu)-temp)/temp1
hplus<-(uu+.5*cc*(1-lambda)*(1-2*lambda*uu)+temp)/temp1
isub<-max(0,floor(length(y)*hminus)+1)
l[ivec]<-ysort[isub+1]-xsort[ivec]
if(hminus<0)l[ivec]=NA
isub<-max(0,floor(length(y)*hplus)+1)
u[ivec]<-ysort[isub+1]-xsort[ivec]
}
num<-length(l[l>0 & !is.na(l)])+length(u[u<0 & !is.na(u)])
qhat<-c(1:length(x))/length(x)
m<-matrix(c(qhat,l,u),length(x),3)
dimnames(m)<-list(NULL,c('qhat','lower','upper'))
if(plotit){
temp2 <- m[, 2]
temp2 <- temp2[!is.na(temp2)]
xsort<-sort(x)
ysort<-sort(y)
del<-0
for (i in 1:length(x)){
ival<-round(length(y)*i/length(x))
if(ival<=0)ival<-1
if(ival>length(y))ival<-length(y)
del[i]<-ysort[ival]-xsort[i]
}
xaxis<-c(xsort,xsort,xsort)
yaxis<-c(del,m[,2],m[,3])
plot(xaxis,yaxis,type='n',ylab='delta',xlab='x (first group)')
lines(xsort,del)
lines(xsort,m[,2],lty=2)
lines(xsort,m[,3],lty=2)
temp <- summary(x)
                text(temp[3], min(temp2), '+')
                text(temp[2], min(temp2), 'o')
                text(temp[5], min(temp2), 'o')
}
list(m=m,crit=crit,numsig=num,prob.coverage=1-kswsig(n1,n2,crit))
}


whimed <- function(a,iw,n)
{
	acand=0
	iwcand=0
	nn=n
	wtotal=sum(iw[1:nn])
	wrest=0
	IsFound=0
	while (IsFound==0)
	{
		trial=pull(a,nn,floor(nn/2)+1)

		wleft=sum(iw[c(a[1:nn]<trial,rep(F,n-nn))])
		wright=sum(iw[c(a[1:nn]>trial,rep(F,n-nn))])
		wmid=sum(iw[c(a[1:nn]==trial,rep(F,n-nn))])

		if ((2*wrest+2*wleft)>wtotal)
		{
			i=c(a[1:nn]<trial,rep(F,n-nn))
			acand=a[i]
			iwcand=iw[i]
#			nn=kcand_length(acand)
			nn=length(acand)

		}
		else
		{
			if ((2*wrest+2*wleft+2*wmid)>wtotal)
			{
				whmed=trial
				IsFound=1
			}
			else
			{
				i=c(a[1:nn]>trial,rep(F,n-nn))
				acand=a[i]
				iwcand=iw[i]
	nn=length(acand)
#		nn_kcand_length(acand)
				wrest=wrest+wleft+wmid
			}
		}
		a[1:nn]=acand[1:nn]
		iw[1:nn]=iwcand[1:nn]
	}
	whmed
}




# ----------------------------------------------------------------------------

# wilbinomci

# ----------------------------------------------------------------------------

wilbinomci<-function(x = sum(y), n = length(y), y = NULL, alpha = 0.05){
#
#  Wilson type confidence interval used by Zou et al. (2009)
if(!is.null(y[1])){
y=elimna(y)
n=length(y)
}
if(n==1)stop('Something is wrong: number of observations is only 1')
cr=qnorm(alpha/2)
phat=x/n
term=phat*(1-phat)+cr^2/(4*n)
lower=(phat+cr^2/(2*n)+cr*sqrt(term/n))/(1+cr^2/n)
upper=(phat+cr^2/(2*n)-cr*sqrt(term/n))/(1+cr^2/n)
if(x==0){  #Use Clopper-Pearson
lower<-0
upper<-1-alpha^(1/n)
}
if(x==1){
upper<-1-(alpha/2)^(1/n)
lower<-1-(1-alpha/2)^(1/n)
}
if(x==n-1){
lower<-(alpha/2)^(1/n)
upper<-(1-alpha/2)^(1/n)
}
if(x==n){
lower<-alpha^(1/n)
upper<-1
}
phat=x/n
list(phat=phat,ci=c(lower,upper),n=n)
}




# ----------------------------------------------------------------------------

# wlogreg

# ----------------------------------------------------------------------------

wlogreg<-function(x,y,initwml=FALSE,const=0.5,kmax=1e3,maxhalf=10){
#
#
#  Bianco and Yohai (1996) in logistic regression
#
#
options(warn=-1)
xy=cbind(x,y)
p1=ncol(xy)
xy=elimna(xy)
p=p1-1
if(p==1){
library(robustbase)
a=BYlogreg(x,y,initwml=initwml,const=const,kmax=kmax,maxhalf=maxhalf)
}
else
a=wlogregv2(x,y,initwml=initwml,const=const,kmax=kmax,maxhalf=maxhalf)
options(warn=0)
a
}






wlogregci<-function(x,y,nboot=400,alpha=.05,SEED=TRUE,MC=FALSE,
xlab="Predictor 1",ylab="Predictor 2",xout=FALSE,outfun=out,...){
#
#   Compute a  confidence interval for each of the parameters of
#   a log linear model based on a robust estimator
#
#   The predictor values are assumed to be in the n by p matrix x.
#

x<-as.matrix(x)
p1<-ncol(x)+1
p<-ncol(x)
xy<-cbind(x,y)
xy<-elimna(xy)
x<-xy[,1:p]
y<-xy[,p1]
if(xout){
m<-cbind(x,y)
flag<-outfun(x,plotit=FALSE)$keep
m<-m[flag,]
x<-m[,1:p]
y<-m[,p1]
}
if(SEED)set.seed(2) # set seed of random number generator so that
#             results can be duplicated.
n=length(y)
data<-matrix(sample(n,size=length(y)*nboot,replace=TRUE),nrow=n,ncol=nboot)
data=listm(data)
if(MC)bvec<-mclapply(data,wlogreg.sub,x,y,mc.preschedule=TRUE)
if(!MC)bvec<-lapply(data,wlogreg.sub,x,y)
bvec=matl(bvec)
#
# bvec is a p+1 by nboot matrix. The first row
#                     contains the bootstrap intercepts, the second row
#                     contains the bootstrap values for first predictor, etc.
x=as.matrix(x)
p1<-ncol(x)+1
regci<-matrix(0,p1,3)
VAL<-c("intercept",rep("X",ncol(x)))
dimnames(regci)<-list(VAL,c("Est.","ci.low","ci.up"))
se<-NA
sig.level<-NA
for(i in 1:p1){
bna=elimna(bvec[i,])
nbn=length(bna)
ilow<-round((alpha/2) * nbn)
ihi<-nbn - ilow
ilow<-ilow+1
temp<-mean(bna<0)
sig.level[i]<-2*(min(temp,1-temp))
bna<-sort(bna)
regci[i,2]<-bna[ilow]
regci[i,3]<-bna[ihi]
se[i]<-sqrt(var(elimna(bvec[i,])))
}
regci[,1]=wlogreg(x,y)$coef
list(conf.interval=regci,p.values=sig.level,se=se)
}
wlogreg.sub<-function(data,x,y){
x=as.matrix(x)
vals=wlogreg(x[data,],y[data])$coef
}



# original version of logreg.plot is stored in logreg_plot_orig_chk.tex


wmve<-function(m,SEED=TRUE){
#
# Compute skipped measure of location and scatter
# using MVE method
#
if(is.matrix(m))n<-nrow(m)
if(is.vector(m))n<-length(m)
flag<-rep(TRUE,n)
vec<-out(m,plotit=FALSE,SEED=SEED)$out.id
flag[vec]<-FALSE
if(is.vector(m)){
center<-mean(m[flag])
scatter<-var(m[flag])
}
if(is.matrix(m)){
center<-apply(m[flag,],2,mean)
scatter<-var(m[flag,])
}
list(center=center,cov=scatter)
}

wsumsq<-function(x,nval){
#
#  Compute the weighted sum of squared differences from the mean.
#  This function is used by b1way
#
wsumsq<-sum(nval*(x-mean(x))^2)/sum(nval)
wsumsq
}

wt.bt <- function(x,c1,M)
{
    x1 <- (x-M)/c1
    ivec1 <- (x1 < 0)
    ivec2 <- (x1 >  1)
    return(ivec1+(1-ivec1-ivec2)*(1-x1^2)^2)
}




# ----------------------------------------------------------------------------

# wwiQS

# ----------------------------------------------------------------------------

wwiQS<-function(J,K,x,locfun=median,...){
#
#  Quantile shift  measure of effect size for interactions in a
#  within-by-within design
#
#  The R variable x is assumed to contain the raw
#  data stored in list mode or in a matrix.
#  If in list mode, x[[1]] contains the data
#  for the first level of both factors: level 1,1.
#  x[[2]] is assumed to contain the data for level 1 of the
#  first factor and level 2 of the second: level 1,2
#  x[[K]] is the data for level 1,K
#  x[[K+1]] is the data for level 2,1, x[[2K]] is level 2,K, etc.
#
#  If the data are in a matrix, column 1 is assumed to
#  correspond to x[[1]], column 2 to x[[2]], etc.
#
x=elimna(x)
JK<-J*K
MJ<-(J^2-J)/2
MK<-(K^2-K)/2
MJMK<-MJ*MK
if(is.matrix(x) || is.data.frame(x))x=listm(x)
if(JK!=length(x))stop('Something is wrong. Expected ',JK,' groups but x contains ', length(x), ' groups instead.')
m=matrix(c(1:JK),nrow=J,byrow=TRUE)
output=matrix(NA,ncol=5,nrow=MJMK)
dimnames(output)<-list(NULL,c('A','A','B','B','Q.Effect'))
ic=0
for(j in 1:J){
for(jj in 1:J){
if(j<jj){
for(k in 1:K){
for(kk in 1:K){
if(k<kk){
ic<-ic+1
output[ic,1]<-j
output[ic,2]<-jj
output[ic,3]<-k
output[ic,4]<-kk
x1<-x[[m[j,k]]]-x[[m[j,kk]]]-x[[m[jj,k]]]+x[[m[jj,kk]]]
output[ic,5]<-depQS(x1,locfun=locfun,...)$Q.effect
}}}}}}
output
}




# ----------------------------------------------------------------------------

# wwQS

# ----------------------------------------------------------------------------

wwQS<-function(J,K,x,locfun=median,pr=TRUE){
#
#  Compute quantile shift measure of effect size for all pairwise comparisons
#  in a within-by-within design plus interactions.
#
if(pr){
print('output: A[[1]] contains results for level 1 of Factor A;')
print(' all pairwise comparisons over Factor B')
print('A[[2]] contains results for level 2, etc.')
print(' ')
print('Note: Under normality and homoscedasticity, Cohen d= 0, .2, .5, .8')
print('correspond approximately to Q.effect = 0.5, 0.55, 0.65 and 0.70, respectively')
}
A=list()
B=list()
AB=list()
if(is.matrix(x) || is.data.frame(x))x<-listm(x)
JK=J*K
ID=matrix(c(1:JK),nrow=J,ncol=K,byrow=TRUE)
A=list()
for (j in 1:J)A[[j]]=wmcpQS(x[ID[j,]],locfun=locfun)
B=list()
for(k in 1:K)B[[k]]=wmcpQS(x[ID[,k]],locfun=locfun)
AB=wwiQS(J,K,x)
list(Factor.A=A,Factor.B=B,interactions=AB)
}


wwtrim<-function(J,K,x,grp=c(1:p),p=J*K,tr=.2){
#
#  Perform a J by K anova using trimmed means with
#  repeated measures on both factors.
#
#  tr=.2 is default trimming
#
#  The R variable data is assumed to contain the raw
#  data stored in list mode. data[[1]] contains the data
#  for the first level of both factors: level 1,1.
#  data[[2]] is assumed to contain the data for level 1 of the
#  first factor and level 2 of the second: level 1,2
#  data[[K]] is the data for level 1,K
#  data[[K+1]] is the data for level 2,1, data[2K] is level 2,K, etc.
#
#  It is assumed that data has length JK, the total number of
#  groups being tested, but a subset of the data can be analyzed
#  using grp
#
if(is.data.frame(x))x=as.matrix(x)
if(is.list(x))x<-elimna(matl(x))
if(is.matrix(x))x<-elimna(x)
data<-x
if(is.matrix(data))data<-listm(data)
if(!is.list(data))stop("Data are not stored in list mode or a matrix")
if(p!=length(data)){
print("The total number of groups, based on the specified levels, is")
print(p)
print("The number of groups stored in x is")
print(length(data))
print("Warning: These two values are not equal")
}
if(p!=length(grp))stop("Apparently a subset of the groups was specified that does not match the total number of groups indicated by the values for J and K.")
tmeans<-0
h<-length(data[[grp[1]]])
v<-matrix(0,p,p)
for (i in 1:p)tmeans[i]<-mean(data[[grp[i]]],tr=tr,na.rm=TRUE)
v<-covmtrim(data,tr=tr)
ij<-matrix(c(rep(1,J)),1,J)
ik<-matrix(c(rep(1,K)),1,K)
jm1<-J-1
cj<-diag(1,jm1,J)
for (i in 1:jm1)cj[i,i+1]<-0-1
km1<-K-1
ck<-diag(1,km1,K)
for (i in 1:km1)ck[i,i+1]<-0-1
#  Do test for factor A
cmat<-kron(cj,ik)  # Contrast matrix for factor A
Qa<-trimww.sub(cmat,tmeans,v,h,J,K)
Qa.siglevel<-1-pf(Qa,J-1,999)
# Do test for factor B
cmat<-kron(ij,ck)  # Contrast matrix for factor B
Qb<-trimww.sub(cmat,tmeans,v,h,J,K)
Qb.siglevel<-1-pf(Qb,K-1,999)
# Do test for factor A by B interaction
cmat<-kron(cj,ck)  # Contrast matrix for factor A by B
Qab<-trimww.sub(cmat,tmeans,v,h,J,K)
Qab.siglevel<-1-pf(Qab,(J-1)*(K-1),999)
list(Qa=Qa,Qa.siglevel=Qa.siglevel,
Qb=Qb,Qb.siglevel=Qb.siglevel,
Qab=Qab,Qab.siglevel=Qab.siglevel)
}


wwtrimbt<-function(J, K, x, tr = 0.2, JK = J*K, con = 0,
 alpha = 0.05, grp =c(1:JK), nboot = 599,SEED = TRUE, ...){
        #
        # A bootstrap-t for a within-by-within omnibus tests
        #  for all main effects and interactions
        #
        #  The R variable x is assumed to contain the raw
        #  data stored in list mode or in a matrix.
        #  If in list mode, x[[1]] contains the data
        #  for the first level of both factors: level 1,1.
        #  x[[2]] is assumed to contain the data for level 1 of the
        #  first factor and level 2 of the second: level 1,2
        #  x[[K]] is the data for level 1,K
        #  x[[K+1]] is the data for level 2,1, x[[2K]] is level 2,K, etc.
        #
        #  If the data are in a matrix, column 1 is assumed to
        #  correspond to x[[1]], column 2 to x[[2]], etc.
        #
        #  When in list mode x is assumed to have length JK, the total number
        #  groups being tested, but a subset of the data can be analyzed
        #  using grp
        #
if(is.data.frame(x))x=as.matrix(x)
        if(is.matrix(x)) {
                y <- list()
                for(j in 1:ncol(x))
                        y[[j]] <- x[, j]
x=y
}
ncon=ncol(con)
 p <- J*K
JK=p
if(p>length(x))stop("JK is less than the Number of groups")
JK=J*K
        data <- list()
xx=list()
        for(j in 1:length(x)) {
xx[[j]]=x[[grp[j]]] # save input data
data[[j]] = xx[[j]] - mean(xx[[j]], tr = tr)
#                # Now have the groups in proper order.
        }
        if(SEED)set.seed(2)
        # set seed of random number generator so that
        #             results can be duplicated.
        bsam = list()
        bdat = list()
aboot=NA
bboot=NA
cboot=NA
abboot=NA
test.stat=wwtrim(J,K,xx,tr=tr)
nv=length(x[[1]])
        for(ib in 1:nboot) {
bdat[[j]] = sample(nv, size = nv, replace =TRUE)
for(k in 1:JK) bsam[[k]] = data[[k]][bdat[[j]]]
temp=wwtrim(J,K,bsam,tr=tr)
aboot[ib]=temp$Qa
bboot[ib]=temp$Qb
abboot[ib]=temp$Qab
}
pbA=NA
pbB=NA
pbAB=NA
pbA=mean(test.stat$Qa[1,1]<aboot)
pbB=mean(test.stat$Qb[1,1]<bboot)
pbAB=mean(test.stat$Qab[1,1]<abboot)
list(p.value.A=pbA,p.value.B=pbB,p.value.AB=pbAB)
}

ksties.crit<-function(x,y,alpha=.05){
#
# Compute a critical value so that probability coverage is approximately
# 1-alpha
#
n1=length(x)
n2=length(y)
START=sqrt(0-log(alpha/2)*(n1+n2)/(2*n1*n2))
crit=optim(START,ksties.sub,x=x,y=y,alpha=alpha,lower=.001,upper=.86,method='Brent')$par
crit
}

ksties.sub<-function(crit,x,y,alpha){
v=kstiesig(x,y,crit)
dif=abs(alpha-v)
dif
}




# ----------------------------------------------------------------------------

# wwwmcppb.OLD

# ----------------------------------------------------------------------------

wwwmcppb.OLD<-function(J, K,L, x, est=tmean,JKL = J * K*L,
 alpha = 0.05, grp =c(1:JKL), nboot = 2000, bhop=FALSE,SEED = TRUE,...)
{
        #
        # A percentile bootstrap for
        # multiple comparisons for all main effects and interactions
        # The analysis is done by generating bootstrap samples and
        # using an appropriate linear contrast.
        #
        #  The R variable x is assumed to contain the raw
        #  data stored in list mode or in a matrix.
        #  If in list mode, x[[1]] contains the data
        #  for the first level of both factors: level 1,1.
        #  x[[2]] is assumed to contain the data for level 1 of the
        #  first factor and level 2 of the second: level 1,2
        #  x[[K]] is the data for level 1,K
        #  x[[K+1]] is the data for level 2,1, x[[2K]] is level 2,K, etc.
        #
        #  If the data are in a matrix, column 1 is assumed to
        #  correspond to x[[1]], column 2 to x[[2]], etc.
        #
        #  When in list mode x is assumed to have length JK, the total number
        #  groups being tested, but a subset of the data can be analyzed
        #  using grp
        #
con=con3way(J,K,L)
A=wwwmcppb.sub(J=J, K=K,L=L, x, est=est,con=con$conA,
 alpha = alpha, nboot = nboot, bhop=bhop,SEED = SEED,grp=grp,...)
B=wwwmcppb.sub(J=J, K=K,L=L, x, est=est,con=con$conB,
 alpha = alpha, nboot = nboot, bhop=bhop,SEED = SEED,grp=grp,...)
C=wwwmcppb.sub(J=J, K=K,L=L, x, est=est,con=con$conC,
 alpha = alpha, nboot = nboot, bhop=bhop,SEED = SEED,grp=grp,...)
AB=wwwmcppb.sub(J=J, K=K,L=L, x, est=est,con=con$conAB,
 alpha = alpha, nboot = nboot, bhop=bhop,SEED = SEED,grp=grp,...)
AC=wwwmcppb.sub(J=J, K=K,L=L, x, est=est,con=con$conAC,
 alpha = alpha, nboot = nboot, bhop=bhop,SEED = SEED,grp=grp,...)
BC=wwwmcppb.sub(J=J, K=K,L=L, x, est=est,con=con$conBC,
 alpha = alpha, nboot = nboot, bhop=bhop,SEED = SEED,grp=grp,...)
ABC=wwwmcppb.sub(J=J, K=K,L=L, x, est=est,con=con$conABC,
 alpha = alpha, nboot = nboot, bhop=bhop,SEED = SEED,grp=grp,...)
list(Fac.A=A,Fac.B=B,Fac.C=C,Fac.AB=AB,Fac.AC=AC,Fac.BC=BC,Fac.ABC=ABC)
}

wwwtrim<-function(J,K,L,data,tr=.2,grp=c(1:p),alpha=.05,p=J*K*L){
#  Perform a within by within by within (three-way) anova on trimmed means.
#
#  That is, there are three factors with a total of JKL dependent groups.
#
#  The argument data is assumed to contain the raw
#  data stored in list mode. data[[1]] contains the data
#  for the first level of all three factors: level 1,1,1.
#  data][2]] is assumed to contain the data for level 1 of the
#  first two factors and level 2 of the third factor: level 1,1,2
#  data[[L]] is the data for level 1,1,L
#  data[[L+1]] is the data for level 1,2,1. data[[2L]] is level 1,2,L.
#  data[[KL+1]] is level 2,1,1, etc.
#
#  The default amount of trimming is tr=.2
#
#  It is assumed that data has length JKL, the total number of
#  groups being tested.
#
if(is.data.frame(data))data=as.matrix(data)
if(is.list(data))data=listm(elimna(matl(data)))
if(is.matrix(data))data=listm(elimna(data))
if(!is.list(data))stop("Data are not stored in list mode or a matrix")
if(p!=length(data)){
print("The total number of groups, based on the specified levels, is")
print(p)
print("The number of groups in data is")
print(length(data))
print("Warning: These two values are not equal")
}
tmeans<-0
h<-0
v<-0
for (i in 1:p){
tmeans[i]<-mean(data[[grp[i]]],tr)
h[i]<-length(data[[grp[i]]])-2*floor(tr*length(data[[grp[i]]]))
#    h is the effective sample size
}
v=covmtrim(data,tr=tr)
ij<-matrix(c(rep(1,J)),1,J)
ik<-matrix(c(rep(1,K)),1,K)
il<-matrix(c(rep(1,L)),1,L)
jm1<-J-1
cj<-diag(1,jm1,J)
cj<-diag(1,jm1,J)
for (i in 1:jm1)cj[i,i+1]<-0-1
km1<-K-1
ck<-diag(1,km1,K)
for (i in 1:km1)ck[i,i+1]<-0-1
lm1<-L-1
cl<-diag(1,lm1,L)
for (i in 1:lm1)cl[i,i+1]<-0-1
#  Do test for factor A
cmat<-kron(cj,kron(ik,il))  # Contrast matrix for factor A
Qa=bwwtrim.sub(cmat, tmeans, v, h,p)
Qa.siglevel <- 1 - pf(Qa, J - 1, 999)
# Do test for factor B
cmat<-kron(ij,kron(ck,il))  # Contrast matrix for factor B
Qb=bwwtrim.sub(cmat, tmeans, v, h,p)
 Qb.siglevel <- 1 - pf(Qb, K - 1, 999)
# Do test for factor C
cmat<-kron(ij,kron(ik,cl))  # Contrast matrix for factor C
Qc<-bwwtrim.sub(cmat, tmeans, v, h,p)
Qc.siglevel <- 1 - pf(Qc, L - 1, 999)
# Do test for factor A by B interaction
cmat<-kron(cj,kron(ck,il))  # Contrast matrix for factor A by B
Qab<-bwwtrim.sub(cmat, tmeans, v, h,p)
Qab.siglevel <- 1 - pf(Qab, (J - 1) * (K - 1), 999)
# Do test for factor A by C interaction
cmat<-kron(cj,kron(ik,cl))  # Contrast matrix for factor A by C
Qac<-bwwtrim.sub(cmat, tmeans, v, h,p)
Qac.siglevel <- 1 - pf(Qac, (J - 1) * (L - 1), 999)
# Do test for factor B by C interaction
cmat<-kron(ij,kron(ck,cl))  # Contrast matrix for factor B by C
Qbc<-bwwtrim.sub(cmat, tmeans, v, h,p)
Qbc.siglevel <- 1 - pf(Qbc, (K - 1) * (L - 1), 999)
# Do test for factor A by B by C interaction
cmat<-kron(cj,kron(ck,cl))  # Contrast matrix for factor A by B by C
Qabc<-bwwtrim.sub(cmat, tmeans, v, h,p)
Qabc.siglevel <-1-pf(Qabc,(J-1)*(K-1)*(L-1), 999)
list(Qa=Qa,Qa.p.value=Qa.siglevel,Qb=Qb,Qb.p.value=Qb.siglevel,
Qc=Qc,Qc.p.value=Qc.siglevel,Qab=Qab,Qab.p.value=Qab.siglevel,
Qac=Qac,Qac.p.value=Qac.siglevel,Qbc=Qbc,Qbc.p.value=Qbc.siglevel,
Qabc=Qabc,Qabc.p.value=Qabc.siglevel)
}




# ----------------------------------------------------------------------------

# wwwtrimbt

# ----------------------------------------------------------------------------

wwwtrimbt<-function(J, K,L, x, tr = 0.2, JKL = J * K*L, con = 0,
 alpha = 0.05, grp =c(1:JKL), nboot = 599,SEED = TRUE, ...){
        #
        # A bootstrap-t for a within-by-within-by-within omnibus tests
        #  for all main effects and interactions
        #
        #  The R variable x is assumed to contain the raw
        #  data stored in list mode or in a matrix.
        #  If in list mode, x[[1]] contains the data
        #  for the first level of both factors: level 1,1.
        #  x[[2]] is assumed to contain the data for level 1 of the
        #  first factor and level 2 of the second: level 1,2
        #  x[[K]] is the data for level 1,K
        #  x[[K+1]] is the data for level 2,1, x[[2K]] is level 2,K, etc.
        #
#
#  within-by-within-by-within design
#
#  JKL dependent groups
#

       #  If the data are in a matrix, column 1 is assumed to
        #  correspond to x[[1]], column 2 to x[[2]], etc.
        #
        #  When in list mode x is assumed to have length JK, the total number
        #  groups being tested, but a subset of the data can be analyzed
        #  using grp
        #
if(is.data.frame(x))x=as.matrix(x)
        if(is.matrix(x)) {
                y <- list()
                for(j in 1:ncol(x))
                        y[[j]] <- x[, j]
x=y
}
ncon=ncol(con)
 p <- J*K*L
JKL=p
if(p>length(x))stop("JKL is less than the Number of groups")
JK=J*K
KL=K*L
#        v <- matrix(0, p, p)
        data <- list()
xx=list()
        for(j in 1:length(x)) {
xx[[j]]=x[[grp[j]]] # save input data
data[[j]] = xx[[j]] - mean(xx[[j]], tr = tr)
#                # Now have the groups in proper order.
        }
        if(SEED)set.seed(2)
        # set seed of random number generator so that
        #             results can be duplicated.
        # Next determine the n_j values
        bsam = list()
        bdat = list()
aboot=NA
bboot=NA
cboot=NA
abboot=NA
acboot=NA
bcboot=NA
abcboot=NA
test.stat=wwwtrim(J,K,L,xx,tr=tr)
nv=length(x[[1]])
        for(ib in 1:nboot) {
bdat[[j]] = sample(nv, size = nv, replace =TRUE)
for(k in 1:JKL) bsam[[k]] = data[[k]][bdat[[j]]]
temp=wwwtrim(J,K,L,bsam,tr=tr)
aboot[ib]=temp$Qa
bboot[ib]=temp$Qb
cboot[ib]=temp$Qc
acboot[ib]=temp$Qac
bcboot[ib]=temp$Qbc
abboot[ib]=temp$Qab
abcboot[ib]=temp$Qabc
}
pbA=NA
pbB=NA
pbC=NA
pbAB=NA
pbAC=NA
pbBC=NA
pbABC=NA
pbA=mean(test.stat$Qa[1,1]<aboot)
pbB=mean(test.stat$Qb[1,1]<bboot)
pbC=mean(test.stat$Qc[1,1]<cboot)
pbAB=mean(test.stat$Qab[1,1]<abboot)
pbAC=mean(test.stat$Qac[1,1]<acboot)
pbBC=mean(test.stat$Qbc[1,1]<bcboot)
pbABC=mean(test.stat$Qabc[1,1]<abcboot)
list(p.value.A=pbA,p.value.B=pbB,p.value.C=pbC,p.value.AB=pbAB,
p.value.AC=pbAC,p.value.BC=pbBC,p.value.ABC=pbABC)
}



xi2cohen<-function(xi){
delta=sqrt((4*xi^2)/(2-xi^2))
delta
}

ydbt<-function(x,y,tr=.2,alpha=.05,nboot=599,side=TRUE,plotit=FALSE,op=1,SEED=TRUE){
#
#   Using the bootstrap-t method,
#   compute a .95 confidence interval for the difference between
#   the marginal trimmed means of paired data.
#   By default, 20% trimming is used with B=599 bootstrap samples.
#
#   side=F returns equal-tailed ci
#   side=T returns symmetric ci.
#
side<-as.logical(side)
if(length(x)!=length(y))stop("Must have equal sample sizes.")
m<-cbind(x,y)
m<-elimna(m)
x<-m[,1]
y<-m[,2]
if(sum(c(!is.na(x),!is.na(y)))!=(length(x)+length(y)))stop("Missing values are not allowed.")
if(SEED)set.seed(2) # set seed of random number generator so that
#             results can be duplicated.
#print("Taking bootstrap samples. Please wait.")
data<-matrix(sample(length(y),size=length(y)*nboot,replace=TRUE),nrow=nboot)
e1=mean(x,tr)
e2=mean(y,tr)
xcen<-x-mean(x,tr)
ycen<-y-mean(y,tr)
bvec<-apply(data,1,tsub,xcen,ycen,tr)
# bvec is a 1 by nboot matrix containing the bootstrap test statistics.
dotest=yuend(x,y,tr=tr)
estse<-dotest$se
p.value=NULL
dif<-mean(x,tr)-mean(y,tr)
if(!side){
ilow<-round((alpha/2)*nboot)
ihi<-nboot-ilow
bsort<-sort(bvec)
ci<-0
ci[1]<-dif-bsort[ihi]*estse
ci[2]<-dif-bsort[ilow+1]*estse
}
if(side){
bsort<-sort(abs(bvec))
ic<-round((1-alpha)*nboot)
ci<-0
ci[1]<-dif-bsort[ic]*estse
ci[2]<-dif+bsort[ic]*estse
p.value<-(sum(abs(dotest$teststat)<=abs(bvec)))/nboot
}
if(plotit){
if(op==1)akerd(bsort)
if(op==2)rdplot(bsort)
if(op==3)boxplot(bsort)
}
list(ci=ci,Est.1=e1,Est.2=e2,dif=dif,p.value=p.value)
}




# ----------------------------------------------------------------------------

# yhall

# ----------------------------------------------------------------------------

yhall<-function(x,y,tr=.2,alpha=.05){
#
#  Perform Yuen's test for trimmed means on the data in x and y
#  in conjunction with Hall's transformation.
#  The default amount of trimming is 20%
#  Missing values (values stored as NA) are automatically removed.
#
#  A confidence interval for the trimmed mean of x minus the
#  the trimmed mean of y is computed and returned in yuen$ci.
#
x<-x[!is.na(x)]  # Remove any missing values in x
y<-y[!is.na(y)]  # Remove any missing values in y
winx<-winval(x,tr=tr)
winy<-winval(y,tr=tr)
m3x<-sum((winx-mean(winx))^3)/length(x)
m3y<-sum((winy-mean(winy))^3)/length(y)
h1<-length(x)-2*floor(tr*length(x))
h2<-length(y)-2*floor(tr*length(y))
mwx<-length(x)*m3x/h1
mwy<-length(y)*m3y/h2
q1<-(length(x)-1)*winvar(x,tr)/(h1*(h1-1))
q2<-(length(y)-1)*winvar(y,tr)/(h2*(h2-1))
sigtil<-q1+q2
mtil<-(mwx/h1^2)-(mwy/h2^2)
dif<-mean(x,tr)-mean(y,tr)
thall<-dif+mtil/(6*sigtil)+mtil*dif^2/(3*sigtil^2)+mtil^2*dif^3/(27*sigtil^4)
thall<-thall/sqrt(sigtil)
nhat<-mtil/sigtil^1.5
list(test.stat=thall,nu.tilda=nhat,sig.tilda=sqrt(sigtil))
}

yhbt<-function(x,y,tr=.2,alpha=.05,nboot=600,SEED=TRUE,PV=FALSE){
#
#  Compute a 1-alpha confidence interval for the difference between
#  the trimmed means corresponding to two independent groups.
#  The bootstrap-t method with Hall's transformation is used.
#
if(SEED)set.seed(2) # set seed of random number generator so that
#                    results can be duplicated.
x<-x[!is.na(x)]  # Remove missing values in x
y<-y[!is.na(y)]  # Remove missing values in y
xcen<-x-mean(x,tr)
ycen<-y-mean(y,tr)
print("Taking bootstrap samples. Please wait.")
datax<-matrix(sample(xcen,size=length(x)*nboot,replace=TRUE),nrow=nboot)
datay<-matrix(sample(ycen,size=length(y)*nboot,replace=TRUE),nrow=nboot)
val<-NA
for(ib in 1:nboot)val[ib]<-yhall(datax[ib,],datay[ib,],tr=tr,alpha)$test.stat
temp<-yhall(x,y,tr=tr)
sigtil<-temp$sig.tilda
nhat<-temp$nu.tilda
val<-sort(val)
dif<-mean(x,tr=tr)-mean(y,tr=tr)
ilow<-round(alpha*nboot/2)
il<-ilow+1
uval<-nboot-ilow
b.low<-3*((1+nhat*val[il]-nhat/6)^{1/3})/nhat-3/nhat
uval<-nboot-ilow
b.low<-3*((1+nhat*val[il]-nhat/6)^{1/3})/nhat-3/nhat
b.hi<-3*((1+nhat*val[uval]-nhat/6)^{1/3})/nhat-3/nhat
ci.LOW<-dif-sigtil*b.hi
ci.UP<-dif-sigtil*b.low
pv=NULL
if(PV){
#  Determine p-value
pv=1
flag=F
if(dif !=0){
alpha=seq(1:100)/1000
for(i in 1:length(alpha)){
ilow<-round(alpha[i]*nboot/2)
il<-ilow+1
uval<-nboot-ilow
b.low<-3*((1+nhat*val[il]-nhat/6)^{1/3})/nhat-3/nhat
b.hi<-3*((1+nhat*val[uval]-nhat/6)^{1/3})/nhat-3/nhat
ci.low<-dif-sigtil*b.hi
ci.up<-dif-sigtil*b.low
if(ci.low>0 || ci.up<0){
pv=alpha[i]
flag=T
}
if(flag)break
}
if(!flag){
alpha=c(1:99)/100
for(i in 1:length(alpha)){
ilow<-round(alpha[i]*nboot/2)
il<-ilow+1
uval<-nboot-ilow
b.low<-3*((1+nhat*val[il]-nhat/6)^{1/3})/nhat-3/nhat
b.hi<-3*((1+nhat*val[uval]-nhat/6)^{1/3})/nhat-3/nhat
ci.low<-dif-sigtil*b.hi
ci.up<-dif-sigtil*b.low
if(ci.low>0 || ci.up<0){
pv=alpha[i]
flag=T
}
if(flag)break
}
}}}
list(est.dif=dif,conf.interval=c(ci.LOW,ci.UP),p.value=pv)
}

YYmcp<-function(x,alpha=.05,grp=NA,tr=.2,bhop=FALSE,J=NULL,p=NULL,...){
#
#   All pairwise  comparisons among J independent groups using trimmed means
#   with multivariate data for each group.
#   The method applies the Yanagihara - Yuan for each pair of groups
#   and controls FWE via Rom's method if bhop=F.
#   bhop=T, use Benjamini-Hochberg method
#
#   The data are assumed to be stored in x
#   which  has list mode,
#   x[[1]] contains the data for the first group in the form of a
#   matrix, x[[2]] the data
#   for the second group, etc., each matrix having the same
#   number of columns Length(x)=the number of groups = J.
#
#   The data can be stored in a single matrix having Jp columns
#   J = number of groups.
#   If this is the case, specify the argument J or p(number of variables)

#   est is the measure of location and defaults to the median
#   ... can be used to set optional arguments associated with est
#
#   The argument grp can be used to analyze a subset of the groups
#   Example: grp=c(1,3,5) would compare groups 1, 3 and 5.
#
#   Missing values are automatically removed.
#
con<-as.matrix(con)
if(is.matrix(x) || is.data.frame(x)){
if(is.null(J) && is.null(p))stop("Specify J or P")
x=MAT2list(x,p=p,J=J)
}
if(!is.list(x))stop("Data must be stored in list mode.")
if(!is.na(sum(grp))){  # Only analyze specified groups.
xx<-list()
for(i in 1:length(grp))xx[[i]]<-x[[grp[i]]]
x<-xx
}
J<-length(x)
nullvec=rep(0,ncol(x[[1]]))
bplus=nboot+1
tempn<-0
mvec<-list
for(j in 1:J){
x[[j]]<-elimna(x[[j]])
}
Jm<-J-1
#
# Determine contrast matrix
#
ncon<-(J^2-J)/2
if(!bhop){
if(alpha==.05){
dvec<-c(.05,.025,.0169,.0127,.0102,.00851,.0073,.00639,.00568,.00511)
if(ncon > 10){
avec<-.05/c(11:ncon)
dvec<-c(dvec,avec)
}}
if(alpha==.01){
dvec<-c(.01,.005,.00334,.00251,.00201,.00167,.00143,.00126,.00112,.00101)
if(ncon > 10){
avec<-.01/c(11:ncon)
dvec<-c(dvec,avec)
}}
if(alpha != .05 && alpha != .01){
dvec<-alpha/c(1:ncon)
}
}
if(bhop)dvec<-(ncon-c(1:ncon)+1)*alpha/ncon
#
output<-matrix(0,ncon,4)
dimnames(output)<-list(NULL,c("Group","Group","p.value","p.crit"))
ic=0
for (j in 1:Jm){
jp<-j+1
for (k in jp:J){
ic=ic+1
output[ic,1]=j
output[ic,2]=k
output[ic,3]<-YYmanova(x[[j]],x[[k]],tr=tr)$p.value
}}
test=output[,3]
temp2<-order(0-test)
zvec<-dvec[1:ncon]
sigvec<-(test[temp2]>=zvec)
output[temp2,4]<-zvec
num.sig<-sum(output[,3]<=output[,4])
list(output=output,num.sig=num.sig)
}


ZL.break<-function(x,y,tau=median(x),xout=FALSE,outfun=outbox,...){
#
# Feipeng Zhanga, Qunhua Li (2017)
# Robust bent line regression
#  Journal of Statistical Planning and Inference 185 41--55
#
xx<-cbind(x,y)
xx<-elimna(xx)
if(xout){
if(identical(outfun,outblp))flag=outblp(xx[,1:p],xx[,p1],plotit=FALSE)$keep
else
flag=outfun(xx[,1],plotit=FALSE,...)$keep
xx=xx[flag,]
}
x<-xx[,1]
y<-xx[,2]
q1=qest(x,.1)
q9=qest(x,.9)
if(tau<q1)tau=q1
if(tau>q9)tau=q9
n=length(y)
term =x-tau
flag=x-tau<0
term[flag]=0
term =x-tau
flag=x-tau<=0
term[flag]=0
term2=-1*as.numeric(x>tau)
X=cbind(x,term,term2)
coef=Rfit(X,y)$coef
oldtau=tau
tau=oldtau+coef[4]/coef[3]
if(tau<q1)tau=q1
if(tau>q9)tau=q9
ic=0
while(abs(oldtau-tau)>.1){
ic=ic+1
if(tau<q1)tau=q1
if(tau>q9)tau=q9
term =x-tau
flag=x-tau<=0
term[flag]=0
term2=-1*as.numeric(x>tau)
X=cbind(x,term,term2)
coef=Rfit(X,y)$coef
oldtau=tau
tau=oldtau+coef[4]/coef[3]
if(ic>10){
if(tau<q1)tau=q1
if(tau>q9)tau=q9
break
}
}
tau
}




# ----------------------------------------------------------------------------

# zonoid

# ----------------------------------------------------------------------------

zonoid<-function(x,pts=x,SEED=TRUE){
#
#   Requires  ddalpha
#
#  SEED, included for convenience when this function is used with certain classification techniques.
#
#  Compute zonoid depths
#  Dyckerhoff, R., Koshevoy, G., and Mosler, K. (1996).
#  Zonoid data depth: theory and computation.
#  In: Prat A. (ed), COMPSTAT 1996. Proceedings in computational
#   statistics, Physica-Verlag (Heidelberg), 235--240.

# Koshevoy, G. and Mosler, K. (1997). Zonoid trimming
# for multivariate distributions Annals of Statistics 25 1998--2017.
# The code was written by Rainer Dyckerhoff.
#
#  This function is for convenience: The arguments are consistent with
#  other depth functions in Rallfun and the R package WRS
#
library(ddalpha)
depth.zonoid(pts,x)
}




# ----------------------------------------------------------------------------

# zwe

# ----------------------------------------------------------------------------

zwe<-function(x,k=3,C=0.2){
#
#  Zuo's  (2010) weighted estimator
#
x=elimna(x)
SD=abs((x-median(x)))/mad(x,constant=1)
D=1/(SD+1)
n=length(x)
IDGE =rep(0,n)
flag=D >= C
IDGE[flag]=1
IDLT=rep(0,n)
flag=D<C
IDLT[flag]=1
top=exp(-1*k*(1-D^2/C^2)^2) - exp(-1*k)*IDLT
bot=(1-exp(-1*k))
w=IDGE+top/bot
e=sum(w*x)/sum(w)
e
}

rmbestVAR.DO<-function(x,est=winvar,nboot=NA,SEED=TRUE,pr=TRUE,...){
#
#  Determine whether it is reasonable to
#  decide which group has smallest robust measure of variation
#
#
if(is.list(x))x=matl(x)
x=elimna(x)
x<-listm(x)
J=length(x)
e=lapply(x,est,...)
e=pool.a.list(e)
id=which(e==min(e))
id=id[1]
e=lapply(x,est,...)
e=pool.a.list(e)
CON=conCON(J,id)$conCON
pv=rmmcppb(x,est=est,con=CON,dif=FALSE,pr=FALSE)$output[,3]
pv=max(pv)
list(Group.smallest=id,Est.=e,p.value=pv)
}

ZYmediate<-function(x,y,nboot=2000,alpha=.05,kappa=.05,SEED=TRUE,xout=FALSE,outfun=out){
#
# Robust mediation analysis using M-estimator as
# described in Zu and Yuan, 2010, MBR, 45, 1--44.
#
# x[,1] is predictor
# x[,2] is mediator variable
#  y is outcome variable.
ep=0.00000001  # convergence criteria
B=nboot         # the number of bootstrap replications
kappa    # the percent of cases to be controlled when robust method is used
               # Zu and Yuan used .05, so this is the default value used here.
level=alpha    # alpha level
if(SEED)set.seed(2)
Z=elimna(cbind(x,y))
if(xout){
flag<-outfun(Z[,1],plotit=FALSE,SEED=SEED)$keep
Z<-Z[flag,]
}
p=3
n=nrow(Z)
HT=HuberTun(kappa,p)
r=HT$r
tau=HT$tau
H=robEst(Z,r,tau,ep)
R.v=H$u2*tau
oH=order(R.v)
oCaseH=(1:n)[oH]        # case number with its Ri increases
oR.v=R.v[oH]

thetaH=H$theta
aH=thetaH[1]
bH=thetaH[2]
abH=aH*bH

muH=H$mu
SigmaH=H$Sigma
dH=H$d


### Use robust method
# point estimate
thetaH=H$theta
aH=thetaH[1]
bH=thetaH[2]
abH=aH*bH

muH=H$mu
SigmaH=H$Sigma
dH=H$d

#Standard errors
RH=SErob(Z,muH,SigmaH,thetaH,dH,r,tau)

Zr=RH$Zr
SEHI=RH$inf
SEHS=RH$sand

#Standard errors
RH=SErob(Z,muH,SigmaH,thetaH,dH,r,tau)

Zr=RH$Zr
SEHI=RH$inf
SEHS=RH$sand

#Standard errors
RH=SErob(Z,muH,SigmaH,thetaH,dH,r,tau)

Zr=RH$Zr
SEHI=RH$inf
SEHS=RH$sand
ParEstH<-round(cbind(thetaH,SEHI[1:6],SEHS[1:6]),3)
rnames<-c("a","b","c","vx","vem","vey")
ParEstH<-cbind(rnames,ParEstH)
res=t(ParEstH)
#
Res=BCI(Z,Zr,ab=3,abH,B,level)
list(CI.ab=Res$CI,p.value=Res$pv,a.est=aH,b.est=bH,ab.est=abH)
}


#------------------------------------------------------------
# Tunning parameter when use Huber type weight
#------------------------------------------------------------
# Input:
	#kappa: the proportion of cases to be controlled
	#p: the number of variables
# Output
	# r: the critical value of Mahalalanobis distance, as defined in (20)
	# tau: the constant to make the robust estimator of Sigma to be unbiased, as defined in (20)

HuberTun=function(kappa,p){
	prob=1-kappa
	chip=qchisq(prob,p)
	r=sqrt(chip)
	tau=(p*pchisq(chip,p+2)+ chip*(1-prob))/p
	Results=list(r=r,tau=tau)
	return(Results)
}

robEst=function(Z,r,tau,ep){

      p=ncol(Z)
      n=nrow(Z)
      # Starting values
      mu0=MeanCov(Z)$zbar
      Sigma0=MeanCov(Z)$S
      Sigin=solve(Sigma0)

      diverg=0 # convergence flag

      for (k in 1:200) {
		sumu1=0
		mu=matrix(0,p,1)
		Sigma=matrix(0,p,p)
		d=rep(NA,n)
		u1=rep(NA,n)
		u2=rep(NA,n)

   		for (i in 1:n) {			zi=Z[i,]
  			zi0=zi-mu0
  			di2=t(zi0)%*%Sigin%*%zi0
  			di=as.numeric(sqrt(di2))
  			d[i]=di

    		#get u1i,u2i
			if (di<=r) {
     			   u1i=1.0
     			   u2i=1.0/tau
			}else {
     			   u1i=r/di
     			   u2i=u1i^2/tau
 		   }
    			u1[i]=u1i
    			u2[i]=u2i

  			sumu1=sumu1+u1i
  			mu=mu+u1i*zi
  			Sigma=Sigma+u2i*zi0%*%t(zi0)

   		} # end of loop i

  		mu1=mu/sumu1
  		Sigma1=Sigma/n
 		Sigdif=Sigma1-Sigma0
  		dt=sum(Sigdif^2)

  		mu0=mu1
  		Sigma0=Sigma1
  		Sigin=solve(Sigma0)
  if (dt<ep) {break}

	  } # end of loop k


       if (k==200) {
  			diverg=1
  			mu0=rep(0,p)
   			sigma0=matrix(NA,p,p)

  	  }

       theta=MLEst(Sigma0)

       Results=list(mu=mu0,Sigma=Sigma0,theta=theta,d=d,u1=u1,u2=u2,diverg=diverg)
       return(Results)
}

SErob=function(Z,mu,Sigma,theta,d,r,tau){
	n=nrow(Z)
	p=ncol(Z)
	ps=p*(p+1)/2
	q=6
	Dup=Dp(p)
	DupPlus=solve(t(Dup)%*%Dup)%*%t(Dup)

   	InvSigma=solve(Sigma)
	sigma=vech(Sigma)
   	W=0.5*t(Dup)%*%(InvSigma%x%InvSigma)%*%Dup

   	Zr=matrix(NA,n,p) # robustly transformed data
   	A=matrix(0,p+q,p+q)
   	B=matrix(0,p+q,p+q)
 	sumg=rep(0,p+q)

   	for (i in 1:n) {
    	zi=Z[i,]
    	zi0=zi-mu
    	di=d[i]

     	if (di<=r) {
     			u1i=1.0
     			u2i=1.0/tau
     			du1i=0
     			du2i=0
		}else {
     			u1i=r/di
     			u2i=u1i^2/tau
     			du1i=-r/di^2
     			du2i=-2*r^2/tau/di^3
 		}

      	#robust transformed data
    	Zr[i,]=sqrt(u2i)*t(zi0)

    	####	gi

    	g1i=u1i*zi0	# defined in (24)
      	vTi=vech(zi0%*%t(zi0))
    	g2i=u2i*vTi-sigma	# defined in (25)
	gi=rbind(g1i,g2i)
    	sumg=gi+sumg

		B=B+gi%*%t(gi)

    	####	gdoti

    	#	derivatives of di with respect to mu and sigma
    	ddmu=-1/di*t(zi0)%*%InvSigma
    	ddsigma=-t(vTi)%*%W/di

    	#
    	dg1imu=-u1i*diag(p)+du1i*zi0%*%ddmu
    	dg1isigma=du1i*zi0%*%ddsigma
    	dg2imu=-u2i*DupPlus%*%(zi0%x%diag(p)+diag(p)%x%zi0)+du2i*vTi%*%ddmu
    	dg2isigma=du2i*vTi%*%ddsigma-diag(q)

    	dgi=rbind(cbind(dg1imu,dg1isigma),cbind(dg2imu,dg2isigma))
    	A=A+dgi
   } # end of loop n

	A=-1*A/n
	B=B/n
	invA=solve(A)
	OmegaSW=invA%*%B%*%t(invA)
   	OmegaSW=OmegaSW[(p+1):(p+q),(p+1):(p+q)]


   SEsw=getSE(theta,OmegaSW,n)
	SEinf=SEML(Zr,theta)$inf

	Results=list(inf=SEinf,sand=SEsw,Zr=Zr)
   return(Results)

}

MeanCov=function(Z){
	n=nrow(Z)
	p=ncol(Z)

	zbar=t(Z)%*%matrix(1/n,n,1)
	S=t(Z)%*%(diag(n)-matrix(1/n,n,n))%*%Z/n

   	Results=list(zbar=zbar,S=S)
   	return(Results)
}

#-----------------------------------------------------------------------
# Obtaining normal-theory MLE of parameters in the mediation model
#-----------------------------------------------------------------------
# Input:
	# S: sample covariance
# Output:
   # thetaMLE: normal-theory MLE of theta. theta is defined in the subsection: MLEs of a,b, and c

MLEst=function(S){
	ahat=S[1,2]/S[1,1]
	vx=S[1,1]
	# M on X
	Sxx=S[1:2,1:2]
	sxy=S[1:2,3]
	vem=S[2,2]-S[2,1]*S[1,2]/S[1,1]

	# Y on X and M
	invSxx=solve(Sxx)
	beta.v=invSxx%*%sxy # chat, bhat
	vey=S[3,3]-t(sxy)%*%invSxx%*%sxy
	thetaMLE=c(ahat,beta.v[2],beta.v[1],vx,vem,vey)
    	return(thetaMLE)
}

Dp=function(p){
    p2=p*p
    ps=p*(p+1)/2
  	Dup=matrix(0,p2,ps)
	count=0
	for (j in 1:p){
    	for (i in j:p){
      		count=count+1
      		if (i==j){
      			Dup[(j-1)*p+j, count]=1
      		}else{
      			Dup[(j-1)*p+i, count]=1
                Dup[(i-1)*p+j, count]=1
      		}
      	}
	}

	return(Dup)
}

vech=function(A){
	l=0
	p=nrow(A)
	ps=p*(p+1)/2
	vA=matrix(0,ps,1)
	for (i in 1:p) {
		for (j in i:p) {
             l=l+1
             vA[l,1]=A[j,i]
	    }
	}

	return(vA)
}


   getSE=function(theta,Omega,n){

	hdot=gethdot(theta)
	COV=hdot%*%(Omega/n)%*%t(hdot)  # delta method
	se.v=sqrt(diag(COV)) # se.v of theta

	a=theta[1]
	b=theta[2]
	SobelSE=sqrt(a^2*COV[2,2]+b^2*COV[1,1])

	se.v=c(se.v,SobelSE) # including Sobel SE

   return(se.v)

}
gethdot=function(theta){

p=3
ps=p*(p+1)/2
q=ps

a=theta[1]
b=theta[2]
c=theta[3]
#ab=a*b
vx=theta[4]
vem=theta[5]
vey=theta[6]

sigmadot=matrix(NA,ps,q)
sigmadot[1,]=c(0,0,0,1,0,0)
sigmadot[2,]=c(vx,0,0,a,0,0)
sigmadot[3,]=c(b*vx,a*vx,vx,a*b+c,0,0)
sigmadot[4,]=c(2*a*vx,0,0,a^2,1,0)
sigmadot[5,]=c((2*a*b+c)*vx,a^2*vx+vem,a*vx,a^2*b+a*c,b,0)
sigmadot[6,]=c((2*b*c+2*a*b^2)*vx,(2*c*a+2*a^2*b)*vx+2*b*vem,(2*a*b+2*c)*vx,c^2+2*c*a*b+a^2*b^2,b^2,1)

hdot=solve(sigmadot)

return(hdot)
}
SEML=function(Z,thetaMLE){
	n=nrow(Z)
	p=ncol(Z)
	ps=p*(p+1)/2
   	q=ps
	zbar=MeanCov(Z)$zbar
	S=MeanCov(Z)$S
	Dup=Dp(p)
	InvS=solve(S)
	W=0.5*t(Dup)%*%(InvS%x%InvS)%*%Dup
	OmegaInf=solve(W)  # only about sigma, not mu


	# Sandwich-type Omega
	S12=matrix(0,p,ps)
	S22=matrix(0,ps,ps)

	for (i in 1:n){
		zi0=Z[i,]-zbar
		difi=zi0%*%t(zi0)-S
   	   	vdifi=vech(difi)
		S12=S12+zi0%*%t(vdifi)
		S22=S22+vdifi%*%t(vdifi)
	}

	OmegaSW=S22/n # only about sigma, not mu

   	SEinf=getSE(thetaMLE,OmegaInf,n)
   	SEsw=getSE(thetaMLE,OmegaSW,n)

	Results=list(inf=SEinf,sand=SEsw)
   	return(Results)

}
BCI=function(Z,Zr,ab=NULL,abH,B,level){
	p=ncol(Z)
	n=nrow(Z)
#	abhat.v=rep(NA,B) # save MLEs of a*b in the B bootstrap samples
	abhatH.v=matrix(NA,B)
	Index.m=matrix(NA,n,B)

	t1=0
	t2=0
    for(i in 1:B){
	    U=runif(n,min=1,max=n+1)
       index=floor(U)
		Index.m[,i]=index
		#H(.05)
		Zrb=Zr[index,]
		SH=MeanCov(Zrb)$S
		thetaH=MLEst(SH)
		abhatH=thetaH[1]*thetaH[2]
		abhatH.v[i]=abhatH
		if (abhatH<abH){
			t2=t2+1
		}

	 } # end of B loop

	abhatH.v=abhatH.v[!is.na(abhatH.v)]
	SEBH=sd(abhatH.v)

	# bootstrap confidence intervals using robust method
	CI2 =BpBCa(Zr,abhatH.v,t2,level)
#    Results=list(CI=CI2)
    Results=list(CI=CI2[[1]],pv=CI2[[2]])
 	return(Results)

}# end of function


BpBCa=function(Z,abhat.v,t,level){
	# Bootstrap percentile
	oab.v=sort(abhat.v)
	B=length(abhat.v)

	ranklowBp=round(B*level/2)

	if(ranklowBp==0){
		ranklowBp=1
	}

	Bpl=oab.v[ranklowBp]
	Bph=oab.v[round(B*(1-level/2))]
	BP=c(Bpl,Bph)
pstar=mean(oab.v>0)
pv=2*min(c(pstar,1-pstar))
#	Results=list(BP=BP)
#    return(Results)
list(BP,pv)
}




